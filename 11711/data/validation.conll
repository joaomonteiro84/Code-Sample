Balancing O
Multi O
- O
Domain O
Corpora O
Learning O
for O
Open B-TaskName
- I-TaskName
Domain I-TaskName
Response I-TaskName
Generation I-TaskName

Open O
- O
domain O
conversational O
systems O
are O
assumed O
to O
generate O
equally O
good O
responses O
on O
multiple O
domains O
. O
Previous O
work O
achieved O
good O
performance O
on O
the O
single O
corpus O
, O
but O
training O
and O
evaluating O
on O
multiple O
corpora O
from O
different O
domains O
are O
less O
studied O
. O
This O
paper O
explores O
methods O
of O
generating B-TaskName
relevant O
responses O
for O
each O
of O
multiple O
multi O
- O
domain O
corpora O
. O
We O
first O
examine O
interleaved O
learning O
which O
intermingles O
multiple O
corpora O
as O
the O
baseline O
. O
We O
then O
investigate O
two O
multidomain O
learning O
methods O
, O
labeled B-MethodName
learning I-MethodName
and O
multi B-MethodName
- I-MethodName
task I-MethodName
labeled I-MethodName
learning I-MethodName
, O
which O
encode O
each O
corpus O
through O
a O
unique O
corpus O
embedding O
. O
Furthermore O
, O
we O
propose O
Domainspecific B-MetricName
Frequency I-MetricName
( O
DF B-MetricName
) O
, O
a O
novel O
word O
- O
level O
importance O
weight O
that O
measures O
the O
relative O
importance O
of O
a O
word O
for O
a O
specific O
corpus O
compared O
to O
other O
corpora O
. O
Based O
on O
DF B-MetricName
, O
we O
propose O
weighted B-MethodName
learning I-MethodName
, O
a O
method O
that O
integrates O
DF B-MetricName
to O
the O
loss O
function O
. O
We O
also O
adopt O
DF B-MetricName
as O
a O
new O
evaluation O
metric O
. O
Extensive O
experiments O
show O
that O
our O
methods O
gain O
significant O
improvements O
on O
both O
automatic O
and O
human O
evaluation O
. O
We O
share O
our O
code O
and O
data O
for O
reproducibility O
. O
1 O
* O
This O
work O
was O
done O
prior O
to O
the O
author O
joining O
Amazon O
. O
1 O
https O
: O
/ O
/ O
github.com O
/ O
yujie-xing O
/ O
Balancing_Multi_Domain_Corpus_Learning O
_ O
for_Open_Domain_Response_GenerationWhat O
are O
you O
going O
to O
do O
on O
the O
remote O
system O
exactly O
? O
PersonaChat O
I O
am O
going O
to O
be O
a O
pilot O
. O
I O
am O
going O
to O
fly O
planes O
. O
4 O
corpora O
( O
concatenated O
) O
I O
am O
going O
to O
go O
to O
the O
beach O
. O

Introduction O

Recent O
work O
has O
achieved O
improvements O
in O
general O
performance O
for O
open B-TaskName
- I-TaskName
domain I-TaskName
response I-TaskName
generation I-TaskName
( O
Vinyals O
and O
Le O
, O
2015 O
; O
Serban O
et O
al O
. O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2016 O
; O
Xu O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
most O
studies O
are O
restricted O
to O
single O
- O
corpus O
training O
and O
evaluating O
, O
while O
there O
lacks O
studies O
for O
training O
and O
evaluating O
with O
multiple O
corpora O
from O
different O
domains O
. O
Single O
- O
corpus O
training O
has O
intrinsic O
limitations O
. O
For O
example O
, O
a O
corpus O
of O
everyday O
chats O
, O
e.g. O
, O
the O
PersonaChat B-MethodName
corpus O
( O
Dinan O
et O
al O
. O
, O
2019 O
) O
, O
does O
not O
cover O
technical O
topics O
discussed O
in O
Ubuntu O
chatlogs O
( O
Lowe O
et O
al O
. O
, O
2015 O
) O
. O
A O
conversational O
system O
that O
learns O
only O
from O
PersonaChat O
or O
from O
multiple O
corpora O
without O
an O
appropriate O
technique O
is O
not O
likely O
to O
generate O
relevant O
responses O
for O
certain O
topics O
( O
see O
Table O
1 O
) O
. O
Therefore O
, O
it O
is O
necessary O
for O
an O
open O
- O
domain O
conversational O
system O
to O
learn O
from O
multiple O
corpora O
, O
and O
to O
learn O
with O
good O
techniques O
. O
Furthermore O
, O
the O
case O
of O
using O
a O
single O
smallscale O
open O
- O
domain O
corpus O
has O
apparent O
weaknesses O
. O
A O
common O
way O
of O
dealing O
with O
a O
smallscale O
corpus O
is O
through O
fine O
- O
tuning O
( O
Li O
et O
al O
. O
, O
2016 O
; O
Akama O
et O
al O
. O
, O
2017 O
; O
Chu O
et O
al O
. O
, O
2017 O
) O
. O
Fine O
- O
tuning O
on O
a O
single O
corpus O
tends O
to O
make O
the O
model O
overfit O
on O
that O
specific O
corpus O
while O
performing O
worse O
on O
other O
corpora O
. O
Table O
2 O
shows O
the O
result O
of O
a O
GPT-2 B-MethodName
model O
gaining O
good O
performance O
on O
PersonaChat B-MethodName
while O
performing O
poorly O
on O
other O
corpora O
. O

This O
paper O
explores O
how O
to O
train O
and O
evaluate O
on O
multiple O
corpora O
from O
different O
domains O
for O
the O
open B-TaskName
- I-TaskName
domain I-TaskName
response I-TaskName
generation I-TaskName
task O
. O
We O
propose O
several O
methods O
to O
make O
a O
model O
generate O
relevant O
responses O
for O
each O
of O
the O
multiple O
corpora O
. O

Since O
simply O
training O
multiple O
corpora O
one O
by O
one O
does O
not O
solve O
the O
imbalanced O
performance O
( O
as O
shown O
in O
Table O
1 O
and O
2 O
) O
, O
we O
first O
investigate O
interleaved B-MethodName
learning I-MethodName
, O
a O
method O
that O
intermingles O
the O
training O
data O
instead O
of O
simply O
concatenating O
, O
which O
ensures O
a O
model O
learns O
from O
all O
corpora O
evenly O
. O
We O
use O
this O
method O
as O
a O
baseline O
. O
Additionally O
, O
we O
explore O
two O
multi O
- O
domain O
learning O
methods O
: O
labeled B-MethodName
learning I-MethodName
and O
multi B-MethodName
- I-MethodName
task I-MethodName
labeled I-MethodName
learning I-MethodName
. O
Labeled B-MethodName
learning I-MethodName
comes O
from O
a O
control O
technique O
in O
response B-TaskName
generation I-TaskName
( O
Li O
et O
al O
. O
, O
2016 O
; O
Johnson O
et O
al O
. O
, O
2017 O
; O
. O
Previous O
works O
focus O
on O
controlling O
persona O
and O
style O
, O
while O
our O
method O
controls O
corpus O
's O
information O
with O
the O
corpus O
embedding O
. O
Multi B-MethodName
- I-MethodName
task I-MethodName
labeled I-MethodName
learning I-MethodName
is O
inspired O
by O
works O
of O
domain O
adaption O
( O
Luan O
et O
al O
. O
, O
2017 O
; O
Niu O
and O
Bansal O
, O
2018 O
; O
Chu O
and O
Wang O
, O
2018 O
) O
, O
where O
multiple O
losses O
from O
both O
the O
corpus O
classifier O
and O
response O
generator O
are O
minimized O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
paper O
is O
the O
first O
that O
uses O
corpus O
embeddings O
on O
the O
open B-TaskName
- I-TaskName
domain I-TaskName
response I-TaskName
generation I-TaskName
task O
for O
multiple O
corpora O
. O

Furthermore O
, O
we O
propose O
a O
novel O
weighted B-MethodName
learning I-MethodName
with O
Domain B-MetricName
- I-MetricName
specific I-MetricName
Frequency I-MetricName
( O
DF B-MetricName
) O
. O
DF B-MetricName
is O
a O
word O
- O
level O
importance O
weight O
( O
Leopold O
and O
Kindermann O
, O
2002 O
) O
that O
assigns O
different O
weights O
( O
importance O
) O
to O
the O
same O
words O
from O
different O
corpora O
. O
In O
the O
training O
process O
, O
we O
weight O
the O
loss O
of O
a O
model O
with O
DF B-MetricName
, O
so O
that O
the O
model O
focuses O
on O
the O
most O
important O
words O
for O
a O
specific O
corpus O
. O

For O
automatic O
evaluation O
metrics O
, O
we O
eliminate O
the O
stop O
words O
and O
use O
ROUGE-1 B-MetricName
( O
precision B-MetricName
, O
recall B-MetricName
, O
F1 B-MetricName
) O
( O
Lin O
, O
2004 O
) O
to O
measure O
the O
relevance O
of O
the O
generated O
responses O
. O
In O
addition O
, O
we O
adopt O
DF B-MetricName
to O
see O
how O
relevant O
the O
generated O
response O
of O
a O
model O
is O
to O
a O
specific O
corpus O
. O
We O
will O
explain O
DF B-MetricName
as O
an O
evaluation O
metric O
in O
Section O
4.4 O
. O
Results O
show O
that O
for O
overall O
performance O
, O
the O
best O
method O
( O
weighted B-MethodName
learning I-MethodName
) O
improves O
27.4 B-MetricValue
% I-MetricValue
on O
precision B-MetricName
, O
45.5 B-MetricValue
% I-MetricValue
on O
recall B-MetricName
, O
and O
34.1 B-MetricValue
% I-MetricValue
on O
F1 B-MetricName
. O
Further O
, O
it O
has O
at O
least O
20.0 B-MetricValue
% I-MetricValue
higher O
DF B-MetricName
, O
stating O
that O
it O
uses O
more O
important O
words O
from O
the O
" O
correct O
" O
corpus O
. O
We O
also O
conduct O
an O
extensive O
human B-MetricName
evaluation I-MetricName
on O
2400 O
generated O
responses O
. O
The O
human B-MetricName
evaluation I-MetricName
shows O
a O
highly O
significant O
( O
p O
< O
0.001 O
) O
improvement O
on O
all O
of O
our O
proposed O
methods O
, O
especially O
the O
weighted B-MethodName
learning I-MethodName
method O
. O

We O
summarize O
our O
work O
as O
follows O
: O

• O
We O
explore O
the O
problem O
of O
training O
and O
eval O
- O
uating O
on O
multiple O
corpora O
from O
different O
domains O
for O
open B-TaskName
- I-TaskName
domain I-TaskName
response I-TaskName
generation I-TaskName
. O
The O
task O
is O
to O
make O
the O
conversational O
models O
generate O
relevant O
responses O
for O
each O
corpus O
. O

• O
We O
examine O
several O
multi O
- O
domain O
corpora O
learning O
methods O
for O
their O
ability O
to O
solve O
the O
proposed O
task O
. O

• O
We O
propose O
Domain B-MetricName
- I-MetricName
specific I-MetricName
Frequency I-MetricName
( O
DF B-MetricName
) O
as O
in O
weighted B-MethodName
learning I-MethodName
and O
as O
an O
evaluation O
metric O
. O
DF B-MetricName
distinguishes O
important O
words O
for O
each O
corpus O
and O
helps O
a O
model O
to O
focus O
on O
these O
important O
words O
in O
the O
training O
process O
. O

Related O
Work O

Open B-TaskName
- I-TaskName
Domain I-TaskName
Response I-TaskName
Generation I-TaskName
Recent O
work O
of O
open B-TaskName
- I-TaskName
domain I-TaskName
response I-TaskName
generation I-TaskName
generally O
follows O
the O
work O
of O
Ritter O
et O
al O
. O
( O
2011 O
) O
where O
the O
task O
is O
treated O
as O
a O
machine O
translation O
task O
, O
and O
many O
of O
them O
use O
a O
Seq2Seq O
structure O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
following O
previous O
work O
( O
Vinyals O
and O
Le O
, O
2015 O
; O
Shang O
et O
al O
. O
, O
2015 O
; O
Sordoni O
et O
al O
. O
, O
2015 O
) O
. O

In O
recent O
years O
, O
substantial O
improvements O
have O
been O
made O
( O
Serban O
et O
al O
. O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2016 O
; O
Wolf O
et O
al O
. O
, O
2019 O
) O
, O
and O
embeddings O
are O
used O
to O
control O
response O
generation O
on O
extra O
information O
such O
as O
persona O
( O
Li O
et O
al O
. O
, O
2016 O
) O
, O
profiles O
, O
coherence O
( O
Xu O
et O
al O
. O
, O
2018 O
) O
, O
emotions O
( O
Huang O
et O
al O
. O
, O
2018 O
) O
, O
and O
dialogue O
attributes O
like O
response O
- O
relatedness O
( O
See O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
there O
is O
a O
lack O
of O
work O
that O
uses O
embeddings O
to O
control O
response O
generation O
over O
multiple O
corpora O
. O
Our O
work O
follows O
the O
common O
models O
of O
opendomain O
conversational O
systems O
, O
while O
we O
study O
the O
problem O
of O
multiple O
corpora O
of O
different O
domains O
. O

Multi O
- O
Domain O
Learning O
and O
Domain O
Adaption O

Multi O
- O
domain O
learning O
aims O
at O
making O
a O
conversational O
model O
learn O
from O
multiple O
domains O
to O
prevent O
the O
performance O
from O
degrading O
due O
to O
domain O
differences O
( O
Ben O
- O
David O
et O
al O
. O
, O
2007 O
) O
. O
There O
are O
two O
categories O
of O
solutions O
for O
multidomain O
learning O
( O
Joshi O
et O
al O
. O
, O
2012 O
) O
: O
( O
i O
) O
capturing O
domain O
- O
specific O
characteristics O
in O
the O
parameters O
( O
Daumé O
III O
, O
2007 O
) O
; O
( O
ii O
) O
capturing O
the O
relationship O
among O
different O
domains O
( O
Saha O
et O
al O
. O
, O
2011 O
) O
. O
Some O
work O
of O
natural B-TaskName
language I-TaskName
generation I-TaskName
and O
machine O
translation O
is O
related O
to O
multi O
- O
domain O
learning O
. O
Luan O
et O
al O
. O
( O
2017 O
) O
and O
Niu O
and O
Bansal O
( O
2018 O
) O
use O
multi O
- O
task O
learning O
for O
domain O
adaption O
respectively O
on O
speaker O
- O
role O
and O
politeness O
. O
Wen O
et O
al O
. O
( O
2016 O
) O
and O
Akama O
et O
al O
. O
( O
2017 O
) O
utilizes O
finetuning O
as O
a O
common O
way O
of O
domain O
adaption O
for O
language B-TaskName
generator I-TaskName
and O
style O
transferer O
. O
For O
machine O
translation O
, O
in O
order O
to O
deal O
with O
the O
mixeddomain O
parallel O
corpus O
, O
Zeng O
et O
al O
. O
( O
2018 O
) O
adjust O
the O
weights O
of O
target O
words O
in O
the O
training O
objective O
based O
on O
their O
relevance O
to O
different O
domains O
. O
We O
differ O
in O
that O
we O
propose O
DF B-MetricName
and O
we O
deal O
with O
the O
response B-TaskName
generation I-TaskName
task O
. O
Chu O
et O
al O
. O
( O
2017 O
) O
propose O
mixed O
fine O
- O
tuning O
, O
which O
adds O
the O
out O
- O
ofdomain O
pre O
- O
training O
data O
to O
the O
fine O
- O
tuning O
dataset O
, O
and O
they O
observe O
an O
improvement O
of O
performance O
. O
In O
this O
paper O
, O
we O
also O
mix O
small O
- O
scale O
fine O
- O
tuning O
datasets O
with O
out O
- O
of O
- O
domain O
training O
data O
, O
while O
the O
data O
we O
add O
is O
not O
necessarily O
used O
during O
pretraining O
. O
Shi O
et O
al O
. O
( O
2015 O
) O
state O
that O
fine O
- O
tuning O
can O
be O
done O
by O
placing O
the O
corpus O
to O
be O
fine O
- O
tuned O
at O
the O
end O
of O
the O
entire O
corpus O
, O
which O
is O
an O
extension O
of O
curriculum O
learning O
proposed O
by O
Bengio O
et O
al O
. O
( O
2009 O
) O
. O
We O
also O
explore O
how O
the O
order O
of O
multiple O
corpora O
influences O
the O
result O
, O
but O
our O
focus O
is O
on O
balancing O
performance O
. O
Recently O
, O
Smith O
et O
al O
. O
( O
2020 O
) O
investigated O
blending O
conversational O
skills O
with O
knowledge O
and O
empathy O
skills O
, O
where O
they O
mix O
3 O
corpora O
. O
They O
focus O
on O
selecting O
appropriate O
skills O
and O
they O
propose O
a O
blended O
corpus O
with O
labels O
, O
while O
we O
focus O
on O
generating O
responses O
that O
are O
most O
relevant O
to O
a O
specific O
corpus O
. O

Base O
Models O

We O
use O
two O
base O
models O
: O
an O
LSTM B-MethodName
Seq2Seq I-MethodName
model O
with O
attention O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
and O
a O
pre O
- O
trained O
GPT-2 B-MethodName
model O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
. O
The O
LSTM B-MethodName
Seq2Seq I-MethodName
model O
with O
attention O
is O
a O
common O
model O
for O
conversational O
systems O
( O
Li O
et O
al O
. O
, O
2016 O
; O
See O
et O
al O
. O
, O
2019 O
) O
, O
and O
the O
GPT2 B-MethodName
model O
is O
a O
state O
- O
of O
- O
the O
- O
art O
model O
for O
the O
response B-TaskName
generation I-TaskName
task O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Zhao O
et O
al O
. O
, O
2020 O
) O
. O

The O
basic O
task O
of O
response B-TaskName
generation I-TaskName
is O
to O
predict O
the O
next O
word O
given O
the O
past O
and O
current O
words O
of O
the O
context O
and O
response O
, O
and O
to O
make O
the O
generated O
response O
as O
similar O
to O
the O
original O
response O
as O
possible O
. O
The O
task O
can O
be O
described O
as O
follows O
. O
Probability O
of O
response O
Y O
given O
context O
X O
is O
predicted O
as O
: O

P O
( O
Y O
|X O
) O
= O
n O
t=1 O
P O
( O
y O
t O
|y O
1 O
, O
. O
. O
. O
, O
y O
t−1 O
, O
X O
) O
, O
( O
1 O

where O
X O
= O
x O
1 O
, O
. O
. O
. O
, O
x O
m O
and O
Y O
= O
y O
1 O
, O
. O
. O
. O
, O
y O
n O
is O
a O
context O
- O
response O
pair O
. O

LSTM B-MethodName
Seq2Seq I-MethodName
Model I-MethodName
with I-MethodName
Attention I-MethodName

We O
simplify O
an O
LSTM O
with O
attention O
unit O
as O
LSTM O
* O
since O
it O
is O
well O
introduced O
in O
previous O
work O
( O
Li O
et O
al O
. O
, O
2016 O
) O
. O
We O
calculate O
the O
hidden O
vector O
h O
t O
at O
step O
t O
as O
: O

h O
t O
= O
LSTM O
* O
( O
h O
t−1 O
, O
E O
( O
z O
t O
) O
) O
, O
( O
2 O
) O

where O
h O
t−1 O
∈ O
R O
dim O
is O
the O
hidden O
vector O
at O
step O
t O
− O
1 O
, O
dim B-HyperparameterName
is O
the O
dimension O
of O
hidden O
vectors O
, O
and O
E O
( O
z O
t O
) O
is O
the O
word O
embedding O
for O
word O
z O
t O
∈ O
( O
x O
1 O
, O
. O
. O
. O
, O
x O
m O
, O
y O
1 O
, O
. O
. O
. O
, O
y O
n−1 O
) O
. O
We O
apply O
dot O
multiple O
in O
the O
attention O
mechanism O
when O
calculating O
the O
context O
vector O
c O
t O
: O

c O
t O
= O
H O
• O
( O
sof O
tmax O
( O
H O
• O
h O
t O
) O
) O

where O
H O
∈ O
R O
d×m O
is O
the O
concatenation O
of O
hidden O
vectors O
from O
the O
encoder O
. O
c O
t O
is O
input O
to O
the O
next O
step O
t O
+ O
1 O
in O
the O
decoder O
. O
Each O
token O
's O
hidden O
vector O
h O
t O
in O
the O
decoder O
is O
combined O
with O
c O
t O
through O
a O
linear O
layer O
and O
an O
activation O
to O
predict O
the O
next O
token O
. O

GPT-2 B-MethodName

As O
for O
GPT-2 B-MethodName
, O
we O
follow O
the O
adaption O
of O
Wolf O
et O
al O
. O
( O
2019 O
) O
. O
The O
transformer O
block O
of O
GPT-2 B-MethodName
captures O
the O
relation O
of O
multiple O
words O
in O
one O
sentence O
, O
which O
largely O
follows O
Vaswani O
et O
al O
. O
( O
2017 O
) O
. O
The O
hidden O
vector O
to O
be O
input O
to O
the O
transformer O
block O
is O
calculated O
as O
: O

h O
0 O
[ O
t O
] O
= O
E O
( O
X O
, O
Y O
[ O
1 O
: O
t O
] O
) O
+ O
( O
E O
0 O
, O
E O
1 O
) O
+ O
W O
p O
, O
( O
3 O
) O

where O
Y O
[ O
1 O
: O
t O
] O
is O
( O
y O
1 O
, O
. O
. O
. O
, O
y O
t O
) O
, O
E O
( O
X O
, O
Y O
[ O
1 O
: O
t O
] O
) O
is O
the O
subword O
embedding O
for O
context O
X O
and O
response O
Y O
[ O
1 O
: O
t O
] O
. O
E O
0 O
and O
E O
1 O
are O
dialogue O
- O
state O
embeddings O
, O
which O
tutor O
the O
model O
to O
distinguish O
between O
contexts O
and O
responses O
. O
W O
p O
is O
a O
pre O
- O
trained O
position O
embedding O
. O
The O
probability O
of O
the O
subword O
to O
generate O
is O
then O
calculated O
as O
: O

h O
[ O
t O
] O
= O
transformer O
_ O
block O
( O
h O
0 O
[ O
t O
] O
) O
( O
4 O
) O
P O
( O
y O
) O
t+1 O
= O
softmax O
( O
E O
( O
h O
[ O
t O
] O
) O
) O
, O
( O
5 O
) O

where O
y O
∈ O
V O
, O
and O
V O
stands O
for O
the O
sub O
- O
word O
vocabulary O
. O
We O
simplify O
the O
structure O
of O
transformer O
block O
as O
transformer O
_ O
block O
. O
In O
the O
block O
, O
a O
mask O
is O
filled O
in O
the O
attention O
matrix O
, O
which O
bans O
past O
words O
from O
attending O
to O
future O
words O
. O
This O
ensures O
that O
the O
model O
follows O
the O
traditional O
language O
modeling O
. O
The O
hidden O
vector O
of O
t O
th O
sub O
- O
word O
is O
used O
to O
generate O
the O
probability O
distribution O
for O
the O
vocabulary O
( O
P O
( O
y O
) O
, O
y O
∈ O
V O
) O
for O
( O
t O
+ O
1 O
) O
th O
subword O
. O
E O
means O
that O
the O
model O
uses O
the O
sub O
- O
word O
embeddings O
in O
calculating O
sub O
- O
word O
probabilities O
for O
generation O
( O
Press O
and O
Wolf O
, O
2017 O
) O
. O

4 O
Proposed O
Methods O

Interleaved B-MethodName
Learning I-MethodName

Interleaving O
is O
a O
concept O
in O
cognitive O
psychology O
proven O
to O
be O
efficient O
for O
learning O
( O
Kornell O
and O
Bjork O
, O
2008 O
) O
: O
intermingling O
learning O
material O
of O
different O
topics O
helps O
students O
to O
gain O
better O
learning O
results O
than O
learning O
the O
material O
topic O
by O
topic O
. O
Previous O
work O
from O
machine O
learning O
also O
shows O
that O
training O
order O
greatly O
influences O
the O
performance O
( O
Bengio O
et O
al O
. O
, O
2009 O
) O
. O
When O
the O
training O
is O
conducted O
on O
a O
simple O
concatenation O
of O
multiple O
corpora O
, O
the O
model O
tends O
to O
concentrate O
on O
the O
last O
corpus O
( O
Shi O
et O
al O
. O
, O
2015 O
) O
. O
To O
address O
this O
issue O
, O
we O
propose O
interleaved B-MethodName
learning I-MethodName
as O
an O
alternative O
: O
each O
time O
we O
collect O
one O
context O
- O
response O
pair O
from O
each O
of O
the O
corpora O
, O
and O
we O
randomly O
shuffle O
them O
. O
For O
example O
, O
if O
there O
are O
3 O
corpora O
( O
a O
1 O
, O
a O
2 O
, O
... O
) O
, O
( O
b O
1 O
, O
b O
2 O
, O
... O
) O
, O
( O
c O
1 O
, O
c O
2 O
, O
... O
) O
where O
a O
i O
, O
b O
i O
and O
c O
i O
are O
context O
- O
response O
pairs O
, O
the O
resulting O
mixed O
corpus O
might O
be O

( O
b O
1 O
, O
a O
1 O
, O
c O
1 O
, O
c O
2 O
, O
b O
2 O
, O
a O
2 O
, O
... O
) O
. O

Interleaved B-MethodName
learning I-MethodName
guarantees O
that O
the O
combined O
corpus O
is O
evenly O
distributed O
, O
which O
helps O
the O
model O
learn O
from O
multiple O
corpora O
evenly O
. O

Labeled B-MethodName
Learning I-MethodName

We O
propose O
our O
labeled B-MethodName
learning I-MethodName
as O
follows O
: O
each O
corpus O
is O
assigned O
a O
randomly O
initialized O
unique O
embedding O
, O
and O
the O
conversational O
model O
learns O
these O
embeddings O
together O
with O
conversations O
during O
the O
training O
period O
. O
We O
denote O
these O
embeddings O
as O
" O
corpus O
embedding O
" O
, O
or O
E O
c O
. O
A O
model O
captures O
each O
corpus O
's O
characteristics O
through O
the O
corpus O
embedding O
and O
uses O
it O
to O
control O
the O
generated O
responses O
. O
To O
know O
which O
corpus O
embedding O
to O
use O
, O
each O
context O
is O
labeled O
with O
which O
corpus O
it O
comes O
from O
, O
and O
these O
labels O
are O
provided O
to O
the O
model O
both O
in O
the O
training O
and O
generation O
period O
. O
We O
propose O
an O
approach O
for O
each O
of O
our O
base O
models O
for O
encoding O
corpus O
embeddings O
. O

For O
the O
LSTM B-MethodName
model O
, O
following O
Li O
et O
al O
. O
( O
2016 O
) O
, O
we O
input O
the O
corpus O
embedding O
E O
c O
into O
the O
first O
layer O
of O
the O
decoder O
LSTM O
at O
every O
step O
, O
together O
with O
the O
response O
words O
. O
Calculation O
of O
a O
hidden O
vector O
h O
t O
in O
the O
decoder O
LSTM O
is O
then O
adapted O
to O
: O

h O
t O
= O
LSTM O
* O
( O
h O
t−1 O
, O
E O
( O
y O
t O
) O
, O
E O
c O
) O
. O
( O
6 O

The O
structure O
is O
illustrated O
in O
the O
dashed O
red O
rectangle O
of O
Figure O
1a O
. O

For O
the O
GPT-2 B-MethodName
model O
, O
our O
method O
is O
based O
on O
Wolf O
et O
al O
. O
( O
2019 O
) O
. O
Instead O
of O
two O
kinds O
of O
dialogue O
- O
state O
embeddings O
( O
context O
embedding O
E O
0 O
and O
response O
embedding O
E O
1 O
) O
, O
we O
replace O
the O
response O
embedding O
with O
corpus O
embeddings O
E O
c O
. O
As O
a O
result O
, O
the O
model O
is O
aware O
of O
which O
corpus O
the O
response O
belongs O
. O
Calculation O
of O
a O
hidden O
vector O
to O
be O
input O
to O
the O
transformer O
block O
is O
adapted O
to O
: O

h O
0 O
[ O
t O
] O
= O
E O
( O
X O
, O
Y O
[ O
1 O
: O
t O
] O
) O
+ O
( O
E O
0 O
, O
E O
c O
) O
+ O
W O
p O
. O
( O
7 O

The O
structure O
is O
illustrated O
in O
Figure O
1b O
. O

Multi B-MethodName
- I-MethodName
Task I-MethodName
Labeled I-MethodName
Learning I-MethodName

Labeled B-MethodName
learning I-MethodName
needs O
corpus O
labels O
for O
both O
training O
and O
generation O
processes O
. O
To O
avoid O
providing O
labels O
in O
the O
generation O
process O
, O
we O
combine O
multitask B-MethodName
learning I-MethodName
with O
labeled B-MethodName
learning I-MethodName
on O
multiple O
corpora O
. O
Here O
, O
the O
conversational O
model O
has O
to O
predict O
by O
itself O
which O
corpus O
a O
context O
belongs O
to O
, O
which O
is O
expected O
to O
result O
in O
worse O
performance O
, O
but O
less O
information O
is O
required O
. O
In O
the O
encoder O
, O
we O
have O
a O
classifier O
layer O
that O
uses O
the O
sum O
of O
hidden O
vectors O
from O
the O
encoder O
( O
H O
) O
to O
predict O
the O
corpus O
of O
a O
context O
. O
The O
loss O
of O
the O
classifier O
is O
calculated O
as O
: O

L O
c O
= O
−log O
softmax O
H O
• O
W O
[ O
c O
] O
, O
( O
8 O

where O
W O
[ O
c O
] O
∈ O
R O
dim O
is O
the O
part O
from O
the O
classifier O
layer O
for O
target O
corpus O
c. O
L O
c O
is O
summed O
up O
with O
the O
loss O
from O
the O
response O
generator O
. O
The O
predicted O
corpus O
embedding O
is O
input O
into O
the O
decoder O
like O
labeled O
learning O
( O
see O
Section O
4.2 O
) O
. O
The O
simplified O
structure O
is O
illustrated O
in O
Figure O
1a O
. O

Document B-MetricName
- I-MetricName
specific I-MetricName
Frequency I-MetricName
( O
DF B-MetricName
) O

We O
propose O
Domain B-MetricName
- I-MetricName
specific I-MetricName
Frequency I-MetricName
( O
DF B-MetricName
) O
to O
measure O
how O
important O
a O
word O
is O
with O
respect O
to O
a O
different O
corpus O
under O
a O
collection O
of O
corpora O
. O
DF B-MetricName
is O
used O
for O
weighted B-MethodName
learning I-MethodName
and O
evaluation O
. O
It O
is O
calculated O
as O
follows O
: O

f O
( O
w O
) O
d O
= O
freq O
( O
w O
) O
d O
− O
min O
v O
{ O
freq O
( O
v O
) O
d O
} O
( O
9 O
) O
df O
( O
w O
) O
d O
= O
0 O
f O
( O
w O
) O
d O
= O
0 O
f O
( O
w O
) O
d O
d∈D O
f O
( O
w O
) O
d O
f O
( O
w O
) O
d O
= O
0 O
( O
10 O
) O
DF B-MetricName
( I-MetricName
w I-MetricName
) I-MetricName
d I-MetricName
= O
df O
( O
w O
) O
d O
max O
v O
{ O
df O
( O
v O
) O
d O
} O
, O
( O
11 O

where O
freq O
( O
w O
) O
d O
is O
the O
relative O
frequency O
of O
a O
word O
w O
in O
a O
corpus O
d O
, O
and O
D O
represents O
the O
set O
of O
all O
corpora O
. O
It O
is O
easy O
to O
see O
from O
Equation O
10that O
DF B-MetricName
( O
w O
) O
d O
represents O
the O
importance O
of O
word O
w O
for O
corpus O
d O
compared O
to O
other O
corpora O
. O
For O
a O
word O
w O
that O
frequently O
appears O
in O
corpus O
d O
but O
seldom O
A O
word O
that O
frequently O
appears O
in O
all O
corpora O
( O
e.g. O
, O
" O
I O
" O
, O
" O
you O
" O
) O
is O
punished O
, O
resulting O
in O
a O
lower O
DF B-MetricName
( O
w O
) O
d O
. O
A O
word O
that O
seldom O
appears O
in O
corpus O
d O
but O
frequently O
appears O
in O
other O
corpora O
( O
e.g. O
, O
" O
music O
" O
seldom O
appears O
in O
Ubuntu B-DatasetName
corpus O
, O
but O
is O
common O
in O
other O
corpora O
) O
has O
the O
lowest O
DF B-MetricName
( O
w O
) O
d O
. O
Words O
that O
appear O
minimal O
times O
( O
e.g. O
, O
once O
) O
in O
a O
corpus O
are O
ignored O
with O
Equation O
9 O
. O
Words O
that O
appear O
few O
times O
( O
e.g. O
, O
twice O
or O
three O
times O
) O
are O
not O
dealt O
with O
, O
yet O
they O
are O
not O
of O
great O
influence O
in O
our O
experiments O
. O
We O
apply O
a O
normalization O
in O
the O
final O
step O
( O
Equation O
11 O
) O
to O
make O
DF B-MetricName
( O
w O
) O
d O
of O
each O
corpus O
d O
range O
from O
0 O
to O
1 O
. O
We O
show O
DF O
( O
w O
) O
Ubuntu B-DatasetName
and O
DF O
( O
w O
) O
PersonaChat B-DatasetName
of O
some O
words O
in O
Table O
3 O
. O
We O
also O
show O
the O
results O
of O
TF B-MetricName
- I-MetricName
IDF I-MetricName
( O
log O
normalization O
variant O
) O
, O
a O
commonly O
used O
word O
importance O
weight O
, O
as O
a O
comparison O
. O
As O
expected O
, O
for O
the O
corpus O
Ubuntu B-DatasetName
and O
PersonaChat B-DatasetName
, O
most O
unique O
words O
w O
have O
very O
different O
DF B-MetricName
( O
w O
) O
Ubuntu B-DatasetName
and O
DF B-MetricName
( O
w O
) O
PersonaChat B-DatasetName
. O
Unique O
words O
of O
each O
corpus O
get O
the O
highest O
values O
for O
the O
corresponding O
corpus O
, O
like O
" O
upgrade O
" O
for O
the O
Ubuntu B-DatasetName
corpus O
and O
" O
music O
" O
for O
the O
PersonaChat B-DatasetName
corpus O
; O
these O
words O
receive O
the O
lowest O
values O
for O
incorrect O
corpora O
, O
like O
" O
upgrade O
" O
for O
PersonaChat B-DatasetName
and O
" O
music O
" O
for O
Ubuntu B-DatasetName
. O
The O
stress O
on O
unique O
words O
makes O
DF B-MetricName
more O
suitable O
for O
our O
task O
. O

Weighted B-MethodName
Learning I-MethodName
with O
DF B-MetricName
Weighted B-MethodName
learning I-MethodName
weights O
the O
loss O
of O
the O
predication O
y O
for O
each O
target O
word O
w O
using O
DF B-MetricName
( O
w O
) O
d O
. O

L O
weighted O
= O
DF B-MetricName
( O
w O
) O
d O
• O
−log O
softmax O
( O
y O
w O
) O
, O
( O
12 O
) O

where O
y O
w O
represents O
the O
model O
's O
predicted O
score O
for O
the O
target O
word O
w. O
With O
the O
weighted O
loss O
, O
the O
model O
concentrates O
on O
words O
that O
are O
important O
to O
the O
corpus O
of O
the O
current O
context O
, O
and O
focuses O
less O
on O
frequent O
words O
or O
words O
that O
are O
not O
important O
to O
the O
current O
corpus O
. O
The O
structure O
is O
illustrated O
in O
Figure O
1c O
. O

Evaluation O
with O
DF B-MetricName
For O
the O
generated O
responses O
to O
be O
relevant O
to O
a O
specific O
corpus O
, O
they O
have O
to O
be O
similar O
to O
that O
corpus O
, O
which O
includes O
using O
important O
words O
of O
that O
corpus O
( O
e.g. O
, O
responses O
generated O
for O
the O
Ubuntu O
corpus O
should O
have O
more O
technical O
words O
than O
other O
corpora O
) O
. O
Thus O
, O
we O
propose O
DF B-MetricName
as O
an O
evaluation O
metric O
that O
shows O
to O
what O
extent O
the O
generated O
responses O
use O
important O
words O
of O
the O
corresponding O
corpus O
. O
We O
want O
to O
decrease O
the O
influence O
of O
common O
words O
like O
" O
i O
" O
, O
" O
to O
" O
, O
etc O
. O
, O
and O
thus O
address O
the O
important O
words O
. O
So O
we O
adopt O
exponential O
DF B-MetricName
with O
α B-HyperparameterName
as O
the O
base O
( O
αDF B-MetricName
) O
: O

αDF B-MetricName
( O
w O
) O
d O
= O
0 O
DF B-MetricName
( O
w O
) O
d O
= O
0 O
α B-HyperparameterName
DF B-MetricName
( O
w O
) O
d O
DF B-MetricName
( O
w O
) O
d O
= O
0 O
, O
( O
13 O

where O
α B-HyperparameterName
is O
a O
constant O
. O
αDF B-MetricName
( O
w O
) O
d O
rescales O
DF B-MetricName
( O
w O
) O
d O
by O
exponent O
with O
α O
as O
a O
base O
. O
In O
our O
experiments O
, O
we O
set O
α B-HyperparameterName
to O
be O
100 B-HyperparameterValue
, O
which O
transforms O
the O
range O
of O
the O
metric O
from O
( O
0 O
, O
1 O
) O
to O
( O
0 O
, O
100 O
) O
. O
This O
makes O
the O
difference O
between O
high O
and O
low O
αDF B-HyperparameterName
more O
significant O
than O
DF B-MetricName
and O
gives O
a O
100 O
- O
scale O
score O
. O
We O
show O
αDF B-MetricName
( O
w O
) O
Ubuntu B-DatasetName
and O
αDF B-MetricName
( O
w O
) O
PersonaChat B-DatasetName
( O
calculated O
purely O
on O
test O
set O
) O
in O
Table O
3 O
. O
As O
expected O
, O
αDF B-MetricName
has O
a O
more O
significant O
difference O
between O
important O
words O
and O
common O
words O
. O

Is O
DF B-MetricName
a O
Legal O
Evaluation O
Metric O
? O
Although O
DF B-MetricName
is O
used O
for O
both O
weighted B-MethodName
learning I-MethodName
and O
evaluation O
, O
we O
see O
DF B-MetricName
as O
a O
suitable O
evaluation O
metric O
for O
our O
task O
and O
not O
biased O
in O
favor O
of O
weighted B-MethodName
learning I-MethodName
due O
to O
: O
1 O
) O
A O
word O
receives O
multiple O
DF B-MetricName
values O
in O
the O
training O
process O
given O
the O
corpus O
that O
a O
context O
belongs O
to O
; O
2 O
) O
in O
the O
generation O
process O
, O
DF B-MetricName
is O
never O
used O
. O
3 O
) O
In O
the O
evaluation O
process O
, O
DF B-MetricName
can O
be O
calculated O
purely O
on O
the O
test O
sets O
. O
Note O
that O
since O
a O
word O
receives O
multiple O
DF B-MetricName
values O
in O
the O
training O
step O
, O
it O
is O
equivalently O
likely O
for O
the O
model O
trained O
with O
weighted O
learning O
to O
be O
influenced O
by O
DF B-MetricName
weights O
of O
incorrect O
corpus O
. O
Above O
all O
, O
in O
the O
evaluation O
step O
, O
if O
the O
trained O
model O
is O
influenced O
more O
by O
DF B-MetricName
weights O
from O
the O
correct O
corpus O
, O
it O
already O
means O
that O
the O
model O
is O
good O
at O
distinguishing O
which O
corpus O
a O
given O
context O
is O
from O
, O
thus O
is O
suitable O
for O
our O
task O
. O

Experiment O
Setup O

Datasets O
Data O
Collection O

We O
collected O
4 O
commonly O
used O
English O
corpora O
of O
different O
domains O
from O
the O
Par O
- O
lAI O
platform O
( O
Miller O
et O
al O
. O
, O
2017 O
) O
: O
OpenSubtitles B-DatasetName
corpus O
( O
OSDB B-DatasetName
) O
2 O
( O
Lison O
et O
al O
. O
, O
2018 O
) O
, O
Twitter B-DatasetName
corpus O
3 O
( O
Miller O
et O
al O
. O
, O
2017 O
) O
, O
Ubuntu B-DatasetName
chatlogs O
corpus O
( O
Lowe O
et O
al O
. O
, O
2015 O
) O
4 O
, O
and O
PersonaChat B-DatasetName
corpus O
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
from O
the O
NeurIPS O
2018 O
ConvAI2 O
Challenge O
( O
Dinan O
et O
al O
. O
, O
2019 O
) O
. O
Each O
corpus O
contains O
250 O
K O
context O
- O
response O
pairs O
, O
as O
much O
as O
the O
size O
of O
the O
original O
PersonaChat B-DatasetName
used O
in O
ConvAI2 O
competition O
. O
This O
gives O
us O
1 O
M O
contextresponse O
pairs O
in O
total O
. O
The O
corpus O
for O
training O
is O
a O
combination O
of O
these O
4 O
corpora O
. O
For O
comparison O
, O
we O
have O
a O
single O
corpus O
- O
PersonaChat B-DatasetName
- O
trained O
on O
both O
base O
models O
. O
For O
testing O
, O
each O
of O
the O
4 O
corpora O
has O
a O
test O
set O
of O
30 O
K O
context O
- O
response O
pairs O
, O
which O
is O
the O
same O
size O
of O
the O
test O
set O
of O
PersonaChat B-DatasetName
. O

The O
OpenSubtitles B-DatasetName
corpus O
( O
OSDB B-DatasetName
) O
is O
a O
noisy O
dataset O
of O
film O
subtitles O
. O
We O
removed O
films O
that O
belonged O
to O
genres O
that O
usually O
had O
few O
conversations O
, O
such O
as O
musical O
and O
documentary O
films O
. O
We O
regarded O
two O
neighboring O
sentences O
as O
a O
contextresponse O
pair O
following O
Vinyals O
and O
Le O
( O
2015 O
) O
. O
The O
Twitter B-DatasetName
corpus O
contains O
one O
- O
turn O
dialogues O
extracted O
from O
Twitter O
. O
The O
original O
author O
has O
already O
cleaned O
it O
, O
so O
we O
only O
removed O
special O
symbols O
such O
as O
hashtags O
, O
Emojis O
, O
and O
@ O
. O
The O
Ubuntu B-DatasetName
corpus O
contains O
dialogues O
about O
solving O
technical O
problems O
of O
Ubuntu O
. O
The O
PersonaChat B-DatasetName
corpus O
contains O
dialogues O
between O
two O
workers O
acting O
as O
specific O
personas O
; O
we O
focused O
on O
the O
dialogue O
part O
and O
ignored O
the O
persona O
part O
. O
This O
corpus O
allows O
us O
to O
compare O
our O
base O
models O
with O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
These O
4 O
corpora O
have O
very O
different O
characteristics O
, O
confirmed O
by O
the O
imbalanced O
performance O
of O
GPT-2 B-MethodName
fine O
- O
tuned O
on O
a O
single O
corpus O
( O
see O
Table O
2 O
) O
. O

Training O
and O
Decoding O

We O
used O
Pytorch O
( O
Paszke O
et O
al O
. O
, O
2017 O
) O
to O
implement O
the O
LSTM B-MethodName
Seq2Seq I-MethodName
model O
with O
attention O
and O
the O
pre O
- O
trained O
GPT-2 B-MethodName
models O
. O
For O
GPT-2 B-MethodName
, O
we O
adapted O
( O
2019 O
) O
: O
the O
batch B-HyperparameterName
size I-HyperparameterName
was O
32 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
was O
6 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
, O
L2 B-HyperparameterName
weight I-HyperparameterName
decay I-HyperparameterName
set O
to O
0.01 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
linearly O
decreased O
to O
zero O
at O
the O
end O
. O
We O
followed O
these O
hyper O
- O
parameters O
to O
ensure O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
the O
base O
models O
. O
We O
use O
the O
same O
hyper O
- O
parameters O
for O
both O
base O
models O
and O
models O
with O
our O
proposed O
methods O
, O
so O
the O
proposed O
methods O
work O
slightly O
5 O
https O
: O
/ O
/ O
huggingface.co O
/ O
. O

( O
but O
not O
much O
) O
worse O
than O
it O
should O
be O
. O
This O
is O
to O
avoid O
the O
extra O
improvement O
caused O
by O
hyperparameters O
. O
We O
pre O
- O
trained O
the O
LSTM B-MethodName
model O
on O
3 O
large O
- O
scale O
corpora O
( O
OSDB B-DatasetName
, O
Twitter B-DatasetName
and O
Ubuntu B-DatasetName
) O
with O
interleaved B-MethodName
learning I-MethodName
until O
converging O
. O
GPT-2 B-MethodName
is O
already O
pre O
- O
trained O
, O
so O
we O
directly O
used O
it O
for O
finetuning O
( O
details O
about O
pre O
- O
training O
convergence O
can O
be O
found O
in O
Section O
B O
) O
. O
For O
decoding O
, O
we O
adopted O
greedy O
decoding O
for O
all O
the O
models O
to O
ensure O
an O
equal O
condition O
. O

Evaluation O

For O
automatic O
metrics O
, O
to O
measure O
the O
relevance O
of O
the O
generated B-TaskName
responses I-TaskName
, O
we O
eliminated O
punctuation O
and O
stop O
words O
, O
and O
adopted O
Rouge-1 B-MetricName
6 O
( O
precision B-MetricName
, O
recall B-MetricName
, O
F1 B-MetricName
) O
as O
multi O
- O
grams O
become O
meaningless O
without O
stop O
words O
. O
However O
, O
Rouge-1 B-MetricName
compares O
the O
generated O
responses O
with O
the O
golden O
ones O
, O
while O
there O
is O
never O
a O
standard O
response O
for O
any O
context O
, O
so O
in O
addition O
to O
Rouge O
, O
we O
use O
αDF B-MetricName
score O
that O
shows O
to O
what O
extent O
the O
generated O
responses O
use O
important O
words O
of O
the O
corresponding O
corpus O
, O
as O
stated O
in O
Section O
4.4 O
. O
Due O
to O
the O
limitation O
of O
automatic O
evaluation O
methods O
( O
Liu O
et O
al O
. O
, O
2016 O
) O
, O
we O
also O
conduct O
an O
extensive O
human B-MetricName
evaluation I-MetricName
on O
the O
relevance O
of O
generated O
responses O
to O
contexts O
( O
see O
Section O
6.1 O
for O
details O
) O
. O

Results O

Our O
base O
models O
achieve O
perplexity B-MetricName
scores I-MetricName
of O
28.9 B-MetricValue
( O
LSTM B-MethodName
model O
) O
and O
19.6 B-MetricValue
( O
GPT-2 B-MethodName
) O
on O
the O
test O
set O
of O
the O
PersonaChat B-DatasetName
dataset O
from O
the O
ConvAI2 O
competition O
when O
fine O
- O
tuned O
with O
the O
single O
PersonaChat B-DatasetName
corpus O
( O
more O
details O
can O
be O
found O
in O
Section O
C O
) O
. O
These O
results O
would O
likely O
advance O
the O
models O
to O
the O
second O
round O
in O
the O
competition O
. O
Table O
4 O
shows O
that O
models O
trained O
with O
our O
proposed O
methods O
gain O
better O
performance O
on O
Rouge B-MetricName
than O
baselines O
. O
Baselines O
concentrate O
on O
the O
last O
trained O
corpus O
( O
PersonaChat B-DatasetName
) O
, O
while O
with O
the O
proposed O
methods O
, O
performance O
is O
more O
balanced O
on O
multiple O
corpora O
. O
Weighted B-MethodName
learning I-MethodName
has O
the O
best O
overall O
performance O
on O
all O
metrics O
, O
and O
it O
performs O
especially O
well O
on O
the O
Ubuntu B-DatasetName
corpus O
, O
indicating O
that O
it O
might O
be O
good O
at O
distinguishing O
the O
unique O
technical O
words O
from O
the O
Ubuntu B-DatasetName
corpus O
. O
Labeled B-MethodName
learning I-MethodName
is O
the O
second O
best O
with O
stable O
improvement O
from O
interleaved B-MethodName
learning I-MethodName
, O
indicating O
that O
the O
corpus O
embeddings O
function O
as O
expected O
. O
Multi B-MethodName
- I-MethodName
task I-MethodName
labeled I-MethodName
learning I-MethodName
has O
slightly O
worse O
performance O
than O
interleaved B-MethodName
learning I-MethodName
, O
indicating O
that O
predicting O
the O
corpus O
of O
a O
contexts O
is O
not O
easy O
, O
and O
wrong O
predictions O
result O
in O
worse O
performance O
. O

Table O
5 O
shows O
αDF B-MetricName
d O
scores O
for O
generated B-TaskName
responses I-TaskName
of O
each O
corpus O
. O
Full O
results O
can O
be O
found O
in O
Section O
E. O
We O
use O
both O
αDF B-MetricName
d O
calculated O
purely O
on O
the O
train O
set O
( O
train O
- O
set O
- O
αDF B-MetricName
) O
and O
αDF B-MetricName
d O
calculated O
purely O
on O
the O
test O
set O
( O
test O
- O
set O
- O
αDF B-MetricName
) O
. O
The O
black O
scores O
are O
scores O
for O
the O
corresponding O
corpus O
( O
we O
expect O
high O
scores O
for O
these O
parts O
) O
, O
while O
the O
grey O
scores O
are O
scores O
for O
non O
- O
related O
corpus O
- O
PersonaChat B-DatasetName
( O
we O
expect O
low O
scores O
for O
these O
parts O
) O
. O
Note O
that O
scores O
for O
different O
corpora O
are O
in O
different O
scales O
. O
From O
the O
table O
, O
we O
can O
see O
that O
train O
- O
set O
- O
DF B-MetricName
scores O
and O
test O
- O
set O
- O
DF B-MetricName
scores O
are O
similar O
, O
and O
weighted B-MethodName
learning I-MethodName
always O
has O
the O
highest O
score O
, O
indicating O
that O
weighted B-MethodName
learning I-MethodName
distinguishes O
well O
which O
corpus O
a O
context O
comes O
from O
. O
Labeled B-MethodName
learning I-MethodName
is O
the O
second O
best O
, O
indicating O
that O
the O
learned O
corpus O
embeddings O
help O
the O
model O
to O
use O
more O
important O
words O
of O
the O
corresponding O
corpus O
. O
Compared O
to O
the O
concatenated O
corpus O
, O
the O
improvement O
is O
at O
least O
20 B-MetricValue
% I-MetricValue
, O
while O
the O
decrease O
in O
PersonaChat B-DatasetName
is O
just O
9 B-MetricValue
% I-MetricValue
at O
most O
. O

Human B-MetricName
Evaluation I-MetricName

We O
conducted O
a O
human B-MetricName
evaluation I-MetricName
on O
all O
GPT-2 B-MethodName
models O
: O
base O
models O
and O
models O
adapted O
with O
our O
proposed O
methods O
. O
We O
randomly O
picked O
2400 O
responses O
: O
400 O
different O
contexts O
evenly O
from O
4 O
corpora O
with O
6 O
responses O
generated O
by O
each O
of O
our O
models O
. O
3 O
judges O
7 O
are O
asked O
to O
pick O
the O
most O
and O
the O
least O
relevant O
response O
( O
s O
) O
for O
the O
given O
context O
. O
The O
most O
relevant O
response O
( O
s O
) O
are O
given O
score O
3 O
, O
the O
least O
relevant O
response O
( O
s O
) O
are O
given O
score O
1 O
, O
and O
the O
other O
( O
s O
) O
are O
given O
score O
2 O
. O
Table O
6 O
shows O
the O
overall O
scores O
of O
all O
GPT-2 B-MethodName
based O
models O
. O
Table O
7 O
shows O
the O
p O
- O
value O
for O
the O
t O
- O
test O
conducted O
between O
every O
two O
models O
. O
The O
overall O
scores O
of O
our O
proposed O
methods O
are O
all O
highly O
significantly O
( O
p O
< O
0.001 O
) O
higher O
than O
the O
concatenated O
models O
, O
especially O
the O
weighted B-MethodName
learning I-MethodName
method O
. O

Response O
Examples O

The O
generated B-TaskName
responses I-TaskName
from O
better O
methods O
are O
more O
relevant O
to O
the O
corresponding O
corpus O
, O
while O
worse O
methods O
can O
not O
distinguish O
contexts O
from O
different O
corpora O
( O
e.g. O
, O
they O
may O
answer O
any O
questions O
in O
a O
" O
PersonaChat B-DatasetName
" O
way O
) O
. O
To O
show O
an O
intuition O
of O
the O
difference O
among O
our O
proposed O
methods O
, O
we O
present O
some O
response O
examples O
generated O
by O
GPT-2 B-MethodName
in O
Section O
G O
. O

Possible O
Limitations O

Our O
proposed O
methods O
are O
meant O
to O
be O
able O
to O
work O
in O
most O
models O
, O
which O
is O
why O
we O
choose O
the O
most O
common O
conversational O
models O
as O
our O
base O
models O
. O
However O
, O
there O
are O
many O
variants O
of O
conversational O
models O
focusing O
on O
different O
aspects O
, O
such O
as O
integrating O
knowledge O
, O
avoiding O
dull O
responses O
, O
keeping O
the O
speech O
style O
, O
etc O
. O
We O
can O
not O
ensure O
that O
our O
methods O
work O
for O
all O
of O
these O
variant O
models O
. O
Also O
, O
dialogues O
are O
always O
multi O
- O
turn O
, O
while O
we O
focus O
on O
a O
simpler O
task O
: O
single O
- O
turn O
response O
generation O
. O
Furthermore O
, O
the O
methods O
are O
trained O
and O
evaluated O
on O
English O
corpora O
. O
There O
can O
be O
a O
limitation O
on O
applying O
the O
methods O
to O
other O
languages O
. O

Conclusions O

We O
have O
experimented O
with O
4 O
methods O
- O
interleaved B-MethodName
learning I-MethodName
( O
baseline O
) O
, O
labeled B-MethodName
learning I-MethodName
, O
multi B-MethodName
- I-MethodName
task I-MethodName
labeled I-MethodName
learning I-MethodName
, O
and O
weighted B-MethodName
learning I-MethodName
- O
to O
help O
common O
open O
- O
domain O
conversational O
systems O
generate O
relevant O
responses O
for O
multiple O
corpora O
of O
different O
domains O
. O
We O
adopted O
Rouge B-MetricName
( O
precision B-MetricName
, O
recall B-MetricName
, O
F1 B-MetricName
) O
for O
auto O
evaluation O
. O
In O
addition O
, O
we O
used O
DF B-MetricName
to O
evaluate O
how O
well O
a O
model O
uses O
relevant O
words O
for O
a O
corresponding O
corpus O
. O
We O
also O
did O
an O
extensive O
human B-MetricName
evaluation I-MetricName
. O
Our O
results O
show O
significant O
improvement O
in O
performance O
for O
our O
proposed O
methods O
, O
especially O
weighted B-MethodName
learning I-MethodName
. O
Future O
work O
of O
multi O
- O
turn O
response O
generation O
is O
potential O
. O
We O
have O
focused O
on O
one B-TaskName
- I-TaskName
turn I-TaskName
response I-TaskName
generation I-TaskName
, O
while O
dialogue O
is O
naturally O
multi O
- O
turn O
so O
further O
research O
is O
needed O
. O
Example O
words O
are O
divided O
into O
five O
blocks O
. O
The O
first O
block O
has O
frequent O
words O
in O
all O
corpora O
, O
the O
second O
block O
has O
unique O
words O
from O
OSDB B-DatasetName
, O
the O
third O
block O
has O
unique O
words O
from O
Twitter B-DatasetName
, O
the O
fourth O
block O
has O
unique O
words O
from O
Ubuntu B-DatasetName
, O
and O
the O
fifth O
block O
has O
unique O
words O
from O
PersonaChat B-DatasetName
. O
The O
values O
of O
the O
corresponding O
corpus O
are O
marked O
with O
different O
colors O
. O

From O
this O
table O
, O
it O
is O
clear O
that O
the O
commonly O
used O
word O
importance O
weight O
, O
TF B-MetricName
- I-MetricName
IDF I-MetricName
, O
is O
not O
suitable O
for O
our O
task O
. O
This O
is O
due O
to O
the O
vast O
range O
of O
frequency O
, O
which O
leads O
to O
a O
relatively O
small O
penalty O
for O
IDF B-MetricName
( O
Inversed B-MetricName
Document I-MetricName
Frequency I-MetricName
) O
over O
words O
with O
too O
large O
TF B-MetricName
( O
Term B-MetricName
Frequency I-MetricName
) O
. O

Acknowledgements O

This O
paper O
is O
funded O
by O
the O
collaborative O
project O
of O
DNB O
ASA O
and O
Norwegian O
University O
of O
Science O
and O
Technology O
( O
NTNU O
) O
. O
We O
also O
received O
assist O
on O
computing O
resources O
from O
the O
IDUN O
cluster O
of O
NTNU O
( O
Själander O
et O
al O
. O
, O
2019 O
) O
. O
We O
would O
like O
to O
thank O
Aria O
Rahmati O
, O
Zhirong O
Yang O
( O
Norwegian O
Research O
Council O
, O
287284 O
) O
and O
Özlem O
Özgöbek O
for O
their O
helpful O
comments O
. O

In O
the O
pre O
- O
training O
period O
, O
it O
takes O
21 O
epochs O
for O
the O
concatenated O
corpus O
to O
converge O
on O
the O
base O
LSTM O
model O
, O
while O
only O
12 O
epochs O
with O
interleaved B-MethodName
learning I-MethodName
, O
which O
is O
43 O
% O
shorter O
. O
When O
trained O
on O
the O
concatenated O
corpus O
in O
the O
order O
of O
OSDB B-DatasetName
→ O
Twitter B-DatasetName
→ O
Ubuntu B-DatasetName
, O
it O
takes O
20 O
epochs O
for O
the O
perplexity B-MetricName
on O
OSDB B-DatasetName
and O
Ubuntu B-DatasetName
to O
be O
balanced O
, O
while O
with O
interleaved B-MethodName
learning I-MethodName
, O
it O
takes O
less O
than O
one O
epoch O
. O
For O
concatenated O
corpus O
, O
the O
performance O
of O
the O
Ubuntu B-DatasetName
corpus O
is O
sacrificed O
in O
order O
to O
balance O
the O
performance O
of O
the O
two O
corpora O
, O
which O
results O
in O
worse O
overall O
performance O
. O
GPT-2 B-MethodName
PersonaChat B-DatasetName
( O
single O
) O
478.8 O
4.9 O
6.7 O
159.6 O
5.5 O
6.7 O
264.7 O
5.1 O
7.7 O
19.6 O
14.1 O
16.2 O
44.7 O
7 O
. O
Models O
of O
labeled O
, O
multi B-MethodName
- I-MethodName
task I-MethodName
labeled I-MethodName
and I-MethodName
weighted I-MethodName
learning I-MethodName
do O
not O
have O
the O
best O
hyper O
- O
parameters O
, O
but O
the O
same O
hyper O
- O
parameters O
as O
the O
base O
models O
. O
Their O
perplexity B-MetricName
is O
slightly O
worse O
than O
it O
should O
be O
. O

The O
results O
of O
the O
single O
corpus O
PersonaChat B-DatasetName
trained O
with O
the O
LSTM O
model O
confirm O
our O
concern O
on O
a O
small O
fine O
- O
tuning O
corpus O
. O
The O
LSTM O
model O
is O
pre O
- O
trained O
on O
OSDB B-DatasetName
, O
Twitter B-DatasetName
and O
Ubuntu B-DatasetName
; O
however O
, O
the O
performance O
for O
the O
3 O
corpora O
greatly O
decreases O
after O
fine O
- O
tuning O
. O

The O
automatic O
evaluation O
with O
stop O
words O
is O
not O
good O
for O
measuring O
relevance O
, O
since O
stop O
words O
are O
taken O
too O
much O
into O
account O
. O
See O
BLEU B-MetricName
and O
F1 B-MetricName
scores O
of O
PersonChat B-DatasetName
( O
single O
) O
and O
weighted B-MethodName
learning I-MethodName
as O
an O
example O
. O
Models O
trained O
on O
PersonaChat B-DatasetName
( O
single O
) O
can O
not O
answer O
Ubuntu B-DatasetName
technical O
questions O
at O
all O
, O
yet O
they O
receive O
better O
scores O
than O
weighted B-MethodName
learning I-MethodName
. O
But O
once O
the O
stop O
words O
are O
removed O
, O
the O
scores O
of O
weighted B-MethodName
learning I-MethodName
surplus O
PersonaChat B-DatasetName
( O
single O
) O
a O
lot O
. O

D O
Additional O
Results O
of O
automatic O
evaluation O
without O
stop O
words O

McQueen B-DatasetName
: O
a O
Benchmark O
for O
Multimodal B-TaskName
Conversational I-TaskName
Query I-TaskName
Rewrite I-TaskName

The O
task O
of O
query B-TaskName
rewrite I-TaskName
aims O
to O
convert O
an O
in O
- O
context O
query O
to O
its O
fully O
- O
specified O
version O
where O
ellipsis O
and O
coreference O
are O
completed O
and O
referred O
- O
back O
according O
to O
the O
history O
context O
. O
Although O
much O
progress O
has O
been O
made O
, O
less O
efforts O
have O
been O
paid O
to O
real O
scenario O
conversations O
that O
involve O
drawing O
information O
from O
more O
than O
one O
modalities O
. O
In O
this O
paper O
, O
we O
propose O
the O
task O
of O
multimodal B-TaskName
conversational I-TaskName
query I-TaskName
rewrite I-TaskName
( O
McQR B-TaskName
) O
, O
which O
performs O
query O
rewrite O
under O
the O
multimodal O
visual O
conversation O
setting O
. O
We O
collect O
a O
largescale O
dataset O
named O
McQueen B-DatasetName
based O
on O
manual O
annotation O
, O
which O
contains O
15k O
visual O
conversations O
and O
over O
80k O
queries O
where O
each O
one O
is O
associated O
with O
a O
fully O
- O
specified O
rewrite O
version O
. O
In O
addition O
, O
for O
entities O
appearing O
in O
the O
rewrite O
, O
we O
provide O
the O
corresponding O
image O
box O
annotation O
. O
We O
then O
use O
the O
McQueen B-DatasetName
dataset O
to O
benchmark O
a O
state O
- O
of O
- O
the O
- O
art O
method O
for O
effectively O
tackling O
the O
McQR B-TaskName
task O
, O
which O
is O
based O
on O
a O
multimodal O
pre O
- O
trained O
model O
with O
pointer O
generator O
. O
Extensive O
experiments O
are O
performed O
to O
demonstrate O
the O
effectiveness O
of O
our O
model O
on O
this O
task O
1 O
. O

Introduction O

Recent O
years O
have O
witnessed O
an O
increasing O
attention O
in O
conversational O
- O
related O
tasks O
, O
such O
as O
conversational O
question O
answering O
( O
Choi O
et O
al O
. O
, O
2018 O
; O
Reddy O
et O
al O
. O
, O
2019 O
) O
, O
visual O
conversation O
modeling O
( O
Das O
et O
al O
. O
, O
2017 O
) O
, O
etc O
. O
One O
main O
challenge O
in O
multi O
- O
turn O
conversation O
modeling O
is O
that O
information O
from O
context O
history O
is O
easy O
to O
be O
abbreviated O
or O
omitted O
in O
the O
follow O
- O
up O
queries O
, O
causing O
the O
so O
- O
called O
coreference O
and O
ellipsis O
. O
To O
address O
this O
concern O
, O
the O
task O
of O
query B-TaskName
rewrite I-TaskName
( O
Elgohary O
et O
al O
. O
, O
2019 O
; O
Pan O
et O
al O
. O
, O
2019 O
; O
Su O
et O
al O
. O
, O
2019 O
) O
aims O
to O
reconstruct O
the O
original O
query O
to O
a O
fully O
specified O
form O
based O
on O
its O
history O
context O
. O
The O
rewrite O
eliminates O
the O
coreference O
and O
ellipsis O
in O
the O
original O
query O
without O
changing O
its O
semantic O
information O
, O
thus O
helping O
turn O
the O
more O
challenging O
multi O
- O
turn O
conversation O
modeling O
problem O
to O
a O
single O
- O
turn O
version O
. O

Following O
this O
line O
, O
several O
attempts O
have O
been O
made O
in O
the O
query B-TaskName
rewrite I-TaskName
task O
which O
achieve O
decent O
performance O
on O
the O
natural O
language O
level O
. O
Nevertheless O
, O
conversations O
in O
real O
scenario O
tend O
to O
involve O
knowledge O
from O
more O
than O
one O
modalities O
, O
such O
as O
vision O
, O
text O
, O
speech O
, O
etc O
. O
Information O
from O
different O
modalities O
is O
not O
handled O
in O
isolation O
, O
but O
often O
integrated O
together O
to O
improve O
the O
quality O
of O
perception O
and O
understanding O
. O
For O
example O
, O
as O
shown O
in O
Figure O
1 O
, O
in O
the O
first O
turn O
of O
the O
visual O
conversation O
, O
with O
the O
lack O
of O
context O
, O
the O
user O
directly O
uses O
the O
pronoun O
" O
it O
" O
to O
refer O
to O
the O
dog O
in O
the O
image O
. O
In O
the O
third O
turn O
, O
for O
ellipsis O
that O
does O
not O
appear O
in O
the O
context O
history O
, O
in O
order O
to O
perform O
ellipsis O
completion O
, O
one O
also O
needs O
to O
find O
clues O
from O
the O
corresponding O
image O
. O
Rewriting O
the O
query O
under O
the O
circumstance O
where O
grounding O
outside O
the O
text O
information O
is O
needed O
poses O
more O
challenges O
to O
traditional O
query B-TaskName
rewrite I-TaskName
models O
that O
based O
only O
on O
textual O
features O
. O

In O
this O
paper O
, O
we O
propose O
the O
task O
of O
multimodal B-TaskName
conversational I-TaskName
query I-TaskName
rewrite I-TaskName
( O
McQR B-TaskName
) O
, O
which O
aims O
to O
perform O
query B-TaskName
rewrite I-TaskName
under O
the O
multimodal O
visual O
conversation O
setting O
. O
To O
achieve O
this O
goal O
, O
we O
collect O
a O
large O
- O
scale O
dataset O
called O
McQueen B-DatasetName
. O
Specifically O
, O
for O
each O
visual O
conversation O
consisting O
of O
an O
image O
and O
the O
corresponding O
history O
questionanswer O
context O
, O
we O
provide O
manual O
rewrite O
for O
the O
query O
, O
where O
the O
coreference O
resolution O
and O
ellipsis O
completion O
are O
performed O
respectively O
. O
Furthermore O
, O
in O
order O
to O
assist O
downstream O
tasks O
such O
as O
coreference O
entity O
detection O
, O
for O
all O
the O
entities O
appearing O
in O
the O
rewrite O
, O
we O
annotate O
the O
image O
boxes O
for O
representing O
their O
corresponding O
image O
area O
. O

We O
then O
use O
the O
McQueen B-DatasetName
dataset O
to O
benchmark O
a O
state O
- O
of O
- O
the O
- O
art O
method O
for O
effectively O
tackling O
the O
McQR B-TaskName
task O
. O
Inspired O
by O
the O
big O
success O
of O
pre O
- O
trained O
models O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
our O
model O
is O
based O
on O
a O
multimodal O
pretrained O
model O
where O
interactions O
between O
different O
modalities O
can O
be O
better O
captured O
. O
Furthermore O
, O
we O
enhance O
the O
model O
with O
a O
pointer O
generator O
specially O
designed O
for O
the O
multimodal O
Transformer O
blocks O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
so O
that O
the O
rewritten O
query O
is O
either O
generated O
from O
scratch O
or O
copied O
from O
certain O
contextual O
parts O
with O
high O
attention O
weights O
. O
Extensive O
experiments O
are O
conducted O
to O
compare O
our O
method O
with O
several O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
Our O
model O
outperforms O
all O
the O
methods O
in O
both O
the O
McQR B-TaskName
task O
and O
two O
subtasks O
. O
We O
further O
perform O
analysis O
to O
investigate O
the O
role O
of O
different O
modalities O
in O
this O
task O
and O
demonstrate O
that O
the O
introduction O
of O
image O
information O
provides O
extra O
guidance O
for O
the O
concerned O
query B-TaskName
rewrite I-TaskName
task O
. O

In O
summary O
, O
the O
contribution O
of O
our O
paper O
lies O
in O
three O
folds O
: O

• O
We O
formally O
define O
the O
task O
of O
multimodal B-TaskName
conversational I-TaskName
query I-TaskName
rewrite I-TaskName
( O
McQR B-TaskName
) O
, O
which O
aims O
to O
generate O
a O
fully O
- O
specified O
rewrite O
query O
based O
on O
both O
the O
context O
history O
and O
the O
visual O
image O
. O

• O
We O
propose O
a O
large O
- O
scale O
dataset O
McQueen B-DatasetName
, O
containing O
15k O
visual O
conversations O
and O
over O
80k O
rewrites O
. O
For O
the O
entities O
appearing O
in O
the O
rewrites O
, O
we O
also O
annotate O
the O
image O
boxes O
for O
representing O
their O
corresponding O
image O
area O
. O

• O
We O
benchmark O
a O
multimodal B-MethodName
Transformer I-MethodName
- I-MethodName
based I-MethodName
model I-MethodName
with I-MethodName
pointer I-MethodName
mechanism I-MethodName
for O
effectively O
tackling O
the O
McQR B-TaskName
task O
. O
Extensive O
analysis O
shows O
the O
role O
of O
different O
modalities O
in O
our O
model O
. O

Related O
Work O

Query B-TaskName
Rewrite I-TaskName

The O
task O
of O
query B-TaskName
rewrite I-TaskName
provides O
reconstructed O
queries O
based O
on O
abbreviated O
in O
- O
context O
queries O
without O
changing O
their O
semantic O
meaning O
. O
First O
introduced O
by O
( O
Elgohary O
et O
al O
. O
, O
2019 O
; O
Su O
et O
al O
. O
, O
2019 O
; O
Pan O
et O
al O
. O
, O
2019 O
) O
, O
most O
works O
formulate O
it O
as O
a O
standard O
generation O
task O
, O
which O
can O
be O
solved O
via O
a O
Sequence O
- O
to O
- O
Sequence O
model O
( O
Quan O
et O
al O
. O
, O
2019 O
; O
. O
Some O
attempts O
have O
been O
made O
to O
introduce O
a O
multitask O
learning O
setup O
in O
order O
to O
enhance O
the O
training O
process O
( O
Rastogi O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O
Some O
works O
seek O
to O
focus O
on O
query B-TaskName
rewrite I-TaskName
under O
the O
low O
- O
resource O
scenario O
Voskarides O
et O
al O
. O
, O
2020 O
; O
. O

For O
modeling O
the O
linguistic O
knowledge O
in O
conversational O
context O
more O
effectively O
, O
prior O
knowledge O
is O
leveraged O
such O
as O
using O
semantic O
role O
labeling O
to O
provide O
extra O
guidance O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
, O
and O
reducing O
the O
generation O
search O
space O
via O
sequence O
tagging O
. O
Although O
these O
works O
have O
achieved O
great O
performance O
on O
their O
corresponding O
task O
, O
query B-TaskName
rewrite I-TaskName
under O
the O
multimodal O
setting O
has O
not O
been O
explored O
. O

Visual B-TaskName
Coreference I-TaskName
Resolution I-TaskName

Visual O
dialog O
entails O
answering O
a O
set O
of O
questions O
grounded O
by O
an O
image O
( O
Das O
et O
al O
. O
, O
2017 O
) O
. O
Based O
on O
that O
, O
visual B-TaskName
coreference I-TaskName
resolution I-TaskName
involves O
linking O
the O
words O
in O
the O
text O
( O
usually O
nouns O
and O
pronouns O
) O
to O
a O
certain O
area O
in O
the O
image O
( O
Kong O
et O
al O
. O
, O
2014 O
; O
Kottur O
et O
al O
. O
, O
2018 O
) O
. O
Following O
this O
line O
, O
Li O
et O
al O
. O
( O
2021 O
) O
restrict O
coreference O
resolution O
to O
pronouns O
and O
resolve O
coreferences O
in O
visual O
dialog O
in O
an O
unsupervised O
way O
. O
Yu O
et O
al O
. O
( O
2019 O
) O

The O
McQueen B-DatasetName
Dataset O

Dataset O
Overview O

Our O
dataset O
is O
based O
on O
a O
visual O
dialog O
dataset O
called O
VisDial B-DatasetName
( O
Das O
et O
al O
. O
, O
2017 O
) O
. O
The O
original O
VisDial B-DatasetName
dataset O
consists O
of O
over O
133k O
dialogs O
, O
each O
associated O
with O
an O
image O
and O
10 O
rounds O
of O
questionanswer O
pairs O
. O
All O
question O
- O
answer O
pairs O
are O
conducted O
in O
a O
conversational O
format O
and O
revolve O
around O
the O
content O
of O
the O
picture O
. O

Our O
dataset O
randomly O
selects O
15k O
conversations O
from O
the O
VisDial B-DatasetName
dataset O
with O
the O
total O
of O
over O
80k O
rewrite O
utterances O
. O
For O
each O
query O
in O
a O
visual O
conversation O
, O
we O
conduct O
manual O
annotation O
to O
resolve O
the O
information O
omission O
. O
The O
query O
is O
reconstructed O
based O
on O
the O
image O
as O
well O
as O
the O
history O
context O
so O
that O
the O
coreference O
and O
ellipsis O
are O
referred O
- O
back O
or O
completed O
. O
For O
negative O
queries O
that O
do O
not O
contain O
any O
information O
omission O
, O
the O
rewrite O
stays O
the O
same O
as O
the O
original O
query O
. O
In O
addition O
, O
for O
all O
the O
entities O
appearing O
in O
the O
coreference O
and O
ellipsis O
, O
we O
annotate O
the O
image O
boxes O
for O
representing O
their O
corresponding O
image O
areas O
. O

Dataset O
Construction O

Text O
Rewrite O
Annotation O

For O
manual O
annotation O
, O
we O
hire O
16 O
annotators O
in O
total O
. O
Before O
the O
annotation O
starts O
, O
we O
provide O
100 O
examples O
for O
all O
the O
annotators O
to O
refer O
to O
. O
We O
also O
provide O
a O
guideline O
and O
some O
tutorials O
by O
listing O
some O
typical O
coreference O
and O
ellipsis O
cases O
so O
that O
the O
bias O
and O
language O
style O
shift O
between O
individuals O
are O
minimized O
as O
possible O
. O
After O
that O
, O
the O
annotators O
start O
working O
on O
a O
small O
portion O
of O
data O
where O
query O
rewrite O
is O
performed O
. O
After O
all O
the O
results O
are O
returned O
and O
the O
data O
quality O
is O
checked O
, O
the O
main O
annotation O
phase O
begins O
and O
the O
rest O
of O
data O
is O
labeled O
. O
On O
average O
, O
each O
annotator O
is O
in O
charge O
with O
the O
rewrite O
of O
5059 O
queries O
. O
The O
rewrite O
annotation O
interface O
can O
be O
seen O
in O
Appendix O
A O
. O

Image O
Box O
Annotation O

Besides O
the O
rewrite O
annotation O
, O
we O
also O
provide O
image O
annotation O
to O
assist O
downstream O
or O
related O
tasks O
( O
e.g. O
coreference O
entity O
detection O
) O
. O
The O
image O
box O
annotation O
begins O
right O
after O
the O
rewrite O
annotation O
. O
The O
overall O
procedure O
also O
follows O
the O
( O
1 O
) O
tutorial O
( O
2 O
) O
trial O
phase O
( O
3 O
) O
main O
phase O
pipeline O
. O
Specifically O
, O
the O
annotators O
have O
to O
extract O
the O
entities O
in O
the O
ellipsis O
and O
coreference O
part O
and O
draw O
the O
bounding O
boxes O
of O
them O
in O
the O
image O
. O
Each O
annotator O
is O
in O
charge O
of O
the O
image O
annotation O
of O
the O
rewrites O
written O
by O
him O
/ O
herself O
. O
The O
image O
annotation O
interface O
can O
be O
seen O
in O
Appendix O
B O
. O

Quality O
Control O

After O
the O
all O
the O
annotation O
is O
finished O
, O
we O
re O
- O
group O
and O
shuffle O
the O
annotators O
to O
perform O
cross O
quality O
inspection O
. O
Each O
group O
is O
asked O
to O
check O
the O
annotation O
results O
of O
other O
groups O
. O
In O
addition O
, O
two O
new O
annotators O
who O
do O
not O
take O
part O
in O
the O
annotation O
phase O
are O
recruited O
to O
check O
the O
quality O
of O
all O
the O
annotation O
results O
. O
The O
annotators O
have O
to O
answer O
three O
questions O
for O
each O
query O
: O
( O
1 O
) O
Is O
the O
rewrite O
result O
correct O
or O
not O
? O
( O
2 O
) O
Are O
all O
the O
coreference O
and O
ellipsis O
resolved O
in O
the O
rewrite O
? O
( O
3 O
) O
Are O
the O
entities O
in O
the O
coreference O
and O
ellipsis O
correctly O
annotated O
in O
the O
image O
? O
All O
the O
conversation O
rewrites O
must O
get O
the O
all O
" O
yes O
" O
result O
from O
all O
the O
annotators O
before O
official O
acceptance O
, O
otherwise O
they O
are O
collected O
to O
be O
revised O
and O
re O
- O
checked O
( O
the O
questionnaire O
interface O
is O
shown O
in O
Appendix O
C O
) O
. O
The O
whole O
check O
- O
revise O
process O
lasts O
for O
three O
iterations O
. O
Considering O
chance O
agreement O
, O
we O
measured O
the O
Inter O
- O
Annotator O
Agreement O
( O
IAA O
) O
in O
terms O
of O
Cohen O
's O
κ O
( O
Cohen O
, O
1960 O
) O
. O
The O
final O
κ O
score O
is O
0.82 O
, O
reaching O
the O
" O
almost O
perfect O
" O
level O
2 O
. O
Besides O
, O
after O
each O
quality O
check O
iteration O
, O
we O
randomly O
sample O
100 O
conversations O
from O
the O
dataset O
and O
manually O
evaluate O
the O
utterance O
- O
level O
precision O
and O
recall O
rate O
, O
where O
precision O
denotes O
the O
rate O
of O
the O
retrieved O
rewrites O
being O
correct O
, O
while O
the O
recall O
rate O
records O
the O
portion O
that O
the O
coreference O
and O
ellipsis O
being O
handled O
. O
The O
precision B-MetricName
and O
recall B-MetricName
rate O
in O
the O
1st O
/ O
2nd O
/ O
3rd O
iteration O
are O
( O
89.0 B-MetricValue
% I-MetricValue
, O
87.1 B-MetricValue
% I-MetricValue
) O
/ O
( O
95.5 B-MetricValue
% I-MetricValue
, O
94.2 B-MetricValue
% I-MetricValue
) O
/ O
( O
98.3 B-MetricValue
% I-MetricValue
, O
98.2 B-MetricValue
% I-MetricValue
) O
, O
respectively O
. O

Annotation O
Cost O
and O
Duration O

The O
overall O
phase O
including O
the O
annotation O
and O
quality O
check O
spanned O
for O
10 O
weeks O
( O
from O
March O
to O
May O
2022 O
) O
, O
where O
the O
annotation O
guidance O
lasts O
for O
2 O
weeks O
, O
data O
annotation O
lasts O
for O
5 O
weeks O
, O
quality O
check O
lasts O
for O
3 O
weeks O
. O
All O
the O
annotators O
are O
English O
native O
speakers O
recruited O
from O
a O
professional O
data O
management O
company O
Appen O
3 O
. O
The O
annotation O
costs O
$ O
5942 O
US O
dollars O
in O
total O
, O
with O
$ O
0.31 O
per O
utterance O
rewrite O
, O
$ O
0.03 O
per O
image O
box O
annotation O
. O
occurs O
in O
the O
query O
. O
Table O
3 O
lists O
the O
statistics O
of O
our O
dataset O
, O
where O
each O
visual O
conversation O
contains O
5.40 O
Q O
- O
A O
turns O
with O
2.02 O
image O
boxes O
on O
average O
. O
Table O
4 O
lists O
the O
number O
of O
rewrites O
in O
our O
dataset O
under O
different O
history O
context O
lengths O
, O
where O
most O
of O
the O
data O
contains O
context O
from O
3 O
- O
4 O
turns O
, O
and O
over O
16k O
data O
contains O
context O
over O
9 O
turns O
. O
Futhermore O
, O
we O
compare O
our O
dataset O
with O
existing O
datasets O
from O
related O
works O
. O
According O
to O
Table O
1 O
, O
our O
dataset O
is O
: O
( O
1 O
) O
more O
complete O
-we O
provide O
manual O
annotation O
on O
both O
image O
and O
text O
levels O
, O
where O
the O
rewrite O
query O
and O
entity O
boxes O
are O
both O
provided O
; O
( O
2 O
) O
more O
diverse O
-since O
our O
dataset O
is O
designed O
not O
only O
for O
all O
the O
coreference O
cases O
, O
but O
also O
performs O
ellipsis O
completion O
in O
the O
rewrite O
; O

Dataset O
Statistics O

According O
to O

( O
3 O
) O
much O
larger O
-compared O
with O
existing O
VCR O
dataset O
, O
the O
dataset O
has O
a O
larger O
scale O
. O
( O
1 O
) O

Methods O

Model O
Overview O

Figure O
2 O
depicts O
the O
overall O
structure O
of O
our O
proposed O
model O
. O
Our O
model O
is O
based O
on O
a O
multimodal O
pre O
- O
trained O
model O
VL B-MethodName
- I-MethodName
T5 I-MethodName
( O
Cho O
et O
al O
. O
, O
2021 O
) O
, O
where O
a O
Transformer O
- O
based O
encoder O
and O
decoder O
are O
jointly O
trained O
to O
perform O
generation O
. O
Besides O
, O
we O
add O
a O
pointer O
generator O
to O
the O
multimodal O
Transformer O
decoder O
so O
that O
the O
rewrite O
is O
either O
generated O
from O
scratch O
or O
copied O
from O
history O
context O
. O

Image O
and O
Text O
Embeddings O
Text O
Embedding O

The O
text O
input O
of O
our O
model O
includes O
two O
parts O
: O
the O
task O
prefix O
and O
the O
history O
context O
. O
The O
task O
prefix O
is O
a O
short O
prompt O
which O
aims O
to O
differentiate O
the O
concerned O
McQR B-TaskName
task O
from O
other O
tasks O
in O
the O
pre O
- O
training O
phase O
( O
e.g. O
visual O
grounding O
) O
. O
Specifically O
, O
we O
use O
" O
query B-TaskName
rewrite I-TaskName
: O
" O
as O
the O
task O
prefix O
. O
Following O
it O
, O
the O
history O
context O
contains O
all O
the O
utterances O
including O
all O
the O
queries O
and O
answers O
in O
previous O
turns O
. O
The O
input O
text O
is O
then O
tokenized O
and O
embedded O
before O
passed O
into O
the O
encoder O
. O
Following O
the O
setting O
in O
T5 O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
we O
also O
add O
a O
relative O
position O
bias O
to O
each O
self O
- O
attention O
layer O
. O
As O
a O
result O
, O
the O
text O
input O
is O
represented O
as O
e O
t O
. O

Image O
Embedding O
To O
extract O
image O
features O
, O
we O
first O
detect O
several O
object O
regions O
from O
the O
image O
, O
denoted O
as O
Region O
of O
Interest O
( O
ROI O
) O
. O
Following O
previous O
works O
( O
Cho O
et O
al O
. O
, O
2021 O
; O
Lu O
et O
al O
. O
, O
2019 O
) O
, O
we O
also O
use O
Faster O
R- O
CNN O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
pretrained O
on O
the O
Visual O
Genome O
dataset O
( O
Krishna O
et O
al O
. O
, O
2016 O
) O
to O
extract O
ROI O
features O
in O
our O
task O
. O
For O
each O
image O
I O
, O
we O
extract O
n O
= O
36 O
ROIs O
from O
it O
. O

The O
final O
visual O
vector O
can O
be O
represented O
as O
e O
v O
. O

Encoder O
- O
Decoder O
Structure O

Encoder O
. O
We O
use O
a O
multimodal O
Transformer O
encoder O
- O
decoder O
structure O
to O
encode O
the O
image O
text O
features O
and O
generate O
the O
target O
rewrite O
. O
The O
multimodal O
encoder O
is O
a O
stack O
of O
L O
Transformers O
which O
take O
the O
image O
and O
text O
representations O
as O
input O

h O
0 O
= O
[ O
e O
t O
1 O
, O
.. O
, O
e O
t O
m O
, O
e O
v O
1 O
, O
... O
, O
e O
v O
n O
] O
( O
2 O
) O

h O
i O
= O
F O
N O
N O
( O
M O
ultiHead O
( O
h O
i−1 O
, O
h O
i−1 O
, O
h O
i−1 O
) O
) O
( O
3 O
) O
Decoder O
. O

Similarly O
, O
the O
decoder O
is O
also O
a O
stack O
of O
L O
Transformers O
. O
Given O
the O
decoder O
input O
x O
t O
, O
the O
decoding O
step O
consists O
three O
phases O
, O
where O
the O
first O
sub O
- O
layer O
is O
a O
self O
- O
attention O
layer O
which O
can O
be O
represented O
as O

d O
i O
= O
M O
ultiHead O
( O
d O
i−1 O
, O
d O
i−1 O
, O
d O
i−1 O
) O
( O
4 O
) O

where O
d O
0 O
= O
x O
t O
. O
After O
that O
, O
the O
second O
sub O
- O
layer O
calculates O
the O
cross O
attention O
between O
the O
encoder O
outputs O
and O
the O
decoder O
self O
- O
attention O
result O

s O
i O
= O
M O
ultiHead O
( O
d O
i O
, O
h O
L O
, O
h O
L O
) O
( O
5 O
) O

The O
third O
sub O
- O
layer O
is O
a O
position O
- O
wise O
fully O
connected O
feed O
- O
forward O
neural O
network O
, O
followed O
by O
a O
softmax O
layer O
to O
output O
the O
final O
probability O

P O
vocab O
( O
y O
t O
) O
= O
i O
: O
w O
i O
∈V O
β O
t O
, O
i O
= O
Sof O
tmax O
( O
F O
N O
N O
( O
s O
i O
) O
) O
( O
6 O
) O

where O
β O
is O
the O
softmax O
score O
over O
the O
whole O
vocabulary O
V O
. O
Multimodal B-MethodName
Transformer I-MethodName
with I-MethodName
Pointer I-MethodName
Generator I-MethodName
. O
Additionally O
, O
following O
the O
motivation O
that O
most O
part O
of O
the O
rewrite O
sentence O
can O
be O
related O
to O
certain O
parts O
of O
the O
input O
context O
, O
we O
add O
a O
pointer O
generator O
( O
See O
et O
al O
. O
, O
2017 O
) O
to O
the O
multimodal O
Transformer O
such O
that O
the O
rewrite O
is O
either O
generated O
from O
scratch O
or O
directly O
copied O
from O
the O
input O
. O
Specifically O
, O
the O
cross O
attention O
in O
the O
last O
decoder O
layer O
can O
be O
naturally O
taken O
as O
the O
context O
vector O

c O
i O
= O
M O
ultiHead O
( O
d O
i O
, O
h O
U O
L O
, O
h O
U O
L O
) O
( O
7 O
) O

where O
h O
U O
L O
is O
the O
textual O
part O
of O
the O
encoding O
result O
, O
which O
is O
the O
embedding O
of O
tokens O
before O
the O
" O
[ O
SEP O
] O
" O
token O
. O

α O
t O
, O
i O
= O
sof O
tmax O
( O
( O
W O
s O
s O
t O
) O
T O
W O
h O
h O
i O
√ O
d O
) O
( O
8 O

P O
copy O
( O
y O
t O
) O
= O
i O
: O
w O
i O
∈H O
α O
t O
, O
i O
( O
9 O
) O

where O
α O
is O
the O
copy O
distribution O
over O
the O
input O
H O
, O
P O
copy O
determines O
where O
to O
copy O
at O
time O
step O
t. O
By O
incorporating O
the O
pointer O
generator O
, O
the O
final O
probability O
of O
generating O
the O
target O
word O
y O
t O
at O
time O
step O
t O
is O
represented O
as O

P O
( O
y O
t O
) O
= O
λP O
vocab O
( O
y O
t O
) O
+ O
( O
1 O
− O
λ O
) O
P O
copy O
( O
y O
t O
) O
( O
10 O
) O
λ O
= O
sigmoid O
( O
w O
T O
d O
c O
t O
+ O
w O
T O
l O
s O
t O
+ O
w O
T O
a O
x O
t O
) O
( O
11 O
) O

where O
w O
d O
, O
w O
l O
, O
w O
a O
are O
the O
weights O
to O
learn O
. O

Training O

We O
adopt O
the O
standard O
generation O
loss O
when O
finetuning O
the O
pre O
- O
trained O
model O
parameterized O
θ O
on O
our O
McQR B-TaskName
task O
. O
At O
each O
time O
step O
t O
, O
the O
decoder O
output O
token O
is O
determined O
based O
on O
the O
generated O
rewrites O
of O
previous O
time O
steps O
denoted O
as O
y O
< O
t O
, O
the O
input O
image O
and O
text O
encoding O
vector O
e O
v O
and O
e O
t O
. O
We O
minimize O
the O
negative O
log O
- O
likelihood O
of O
generating O

5 O
Experiments O

Compared O
Methods O

We O
compare O
our O
methods O
with O
several O
baselines O
. O

• O
Original B-MethodName
( O
Elgohary O
et O
al O
. O
, O
2019 O
) O
is O
the O
method O
where O
the O
rewrite O
is O
set O
to O
be O
the O
same O
as O
the O
input O
query O
. O

• O
AllenNLP B-MethodName
Coref I-MethodName
( O
Gardner O
et O
al O
. O
, O
2018 O
) O
is O
a O
deep O
learning O
based O
NLP O
tool O
. O
We O
utilize O
its O
coreference O
resolution O
model O
to O
generate O
the O
rewrite O
. O

• O
( B-MethodName
L I-MethodName
/ I-MethodName
T I-MethodName
) I-MethodName
-Gen I-MethodName
( O
Su O
et O
al O
. O
, O
2019 O
) O
denotes O
the O
LSTM O
/ O
Transformer O
based O
encoder O
- O
decoder O
generation O
model O
. O
For O
Transformer O
- O
based O
models O
, O
we O
report O
the O
performance O
of O
two O
variants O
: O
Early O
Fusion O
which O
utilizes O
the O
same O
encoder O
to O
encode O
image O
and O
text O
features O
, O
and O
Late O
Fusion O
which O
first O
embeds O
image O
and O
text O
into O
vector O
spaces O
separately O
and O
then O
performs O
fusion O
into O
a O
joint O
embedding O
. O

• O
( B-MethodName
L I-MethodName
/ I-MethodName
T I-MethodName
) I-MethodName
-Ptr I-MethodName
( O
See O
et O
al O
. O
, O
2017 O
) O
adds O
a O
pointer O
generator O
to O
the O
( B-MethodName
L I-MethodName
/ I-MethodName
T I-MethodName
) I-MethodName
-Gen I-MethodName
model O
. O

• O
VL- B-MethodName
( I-MethodName
Bart I-MethodName
/ I-MethodName
T5 I-MethodName
) I-MethodName
( O
Cho O
et O
al O
. O
, O
2021 O
) O
is O
the O
mulitimodal O
implementation O
of O
large O
pre O
- O
trained O
language O
model O
Bart B-MethodName
/ I-MethodName
T5 I-MethodName
. O

• O
VL- B-MethodName
( I-MethodName
Bart I-MethodName
/ I-MethodName
T5 I-MethodName
) I-MethodName
-Ptr I-MethodName
is O
the O
model O
depicted O
in O
Section O
4.2 O
that O
adds O
a O
pointer O
generator O
to O
the O
pre O
- O
trained O
model O
. O

Experimental O
Settings O

Our O
code O
is O
implemented O
based O
on O
Pytorch O
and O
Huggingface O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
. O
We O
use O
the O
base O
version O
of O
the O
pre O
- O
trained O
VL B-MethodName
- I-MethodName
Bart I-MethodName
/ I-MethodName
T5 I-MethodName
in O
all O
our O
experiments O
. O
Our O
dataset O
is O
split O
into O
the O
training O
, O
testing O
, O
and O
validation O
dataset O
following O
the O
portion O
of O
60 B-HyperparameterValue
% I-HyperparameterValue
, O
20 B-HyperparameterValue
% I-HyperparameterValue
, O
20 B-HyperparameterValue
% I-HyperparameterValue
, O
resulting O
in O
48566 O
queries O
for O
the O
training O
set O
, O
16189 O
queries O
for O
the O
testing O
and O
validation O
set O
. O
By O
default O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
as O
32 B-HyperparameterValue
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
5e-5 B-HyperparameterValue
, O
the O
model O
is O
fine O
- O
tuned O
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
with O
the O
random B-HyperparameterName
seed I-HyperparameterName
of O
42 B-HyperparameterValue
. O
In O
the O
testing O
stage O
, O
all O
models O
decode O
words O
by O
beam O
search O
with O
beam B-HyperparameterName
size I-HyperparameterName
set O
to O
5 B-HyperparameterValue
. O
We O
employ O
several O
evaluation O
metrics O
to O
evaluate O
the O
quality O
of O
the O
generated O
rewrite O
. O
We O
first O
report O
the O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
, O
and O
METEOR B-MetricName
( O
Denkowski O
and O
Lavie O
, O
2014 O
) O
rate O
. O
In O
addition O
, O
we O
report O
the O
Exact B-MetricName
Match I-MetricName
( O
EM B-MetricName
) O
of O
both O
positive O
samples O
that O
involves O
some O
changes O
in O
the O
rewrite O
and O
negative O
samples O
that O
the O
rewrite O
is O
the O
same O
as O
the O
query O
. O
On O
two O
subtasks O
, O
we O
report O
the O
precision B-MetricName
, O
recall B-MetricName
, O
and O
F1 B-MetricName
score I-MetricName
. O

Experiment O
Results O

We O
first O
report O
the O
overall O
performance O
on O
the O
query B-TaskName
rewrite I-TaskName
task O
. O
Then O
we O
perform O
experiments O
on O
two O
subtasks O
including O
coreference B-TaskName
resolution I-TaskName
and O
ellipsis B-TaskName
completion I-TaskName
, O
respectively O
. O

Table O
5 O
demonstrates O
the O
overall O
performance O
of O
different O
models O
on O
the O
McQR B-TaskName
task O
. O
We O
have O
the O
following O
observations O
: O
first O
of O
all O
, O
compared O
with O
LSTM O
based O
models O
( O
e.g. O
L B-MethodName
- I-MethodName
Gen I-MethodName
) O
, O
Transformer O
based O
models O
have O
a O
stronger O
generation O
ability O
( O
e.g. O
T- B-MethodName
( I-MethodName
E I-MethodName
) I-MethodName
Gen I-MethodName
) O
, O
where O
the O
BLEU-2 B-MetricName
score O
increases O
from O
77.14 B-MetricValue
to O
78.03 B-MetricValue
. O
In O
addition O
, O
with O
the O
help O
of O
the O
pointer O
generator O
, O
certain O
parts O
from O
the O
original O
query O
and O
the O
history O
context O
are O
able O
to O
be O
preserved O
in O
the O
rewrite O
, O
where O
the O
negative O
EM B-MetricName
Table O
6 O
shows O
the O
results O
of O
different O
models O
on O
the O
visual B-TaskName
coreference I-TaskName
resolution I-TaskName
( O
VCR B-TaskName
) O
subtask O
. O
We O
further O
utilize O
one O
state O
- O
of O
- O
the O
- O
art O
VCR B-TaskName
method O
named O
MBERT B-MethodName
to O
compare O
( O
Yu O
et O
al O
. O
, O
2022 O
) O
. O
According O
to O
the O
table O
, O
traditional O
text O
- O
only O
coreference O
resolution O
methods O
such O
as O
AllenNLP B-MethodName
Coref I-MethodName
may O
not O
have O
a O
good O
performance O
in O
the O
VCR B-TaskName
task O
. O
The O
reason O
is O
that O
, O
in O
the O
conversations O
, O
many O
pronouns O
refer O
to O
entities O
that O
never O
appear O
in O
the O
history O
context O
, O
but O
can O
be O
found O
clues O
in O
the O
image O
. O
Furthermore O
, O
it O
can O
be O
concluded O
that O
pointer O
network O
also O
has O
a O
positive O
influence O
on O
the O
results O
since O
many O
coreferred O
entities O
can O
be O
directly O
copied O
from O
previous O
turns O
. O

Besides O
VCR B-TaskName
, O
we O
also O
perform O
tests O
on O
the O
visual B-TaskName
ellipsis I-TaskName
completion I-TaskName
task I-TaskName
, O
as O
shown O
in O
Table O
7 O
. O
The O
overall O
condition O
is O
similar O
to O
results O
in O
Table O
6 O
, O
while O
there O
encounters O
a O
performance O
degradation O
in O
this O
task O
. O
This O
may O
result O
from O
the O
fact O
that O
recovering O
the O
omitted O
information O
may O
not O
be O
that O
apparent O
( O
we O
will O
use O
some O
examples O
to O
illustrate O
in O
Section O
6.3 O
) O
. O
In O
conclusion O
, O
our O
model O
shows O
superior O
performance O
on O
both O
two O
tasks O
, O
demonstrating O
the O
effectiveness O
of O
our O
model O
in O
resolving O
the O
information O
omission O
of O
the O
conversations O
. O

6 O
Extensive O
Analysis O

Image O
Role O
Analysis O

In O
order O
to O
further O
investigate O
the O
role O
of O
image O
information O
in O
the O
query O
rewrite O
task O
, O
we O
remove O
the O
image O
features O
and O
compare O
the O
performance O
with O
the O
original O
model O
under O
different O
length O
of O
context O
respectively O
. O
Specifically O
, O
we O
divide O
the O
overall O
16189 O
testing O
data O
into O
five O
categories O
by O
the O
history O
context O
length O
, O
ending O
up O
with O
3136 O
/ O
3380 O
/ O
3180 O
/ O
3185 O
/ O
3308 O
data O
records O
with O
the O
context O
turn B-HyperparameterName
length I-HyperparameterName
from O
0 B-HyperparameterValue
- I-HyperparameterValue
1 I-HyperparameterValue
/ O
2 B-HyperparameterValue
- I-HyperparameterValue
3 I-HyperparameterValue
/ O
4 B-HyperparameterValue
- I-HyperparameterValue
5 I-HyperparameterValue
/ O
6 B-HyperparameterValue
- I-HyperparameterValue
7 I-HyperparameterValue
/ O
8 B-HyperparameterValue
+ I-HyperparameterValue
, O
as O
shown O
in O
Figure O
3 O
. O
According O
to O
the O
figure O
, O
the O
performance O
gets O
improved O
with O
the O
help O
of O
image O
information O
. O
Compared O
with O
Transformer B-MethodName
generator I-MethodName
, O
our O
model O
is O
less O
sensitive O
to O
the O
turn B-HyperparameterName
length I-HyperparameterName
, O
where O
the O
model O
still O
has O
a O
decent O
performance O
when O
the O
conversation O
goes O
deep O
. O
We O
can O
also O
observe O
that O
, O
in O
our O
model O
, O
when O
there O
is O
no O
or O
short O
textual O
context O
that O
the O
information O
in O
conversation O
history O
is O
limited O
, O
the O
gap O
between O
monomodal O
and O
multimodal O
performance O
reaches O
to O
the O
largest O
, O
showing O
that O
the O
model O
is O
more O
dependant O
to O
the O
image O
information O
in O
this O
case O
. O
While O
when O
the O
conversation O
is O
deep O
, O
the O
gap O
decreases O
, O
showing O
that O
the O
model O
learns O
to O
copy O
information O
from O
the O
rich O
history O
context O
when O
rewriting O
the O
query O
. O

Multimodal O
Attention O
Visualization O

We O
utilize O
the O
self O
- O
attention O
weight O
heat O
map O
in O
the O
Transformer O
block O
of O
our O
model O
to O
visualize O
how O
the O
model O
learns O
the O
cross O
- O
modal O
relationship O
of O
the O
conversation O
. O
In O
the O
example O
shown O
in O
Figure O
5 O
( O
the O
full O
conversation O
information O
can O
be O
found O
in O
Appendix O
D O
) O
, O
the O
first O
three O
image O
segments O
correspond O
to O
ROI O
" O
man O
" O
, O
" O
boy O
" O
, O
" O
shorts O
" O
, O
respectively O
. O
According O
to O
the O
map O
, O
the O
pronoun O
" O
they O
" O
is O
correctly O
related O
to O
the O
" O
man O
" O
and O
" O
boy O
" O
ROI O
segments O
and O
also O
has O
a O
high O
attention O
weight O
to O
the O
text O
" O
the O
man O
" O
in O
the O
history O
context O
4 O
. O
The O
learned O
relationship O
between O
visual O
and O
text O
representations O
can O
serve O
as O
the O
reason O
that O
our O
model O
shows O
superior O
performance O
concerning O
the O
multimodal O
feature O
incorporation O
in O
the O
McQR B-TaskName
task O
. O

Case O
Study O

We O
provide O
several O
example O
rewrites O
generated O
by O
different O
methods O
to O
vividly O
demonstrate O
the O
quality O
of O
rewrites O
, O
according O
to O
Figure O
4 O
. O
In O
the O
first O
case O
where O
the O
query O
is O
the O
first O
utterance O
where O
no O
textual O
context O
is O
provided O
, O
non O
pre O
- O
trained O
models O
( O
e.g. O
T B-MethodName
( I-MethodName
L I-MethodName
) I-MethodName
-Ptr I-MethodName
) O
have O
difficulty O
resolving O
the O
pronoun O
" O
he O
" O
in O
the O
query O
. O
The O
results O
imply O
that O
with O
the O
help O
of O
prior O
knowledge O
, O
multimodal O
pretrained O
models O
have O
a O
superior O
ability O
in O
incorporating O
image O
and O
text O
features O
, O
mitigating O
the O
gap O
between O
representations O
of O
multimodal O
heterogeneous O
space O
. O
The O
second O
case O
shows O
that O
traditional O
Transformer B-MethodName
pointer I-MethodName
network I-MethodName
tends O
to O
copy O
incorrect O
context O
segments O
whose O
rewrite O
results O
may O
not O
make O
sense O
to O
human O
readers O
. O
For O
example O
, O
the O
abbreviated O
entity O
after O
" O
except O
for O
" O
should O
apparently O
be O
" O
the O
bear O
" O
instead O
of O
" O
the O
people O
" O
, O
while O
our O
model O
has O
a O
better O
ability O
of O
understanding O
the O
whole O
context O
and O
outputing O
more O
accurate O
results O
. O
Furthermore O
, O
when O
it O
comes O
to O
complicated O
cases O
where O
coreference O
and O
ellipsis O
are O
both O
observed O
, O
our O
model O
is O
both O
capable O
of O
copying O
the O
entities O
from O
history O
to O
solve O
the O
coreference O
( O
e.g. O
the O
red O
part O
in O
the O
third O
case O
) O
and O
generate O
parts O
that O
require O
reasoning O
( O
e.g. O
the O
blue O
part O
in O
the O
third O
case O
) O
from O
scratch O
. O

Error O
Case O
Analysis O

As O
shown O
in O
Figure O
6 O
, O
we O
demonstrate O
the O
most O
common O
error O
cases O
of O
our O
model O
, O
which O
can O
be O
classified O
into O
several O
cases O
including O
bad O
sentence O
structure O
, O
wrong O
coreference O
result O
, O
inaccurate O
omission O
completion O
, O
etc O
. O
According O
to O
the O
figure O
, O
we O
can O
see O
that O
pre O
- O
trained O
language O
models O
can O
generate O
sentences O
that O
have O
a O
proper O
format O
, O
but O
still O
have O
some O
problems O
in O
understanding O
the O
deep O
semantic O
correlation O
in O
the O
visual O
conversations O
. O
For O
example O
, O
in O
the O
top O
two O
cases O
, O
although O
the O
coreference O
seems O
simple O
for O
human O
readers O
, O
it O
poses O
challenges O
for O
machines O
other O
than O
directly O
making O
a O
copy O
from O
context O
. O
Specifically O
, O
for O
com- O
Original O
Query O
: O
Anything O
else O
to O
note O
? O
GT O
: O
Is O
there O
anything O
else O
to O
note O
except O
for O
the O
train O
, O
the O
track O
, O
the O
grass O
, O
the O
water O
, O
and O
the O
mountain O
? O
VLBart B-MethodName
: O
is O
there O
anything O
else O
to O
note O
except O
for O
the O
train O
, O
the O
water O
, O
the O
mountains O
, O
VLBart B-MethodName
- I-MethodName
p I-MethodName
: O
Is O
there O
anything O
else O
to O
note O
except O
for O
the O
train O
, O
the O
tracks O
, O
the O
water O
, O
the O
water O
? O
Original O
Query O
: O
do O
the O
people O
in O
the O
group O
appear O
related O
to O
each O
other O
in O
some O
way O
or O
are O
they O
just O
a O
random O
group O
of O
people O
crossing O
the O
road O
at O
the O
same O
time O
GT O
: O
do O
the O
people O
in O
the O
group O
appear O
related O
to O
each O
other O
in O
some O
way O
or O
are O
they O
just O
a O
random O
group O
of O
people O
crossing O
the O
road O
at O
the O
same O
time O
VLBart B-MethodName
: O
do O
the O
people O
in O
the O
group O
appear O
related O
to O
each O
other O
VLBart B-MethodName
- I-MethodName
p I-MethodName
: O
do O
the O
people O
in O
the O
group O
appear O
related O
to O
each O
other O
in O
some O
way O
plicated O
cases O
such O
as O
when O
coreference O
and O
ellipsis O
appear O
at O
the O
same O
time O
, O
when O
the O
conversation O
is O
long O
, O
and O
when O
the O
omitted O
information O
requires O
reasoning O
between O
history O
entities O
( O
e.g. O
the O
bottom O
cases O
) O
, O
there O
still O
remains O
much O
space O
to O
explore O
. O

Conclusion O
and O
Future O
Work O

We O
propose O
the O
task O
of O
multimodal B-TaskName
conversational I-TaskName
query I-TaskName
rewrite I-TaskName
( O
McQR B-TaskName
) O
, O
which O
aims O
to O
perform O
query O
rewrite O
under O
a O
multi O
- O
turn O
visual O
conversation O
. O
To O
facilitate O
the O
research O
, O
we O
benchmark O
a O
large O
- O
scale O
dataset O
with O
manual O
annotation O
which O
covers O
15k O
visual O
conversations O
with O
more O
than O
80k O
rewrites O
. O
We O
also O
provide O
image O
box O
annotation O
of O
entities O
appearing O
in O
the O
rewrites O
. O
Accordingly O
, O
we O
propose O
a O
model O
based O
on O
an O
existing O
multimodal O
pre O
- O
trained O
model O
and O
further O
enhance O
it O
with O
a O
pointer O
generator O
. O
Extensive O
experiments O
are O
performed O
to O
show O
the O
effectiveness O
of O
our O
model O
. O

Limitations O

Although O
our O
model O
achieves O
over O
90 B-MetricValue
% I-MetricValue
BLEU-2 B-MetricName
rate O
on O
our O
dataset O
, O
there O
is O
still O
room O
for O
improvements O
in O
the O
future O
work O
, O
since O
the O
exact O
match O
for O
positive O
rewrite O
queries O
can O
be O
further O
lifted O
. O
In O
addition O
, O
in O
the O
future O
work O
, O
we O
will O
also O
continue O
to O
explore O
how O
the O
dataset O
and O
model O
can O
further O
benefit O
downstream O
research O
under O
this O
scenario O
. O
Furthermore O
, O
fine O
- O
grained O
image O
features O
such O
as O
the O
image O
box O
annotation O
can O
be O
leveraged O
for O
improving O
the O
performance O
. O

Ethics O
Statement O

All O
data O
records O
of O
our O
dataset O
are O
originally O
from O
the O
Visual B-DatasetName
Dialog I-DatasetName
( O
Das O
et O
al O
. O
, O
2017 O
) O
dataset O
, O
where O
all O
the O
images O
are O
collected O
from O
the O
COCO B-DatasetName
dataset O
. O
When O
annotating O
the O
data O
, O
we O
make O
sure O
that O
the O
annotators O
do O
not O
have O
any O
other O
rights O
except O
for O
the O
conversation O
information O
. O
Upon O
data O
publication O
, O
we O
will O
strictly O
follow O
the O
user O
terms O
of O
the O
Visual B-DatasetName
Dialog I-DatasetName
dataset O
. O

A O
Rewrite O
Annotation O
Interface O

Figure O
7 O
shows O
the O
annotation O
interface O
when O
preparing O
the O
rewrite O
. O
We O
assign O
one O
text O
box O
for O
each O
query O
in O
the O
visual O
conversation O
so O
that O
the O
annotators O
can O
rewrite O
the O
query O
according O
to O
the O
image O
and O
history O
context O
. O

B O
Image O
Box O
Annotation O

Figure O
8 O
shows O
the O
image O
box O
annotation O
interface O
, O
where O
the O
annotators O
extract O
entities O
from O
the O
rewrites O
and O
mark O
them O
on O
the O
image O
in O
the O
format O
of O
box O
. O

Annotation O
area O
Content O

Object O
List O
C O
Quality O
Control O
Questionnaire O

We O
design O
the O
quality O
control O
questionnaire O
as O
shown O
in O
Figure O
9 O
. O
All O
quality O
checkers O
have O
to O
click O
and O
answer O
the O
three O
questions O
. O

Figure O
9 O
: O
The O
questionnaire O
interface O
in O
the O
quality O
control O
phase O
. O

D O
Dataset O
Examples O

We O
list O
some O
examples O
of O
our O
dataset O
to O
vividly O
demonstrate O
the O
characteristic O
of O
the O
task O
. O
All O
the O
examples O
are O
listed O
in O
Figure O
10 O
. O

Movie101 B-DatasetName
: O
A O
New O
Movie O
Understanding O
Benchmark O

To O
help O
the O
visually O
impaired O
enjoy O
movies O
, O
automatic O
movie O
narrating O
systems O
are O
expected O
to O
narrate O
accurate O
, O
coherent O
, O
and O
role O
- O
aware O
plots O
when O
there O
are O
no O
speaking O
lines O
of O
actors O
. O
Existing O
works O
benchmark O
this O
challenge O
as O
a O
normal O
video O
captioning O
task O
via O
some O
simplifications O
, O
such O
as O
removing O
role O
names O
and O
evaluating O
narrations O
with O
ngram O
- O
based O
metrics O
, O
which O
makes O
it O
difficult O
for O
automatic O
systems O
to O
meet O
the O
needs O
of O
real O
application O
scenarios O
. O
To O
narrow O
this O
gap O
, O
we O
construct O
a O
large O
- O
scale O
Chinese O
movie O
benchmark O
, O
named O
Movie101 B-DatasetName
. O
Closer O
to O
real O
scenarios O
, O
the O
Movie B-TaskName
Clip I-TaskName
Narrating I-TaskName
( O
MCN B-TaskName
) O
task O
in O
our O
benchmark O
asks O
models O
to O
generate O
role O
- O
aware O
narration O
paragraphs O
for O
complete O
movie O
clips O
where O
no O
actors O
are O
speaking O
. O
External O
knowledge O
, O
such O
as O
role O
information O
and O
movie O
genres O
, O
is O
also O
provided O
for O
better O
movie O
understanding O
. O
Besides O
, O
we O
propose O
a O
new O
metric O
called O
Movie B-MetricName
Narration I-MetricName
Score I-MetricName
( O
MNScore B-MetricName
) O
for O
movie O
narrating O
evaluation O
, O
which O
achieves O
the O
best O
correlation O
with O
human B-MetricName
evaluation I-MetricName
. O
Our O
benchmark O
also O
supports O
the O
Temporal B-TaskName
Narration I-TaskName
Grounding I-TaskName
( O
TNG B-TaskName
) O
task O
to O
investigate O
clip O
localization O
given O
text O
descriptions O
. O
For O
both O
two O
tasks O
, O
our O
proposed O
methods O
well O
leverage O
external O
knowledge O
and O
outperform O
carefully O
designed O
baselines O
. O
The O
dataset O
and O
codes O
are O
released O
at O
https O
: O
/ O
/ O
github.com O
/ O
yuezih O
/ O
Movie101 B-DatasetName
. O

Introduction O

The O
estimated O
number O
of O
visually O
impaired O
people O
worldwide O
was O
about O
285 O
million O
by O
2020 O
, O
according O
to O
reports O
( O
He O
et O
al O
. O
, O
2020 O
) O
. O
While O
regulations O
are O
in O
place O
to O
ensure O
increased O
access O
for O
these O
audiences O
to O
experience O
the O
culturally O
dominant O
movies O
and O
TV O
shows O
on O
popular O
media O
platforms O
, O
technologies O
that O
provide O
them O
with O
genuine O
experience O
are O
becoming O
increasingly O
important O
. O
Audio O
description O
( O
AD O
, O
also O
known O
as O
video O
description O
) O
* O
* O
Corresponding O
Author O
. O
is O
a O
form O
of O
such O
technology O
intended O
for O
visually O
impaired O
audiences O
to O
experience O
the O
movie O
or O
TV O
show O
by O
hearing O
what O
is O
happening O
on O
- O
screen O
. O
However O
, O
producing O
movie O
narration O
scripts O
is O
not O
trivial O
, O
often O
requiring O
a O
professional O
writer O
to O
oversee O
the O
original O
movie O
. O
The O
high O
cost O
of O
narration O
generation O
( O
Lakritz O
and O
Salway O
, O
2006 O
) O
greatly O
hinders O
the O
production O
of O
movies O
with O
AD O
and O
thus O
limits O
the O
opportunities O
for O
visually O
impaired O
users O
to O
experience O
movies O
. O

To O
address O
this O
issue O
, O
attempts O
have O
been O
carried O
out O
to O
automate O
AD O
production O
. O
Datasets O
of O
movies O
with O
ADs O
are O
constructed O
to O
support O
the O
research O
on O
automatic O
AD O
generation O
, O
including O
the O
MPII O
- O
MD O
dataset O
( O
Rohrbach O
et O
al O
. O
, O
2015 O
) O
and O
M O
- O
VAD O
dataset O
( O
Torabi O
et O
al O
. O
, O
2015 O
) O
, O
with O
shotlevel O
ADs O
or O
scripts O
aligned O
to O
the O
visual O
contents O
of O
movie O
. O
Consequently O
, O
different O
solutions O
for O
automatic O
movie B-TaskName
narrating I-TaskName
have O
been O
proposed O
based O
on O
these O
datasets O
. O

However O
, O
existing O
benchmarks O
suffer O
from O
several O
limitations O
. O
Firstly O
, O
there O
is O
a O
gap O
between O
the O
designed O
tasks O
and O
the O
actual O
movie O
narration O
scenario O
. O
These O
tasks O
mainly O
focus O
on O
generating O
single O
- O
sentence O
narrations O
for O
shots O
of O
a O
few O
seconds O
. O
They O
can O
not O
support O
the O
generation O
of O
coherent O
narrations O
for O
longer O
plots O
, O
which O
is O
critical O
for O
the O
visually O
impaired O
to O
better O
understand O
the O
movie O
, O
and O
the O
timestamps O
of O
these O
shots O
are O
carefully O
annotated O
, O
which O
are O
difficult O
to O
obtain O
for O
new O
movies O
in O
real O
application O
. O
Meanwhile O
, O
these O
tasks O
treat O
the O
very O
distinctive O
movie B-TaskName
narrating I-TaskName
task O
as O
a O
normal O
video O
captioning O
task O
through O
some O
simplifications O
such O
as O
replacing O
role O
names O
with O
SOMEONE O
, O
resulting O
in O
the O
inability O
to O
connect O
roles O
to O
plots O
. O
Secondly O
, O
these O
benchmarks O
evaluate O
the O
generated O
narrations O
with O
ngram O
- O
based O
metrics O
, O
which O
can O
over O
- O
penalize O
a O
semantically O
correct O
but O
textually O
inconsistent O
narration O
, O
especially O
when O
there O
is O
only O
one O
reference O
available O
. O
In O
addition O
, O
these O
existing O
datasets O
are O
all O
in O
English O
. O
However O
, O

Introduction O

At O
the O
wedding O
of O
his O
first O
love O
QiuYa O
, O
XiaLuo O
pretended O
to O
be O
rich O
and O
made O
a O
fool O
of O
himself O
, O
and O
was O
exposed O
by O
his O
wife O
MaDongmei O
... O
about O
one O
- O
fifth O
of O
the O
world O
's O
population O
speaks O
Chinese O
as O
their O
mother O
tongue O
, O
of O
whom O
more O
than O
17 O
million O
are O
visually O
impaired O
( O
Yu O
and O
Bu O
, O
2021 O
) O
. O
Therefore O
, O
building O
a O
Chinese O
movie O
narration O
benchmark O
is O
necessary O
. O

Intending O
to O
address O
the O
limitations O
of O
the O
existing O
narrating O
benchmarks O
, O
in O
this O
work O
, O
we O
propose O
a O
new O
benchmark O
with O
101 O
Chinese O
movies O
for O
movie O
understanding O
, O
named O
Movie101 B-DatasetName
. O
We O
collect O
the O
movies O
from O
the O
barrier O
- O
free O
channel O
on O
Xigua O
Video O
platform O
1 O
, O
where O
normal O
movies O
are O
remastered O
with O
ADs O
. O
Through O
automatic O
process O
and O
manual O
correction O
, O
we O
obtain O
the O
ADs O
and O
actor O
lines O
from O
the O
raw O
videos O
. O
We O
crawl O
rich O
metainformation O
relevant O
to O
the O
movies O
as O
well O
. O
Finally O
, O
Movie101 B-DatasetName
contains O
30,174 O
narration O
clips O
totaling O
92 O
hours O
, O
with O
data O
samples O
as O
shown O
in O
Fig O
. O
1 O
. O
As O
our O
investigation O
shows O
that O
narrations O
mostly O
occur O
at O
those O
times O
when O
no O
actors O
are O
speaking O
( O
see O
Appendix O
A O
) O
, O
to O
achieve O
realistic O
movie B-TaskName
narrating I-TaskName
, O
we O
propose O
the O
Movie B-TaskName
Clip I-TaskName
Narrating I-TaskName
( O
MCN B-TaskName
) O
task O
that O
requires O
a O
model O
to O
narrate O
where O
there O
are O
no O
lines O
. O
It O
brings O
a O
potential O
benefit O
for O
identifying O
where O
to O
narrate O
in O
an O
unlabeled O
new O
movie O
, O
since O
the O
timestamps O
of O
the O
actor O
lines O
are O
easily O
accessible O
2 O
. O
Meanwhile O
, O
in O
order O
for O
the O
audience O
to O
accurately O
comprehend O
the O
role O
- O
related O
plots O
, O
concrete O
role O
names O
should O
be O
contained O
in O
the O
generated O
narration O
. O
For O
the O
MCN B-TaskName
task O
, O
we O
reorganize O
the O
Movie101 B-DatasetName
dataset O
, O
merging O
the O
narration O
clips O
between O
two O
actor O
dialogues O
into O
a O
longer O
clip O
, O
to O
simulate O
real O
- O
scenario O
movie O
narrating O
. O
We O
thus O
obtain O
14,109 O
long O
clips O
of O
variable O
length O
for O
narration O
generation O
. O
Moreover O
, O
to O
better O
evaluate O
the O
quality O
of O
model O
- O
generated O
narrations O
, O
we O
conduct O
1 O
https O
: O
/ O
/ O
www.ixigua.com O
/ O
channel O
/ O
barrier_free O
2 O
The O
timestamps O
of O
the O
lines O
can O
be O
obtained O
from O
the O
movie O
script O
or O
by O
automatic O
methods O
such O
as O
OCR O
and O
ASR O
. O

human O
evaluations O
and O
design O
a O
new O
metric O
specific O
to O
movie O
narrating O
, O
namely O
Movie B-MetricName
Narration I-MetricName
Score I-MetricName
( O
MNScore B-MetricName
) O
, O
which O
well O
aligns O
with O
human B-MetricName
evaluation I-MetricName
. O
In O
addition O
to O
the O
MCN B-TaskName
task O
, O
our O
dataset O
also O
supports O
the O
Temporal B-TaskName
Narration I-TaskName
Grounding I-TaskName
( O
TNG B-TaskName
) O
task O
, O
which O
asks O
a O
model O
to O
locate O
target O
clips O
in O
the O
movie O
according O
to O
some O
text O
descriptions O
. O
For O
both O
tasks O
, O
we O
benchmark O
the O
performance O
of O
existing O
methods O
, O
and O
further O
propose O
our O
improved O
models O
by O
incorporating O
auxiliary O
external O
knowledge O
. O
In O
addition O
to O
MCN B-TaskName
and O
TNG B-TaskName
tasks O
, O
Movie101 B-DatasetName
can O
also O
potentially O
support O
other O
movie O
understanding O
tasks O
such O
as O
visual O
question O
answering O
and O
action O
recognition O
, O
etc O
. O

The O
main O
contributions O
of O
this O
paper O
are O
as O
follows O
: O
1 O
) O
We O
propose O
a O
new O
benchmark O
for O
movie O
understanding O
, O
Movie101 B-DatasetName
, O
with O
a O
large O
number O
of O
video O
- O
aligned O
text O
descriptions O
in O
Chinese O
. O
2 O
) O
We O
propose O
two O
primary O
tasks O
, O
MCN B-TaskName
and O
TNG B-TaskName
, O
and O
a O
new O
narrating O
evaluation O
metric O
MNScore B-MetricName
, O
where O
MCN B-TaskName
is O
more O
in O
line O
with O
the O
needs O
of O
actual O
movie O
narrating O
, O
while O
MNScore B-MetricName
is O
more O
consistent O
with O
human B-MetricName
evaluation I-MetricName
. O
3 O
) O
We O
benchmark O
state O
- O
of O
- O
theart O
models O
and O
propose O
improved O
models O
enhanced O
by O
external O
knowledge O
for O
MCN B-TaskName
and O
TNG B-TaskName
, O
respectively O
. O
We O
expect O
our O
proposed O
Movie101 B-DatasetName
benchmark O
can O
inspire O
more O
explorations O
on O
narrating O
and O
understanding O
a O
whole O
movie O
. O

Related O
Works O

Datasets O
. O
Existing O
datasets O
to O
support O
the O
automatic O
narration O
generation O
task O
include O
M O
- O
VAD O
( O
Torabi O
et O
al O
. O
, O
2015 O
) O
and O
MPII O
- O
MD O
( O
Rohrbach O
et O
al O
. O
, O
2015 O
) O
, O
which O
are O
merged O
into O
LSMDC O
. O
M O
- O
VAD O
, O
which O
is O
collected O
based O
on O
an O
automatic O
AD O
segmentation O
and O
alignment O
method O
, O
contains O
47 O
K O
videos O
from O
92 O
DVDs O
, O
with O
an O
average O
length O
of O
6.2s O
, O
each O
with O
an O
aligned O
narration O
. O
MPII O
- O
MD O
contains O
68 O
K O
videos O
from O
94 O
movies O
with O
an O
average O
duration O
of O
3.9s O
, O
about O
half O
of O
which O
come O
with O
paired O
scripts O
and O
the O
other O
half O
with O
paired O
ADs O
. O
In O
addition O
to O
movies O
, O
TV O
shows O
are O
also O
good O
data O
sources O
for O
automatic O
narration B-TaskName
generation I-TaskName
. O
Lei O
et O
al O
. O
( O
2020 O
) O
propose O
TV O
Show O
Caption O
( O
TVC O
) O
, O
a O
variant O
of O
TV O
Show O
Retrieval O
( O
TVR O
) O
. O
It O
contains O
11 O
K O
short O
videos O
averaging O
9.1s O
in O
length O
, O
and O
26 O
K O
captions O
describing O
the O
visual O
content O
, O
dialogues O
, O
and O
subtitles O
. O
All O
the O
existing O
datasets O
are O
in O
English O
. O

Video O
Captioning O
. O
As O
a O
classic O
vision O
and O
language O
task O
, O
the O
video O
captioning O
task O
requires O
a O
model O
to O
generate O
natural O
language O
descriptions O
for O
given O
videos O
. O
Solutions O
for O
normal O
video O
captioning O
go O
through O
stages O
from O
pre O
- O
designed O
templates O
( O
Kojima O
et O
al O
. O
, O
2002 O
; O
Guadarrama O
et O
al O
. O
, O
2013 O
) O
to O
sequence O
- O
to O
- O
sequence O
generation O
with O
deep O
neural O
networks O
( O
Pasunuru O
and O
Bansal O
, O
2017 O
) O
. O
A O
challenging O
variant O
for O
this O
task O
is O
dense O
video O
captioning O
( O
Krishna O
et O
al O
. O
, O
2017 O
) O
, O
which O
requires O
the O
generation O
of O
multi O
- O
sentence O
descriptions O
for O
long O
multievent O
videos O
. O
The O
two O
- O
stage O
generation O
approach O
, O
which O
firstly O
performs O
proposal O
detection O
on O
the O
video O
and O
then O
generates O
descriptions O
for O
each O
proposal O
separately O
, O
has O
been O
the O
dominant O
approach O
( O
Krishna O
et O
al O
. O
, O
2017 O
; O
Park O
et O
al O
. O
, O
2019 O
; O
Rohrbach O
et O
al O
. O
, O
2014 O
; O
. O
Recently O
, O
some O
works O
avoid O
event O
detection O
and O
generate O
paragraph O
descriptions O
directly O
based O
on O
the O
video O
, O
such O
as O
the O
one O
- O
stage O
paragraphing O
model O
( O
OVP O
) O
( O
Song O
et O
al O
. O
, O
2021 O
) O
, O
obtaining O
competitive O
performance O
compared O
to O
previous O
works O
, O
inspired O
by O
which O
we O
propose O
our O
knowledge O
- O
enhanced O
movie O
narrating O
model O
. O
Identity O
- O
aware O
video O
description O
that O
distinguishes O
different O
persons O
is O
more O
practical O
in O
real O
applications O
. O
Park O
et O
al O
. O
( O
2020 O
) O
attempt O
to O
achieve O
role O
- O
aware O
movie O
narrating O
by O
distinguishing O
different O
people O
using O
labels O
such O
as O
PERSON1 O
, O
PERSON2 O
, O
etc O
. O
However O
, O
it O
fails O
to O
generate O
concrete O
role O
names O
and O
falls O
short O
in O
terms O
of O
practicality O
. O

Temporal B-TaskName
Sentence I-TaskName
Grounding I-TaskName
. O
The O
temporal B-TaskName
sentence I-TaskName
grounding I-TaskName
( O
TSG B-TaskName
) O
task O
aims O
to O
localize O
the O
moment O
in O
a O
video O
based O
on O
a O
natural O
language O
query O
( O
Gao O
et O
al O
. O
, O
2017 O
) O
. O
A O
two O
- O
step O
pipeline O
has O
been O
the O
mainstream O
approach O
, O
which O
first O
produces O
a O
large O
number O
of O
moment O
candidates O
via O
sliding O
windows O
, O
then O
ranks O
them O
with O
their O
similarity O
to O
the O
query O
sentence O
. O
The O
following O
works O
try O
to O
improve O
the O
grounding O
performance O
by O
enhancing O
interaction O
between O
video O
and O
query O
modalities O
or O
introducing O
novel O
detection O
heads O
( O
Lei O
et O
al O
. O
, O
2021 O
; O
Zhang O
et O
al O
. O
, O
2020a O
) O
. O
Specifically O
, O
for O
interaction O
methods O
, O
adopt O
an O
Iterative B-MethodName
Alignment I-MethodName
Network I-MethodName
( O
IA B-MethodName
- I-MethodName
Net I-MethodName
) O
to O
iteratively O
interact O
interand O
intra O
- O
modal O
features O
within O
multiple O
steps O
. O
explicitly O
decompose O
video O
and O
query O
into O
multiple O
structured O
hierarchies O
and O
learn O
finegrained O
semantic O
alignment O
among O
them O
. O
In O
this O
work O
, O
we O
propose O
to O
incorporate O
external O
knowledge O
based O
on O
the O
IA B-MethodName
- I-MethodName
Net I-MethodName
model O
structure O
. O

Dataset O

Data O
Collection O

Movie O
Acquisition O
. O
To O
the O
best O
of O
our O
knowledge O
, O
there O
are O
only O
a O
handful O
of O
platforms O
that O
provide O
accessible O
movies O
in O
Chinese O
. O
The O
barrierfree O
channel O
of O
Xigua O
Video O
is O
one O
such O
platform O
that O
provides O
over O
100 O
accessible O
movies O
online O
, O
and O
new O
movies O
are O
still O
being O
released O
that O
can O
support O
further O
expansion O
of O
our O
dataset O
. O
From O
Xigua O
Video O
, O
we O
collect O
all O
101 O
movies O
available O
to O
date O
and O
crawl O
as O
much O
meta O
information O
as O
possible O
for O
each O
movie O
, O
including O
title O
, O
introduction O
, O
genres O
, O
directors O
, O
actors O
, O
etc O
. O
We O
emphasize O
actors O
in O
particular O
, O
including O
actor O
names O
, O
role O
names O
, O
actor O
portraits O
, O
role O
rankings O
, O
and O
other O
information O
about O
important O
roles O
. O
We O
expect O
such O
information O
can O
benefit O
the O
movie O
narrating O
task O
and O
general O
movie O
understanding O
tasks O
. O
Narrations O
and O
Lines O
Extraction O
. O
As O
the O
movie O
lines O
and O
narrations O
are O
only O
available O
in O
the O
subtitle O
and O
audio O
format O
respectively O
from O
the O
platform O
, O
we O
therefore O
leverage O
OCR O
and O
automatic O
speech O
recognition O
( O
ASR O
) O
tools O
for O
transcription O
. O
For O
lines O
, O
we O
extract O
text O
from O
subtitles O
by O
open O
- O
source O
OCR O
toolkit O
PaddleOCR O
3 O
at O
2.4 O
FPS O
, O
and O
manually O
remove O
the O
irrelevant O
subtitles O
from O
the O
beginning O
and O
the O
end O
of O
each O
movie O
. O
For O
narrations O
, O
we O
extract O
the O
audio O
track O
from O
the O
movie O
and O
utilize O
the O
ASR O
service O
provided O
by O
iFlyTek O
4 O
, O
which O
detects O
the O
speech O
in O
the O
audio O
and O
transcribes O
it O
into O
text O
. O
In O
addition O
, O
the O
service O
supports O
identifying O
different O
speakers O
, O
which O
helps O
discriminate O
the O
narrator O
from O
the O
actors O
. O
However O
, O
the O
ASR O
service O
is O
not O
perfect O
, O
and O
its O
outputs O
contain O
errors O
such O
as O
wrong O
characters O
, O
unreasonable O
sentence O
breaking O
, O
and O
misidentification O
of O
narrations O
as O
movie O
dialogues O
, O
etc O
. O
Therefore O
, O
we O
recruit O
human O
annotators O
to O
further O
correct O
the O
ASR O
transcription O
errors O
and O
remove O
non O
- O
narration O
texts O
manually O
to O
improve O
the O
data O
quality O
. O
We O
also O
delete O
the O
irrelevant O
fragments O
at O
the O
beginning O
( O
e.g. O
, O
movie O
synopsis O
, O
cast O
introductions O
) O
and O
the O
summary O
narration O
at O
the O
end O
. O
For O
coherency O
, O
we O
further O
organize O
the O
narration O
fragments O
at O
the O
clip O
level O
. O
We O
merge O
every O
two O
fragments O
if O
their O
temporal O
gap O
is O
less O
than O
1 O
second O
. O
we O
also O
apply O
a O
paragraph O
- O
length O
threshold O
of O
100 O
characters O
to O
limit O
over O
- O
merging O
to O
avoid O
excessively O
long O
clips O
. O
We O
take O
punctuation O
into O
account O
as O
well O
, O
for O
example O
, O
a O
period O
in O
Chinese O
is O
likely O
to O
mean O
the O
end O
of O
a O
narrative O
paragraph O
. O
Further O
detailed O
descriptions O
of O
data O
quality O
can O
be O
found O
in O
Appendix O
B. O
Movie101 B-DatasetName
- I-DatasetName
N I-DatasetName
and O
Movie101 B-DatasetName
- I-DatasetName
G. I-DatasetName
For O
real O
- O
life O
movie O
narrating O
, O
models O
are O
expected O
to O
narrate O
in O
the O
breaks O
between O
different O
actor O
dialogues O
. O
Thus O
, O
we O
reorganize O
Movie101 B-DatasetName
to O
fit O
this O
task O
format O
. O
Concretely O
, O
we O
first O
merge O
the O
independent O
lines O
in O
Movie101 B-DatasetName
into O
dialogues O
, O
where O
two O
lines O
with O
a O
temporal O
gap O
shorter O
than O
5 O
seconds O
are O
considered O
to O
belong O
to O
one O
dialogue O
. O
Then O
, O
we O
merge O
all O
the O
narration O
clips O
between O
two O
adjacent O
dialogues O
into O
a O
long O
paragraph O
. O
In O
this O
way O
, O
we O
obtain O
Movie101 B-DatasetName
- I-DatasetName
N I-DatasetName
with O
narration O
paragraphs O
separated O
by O
dialogues O
, O
which O
well O
simulates O
the O
practical O
narrating B-TaskName
challenge O
. O
Meanwhile O
, O
with O
rich O
videotext O
pairs O
in O
Movie101 B-DatasetName
, O
we O
create O
another O
variant O
dataset O
to O
support O
the O
temporal B-TaskName
grounding I-TaskName
tasks O
, O
named O
Movie101 B-DatasetName
- I-DatasetName
G I-DatasetName
, O
where O
narrations O
are O
taken O
as O
queries O
and O
aligned O
videos O
serve O
as O
targets O
. O
For O
validation O
and O
testing O
, O
we O
carefully O
select O
10 O
movies O
of O
different O
genres O
for O
each O
respectively O
. O

Dataset O
Statistics O

Movie O
Properties O
. O
Movie101 B-DatasetName
contains O
101 O
movies O
, O
involving O
41 O
genres O
( O
a O
movie O
can O
belong O
to O
up O
to O
4 O
genres O
) O
and O
645 O
roles O
in O
total O
. O
Fig O
. O
2 O
shows O
the O
numbers O
of O
movies O
in O
the O
top O
10 O
most O
popular O
genres O
, O
with O
comedy O
, O
romance O
, O
and O
action O
in O
the O
top O
3 O
. O
Clip O
Properties O
. O
Movie101 B-DatasetName
contains O
a O
total O
of O
1 O
shows O
that O
Movie101 B-DatasetName
- I-DatasetName
N I-DatasetName
contains O
much O
longer O
video O
clips O
and O
text O
descriptions O
than O
existing O
movie O
narrating O
datasets O
, O
while O
the O
length O
distribution O
in O
Fig O
. O
4 O
indicates O
that O
the O
clip O
length O
varies O
a O
lot O
. O
Movie101 B-DatasetName
- I-DatasetName
G I-DatasetName
contains O
30,174 O
clips O
to O
be O
located O
from O
101 O
movies O
. O
The O
average O
video O
length O
of O
6,144 O
seconds O
also O
greatly O
exceeds O
existing O
TSG B-TaskName
datasets O
. O

Movie O
Clip O
Narrating O

Task O
Description O

In O
order O
to O
help O
the O
visually O
impaired O
keep O
up O
with O
the O
plot O
in O
the O
movie O
, O
we O
first O
propose O
a O
Movie B-TaskName
Clip I-TaskName
Narrating I-TaskName
( O
MCN B-TaskName
) O
task O
, O
which O
aims O
to O
generate O
a O
plot O
- O
related O
paragraph O
description O
given O
a O
clip O
in O
Movie101 B-DatasetName
- I-DatasetName
N. I-DatasetName
Besides O
, O
the O
narration O
styles O
may O
vary O
across O
different O
genres O
of O
movies O
. O
The O
role O
portraits O
are O
important O
external O
knowledge O
for O
a O
model O
to O
accurately O
describe O
the O
subject O
of O
actions O
. O
Thus O
, O
we O
also O
provide O
this O
information O
in O
Movie101 B-DatasetName
- I-DatasetName
N I-DatasetName
to O
support O
the O
MCN B-TaskName
task O
. O

Proposed O
Method O

For O
the O
MCN B-TaskName
task O
, O
with O
multimodal O
inputs O
including O
video O
, O
movie O
genres O
, O
role O
names O
, O
and O
actor O
portraits O
, O
we O
propose O
a O
Transformer O
- O
based O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
model O
with O
an O
encoderdecoder O
framework O
, O
namely O
Role B-MethodName
- I-MethodName
pointed I-MethodName
Movie I-MethodName
Narrator I-MethodName
( O
RMN B-MethodName
) O
, O
where O
the O
encoder O
mainly O
encodes O
video O
clips O
and O
the O
decoder O
generates O
narrations O
, O
as O
shown O
in O
Fig O
. O
5 O
( O
a O
) O
. O

On O
the O
encoder O
side O
, O
taking O
into O
account O
the O
frame O
- O
level O
visual O
information O
, O
the O
video O
clip O
is O
embedded O
into O
a O
sequence O
of O
frame O
- O
level O
features O
. O
To O
emphasize O
the O
roles O
, O
we O
extract O
face O
features O
from O
each O
frame O
and O
concatenate O
them O
to O
the O
corresponding O
frame O
feature O
sequentially O
based O
on O
the O
confidence O
scores O
of O
face O
detection O
. O
With O
learnable O
genre O
embeddings O
, O
genres O
are O
also O
represented O
as O
a O
sequence O
of O
genre O
features O
. O
After O
video O
and O
genre O
representation O
, O
we O
apply O
a O
Transformer O
encoder O
to O
perform O
cross O
- O
encoding O
. O
Then O
, O
we O
follow O
the O
One O
- O
stage O
Video O
Paragraphing O
model O
( O
OVP O
) O
( O
Song O
et O
al O
. O
, O
2021 O
) O
to O
use O
a O
dynamic O
memory O
bank O
to O
refine O
the O
video O
- O
part O
representations O
, O
which O
updates O
at O
each O
decoding O
step O
. O

On O
the O
decoder O
side O
, O
in O
addition O
to O
the O
Transformer O
decoder O
, O
we O
enable O
the O
model O
to O
directly O
choose O
a O
complete O
role O
name O
from O
the O
movie O
cast O
according O
to O
context O
during O
token O
- O
by O
- O
token O
generation O
via O
a O
pointer O
network O
( O
Gu O
et O
al O
. O
, O
2016 O
) O
. O
At O
the O
decoding O
step O
t O
, O
with O
the O
decoder O
hidden O
state O
h O
t O
, O
we O
first O
calculate O
the O
token O
scores O
y O
voc O
t O
among O
normal O
vocabulary O
. O
Then O
we O
design O
a O
Role O
Selector O
module O
to O
get O
the O
name O
scores O
among O
external O

y O
t O
= O
f O
( O
[ O
y O
voc O
t O
; O
λy O
role O
t O
] O
) O
( O
1 O
) O

where O
[ O
; O
] O
means O
concatenation O
, O
λ O
is O
a O
gate O
computed O
from O
h O
t O
, O
f O
( O
) O
is O
the O
softmax O
function O
. O

Evaluation O

Existing O
movie B-TaskName
narration I-TaskName
benchmarks O
directly O
adopt O
ngram O
- O
based O
metrics O
including O
CIDEr O
, O
BLEU O
, O
and O
METEOR O
as O
in O
normal O
video O
captioning O
. O
However O
, O
there O
are O
pitfalls O
for O
these O
metrics O
, O
such O
as O
underestimating O
semantically O
correct O
but O
textually O
inconsistent O
phrases O
, O
which O
have O
been O
widely O
reported O
( O
Zhang O
et O
al O
. O
, O
2020b O
; O
Shi O
et O
al O
. O
, O
2022 O
) O
. O
For O
movie O
narrating O
, O
a O
movie O
clip O
can O
be O
narrated O
in O
multiple O
expressions O
, O
while O
there O
is O
only O
one O
reference O
. O
Thus O
, O
text O
matching O
is O
inadequate O
to O
measure O
the O
quality O
of O
a O
narration O
paragraph O
. O

To O
better O
evaluate O
the O
generated O
narrations O
in O
the O
MCN B-TaskName
task O
, O
we O
conduct O
a O
manual B-MetricName
evaluation I-MetricName
to O
investigate O
how O
humans O
assess O
different O
narrations O
. O
We O
randomly O
select O
30 O
movie O
clips O
, O
each O
with O
5 O
candidate O
narrations O
, O
of O
which O
3 O
are O
derived O
from O
the O
predictions O
of O
different O
models O
and O
2 O
are O
obtained O
by O
disturbing O
the O
ground O
truth O
narrations O
. O
Next O
, O
we O
recruit O
10 O
annotators O
to O
individually O
rank O
the O
candidates O
for O
each O
video O
in O
terms O
of O
accuracy B-MetricName
, O
informativeness B-MetricName
, O
and O
textual B-MetricName
quality I-MetricName
. O
Accuracy B-MetricName
defines O
how O
the O
narration O
accurately O
describes O
the O
video O
, O
especially O
roles O
, O
actions O
, O
and O
objects O
; O
informativeness B-MetricName
defines O
how O
richly O
the O
narration O
reveals O
the O
video O
content O
; O
textual B-MetricName
quality I-MetricName
is O
determined O
by O
the O
narration O
fluency O
and O
grammatical O
correctness O
. O

With O
the O
human B-MetricName
evaluation I-MetricName
results O
, O
we O
investigate O
a O
wide O
range O
of O
objective O
metrics O
as O
follows O
: O
( O
1 O
) O
State O
- O
of O
- O
the O
- O
art O
video O
captioning O
metrics O
based O
on O
deep O
neural O
networks O
including O
CLIP B-MetricName
- I-MetricName
Score I-MetricName
( O
Hessel O
et O
al O
. O
, O
2021 O
) O
, O
BERTScore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
and O
EMScore B-MetricName
( O
Shi O
et O
al O
. O
, O
2022 O
) O
, O
which O
are O
reported O
outperforming O
ngram O
- O
based O
metrics O
in O
video O
captioning O
evaluation O
; O
( O
2 O
) O
Textual O
quality O
metrics O
including O
n B-MetricName
- I-MetricName
grams I-MetricName
diversity I-MetricName
( O
Shetty O
et O
al O
. O
, O
2017 O
) O
( O
DIV O
) O
and O
causal B-MetricName
language I-MetricName
model I-MetricName
perplexity I-MetricName
( O
PPL B-MetricName
) O
; O
( O
3 O
) O
F1 B-MetricName
score O
of O
role O
name O
generation O
( O
RoleF1 B-MetricName
) O
. O
For O
every O
two O
candidate O
narrations O
of O
a O
video O
, O
we O
use O
human O
ranking O
as O
a O
reference O
to O
determine O
whether O
these O
metrics O
correctly O
judge O
which O
of O
the O
two O
candidates O
is O
better O
or O
worse O
, O
and O
the O
accuracy B-MetricName
is O
used O
for O
evaluating O
metrics O
' O
correlation O
with O
human B-MetricName
judgment I-MetricName
. O
Finally O
, O
we O
settle O
on O
a O
new O
metric O
Movie B-MetricName
Narration I-MetricName
Score I-MetricName
( O
MNScore B-MetricName
) O
as O
follows O
: O

mns B-MetricName
= O
1 O
• O
ems B-MetricName
+ O
4 O
• O
berts B-MetricName
+ O
1 O
• O
rf B-MetricName
1 I-MetricName
6 O
× O
100 O
( O
2 O
) O

where O
mns B-MetricName
, O
ems B-MetricName
, O
berts B-MetricName
and O
rf B-MetricName
1 I-MetricName
refer O
to O
MNScore B-MetricName
, O
EMScore B-MetricName
, O
BERTScore B-MetricName
and O
RoleF1 B-MetricName
, O
respectively O
. O
As O
shown O
in O
Table O
2 O
, O
BERTScore B-MetricName
outperforms O
ngram O
- O
based O
metrics O
in O
narration O
evaluation O
accuracy B-MetricName
, O
while O
our O
new O
proposed O
MNScore B-MetricName
achieves O
the O
best O
alignment O
with O
human B-MetricName
evaluation I-MetricName
. O
More O
details O
about O
the O
implementation O
of O
the O
candidate O
narrations O
and O
the O
above O
metrics O
are O
presented O
in O
Appendix O
C O
. O

Experiments O

Implementation O
Details O
. O
In O
our O
proposed O
method O
, O
models O
are O
trained O
with O
next O
- O
token O
language O
modeling O
by O
the O
maximum O
likelihood O
estimation O
( O
MLE O
) O
objective O
. O
For O
videos O
, O
we O
use O
CLIP B-MethodName
( O
Radford O
et O
al O
. O
, O
2021 O
) O
pre O
- O
trained O
on O
large O
- O
scale O
image O
- O
text O
pairs O
and O
MIL B-MethodName
- I-MethodName
NCE I-MethodName
( O
Miech O
et O
al O
. O
, O
2020 O
) O
pre O
- O
trained O
on O
HowTo100 O
M O
videos O
( O
Miech O
et O
al O
. O
, O
2019 O
) O
to O
extract O
frame O
- O
level O
CLIP O
and O
S3D O
features O
with O
dimensions O
of O
512 O
and O
1024 O
, O
respectively O
, O
at O
1 O
FPS O
, O
and O
further O
concatenate O
them O
. O
For O
faces O
in O
video O
frames O
and O
portraits O
, O
we O
use O
the O
Arcface B-MethodName
model O
( O
Deng O
et O
al O
. O
, O
2019 O
) O
pre O
- O
trained O
on O
MS1 O
M O
( O
Guo O
et O
al O
. O
, O
2016 O
) O
to O
extract O
face O
features O
. O
When O
there O
are O
insufficient O
faces O
detected O
within O
a O
frame O
, O
the O
3 O
, O
RMN B-MethodName
outperforms O
the O
baselines O
by O
a O
large O
margin O
, O
especially O
on O
RoleF1 B-MetricName
. O
This O
indicates O
that O
our O
model O
learns O
to O
generate O
role O
names O
from O
external O
knowledge O
with O
the O
help O
of O
the O
pointer O
network O
. O
To O
verify O
the O
contribution O
of O
the O
genre O
and O
face O
representations O
in O
our O
RMN B-MethodName
model O
, O
we O
also O
perform O
an O
ablation O
study O
by O
progressively O
adding O
these O
representations O
as O
input O
. O
From O
the O
results O
, O
face O
features O
extracted O
from O
video O
frames O
bring O
significant O
gains O
in O
role O
awareness O
, O
which O
shows O
that O
using O
face O
features O
to O
bridge O
the O
video O
content O
and O
external O
actor O
portraits O
is O
beneficial O
for O
generating O
role O
- O
related O
narrations O
. O
Qualitative O
results O
can O
be O
found O
in O
Appendix O
D O
. O

Temporal B-TaskName
Narration I-TaskName
Grounding I-TaskName

Task O
Description O

To O
help O
people O
locate O
clips O
of O
interest O
during O
movie O
entertainment O
, O
an O
AI O
agent O
should O
be O
able O
to O
understand O
users O
' O
intentions O
and O
locate O
the O
target O
clips O
. O
To O
achieve O
this O
goal O
, O
we O
propose O
the O
Temporal B-TaskName
Narration I-TaskName
Grounding I-TaskName
( O
TNG B-TaskName
) O
task O
. O
Given O
a O
clip O
narration O
as O
the O
query O
, O
TNG B-TaskName
aims O
to O
predict O
the O
starting O
and O
ending O
time O
of O
the O
clip O
in O
the O
whole O
movie O
. O

Proposed O
method O

Existing O
temporal B-TaskName
sentence I-TaskName
grounding I-TaskName
models O
can O
hardly O
handle O
an O
entire O
movie O
input O
with O
limited O
computational O
resources O
. O
Thus O
, O
we O
propose O
a O
twostage O
framework O
for O
the O
TNG B-TaskName
task O
, O
with O
global B-TaskName
shot I-TaskName
retrieval I-TaskName
to O
coarsely O
locate O
the O
target O
clip O
in O
the O
first O
Global B-TaskName
Shot I-TaskName
Retrieval I-TaskName
. O
To O
find O
the O
approximate O
location O
of O
the O
target O
, O
we O
treat O
it O
as O
a O
text B-TaskName
- I-TaskName
video I-TaskName
retrieval I-TaskName
subtask O
. O
We O
divide O
a O
movie O
into O
20s O
- O
long O
shots O
, O
and O
the O
shot O
with O
the O
highest O
similarity O
to O
the O
text O
query O
will O
be O
used O
as O
the O
anchor O
for O
further O
grounding O
in O
the O
second O
stage O
. O
For O
training O
such O
a O
retrieval O
system O
, O
we O
construct O
a O
temporary O
dataset O
Movie101 B-DatasetName
- I-DatasetName
GSR I-DatasetName
( I-DatasetName
temp I-DatasetName
) I-DatasetName
. O
Concretely O
, O
after O
cutting O
the O
movie O
into O
shots O
, O
each O
shot O
and O
each O
annotated O
narration O
in O
Movie101 B-DatasetName
are O
judged O
with O
the O
temporal O
overlap O
whether O
they O
can O
be O
considered O
as O
an O
aligned O
video O
- O
text O
pair O
. O
6 O
We O
build O
the O
retrieval O
model O
by O
transferring O
a O
Chinese O
Vision O
- O
Language O
Pre O
- O
training O
( O
VLP O
) O
model O
ChineseCLIP B-MethodName
) O
( O
CNCLIP B-MethodName
) O
from O
image O
- O
text O
to O
video O
- O
text O
. O
Specifically O
, O
the O
shot O
frames O
are O
separately O
encoded O
as O
image O
features O
by O
the O
visual O
encoder O
of O
CNCLIP B-MethodName
, O
and O
the O
final O
video O
feature O
is O
obtained O
by O
performing O
mean O
pooling O
over O
the O
CLS O
tokens O
of O
all O
frames O
. O
We O
then O
perform O
contrastive O
learning O
between O
the O
video O
and O
text O
features O
on O
Movie101 B-DatasetName
- I-DatasetName
GSR I-DatasetName
( I-DatasetName
temp I-DatasetName
) I-DatasetName
to O
fine O
- O
tune O
the O
modified O
CNCLIP B-MethodName
. O
Local B-TaskName
Temporal I-TaskName
Grounding I-TaskName
. O
After O
obtaining O
the O
anchor O
shot O
in O
the O
first O
stage O
, O
we O
further O
lo- O

Experiments O

Implementation O
Details O
. O
For O
Global B-TaskName
Shot I-TaskName
Retrieval I-TaskName
, O
we O
use O
average B-MetricName
Recall I-MetricName
@ I-MetricName
n I-MetricName
( I-MetricName
n I-MetricName
∈ I-MetricName
1 I-MetricName
, I-MetricName
5 I-MetricName
, I-MetricName
10 I-MetricName
) I-MetricName
to O
evaluate O
the O
retrieval O
performance O
on O
all O
movies O
. O
For O
Local B-TaskName
Temporal I-TaskName
Grounding I-TaskName
, O
following O
previous O
works O
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
, O
we O
use O
" O
R B-MetricName
@ I-MetricName
n I-MetricName
, O
IoU B-MetricName
@ I-MetricName
m I-MetricName
" O
as O
metrics O
, O
which O
are O
defined O
as O
the O
percentage O
of O
at O
least O
one O
of O
top O
- O
n O
proposals O
having O
a O
larger O
temporal O
IoU B-MetricName
than O
m O
with O
the O
ground O
truth O
. O
We O
fine O
- O
tune O
CNCLIP B-MethodName
- O
huge O
on O
our O
Movie101 B-DatasetName
- I-DatasetName
GSR I-DatasetName
( I-DatasetName
temp I-DatasetName
) I-DatasetName
for O
Global B-TaskName
Shot I-TaskName
Retrieval I-TaskName
, O
and O
benchmark O
two O
code O
- O
released O
state O
- O
of O
- O
the O
- O
art O
temporal O
grounding O
models O
2D B-MethodName
- I-MethodName
TAN I-MethodName
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
and O
IA B-MethodName
- I-MethodName
Net I-MethodName
on O
Movie101 B-DatasetName
- I-DatasetName
LTG I-DatasetName
( I-DatasetName
temp I-DatasetName
) I-DatasetName
for O
Local B-TaskName
Temporal I-TaskName
Grounding I-TaskName
. O
In O
our O
RNL B-MethodName
model O
, O
the O
video O
frame O
, O
face O
, O
and O
text O
feature O
extractors O
are O
pre O
- O
trained O
MIL O
- O
NCE O
, O
Arcface O
( O
same O
as O
in O
the O
MCN O
task O
) O
and O
BERT O
- O
base O
- O
Chinese O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
respectively O
. O

Results O
& O
Analysis O
. O
Table O
4 O
and O
Table O
5 O
show O
the O
performance O
of O
models O
on O
Global B-TaskName
Shot I-TaskName
Retrieval I-TaskName
and O
Local B-TaskName
Temporal I-TaskName
Grounding I-TaskName
, O
respectively O
. O
Our O
RNL B-MethodName
outperforms O
baselines O
by O
introducing O
roleaware O
video O
and O
text O
encoding O
, O
indicating O
that O
distinguishing O
actions O
of O
different O
roles O
is O
critical O
for O
grounding O
movie O
narration O
. O
Furthermore O
, O
we O
perform O
an O
ablation O
study O
to O
verify O
the O
effectiveness O
of O
role O
- O
aware O
encoding O
. O
As O
shown O
in O
Table O
5 O
, O
adding O
face O
features O
to O
either O
video O
or O
text O
representations O
outperforms O
our O
base O
method O
IA B-MethodName
- I-MethodName
Net I-MethodName
. O
RNL B-MethodName
with O
both O
role O
- O
aware O
video O
and O
text O
encoding O
achieves O
the O
best O
performance O
. O
Table O
6 O
shows O
the O
performance O
of O
combined O
inference O
by O
Global B-TaskName
Shot I-TaskName
Retrieval I-TaskName
and O
Local B-TaskName
Temporal I-TaskName
Grounding I-TaskName
. O
We O
in O
addition O
show O
the O
performance O
of O
k O
- O
way O
re O
- O
ranking O
, O
where O
the O
top O
- O
k O
shots O
retrieved O
in O
the O
first O
stage O
are O
respectively O
used O
as O
the O
anchors O
in O
the O
second O
stage O
, O
and O
all O
predictions O
obtained O
are O
re O
- O
ranked O
with O
their O
confidence O
scores O
. O
The O
experimental O
results O
show O
that O
k O
- O
way O
re O
- O
ranking O
improves O
Rank O
@ O
5 O
performance O
but O
harms O
Rank O
@ O
1 O
performance O
. O
Qualitative O
results O
can O
be O
found O
in O
Appendix O
D O
. O

Conclusion O

In O
this O
work O
, O
we O
propose O
Movie101 B-DatasetName
, O
a O
Chinese O
large O
- O
scale O
video O
benchmark O
for O
movie O
understanding O
. O
To O
assist O
visually O
impaired O
people O
in O
enjoying O
movies O
, O
we O
propose O
a O
more O
realistic O
Movie O
Clip O
Narrating O
task O
to O
address O
the O
automatic O
movie O
description O
issue O
and O
design O
a O
human O
- O
preferencecompatible O
metric O
MNScore B-MetricName
for O
narrating O
evaluation O
. O
Movie101 B-DatasetName
also O
supports O
the O
Temporal B-TaskName
Narration I-TaskName
Grounding I-TaskName
task O
, O
which O
is O
more O
challenging O
than O
the O
previous O
TSG O
benchmarks O
. O
Furthermore O
, O
our O
experiments O
validate O
the O
importance O
of O
external O
knowledge O
including O
genres O
and O
roles O
for O
movie O
understanding O
. O
However O
, O
there O
is O
still O
a O
significant O
gap O
between O
our O
models O
and O
expert O
annotations O
. O
This O
reveals O
that O
further O
research O
endeavors O
are O
still O
needed O
to O
help O
visually O
impaired O
people O
enjoy O
movies O
by O
AI O
. O

Limitations O

Keeping O
narration O
coherent O
within O
a O
movie O
is O
crucial O
for O
visually O
impaired O
people O
to O
enjoy O
the O
movie O
. O

In O
this O
work O
, O
we O
move O
a O
step O
forward O
for O
this O
target O
by O
setting O
the O
ground O
- O
truth O
texts O
in O
the O
Movie B-TaskName
Clip I-TaskName
Narrating I-TaskName
task O
as O
narration O
paragraphs O
and O
providing O
longer O
video O
clips O
as O
inputs O
. O
However O
, O
how O
to O
ensure O
description O
coherence O
across O
different O
clips O
within O
a O
movie O
has O
not O
been O
studied O
in O
this O
work O
. O
This O
requires O
a O
higher O
- O
level O
comprehending O
ability O
of O
models O
to O
process O
the O
whole O
movie O
and O
connect O
different O
plots O
. O
We O
leave O
this O
to O
our O
future O
investigation O
. O

Ethics O
Statement O

We O
propose O
Movie101 B-DatasetName
, O
a O
new O
benchmark O
to O
support O
exploring O
technologies O
to O
benefit O
the O
accessibility O
of O
the O
visually O
impaired O
. O
There O
are O
two O
potential O
ethical O
issues O
with O
our O
work O
, O
regarding O
data O
source O
and O
crowdsourcing O
services O
, O
respectively O
. O
We O
state O
each O
of O
them O
as O
follows O
: O
Data O
Source O
. O
The O
collected O
movies O
are O
publicly O
available O
from O
Xigua O
Video O
, O
and O
are O
allowed O
to O
be O
crawled O
according O
to O
the O
service O
contract O
of O
the O
website O
7 O
. O
Considering O
the O
copyright O
issue O
, O
we O
will O
only O
release O
the O
url O
list O
of O
movies O
. O
Besides O
, O
our O
data O
source O
does O
not O
contain O
any O
information O
that O
names O
or O
uniquely O
identifiable O
individuals O
or O
offensive O
content O
. O

Crowdsourcing O
Services O
. O
We O
recruited O
20 O
Chinese O
college O
students O
( O
12 O
females O
and O
8 O
males O
) O
via O
social O
media O
. O
For O
ASR O
outputs O
cleaning O
, O
workers O
were O
required O
to O
correct O
errors O
in O
the O
narration O
text O
while O
watching O
the O
movie O
. O
For O
each O
movie O
, O
it O
took O
about O
2 O
hours O
with O
a O
payment O
of O
50 O
RMB O
( O
$ O
7.40 O
USD O
) O
. O
To O
review O
corrections O
, O
for O
each O
movie O
, O
it O
took O
about O
30 O
minutes O
with O
a O
payment O
of O
25 O
RMB O
( O
$ O
3.70 O
USD O
) O
. O
Our O
payment O
is O
fair O
and O
reasonable O
in O
China O
, O
especially O
since O
the O
work O
is O
easy O
and O
fun O
. O
Before O
the O
annotation O
works O
began O
, O
we O
introduced O
the O
future O
use O
of O
the O
data O
in O
the O
task O
document O
to O
ensure O
that O
everyone O
was O
informed O
. O

A O
Narration O
Distribution O

Clips O
where O
' O
no O
actors O
are O
speaking O
' O
refer O
to O
ANY O
scene O
wherein O
no O
verbal O
dialogue O
is O
being O
employed O
by O
the O
actors O
, O
regardless O
of O
whether O
they O
are O
visually O
present O
or O
absent O
. O
This O
definition O
encompasses O
, O
for O
example O
, O
a O
scene O
focused O
solely O
on O
a O
depiction O
of O
the O
sky O
. O
We O
detail O
the O
dialogues O
and O
narrations O
in O
the O
101 O
collected O
movies O
. O
By O
merging O
the O
actor O
lines O
, O
we O
obtain O
a O
total O
of O
15,307 O
dialogues O
, O
constituting O
15,206 O
dialogue O
gaps O
with O
a O
total O
duration O
of O
99.4 O
hours O
. O
The O
30,174 O
narration O
clips O
we O
collect O
fill O
in O
95.3 O
% O
of O
the O
dialogue O
gaps O
in O
terms O
of O
quantity O
and O
cover O
92.9 O
% O
in O
terms O
of O
duration O
. O
Therefore O
, O
it O
is O
reasonable O
to O
assume O
that O
where O
there O
are O
no O
lines O
, O
there O
is O
a O
need O
for O
narration O
. O

B O
Dataset O
Quality O
Description O

We O
adopt O
a O
two O
- O
stage O
annotation O
process O
to O
ensure O
the O
quality O
of O
the O
narrations O
. O
In O
the O
first O
stage O
, O
a O
group O
of O
workers O
is O
recruited O
to O
clean O
the O
data O
according O
to O
our O
guidelines O
. O
In O
the O
second O
stage O
, O
another O
group O
of O
workers O
further O
checks O
and O
corrects O
the O
annotation O
data O
. O
Our O
heuristics O
used O
to O
divide O
the O
paragraphs O
are O
designed O
based O
on O
our O
observation O
experience O
. O
We O
further O
conduct O
a O
manual O
evaluation O
of O
the O
narration O
quality O
. O
Of O
the O
randomly O
sampled O
300 O
paragraphs O
, O
( O
1 O
) O
in O
terms O
of O
narration O
recognition O
, O
96.7 O
% O
are O
textually O
consistent O
with O
original O
ADs O
; O
( O
2 O
) O
as O
for O
the O
paragraph O
coherence O
, O
90 O
% O
maintain O
complete O
and O
coherent O
semantics O
, O
7.7 O
% O
should O
be O
merged O
with O
contexts O
, O
and O
2.3 O
% O
should O
be O
divided O
into O
multiple O
paragraphs O
. O
Thus O
, O
the O
narration O
is O
of O
good O
quality O
to O
support O
downstream O
tasks O
. O

C O
Implementation O
Details O

Candidate O
Narrations O
. O
In O
Section O
4.3 O
, O
We O
provide O
5 O
different O
candidate O
narrations O
for O
each O
sampled O
movie O
clip O
for O
human O
evaluators O
to O
rank O
. O
These O
candidates O
are O
created O
as O
follows O
: O

1 O
. O
generated O
by O
the O
Vanilla B-MethodName
Transformer I-MethodName
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
; O
2 O
. O
generated O
by O
the O
OVP B-MethodName
model O
( O
Song O
et O
al O
. O
, O
2021 O
) O
; O
3 O
. O
generated O
by O
our O
proposed O
RMN B-MethodName
model O
; O
4 O
. O
generated O
by O
disturbing O
the O
ground O
truth O
with O
role O
name O
removal O
and O
replacement O
; O
5 O
. O
generated O
by O
disturbing O
the O
ground O
truth O
with O
nouns O
and O
verbs O
replacement O
. O

Metrics O
Implementation O
. O
For O
CLIP O
- O
based O
metrics O
including O
CLIPScore B-MetricName
and O
EMScore B-MetricName
, O
we O
finetune O
ChineseCLIP B-MethodName
- O
huge O
on O
our O
dataset O
in O
the O
same O
way O
as O
in O
Section O
5.2 O
. O
For O
each O
movie O
clip O
and O
generated O
narration O
, O
CLIP B-MetricName
- I-MetricName
Score I-MetricName
is O
calculated O
with O
the O
mean O
pooled O
feature O
of O
10 O
uniformly O
selected O
frames O
and O
the O
overall O
text O
feature O
, O
while O
EMScore B-MetricName
is O
calculated O
with O
all O
selected O
frame O
features O
and O
textual O
token O
features O
. O

For O
BERTScore B-MetricName
, O
we O
use O
the O
BERT B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
Chinese I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
model O
checkpoint O
to O
calculate O
, O
and O
rescale O
the O
raw O
BERTScore B-MetricName
with O
baseline O
8 O
. O
For O
DIV O
, O
we O
calculate O
1 O
- O
gram O
diversity O
and O
2 O
- O
gram O
diversity O
following O
Shetty O
et O
al O
. O
( O
2017 O
) O
, O
and O
average O
them O
. O
For O
PPL B-MetricName
, O
we O
obtain O
the O
perplexity O
of O
each O
narration O
with O
the O
causal O
Ernie O
3.0 O
model O
( O
Sun O
et O
al O
. O
, O
2021 O
) O
following O
the O
calculation O
of O
Hugging O
- O
Face O
9 O
. O
For O
RoleF1 B-MetricName
, O
we O
extract O
role O
names O
from O
the O
ground O
truth O
and O
the O
generated O
narration O
. O
We O
measure O
how O
the O
generated O
narration O
covers O
the O
roles O
appearing O
in O
the O
movie O
clip O
by O
Recall B-MetricName
; O
given O
that O
these O
generated O
role O
names O
may O
also O
come O
from O
the O
model O
's O
hallucination O
, O
for O
example O
from O
a O
wrong O
movie O
, O
we O
also O
take O
Precision B-MetricName
into O
account O
. O
Finally O
, O
we O
calculate O
the O
F1 B-MetricName
score O
with O
Precision B-MetricName
and O
Recall B-MetricName
. O
Hyperparameters O
and O
Computation O
. O
We O
detail O
the O
key O
hyperparameters O
and O
computational O
burden O
for O
the O
models O
training O
in O
Table O
7 O
. O
For O
each O
model O
, O
the O
results O
are O
derived O
from O
a O
single O
run O
. O

D.2 O
Temporal B-TaskName
Narration I-TaskName
Grounding I-TaskName

Fig O
. O
7 O
shows O
the O
qualitative O
results O
of O
our O
proposed O
two O
- O
stage O
method O
. O
Through O
Global B-TaskName
Shot I-TaskName
Retrieval I-TaskName
, O
we O
obtain O
an O
anchor O
shot O
near O
the O
target O
clip O
from O
the O
whole O
movie O
, O
which O
further O
helps O
Local B-TaskName
Temporal I-TaskName
Grounding I-TaskName
to O
locate O
the O
final O
target O
. O
GT O
: O
现场 O
⼤ O
屏幕上的数字转了起来， O
⻩ O
达和主持 O
⼈ O
转身看向 O
⼤ O
屏幕，数字转动了 O
⼀ O
会 O
⼉ O
之后停了下来， O
( O
The O
numbers O
on O
the O
big O
screen O
turn O
up O
, O
and O
HuangDa O
and O
the O
host O
turn O
to O
look O
at O
the O
big O
screen O
, O
the O
numbers O
turns O
for O
a O
while O
and O
then O
stops O
. O
) O

VT B-MethodName
: O
他们在台上观察着，时间间的位置上台下的观 O
⾳ O
室内， O
( O
They O
watch O
from O
the O
stage O
, O
the O
position O
between O
time O
on O
the O
stage O
in O
the O
chamber O
of O
the O
observer O
, O
) O

OVP B-MethodName
: O
第 O
⼆ O
天，三 O
⼈ O
来到现场，孟云和余 O
⻜ O
⼀ O
起看着屏幕上的选择题，三 O
⼈ O
离开了。 O
( O
The O
next O
day O
, O
the O
three O
come O
to O
the O
scene O
, O
MengYun O
and O
YuFei O
look O
at O
the O
multiple O
choice O
questions O
on O
the O
screen O
together O
, O
the O
three O
leave O
. O
) O
RMN O
: O
⻩ O
达看着台下的电脑，这时余 O
⻜ O
和 O
⻩ O
达拉着 O
⼿ O
来到台球厅，他们相互打量着这 O
⼀ O
切， O
( O
HuangDa O
looks O
at O
the O
computer O
under O
the O
stage O
, O
at O
this O
time O
, O
YuFei O
and O
HuangDa O
come O
to O
the O
billiard O
room O
with O
hands O
holding O
, O
they O
survey O
all O
this O
each O
other O
) O

GT O
: O
王多 O
⻥ O
站在保险公司 O
⼤ O
厦最顶层，穿着红 O
⾊ O
⾐ O
服绿裤衩， O
⼀ O
只 O
⼿ O
背在后 O
⾯ O
，另 O
⼀ O
只 O
⼿ O
扶着巨 O
⼤ O
的 O
" O
瘦 O
" O
字，双腿交叉带着 O
⼀ O
脸享受闭上双眼，倚靠在 O
" O
瘦 O
" O
字上 O
⾯ O
。镜头缓 O
缓上升拉伸，王多 O
⻥ O
变得越来越渺 O
⼩ O
，最后完全看不 O
⻅ O
了。 O
( O
WangDuoyu O
stands O
at O
the O
top O
of O
the O
insurance O
company O
building O
, O
wearing O
red O
clothes O
and O
green O
pants O
, O
one O
hand O
behind O
the O
back O
, O
the O
other O
hand O
holding O
the O
huge O
" O
thin O
" O
character O
, O
legs O
crossing O
, O
with O
a O
face O
of O
enjoyment O
, O
closing O
eyes O
, O
leaning O
on O
the O
" O
thin O
" O
character O
. O
The O
camera O
slowly O
rises O
, O
WangDuoyu O
becomes O
smaller O
and O
smaller O
, O
and O
finally O
completely O
invisible O
. O
) O

VT B-MethodName
: O
在众 O
⼈ O
的 O
⾼ O
举 O
⾏ O
下，张彪也在这 O
⼀ O
场，下 O
⾯ O
的 O
⾼ O
楼下，阿俊也摔倒在地上。 O
( O
In O
the O
crowd O
hold O
under O
the O
high O
, O
ZhangBiao O
is O
also O
in O
this O
scene O
, O
below O
the O
high O
floor O
, O
Arjun O
also O
falls O
to O
the O
ground O
. O
) O

OVP B-MethodName
: O
随着 O
⻜ O
机的轰鸣声，继续朝下抓捕， O
⽽ O
在楼梯上的临时，他选择了 O
⼀ O
个 O
⾼ O
挑的身 O
影，这时，焦急的他选择了 O
⼀ O
个按钮， O
⼤ O
厦向下 O
⻜ O
去。 O
( O
With O
the O
roar O
of O
the O
plane O
, O
continues O
capture O
downward O
, O
while O
on O
the O
stairs O
of O
the O
temporary O
, O
he O
chooses O
a O
tall O
figure O
, O
at O
this O
point O
, O
anxious O
he O
chooses O
a O
button O
, O
the O
building O
flies O
downward O
. O
) O

RMN B-MethodName
: O
随后，王多 O
⻥ O
在天台上朗着升机，踏上了 O
⾏ O
程，登基本的装饰演员，王多 O
⻥ O
独 O
⾃ O
在空中 O
， O
美丽在云 O
⼤ O
楼 O
⾥ O
摆着各种姿势 O
， O
王多 O
⻥ O
顺着绳索向上攀爬 O
( O
Subsequently O
, O
WangDuoyu O
boards O
on O
the O
rooftop O
… O
, O
embarking O
on O
a O
trip O
, O
… O
decorative O
actors O
, O
WangDuoyu O
is O
in O
the O
air O
alone O
, O
posing O
in O
a O
variety O
of O
positions O
beautifully O
in O
the O
cloud O
building O
; O
WangDuoyu O
climbs O
upward O
along O
the O
rope O
) O

GT O
: O
桃 O
⼦ O
边说话边拦下 O
⼀ O
辆出租 O
⻋ O
，然后坐上 O
⻋ O
快速离去了。 O
⻩ O
达 O
⼀ O
个 O
⼈ O
愣在原地 O
。 O
( O
Taozi O
stops O
a O
cab O
as O
she O
talks O
, O
then O
gets O
in O
and O
quickly O
leaves O
. O
HuangDa O
freezes O
alone O
. O
) O

VT B-MethodName
: O
卢 O
⼩ O
⻥ O
看到了他的眼神，他低头看着他，然后低下头， O
( O
LuXiaoyu O
sees O
the O
look O
in O
his O
eyes O
, O
and O
he O
looks O
down O
at O
him O
, O
then O
lowers O
his O
head O
, O
) O

OVP B-MethodName
: O
江丰回过头来看着他，然后叹了 O
⼝ O
⽓ O
， O
( O
JiangFeng O
looks O
back O
at O
him O
, O
then O
sighs O
, O
) O
In O
the O
narration O
texts O
, O
green O
and O
red O
characters O
denote O
the O
correctly O
and O
wrongly O
generated O
role O
names O
, O
respectively O
. O
In O
the O
tables O
, O
metrics O
in O
green O
indicate O
that O
the O
ranking O
of O
candidates O
by O
the O
metric O
is O
consistent O
with O
human O
ranking O
, O
while O
red O
indicates O
inconsistency O
. O

Acknowledgements O

This O
work O
was O
partially O
supported O
by O
the O
National O
Key O
R O
& O
D O
Program O
of O
China O
( O
No.2020AAA0108600 O
) O
and O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62072462 O
) O
. O

Query O
: O
影 O
⽚ O
开始挂满鲜花的欧式 O
⼤ O
铁 O
⻔ O
缓缓打开，铁 O
⻔ O
内是 O
⼀ O
座欧式建筑 O
， O
⼀ O
辆 O
⼩ O
汽 O
⻋ O
从 O
⻔ O
外 O
⾏ O
驶 O
⽽ O
进，它穿过摆满花束的院 O
⼦ O
中， O
⼀ O
名保安在礼堂前 O
⼀ O
⼿ O
拿起路障， O
⼀ O
⼿ O
指挥着汽 O
⻋ O
向前，这辆 O
⻋ O
没有停下 O
。 O
( O
At O
the O
beginning O
of O
the O
film O
, O
a O
large O
European O
- O
style O
iron O
gate O
full O
of O
flowers O
slowly O
opens O
. O
Inside O
the O
iron O
gate O
is O
a O
European O
- O
style O
building O
. O
A O
car O
drives O
in O
through O
the O
gate O
, O
and O
crosses O
the O
courtyard O
full O
of O
flowers O
. O
A O
security O
guard O
holds O
a O
barricade O
in O
front O
of O
the O
auditorium O
with O
one O
hand O
and O
directs O
the O
car O
forward O
with O
the O
other O
; O
this O
car O
does O
not O
stop O
. O
) O

( O
Another O
day O
, O
a O
paper O
airplane O
flies O
through O
the O
roof O
of O
the O
school O
building O
. O
QiuYa O
is O
standing O
alone O
on O
the O
roof O
, O
when O
YuanHua O
slowly O
walks O
over O
. O
QiuYa O
takes O
a O
look O
at O
YuanHua O
and O
then O
frowns O
, O
while O
touching O
her O
pigtails O
, O
while O
lowering O
her O
head O
, O
YuanHua O
spits O
out O
his O
mouth O
and O
asks O
with O
tears O
. O
) O
Section O
: O
Appendix O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Section O
: O
Appendix O

GT O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Deconfounding O
Legal B-TaskName
Judgment I-TaskName
Prediction I-TaskName
for O
European O
Court O
of O
Human O
Rights O
Cases O
Towards O
Better O
Alignment O
with O
Experts O

This O
work O
demonstrates O
that O
Legal B-TaskName
Judgement I-TaskName
Prediction I-TaskName
systems O
without O
expert O
- O
informed O
adjustments O
can O
be O
vulnerable O
to O
shallow O
, O
distracting O
surface O
signals O
that O
arise O
from O
corpus O
construction O
, O
case O
distribution O
, O
and O
confounding O
factors O
. O
To O
mitigate O
this O
, O
we O
use O
domain O
expertise O
to O
strategically O
identify O
statistically O
predictive O
but O
legally O
irrelevant O
information O
. O
We O
adopt O
adversarial O
training O
to O
prevent O
the O
system O
from O
relying O
on O
it O
. O
We O
evaluate O
our O
deconfounded O
models O
by O
employing O
interpretability O
techniques O
and O
comparing O
to O
expert O
annotations O
. O
Quantitative O
experiments O
and O
qualitative O
analysis O
show O
that O
our O
deconfounded O
model O
consistently O
aligns O
better O
with O
expert O
rationales O
than O
baselines O
trained O
for O
prediction O
only O
. O
We O
further O
contribute O
a O
set O
of O
reference O
expert O
annotations O
to O
the O
validation O
and O
testing O
partitions O
of O
an O
existing O
benchmark O
dataset O
of O
European O
Court O
of O
Human O
Rights O
cases O
. O

Introduction O

The O
task O
of O
Legal B-TaskName
Judgment I-TaskName
Prediction I-TaskName
( O
LJP B-TaskName
) O
has O
recently O
gained O
increasing O
attention O
in O
the O
legal O
and O
mainstream O
NLP O
communities O
( O
Aletras O
et O
al O
. O
, O
2016 O
; O
Zhong O
et O
al O
. O
, O
2018 O
; O
Medvedeva O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Sert O
et O
al O
. O
, O
2021 O
) O
. O
Legal O
cases O
are O
resolved O
through O
the O
exchange O
of O
arguments O
in O
front O
of O
a O
decision O
body O
by O
lawyers O
who O
represent O
litigating O
parties O
. O
This O
typically O
involves O
evidential O
reasoning O
, O
the O
determination O
of O
relevant O
rules O
from O
sources O
of O
law O
( O
e.g. O
, O
codes O
, O
regulations O
, O
precedent O
) O
, O
their O
application O
to O
the O
case O
, O
and O
the O
balancing O
of O
legal O
and O
societal O
values O
. O
In O
the O
NLP O
context O
, O
LJP B-TaskName
takes O
the O
form O
of O
classifying O
the O
outcome O
of O
a O
case O
from O
some O
textual O
representation O
of O
its O
specific O
facts O
, O
effectively O
skipping O
legal O
reasoning O
. O
This O
forms O
a O
counterpoint O
to O
knowledge O
- O
focused O
approaches O
to O
outcome O
prediction O
( O
e.g. O
, O
Brüninghaus O
and O
Ashley O
, O
2005 O
; O
Branting O
, O
2013 O
; O
Grabmair O
, O
Figure O
1 O
: O
Our O
deconfounding O
experiment O
architecture O
2017 O
) O
that O
connect O
to O
a O
lawyer O
's O
understanding O
of O
the O
domain O
but O
also O
require O
substantial O
knowledge O
engineering O
. O

This O
carries O
particular O
risk O
in O
the O
legal O
domain O
, O
where O
systems O
may O
rely O
on O
data O
elements O
that O
are O
statistically O
predictive O
but O
legally O
irrelevant O
, O
or O
even O
forbidden O
as O
decision O
criteria O
( O
e.g. O
, O
the O
race O
of O
an O
accused O
person O
) O
. O
This O
can O
lead O
to O
undesirable O
consequences O
, O
ranging O
from O
suboptimal O
litigation O
strategy O
decisions O
, O
flawed O
inference O
about O
factors O
predictive O
for O
the O
outcome O
, O
to O
disparate O
impact O
of O
decisions O
across O
groups O
that O
are O
to O
be O
treated O
equally O
. O
If O
legal O
decisions O
are O
to O
be O
informed O
by O
predictive O
systems O
processing O
textual O
case O
descriptions O
, O
then O
such O
systems O
must O
strive O
to O
be O
as O
closely O
aligned O
with O
legally O
relevant O
and O
permissible O
parts O
of O
the O
input O
as O
possible O
. O

In O
this O
work O
, O
we O
focus O
on O
LJP B-TaskName
for O
the O
European O
Court O
of O
Human O
Rights O
( O
ECtHR O
) O
, O
which O
adjudicates O
complaints O
by O
individuals O
against O
states O
about O
alleged O
violations O
of O
their O
rights O
as O
enshrined O
in O
the O
European O
Convention O
of O
Human O
Rights O
. O
We O
trained O
deep O
neural O
models O
on O
four O
tasks O
across O
two O
existing O
, O
related O
datasets O
( O
Chalkidis O
et O
al O
. O
, O
2019 O
( O
Chalkidis O
et O
al O
. O
, O
, O
2022a O
around O
predicting O
such O
violations O
alleged O
by O
the O
claimant O
and O
decided O
by O
the O
court O
. O
We O
find O
that O
the O
models O
substantially O
base O
their O
predictions O
on O
aspects O
of O
the O
text O
that O
correlate O
with O
the O
outcome O
but O
either O
have O
no O
legal O
bearing O
or O
are O
forbidden O
nationality O
- O
related O
information O
that O
stem O
from O
the O
distribution O
of O
cases O
arising O
at O
the O
court O
. O

To O
improve O
the O
alignment O
of O
model O
focus O
with O
legal O
expert O
understanding O
, O
we O
apply O
a O
series O
of O
deconfounding O
measures O
, O
including O
a O
vocabularybased O
method O
which O
identifies O
predictive O
tokens O
using O
a O
simple O
model O
. O
The O
third O
author O
, O
who O
is O
an O
ECtHR O
expert O
, O
then O
identifies O
distractors O
among O
them O
. O
The O
distracting O
signal O
can O
subsequently O
be O
removed O
from O
the O
encodings O
via O
adversarial O
training O
. O
This O
procedure O
is O
an O
effective O
way O
of O
engaging O
with O
domain O
experts O
and O
obtaining O
information O
about O
what O
the O
model O
should O
be O
steered O
away O
from O
by O
means O
of O
deconfounding O
, O
rather O
than O
trying O
to O
attract O
the O
model O
towards O
relevant O
elements O
via O
expensive O
data O
collection O
for O
supervised O
training O
. O
For O
simplicity O
, O
throughout O
this O
paper O
, O
we O
use O
' O
deconfounding O
' O
in O
an O
inclusive O
sense O
as O
the O
mitigation O
of O
distracting O
effects O
of O
( O
a O
) O
confounders O
in O
the O
statistical O
sense O
that O
influence O
both O
the O
dependent O
and O
independent O
variables O
, O
( O
b O
) O
reverse O
causation O
relationships O
, O
and O
( O
c O
) O
other O
attributes O
that O
spuriously O
correlate O
with O
the O
target O
variable O
. O
See O
Fig O
. O
1 O
for O
an O
overview O
of O
our O
experiment O
design O
. O

We O
evaluate O
our O
trained O
and O
deconfounded O
models O
with O
regard O
to O
an O
alignment O
of O
its O
explanation O
rationales O
with O
( O
1 O
) O
a O
dataset O
of O
expert O
passage O
relevance O
assessments O
we O
collected O
and O
will O
make O
available O
to O
community O
as O
a O
supplement O
to O
Chalkidis O
et O
al O
. O
( O
2019 O
) O
, O
and O
( O
2 O
) O
on O
expert O
relevance O
assessments O
published O
as O
part O
of O
Chalkidis O
et O
al O
. O
( O
2021 O
) O
. O
Our O
results O
show O
that O
our O
deconfounding O
steps O
succeed O
in O
improving O
the O
model O
focus O
alignment O
with O
expert O
- O
identified O
, O
relevant O
patterns O
on O
both O
sets O
of O
reference O
annotations O
. O

In O
sum O
, O
we O
make O
the O
following O
contributions O
: O

• O
We O
introduce O
an O
expert B-MethodName
- I-MethodName
informed I-MethodName
deconfounding I-MethodName
method I-MethodName
which O
identifies O
distracting O
effects O
from O
confounders O
and O
spurious O
correlations O
using O
a O
simple O
model O
, O
and O
mitigates O
them O
through O
adversarial O
training O
, O
thus O
helping O
to O
improve O
the O
alignment O
of O
the O
model O
focus O
with O
legal O
expert O
rationales O
. O

• O
We O
empirically O
evaluate O
this O
method O
on O
four O
tasks O
in O
legal B-TaskName
judgment I-TaskName
prediction I-TaskName
on O
ECtHR B-DatasetName
data O
and O
show O
that O
our O
model O
consistently O
aligns O
better O
with O
expert O
rationales O
than O
a O
baseline O
trained O
for O
the O
prediction O
target O
only O
. O

• O
We O
release O
a O
set O
of O
gold O
rationales O
annotated O
by O
an O
ECtHR B-DatasetName
expert O
as O
a O
supplement O
to O
an O
existing O
dataset O
to O
facilitate O
future O
work O
on O
deriving O
more O
useful O
insight O
from O
trained O
predictive O
systems O
in O
the O
legal O
domain O
. O
* O

Related O
Work O

LJP B-TaskName
as O
an O
NLP O
task O
has O
been O
tackled O
using O
ngram O
representations O
( O
e.g. O
, O
Aletras O
et O
al O
. O
, O
2016 O
; O
Medvedeva O
et O
al O
. O
, O
2020 O
) O
, O
word O
embeddings O
and O
domain O
models O
( O
Branting O
et O
al O
. O
, O
2021 O
) O
, O
and O
deep O
neural O
networks O
( O
e.g. O
, O
Chalkidis O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2020 O
) O
. O
Special O
attention O
must O
be O
given O
to O
the O
origin O
of O
the O
text O
from O
which O
the O
prediction O
is O
to O
be O
made O
. O
Medvedeva O
et O
al O
. O
( O
2021Medvedeva O
et O
al O
. O
( O
, O
2022 O
recharacterize O
LJP B-TaskName
on O
texts O
produced O
before O
the O
outcome O
is O
known O
as O
' O
forecasting O
' O
and O
observes O
that O
most O
current O
works O
' O
classify O
' O
judgments O
based O
on O
the O
data O
compiled O
after O
the O
outcome O
has O
been O
determined O
. O
They O
also O
find O
that O
forecasting O
is O
a O
harder O
task O
. O
This O
result O
is O
consistent O
with O
our O
finding O
of O
confounding O
effects O
from O
text O
production O
by O
the O
ECtHR B-DatasetName
, O
resulting O
in O
a O
prediction O
from O
fact O
descriptions O
that O
were O
influenced O
by O
the O
decision O
. O
Moverover O
, O
the O
relationship O
between O
the O
information O
LJP B-TaskName
models O
rely O
on O
and O
legal O
expert O
analysis O
of O
texts O
remains O
underexplored O
. O
Bhambhoria O
et O
al O
. O
( O
2021 O
) O
find O
that O
transformer O
- O
based O
models O
exploit O
spurious O
correlations O
and O
that O
simple O
models O
, O
such O
as O
XGBoost O
, O
can O
achieve O
similar O
performance O
. O
Chalkidis O
et O
al O
. O
( O
2021 O
) O
extract O
model O
rationales O
for O
alleged O
violation O
prediction O
and O
observes O
limited O
overlap O
with O
expert O
markup O
. O
Similarly O
, O
a O
small O
study O
in O
Branting O
et O
al O
. O
( O
2021 O
) O
finds O
that O
users O
do O
not O
perceive O
case O
prediction O
- O
derived O
highlighting O
as O
useful O
in O
making O
predictions O
themselves O
. O
Our O
work O
contributes O
to O
this O
state O
of O
the O
art O
by O
using O
adversarial O
deconfounding O
to O
improve O
the O
overlap O
between O
what O
systems O
predict O
from O
with O
what O
legal O
experts O
consider O
relevant O
. O
Deconfounding O
A O
growing O
number O
of O
works O
have O
raised O
awareness O
that O
deep O
neural O
models O
may O
exploit O
spurious O
statistical O
patterns O
and O
take O
erroneous O
shortcuts O
( O
McCoy O
et O
al O
. O
, O
2019 O
; O
Bender O
and O
Koller O
, O
2020 O
; O
Geirhos O
et O
al O
. O
, O
2020 O
) O
. O
A O
common O
method O
of O
mitigating O
this O
is O
adversarial O
learning O
. O
Pryzant O
et O
al O
. O
2018 O
use O
a O
gradient O
reversal O
layer O
( O
Ganin O
et O
al O
. O
, O
2016 O
) O
to O
deconfound O
lexicons O
in O
text O
classification O
. O
Other O
domains O
that O
adopt O
adversarial O
training O
to O
eliminate O
confounders O
include O
bioinformatics O
( O
Dincer O
et O
al O
. O
, O
2020 O
) O
and O
political O
science O
( O
Roberts O
et O
al O
. O
, O
2020 O
) O
. O
Many O
existing O
works O
on O
identifying O
shortcuts O
focus O
on O
situations O
where O
these O
patterns O
are O
known O
in O
advance O
and O
may O
require O
potentially O
expensive O
data O
collection O
. O
In O
fairness O
- O
focused O
legal O
NLP O
, O
Chalkidis O
et O
al O
. O
( O
2022b O
) O
observe O
and O
remedy O
group O
disparities O
in O
LJP B-TaskName
performance O
on O
the O
EC O
- O
tHR O
informed O
by O
metadata O
attributes O
( O
respondent O
state O
, O
applicant O
gender O
, O
applicant O
age O
) O
. O
We O
extend O
this O
to O
explainability O
in O
LJP B-TaskName
by O
involving O
a O
legal O
expert O
in O
a O
procedure O
that O
allows O
an O
efficient O
, O
incremental O
identification O
of O
distracting O
information O
, O
as O
well O
as O
its O
removal O
via O
adversarial O
training O
. O
Interpretability O
We O
employ O
interpretability O
techniques O
to O
evaluate O
model O
alignment O
with O
expert O
rationales O
. O
Danilevsky O
et O
al O
. O
( O
2020 O
) O
reviews O
and O
categorizes O
the O
main O
current O
interpretability O
methods O
. O
Though O
initial O
works O
( O
Ghaeini O
et O
al O
. O
, O
2018 O
; O
Lee O
et O
al O
. O
, O
2017 O
) O
used O
attention O
scores O
as O
explanation O
for O
model O
decisions O
, O
Bastings O
and O
Filippova O
( O
2020 O
) O
; O
Serrano O
and O
Smith O
( O
2019 O
) O
point O
out O
that O
saliency O
methods O
, O
such O
as O
gradient O
based O
methods O
( O
Sundararajan O
et O
al O
. O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2016 O
) O
, O
propagation O
based O
methods O
( O
Bach O
et O
al O
. O
, O
2015 O
) O
, O
occlusion O
based O
methods O
( O
Zeiler O
and O
Fergus O
, O
2014 O
) O
, O
and O
surrogate O
model O
based O
methods O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
are O
better O
suited O
for O
explainability O
analysis O
. O
However O
, O
the O
reliability O
and O
informativeness O
of O
these O
methods O
remains O
an O
open O
research O
problem O
. O
Our O
model O
uses O
the O
currently O
most O
commonly O
used O
Integrated O
Gradients O
( O
IG O
) O
( O
Sundararajan O
et O
al O
. O
, O
2017 O
) O
, O
which O
computes O
the O
gradient O
of O
the O
model O
's O
output O
with O
respect O
to O
its O
input O
features O
. O

ECtHR B-DatasetName
Tasks O
& O
Datasets O

The O
ECtHR B-DatasetName
has O
been O
the O
subject O
of O
substantial O
prior O
work O
in O
LJP B-TaskName
. O
We O
use O
two O
datasets O
for O
model O
training O
and O
evaluation O
: O
First O
, O
for O
binary O
violation O
we O
use O
the O
dataset O
by O
Chalkidis O
et O
al O
. O
( O
2019 O
) O
of O
approx O
. O
11k O
case O
fact O
statements O
, O
where O
the O
target O
is O
to O
predict O
whether O
the O
court O
has O
found O
at O
least O
one O
convention O
article O
to O
be O
violated O
. O
To O
evaluate O
alignment O
, O
we O
annotate O
50 O
( O
25 O
each O
) O
expert O
rationales O
for O
cases O
from O
both O
the O
development O
and O
test O
partitions O
( O
See O
App O
. O
C O
for O
the O
annotation O
process O
) O
. O
Second O
, O
for O
article O
- O
specific O
violation O
, O
we O
use O
the O
LexGLUE B-DatasetName
dataset O
by O
Chalkidis O
et O
al O
. O
( O
2022a O
) O
, O
which O
consists O
of O
11k O
case O
fact O
statements O
along O
with O
information O
about O
which O
convention O
articles O
have O
been O
alleged O
to O
be O
violated O
, O
and O
which O
the O
court O
has O
found O
to O
be O
violated O
. O
For O
alignment O
, O
we O
merge O
this O
data O
with O
the O
50 O
test O
set O
rationales O
from O
Chalkidis O
et O
al O
. O
( O
2021 O
) O
. O
While O
both O
datasets O
stem O
from O
the O
EC B-DatasetName
- I-DatasetName
tHR I-DatasetName
's O
public O
database O
, O
they O
differ O
in O
case O
facts O
and O
outcome O
distribution O
as O
we O
explain O
in O
Sec O
. O
3.1 O
. O
The O
input O
texts O
consist O
of O
each O
case O
's O
FACTS O
section O
extracted O
from O
ECtHR B-DatasetName
judgments O
. O
This O
section O
is O
drafted O
by O
court O
staff O
over O
the O
course O
of O
the O
case O
proceedings O
. O
While O
it O
does O
not O
contain O
the O
outcome O
explicitly O
, O
it O
is O
not O
finalized O
before O
the O
final O
decision O
has O
been O
determined O
, O
potentially O
creating O
confounding O
effects O
. O

We O
conduct O
experiments O
on O
four O
LJP B-TaskName
tasks O
: O
Task O
J O
-Binary B-TaskName
Violation I-TaskName
For O
our O
task O
J O
, O
the O
model O
is O
given O
a O
fact O
statement O
and O
is O
asked O
to O
predict O
whether O
or O
not O
any O
article O
of O
the O
convention O
has O
been O
violated O
. O
We O
train O
our O
models O
on O
Chalkidis O
et O
al O
. O
( O
2019 O
) O
and O
evaluate O
alignment O
on O
the O
set O
of O
expert O
rationales O
we O
collected O
. O
Task O
B O
-Article B-TaskName
Allegation I-TaskName
We O
train O
and O
evaluate O
on O
LexGLUE B-DatasetName
's O
ECtHR B-DatasetName
B O
, O
* O
where O
the O
fact O
description O
is O
the O
basis O
to O
predict O
the O
set O
of O
convention O
articles O
that O
the O
claimant O
alleges O
to O
have O
been O
violated O
. O
It O
can O
be O
conceptualized O
as O
topic O
classification O
in O
that O
the O
system O
needs O
to O
identify O
suitable O
candidate O
articles O
( O
e.g. O
, O
the O
right O
to O
respect O
for O
private O
and O
family O
life O
) O
from O
fact O
statements O
( O
e.g. O
, O
about O
government O
surveillance O
) O
. O
We O
test O
alignment O
on O
the O
expert O
rationales O
by O
Chalkidis O
et O
al O
. O
( O
2021 O
) O
. O
Task O
A O
-Article B-TaskName
Violation I-TaskName
We O
also O
experiment O
with O
LexGLUE B-DatasetName
's O
ECtHR B-DatasetName
A O
, O
which O
is O
to O
predict O
which O
of O
the O
convention O
's O
articles O
has O
been O
deemed O
violated O
by O
the O
court O
from O
a O
case O
's O
fact O
description O
. O
Task O
A O
is O
a O
more O
difficult O
version O
of O
task O
B O
, O
where O
both O
an O
identification O
of O
suitable O
articles O
and O
a O
prediction O
of O
their O
violation O
must O
be O
performed O
. O
For O
alignment O
, O
we O
again O
use O
the O
expert O
rationales O
by O
Chalkidis O
et O
al O
. O
( O
2021 O
) O
, O
which O
are O
technically O
intended O
for O
task O
ECtHR B-DatasetName
B O
, O
but O
which O
we O
consider O
to O
also O
be O
suitable O
for O
an O
evaluation O
of O
task O
A. O
* O
Task O
A|B O
-Article B-TaskName
Violation I-TaskName
given I-TaskName
Allegation I-TaskName
We O
further O
disentangle O
the O
LexGLUE B-DatasetName
tasks O
and O
pose O
ECtHR B-DatasetName
A|B. O
Given O
the O
facts O
of O
a O
case O
and O
the O
allegedly O
violated O
articles O
, O
the O
model O
should O
predict O
which O
( O
if O
any O
) O
specific O
articles O
have O
been O
violated O
. O
This O
task O
reflects O
the O
legal O
process O
, O
as O
the O
court O
is O
aware O
of O
allegations O
made O
by O
the O
applicants O
when O
deciding O
. O
Providing O
information O
about O
the O
allegations O
shifts O
the O
nature O
of O
the O
task O
from O
topic O
classification O
to O
article O
- O
specific O
violation O
/ O
non O
- O
violation O
prediction O
, O
thus O
refocusing O
the O
model O
and O
ideally O
leading O
to O
violation O
- O
specific O
explanations O
. O

Data O
Distribution O
& O
Preprocessing O

In O
order O
to O
facilitate O
model O
alignment O
, O
we O
worked O
with O
our O
ECtHR B-DatasetName
expert O
to O
identify O
shallow O
prediction O
signals O
in O
the O
fact O
statements O
that O
are O
unrelated O
to O
the O
legal O
merits O
of O
the O
complaint O
. O

Length O
and O
Respondent O
State O

For O
the O
task O
J O
dataset O
of O
Chalkidis O
et O
al O
. O
2019 O
, O
we O
find O
that O
the O
distribution O
of O
fact O
description O
length O
( O
number O
of O
sentences O
) O
and O
the O
distribution O
of O
respondent O
states O
are O
different O
between O
the O
two O
classes O
( O
see O
Appendix O
A O
) O
. O
We O
hence O
account O
for O
the O
identity O
of O
the O
respondent O
state O
and O
the O
length O
of O
the O
fact O
descriptions O
via O
our O
deconfounding O
procedure O
for O
both O
datasets O
. O

Accounting O
for O
Inadmissible O
Cases O

We O
also O
observe O
in O
the O
task O
J O
dataset O
that O
the O
magnitudes O
of O
the O
running O
paragraph O
numbers O
differ O
between O
the O
classes O
, O
and O
that O
the O
single O
word O
" O
represented O
" O
strongly O
correlates O
with O
the O
positive O
class O
. O
This O
phenomenon O
arises O
because O
2.6k O
of O
the O
7k O
training O
cases O
are O
' O
inadmissible O
' O
cases O
labeled O
as O
' O
non O
- O
violation O
' O
. O
Legally O
, O
inadmissible O
cases O
are O
not O
necessarily O
' O
non O
- O
violation O
' O
as O
inadmissibility O
relates O
to O
complaints O
not O
fulfilling O
the O
court O
's O
formal O
or O
procedural O
criteria O
. O
* O
In O
such O
cases O
, O
the O
court O
does O
not O
examine O
the O
merits O
of O
the O
application O
. O
The O
more O
interesting O
non O
- O
violation O
cases O
are O
such O
that O
are O
admissible O
, O
but O
in O
which O
no O
violation O
of O
the O
convention O
has O
been O
found O
. O
The O
single O
negative O
class O
contains O
instances O
of O
both O
inadmissible O
and O
admissible O
- O
but O
- O
no O
- O
violation O
- O
found O
cases O
. O
As O
explained O
above O
, O
the O
input O
texts O
of O
Chalkidis O
et O
al O
. O
2019 O
are O
extracted O
from O
the O
FACTS O
section O
of O
full O
ECtHR B-DatasetName
decisions O
. O
In O
inadmissible O
cases O
, O
the O
applicant O
's O
background O
information O
can O
typically O
be O
found O
at O
the O
beginning O
of O
that O
section O
. O
We O
found O
that O
almost O
all O
inadmissible O
case O
facts O
start O
with O
* O
For O
example O
, O
the O
applicants O
lodge O
the O
complaint O
outside O
the O
time O
limit O
after O
the O
final O
domestic O
judicial O
decision O
or O
fail O
to O
exhaust O
required O
domestic O
remedies O
before O
complaining O
to O
the O
ECtHR B-DatasetName
, O
etc O
. O
It O
should O
be O
noted O
that O
the O
majority O
of O
inadmissible O
cases O
are O
decided O
by O
single O
judges O
and O
not O
available O
on O
the O
public O
database O
HUDOC O
. O

the O
same O
formulaic O
sentence O
stating O
the O
applicant O
's O
name O
, O
nationality O
, O
and O
legal O
representation O
. O
This O
specific O
sentence O
is O
absent O
from O
the O
texts O
of O
admissible O
cases O
( O
violation O
and O
non O
- O
violation O
) O
, O
where O
that O
information O
is O
part O
of O
a O
separate O
PROCEDURE O
section O
not O
included O
in O
the O
dataset O
. O
Moreover O
, O
due O
to O
the O
PROCEDURE O
section O
preceding O
the O
FACTS O
section O
in O
admissible O
cases O
, O
the O
running O
paragraph O
numbers O
appearing O
in O
FACTS O
sections O
of O
inadmissible O
cases O
are O
smaller O
than O
those O
of O
the O
admissible O
cases O
. O
If O
not O
remedied O
, O
these O
phenomena O
provide O
a O
considerable O
predictive O
signal O
for O
the O
label O
and O
distract O
the O
system O
from O
legally O
relevant O
information O
. O
In O
our O
experiments O
, O
we O
hence O
remove O
paragraph O
numbers O
from O
the O
input O
via O
preprocessing O
and O
account O
for O
distractor O
vocabulary O
via O
our O
deconfounding O
procedure O
described O
in O
Sec O
. O
4 O
. O
Still O
, O
the O
nature O
of O
task O
J O
remains O
unchanged O
and O
requires O
the O
system O
to O
classify O
the O
outcomes O
of O
a O
collection O
of O
both O
admissible O
and O
inadmissible O
cases O
. O

Article O
- O
Specific O
Violation O

By O
contrast O
, O
the O
more O
recent O
LexGLUE B-DatasetName
dataset O
only O
contains O
admissible O
cases O
and O
corresponding O
information O
about O
which O
articles O
the O
claimant O
has O
alleged O
to O
have O
been O
violated O
( O
for O
task O
B O
) O
along O
with O
those O
that O
the O
court O
has O
found O
to O
have O
been O
violated O
, O
if O
any O
( O
task O
A O
) O
. O
The O
collection O
covers O
10 O
different O
convention O
articles O
that O
make O
up O
the O
largest O
share O
of O
ECtHR B-DatasetName
jurisprudence O
. O
Each O
article O
has O
been O
alleged O
in O
a O
partition O
of O
the O
cases O
, O
and O
has O
been O
found O
to O
be O
violated O
in O
a O
subset O
of O
these O
. O
* O
For O
a O
given O
article O
in O
task O
B O
, O
all O
cases O
in O
which O
it O
has O
been O
alleged O
can O
be O
considered O
positive O
instances O
while O
the O
remaining O
cases O
are O
negatives O
. O
We O
consider O
task O
B O
as O
akin O
to O
topic O
classification O
, O
where O
the O
rights O
enshrined O
in O
the O
convention O
articles O
( O
e.g. O
, O
Art O
. O
6 O
: O
right O
to O
a O
fair O
trial O
; O
Art O
. O
1 O
Protocol O
1 O
: O
protection O
of O
property O
, O
etc O
. O
) O
may O
correlate O
with O
certain O
case O
fact O
language O
( O
e.g. O
, O
related O
to O
law O
enforcement O
or O
expropriation O
, O
respectively O
) O
. O
Task O
A O
incorporates O
this O
step O
and O
adds O
violation O
prediction O
per O
article O
, O
which O
is O
more O
difficult O
in O
principle O
. O
However O
, O
we O
observe O
that O
a O
few O
articles O
account O
for O
a O
large O
portion O
of O
the O
data O
and O
the O
conditional O
probability O
of O
a O
positive O
violation O
label O
in O
task O
A O
given O
its O
allegation O
labels O
from O
task O
B O
can O
be O
very O
high O
( O
see O
App O
. O
B O
) O
. O
This O
makes O
an O
analysis O
of O
what O
trained O
models O
focus O
on O
more O
difficult O
, O
since O
they O
may O
learn O
to O
identify O
these O
dominant O
articles O
with O
high O
conditional O
violation O
probability O
, O
and O
be O
distracted O
from O
focusing O
on O
information O
that O
specifically O
signals O
violations O
of O
those O
articles O
. O
To O
remedy O
this O
, O
we O
propose O
task O
A|B O
that O
provides O
models O
an O
easy O
access O
to O
the O
label O
information O
of O
B O
, O
facilitating O
their O
focus O
only O
on O
determining O
whether O
the O
court O
finds O
a O
violation O
of O
given O
articles O
. O
This O
task O
is O
realistic O
since O
the O
allegations O
by O
the O
claimant O
are O
known O
to O
the O
court O
at O
the O
time O
that O
it O
decides O
whether O
the O
respondent O
state O
has O
violated O
the O
convention O
in O
the O
case O
. O

Expert B-MethodName
- I-MethodName
Informed I-MethodName
Deconfounding I-MethodName

We O
apply O
an O
expert B-MethodName
- I-MethodName
informed I-MethodName
deconfounding I-MethodName
method O
designed O
to O
mitigate O
the O
distracting O
effects O
of O
confounding O
elements O
and O
spurious O
correlations O
. O
As O
Pryzant O
et O
al O
. O
( O
2018 O
) O
observe O
, O
accounting O
for O
confounders O
is O
common O
practice O
throughout O
many O
data O
analysis O
tasks O
to O
capture O
the O
intended O
signal O
and O
facilitate O
explainable O
models O
. O
In O
LJP B-TaskName
, O
we O
understand O
confounding O
elements O
as O
such O
that O
influence O
both O
the O
observed O
legal O
outcome O
( O
convention O
violations O
found O
by O
the O
court O
as O
coded O
in O
the O
dataset O
) O
and O
the O
input O
text O
from O
which O
this O
outcome O
is O
to O
be O
predicted O
( O
here O
: O
ECtHR B-DatasetName
fact O
statements O
) O
. O
Already O
covered O
examples O
are O
the O
different O
distribution O
of O
information O
across O
sections O
for O
admissible O
and O
inadmissible O
cases O
and O
the O
length O
of O
the O
fact O
descriptions O
( O
inadmissible O
cases O
tend O
to O
require O
less O
factual O
information O
to O
be O
dismissed O
) O
. O
* O
An O
example O
of O
a O
spurious O
correlation O
is O
the O
identity O
of O
the O
respondent O
state O
( O
certain O
article O
violations O
will O
be O
claimed O
more O
often O
against O
a O
small O
number O
of O
governments O
, O
leading O
to O
a O
correlation O
) O
. O
They O
each O
should O
have O
no O
bearing O
on O
the O
probability O
of O
an O
outcome O
in O
a O
given O
case O
as O
a O
judge O
will O
not O
decide O
against O
a O
violation O
because O
the O
facts O
are O
short O
, O
or O
because O
the O
case O
is O
against O
a O
particular O
government O
. O

Confounding O
effects O
and O
spurious O
information O
in O
LJP B-TaskName
may O
not O
be O
known O
ahead O
of O
time O
, O
especially O
if O
the O
legal O
decision O
is O
not O
made O
on O
the O
basis O
of O
an O
immutable O
a O
priori O
document O
, O
but O
rather O
on O
the O
basis O
of O
text O
that O
is O
technically O
a O
part O
of O
the O
eventual O
judgment O
. O
Our O
expert O
- O
informed O
method O
is O
intended O
to O
mitigate O
such O
situations O
where O
spurious O
correlations O
are O
introduced O
in O
the O
text O
production O
but O
may O
not O
be O
known O
in O
advance O
as O
explicit O
confounders O
. O

Our O
method O
consists O
of O
two O
steps O
: O
( O
i O
) O
Identification O
of O
distracting O
attributes O
for O
deconfounding O
through O
a O
combination O
of O
simple O
model O
training O
and O
minimal O
expert O
markup O
, O
and O
( O
ii O
) O
mitigation O
of O
these O
effects O
through O
adversarial O
training O
. O

Step O
1 O
: O
Identification O
of O
Distracting O
Attributes O
and O
Tokens O

We O
first O
identify O
input O
attributes O
and O
categorize O
them O
as O
either O
distracting O
or O
genuinely O
legally O
relevant O
in O
an O
expert O
consultation O
. O
' O
Distracting O
' O
attributes O
are O
highly O
correlated O
with O
the O
task O
label O
but O
not O
relevant O
in O
a O
human O
expert O
prediction O
. O
Attributes O
can O
be O
either O
( O
i O
) O
explicit O
in O
the O
text O
( O
such O
as O
vocabulary O
tokens O
) O
or O
( O
ii O
) O
implicit O
( O
e.g. O
, O
country O
, O
text O
length O
, O
etc O
. O
) O
. O
Implicit O
attributes O
can O
be O
derived O
from O
available O
metadata O
or O
a O
corpus O
analysis O
. O

For O
textual O
attributes O
, O
we O
apply O
depth O
- O
limited O
decision O
trees O
on O
an O
n O
- O
gram O
representation O
of O
the O
fact O
statement O
to O
predict O
the O
case O
outcome O
. O
We O
extract O
all O
tokens O
that O
appear O
in O
the O
trees O
and O
iterate O
, O
successively O
removing O
tokens O
identified O
as O
predictive O
. O
Compared O
to O
extracting O
tokens O
from O
a O
single O
larger O
tree O
, O
this O
process O
is O
better O
suited O
to O
remove O
high O
- O
entropy O
- O
reducing O
tokens O
one O
typically O
finds O
near O
the O
root O
of O
trees O
. O
The O
list O
of O
removed O
tokens O
is O
then O
presented O
to O
a O
legal O
expert O
, O
who O
categorizes O
them O
into O
spurious O
and O
legally O
genuine O
( O
see O
Appendix O
Sec O
. O
F O
for O
the O
list O
of O
spurious O
vocabulary O
identified O
by O
the O
expert O
and O
the O
rationale O
behind O
the O
choices O
) O
. O
This O
requires O
substantially O
less O
effort O
from O
the O
expert O
compared O
to O
other O
methods O
, O
such O
as O
data O
annotation O
or O
manual O
creation O
of O
counterfactuals O
. O
To O
prevent O
trees O
from O
picking O
up O
very O
sparse O
tokens O
, O
we O
filter O
the O
extracted O
terms O
using O
local O
mutual O
information O
( O
LMI O
) O
( O
Schuster O
et O
al O
. O
, O
2019 O
) O
, O
a O
re O
- O
weighted O
version O
of O
pointwise O
mutual O
information O
( O
PMI O
) O
( O
Church O
and O
Hanks O
, O
1990 O
) O
. O
We O
calculate O
LMI O
for O
each O
pair O
of O
token O
and O
label O
as O
illustrated O
in O
Appendix O
Sec O
. O
G O
. O

Step O
2 O
: O
Mitigation O
of O
Distracting O
Attributes O

We O
assume O
a O
neural O
NLP O
model O
M O
consisting O
of O
a O
feature O
extractor O
F O
and O
classifier O
C O
with O
parameters O
θ O
f O
and O
θ O
c O
, O
respectively O
. O
For O
each O
confounder O
k O
, O
we O
apply O
a O
discriminator O
D O
k O
with O
parameters O
θ O
d O
K O
to O
the O
feature O
extractors O
. O
We O
use O
adversarial O
training O
to O
maximize O
the O
feature O
extractor O
's O
ability O
to O
capture O
information O
for O
the O
main O
classification O
target O
while O
minimizing O
its O
ability O
to O
predict O
the O
value O
of O
distractor O
attributes O
. O
This O
encourages O
the O
model O
to O
generate O
distractor O
- O
invariant O
feature O
representation O
for O
the O
classifier O
. O
We O
use O
the O
following O
adversarial O
training O
objective O
: O

k O
arg O
min O
θ O
d O
k O
L O
( O
D O
k O
( O
F O
( O
x O
) O
) O
, O
y O
k O
) O
( O
1 O
) O
arg O
min O
θ O
f O
, O
θc O
[ O
L O
( O
C O
( O
F O
( O
x O
) O
) O
, O
y O
c O
) O
− O
k O
λ O
k O
L O
( O
D O
k O
( O
F O
( O
X O
) O
) O
, O
y O
k O
) O
] O
( O
2 O
) O

where O
L O
represents O
the O
loss O
, O
λ B-HyperparameterName
is O
a O
hyperparameter O
, O
x O
is O
the O
input O
, O
y O
c O
is O
the O
label O
, O
and O
y O
k O
is O
the O
distracting O
attribute O
k. O
The O
above O
optimization O
is O
performed O
using O
a O
gradient O
reversal O
layer O
( O
GRL O
) O
( O
Ganin O
and O
Lempitsky O
, O
2015 O
) O
to O
jointly O
optimize O
all O
the O
components O
instead O
of O
alternately O
updating O
the O
components O
as O
in O
GANs O
( O
Goodfellow O
et O
al O
. O
, O
2014 O
) O
. O
The O
GRL O
is O
inserted O
between O
the O
feature O
extractor O
and O
discriminators O
. O
It O
acts O
as O
the O
identity O
during O
the O
forward O
pass O
but O
, O
during O
the O
backward O
pass O
, O
scales O
the O
gradients O
flowing O
through O
by O
−λ B-HyperparameterName
, O
making O
the O
feature O
extractor O
receive O
the O
opposite O
gradients O
from O
the O
discriminator O
. O
This O
changes O
the O
overall O
objective O
function O
to O
: O

arg O
min O
θ O
f O
, O
θc O
, O
θ O
D O
[ O
L O
( O
C O
( O
F O
( O
x O
) O
) O
, O
y O
c O
) O
+ O
k O
λ O
k O
L O
( O
D O
k O
( O
GRL O
( O
( O
F O
( O
X O
) O
) O
) O
, O
y O
k O
) O
] O

( O
3 O
) O
We O
hypothesize O
that O
learning O
distractor O
- O
invariant O
feature O
representations O
through O
adversarial O
learning O
will O
help O
the O
model O
to O
focus O
on O
parts O
of O
the O
input O
that O
experts O
consider O
relevant O
. O

Experiments O
& O
Discussion O

In O
this O
section O
we O
describe O
our O
experiments O
in O
using O
our O
proposed O
deconfounding O
methodology O
to O
improve O
the O
alignment O
of O
model O
focus O
on O
the O
input O
with O
expert O
rationales O
on O
our O
set O
of O
LJP B-TaskName
tasks O
. O

Models O

Baseline O
: O
We O
use O
the O
BERT B-MethodName
variant I-MethodName
of I-MethodName
Hierarchical I-MethodName
Attention I-MethodName
Networks I-MethodName
( O
Yang O
et O
al O
. O
, O
2016 O
) O
as O
a O
baseline O
model O
. O
To O
segment O
our O
very O
long O
input O
texts O
we O
resort O
to O
a O
greedy O
sentence O
packing O
strategy O
in O
which O
we O
pack O
as O
many O
sentences O
as O
possible O
into O
one O
packet O
until O
it O
reaches O
the O
predefined O
maximum B-HyperparameterName
length I-HyperparameterName
( O
512 B-HyperparameterValue
tokens O
constrained O
BERT B-MethodName
) O
. O
When O
a O
sentence O
exceeds O
this O
maximum O
, O
we O
split O
it O
into O
parts O
to O
fit O
into O
multiple O
packets O
. O
We O
encode O
each O
packet O
with O
LegalBERT B-MethodName
( O
Chalkidis O
et O
al O
. O
, O
2020 O
) O
to O
obtain O
the O
token O
level O
representations O
. O
Following O
Yang O
et O
al O
. O
, O
2016 O
, O
we O
use O
a O
token O
attention O
layer O
aggregating O
the O
representation O
of O
the O
tokens O
and O
form O
a O
sentence O
( O
packet O
) O
vector O
. O
We O
pass O
these O
sentence O
vectors O
through O
a O
GRU O
encoder O
to O
obtain O
contextual O
representations O
. O
These O
are O
aggregated O
at O
the O
document O
level O
using O
a O
sentence O
attention O
layer O
. O
This O
model O
constitutes O
the O
feature O
extractor O
component O
F O
in O
our O
architecture O
. O
The O
obtained O
document O
representation O
is O
passed O
through O
dense O
layers O
for O
the O
final O
target O
prediction O
, O
constituting O
our O
classifier O
component O
C. O
paraRem B-MethodName
: O
Same O
as O
the O
baseline O
model O
but O
trained O
on O
data O
from O
which O
the O
paragraph O
number O
artifacts O
have O
been O
removed O
( O
see O
Sec O
. O
3.1 O
) O
. O
gradCou B-MethodName
: O
paraRem B-MethodName
model O
extended O
with O
a O
multiclass O
discriminator O
with O
a O
cross O
- O
entropy O
loss O
predicting O
the O
identity O
of O
the O
respondent O
government O
, O
and O
a O
corresponding O
deconfounding O
GRL O
. O
gradLen B-MethodName
: O
paraRem B-MethodName
model O
extended O
with O
a O
length O
discriminator O
predicting O
the O
length O
( O
number O
of O
sentences O
) O
of O
the O
document O
via O
a O
set O
of O
bins O
and O
a O
cross O
- O
entropy O
loss O
, O
and O
a O
corresponding O
GRL O
to O
predict O
the O
bin O
value O
. O
gradVocab B-MethodName
: O
paraRem B-MethodName
model O
extended O
with O
a O
vocabulary O
discriminator O
to O
predict O
the O
presence O
of O
identified O
spurious O
tokens O
, O
and O
associated O
GRL O
. O
As O
there O
can O
be O
multiple O
spurious O
tokens O
in O
a O
document O
, O
we O
employ O
binary O
entropy O
loss O
per O
token O
as O
it O
is O
a O
multi O
- O
label O
classification O
. O
We O
refer O
to O
the O
above O
three O
deconfounded O
models O
collectively O
as O
singleGrad B-MethodName
models O
. O
gradAll B-MethodName
: O
paraRem B-MethodName
model O
extended O
with O
all O
country O
, O
length O
, O
and O
vocabulary O
discriminators O
in O
parallel O
, O
and O
associated O
GRLs O
. O
Please O
refer O
to O
Appendix O
Sec O
. O
H O
for O
details O
on O
model O
configuration O
and O
training O
. O

Quantitative O
Evaluation O
& O
Discussion O

Expert O
Alignment O
Evaluation O

Our O
main O
objective O
is O
to O
evaluate O
the O
alignment O
of O
the O
model O
's O
focus O
on O
the O
input O
text O
with O
legal O
expert O
rationales O
( O
i.e. O
, O
selected O
subsets O
of O
relevant O
segments O
of O
the O
input O
) O
. O
Following O
Chalkidis O
et O
al O
. O
2021 O
, O
we O
measure O
the O
model O
's O
ability O
to O
identify O
the O
correct O
rationales O
at O
the O
paragraph O
level O
, O
which O
is O
the O
natural O
granularity O
of O
ECtHR B-DatasetName
fact O
sections O
. O
To O
extract O
the O
importance O
score O
for O
each O
paragraph O
, O
we O
rely O
on O
an O
interpretability O
technique O
which O
quantifies O
the O
impact O
of O
a O
particular O
input O
token O
towards O
the O
final O
prediction O
of O
the O
model O
. O

We O
use O
integrated B-MetricName
gradients I-MetricName
( O
Sundararajan O
et O
al O
. O
, O
2017 O
) O
to O
obtain O
a O
token O
- O
level O
focus O
score O
and O
ag- O
gregate O
paragraph O
- O
level O
scores O
as O
the O
squared B-MetricName
L2norm I-MetricName
of O
token O
scores O
in O
the O
paragraph O
divided O
by O
the O
square O
root O
of O
its O
number O
of O
tokens O
to O
account O
for O
length O
variation O
. O
We O
compute O
precision B-MetricName
@ I-MetricName
k I-MetricName
conditioned O
on O
some O
fixed O
k B-HyperparameterName
between O
the O
top O
- O
k B-HyperparameterName
paragraphs O
based O
on O
paragraph O
scores O
and O
golden O
paragraph O
rationales O
. O
The O
number O
of O
relevant O
paragraphs O
in O
gold O
rationales O
varies O
considerably O
, O
so O
a O
predefined O
k B-HyperparameterName
is O
inadequate O
. O
Thus O
, O
we O
compute O
precision B-MetricName
@ I-MetricName
Oracle I-MetricName
following O
Chalkidis O
et O
al O
. O
, O
2021 O
, O
where O
Oracle O
is O
the O
number O
of O
relevant O
paragraphs O
in O
the O
gold O
rationales O
. O
For O
tasks O
J O
, O
A O
, O
and O
A|B O
, O
the O
negative O
label O
( O
i.e. O
, O
non O
- O
violation O
) O
is O
of O
similar O
interest O
as O
the O
positive O
label O
. O
In O
task O
B O
, O
however O
, O
the O
negative O
label O
merely O
indicates O
that O
a O
specific O
article O
has O
not O
been O
alleged O
, O
which O
is O
legally O
largely O
uninteresting O
. O
Hence O
, O
we O
reduce O
negative O
IG B-MetricName
scores I-MetricName
of O
tokens O
( O
indicating O
a O
negative O
contribution O
to O
the O
prediction O
) O
to O
zero B-MetricValue
. O

Prediction O
Performance O
Evaluation O

We O
also O
report O
the O
models O
' O
performance O
on O
the O
main O
four O
LJP B-TaskName
tasks O
. O
For O
Task O
J O
, O
we O
report O
the O
macro B-MetricName
F1 I-MetricName
- I-MetricName
score I-MetricName
for O
binary O
violation O
prediction O
. O
For O
Task O
A O
and O
B O
, O
following O
( O
Chalkidis O
et O
al O
. O
, O
2022a O
) O
, O
we O
report O
micro B-MetricName
- I-MetricName
F1 I-MetricName
( O
µ-F1 B-MetricName
) O
and O
macro B-MetricName
- I-MetricName
F1 I-MetricName
( O
m B-MetricName
- I-MetricName
F1 I-MetricName
) O
scores O
. O
For O
Task O
A|B O
, O
we O
also O
report O
micro B-MetricName
- I-MetricName
F1 I-MetricName
and O
macro B-MetricName
- I-MetricName
F1 I-MetricName
scores O
. O
In O
computing O
the O
above O
metrics O
for O
tasks O
A O
and O
A|B O
, O
we O
consider O
the O
cases O
in O
which O
a O
particular O
article O
has O
been O
deemed O
violated O
as O
pos O
- O
itive O
instances O
and O
the O
rest O
of O
the O
instances O
as O
negatives O
. O
We O
also O
introduce O
hard B-MetricName
- I-MetricName
macro I-MetricName
- I-MetricName
F1 I-MetricName
( O
hm B-MetricName
- I-MetricName
F1 I-MetricName
) O
for O
both O
Task O
A O
and O
A|B O
, O
in O
which O
F1 B-MetricName
is O
computed O
for O
each O
article O
using O
only O
those O
instances O
as O
negatives O
where O
an O
article O
has O
been O
alleged O
as O
violated O
but O
not O
found O
so O
by O
the O
court O
. O

Quantitative O
Evaluation O
Results O

Table O
1 O
and O
Table O
2 O
show O
the O
performance O
of O
different O
models O
on O
expert O
alignment O
and O
outcome O
prediction O
, O
respectively O
. O
paraRem B-MethodName
vs. O
baseline O
: O
We O
observe O
that O
paraRem B-MethodName
outperforms O
the O
baseline O
model O
in O
expert O
alignment O
across O
all O
tasks O
with O
a O
minimal O
drop O
in O
prediction O
performance O
. O
Task O
J O
stands O
out O
in O
that O
removing O
distracting O
signals O
via O
paragraph O
number O
removal O
even O
leads O
to O
a O
marginal O
improvement O
. O
Notably O
, O
we O
separately O
confirm O
the O
vulnerability O
of O
the O
baseline O
model O
by O
applying O
it O
to O
the O
test O
set O
with O
paragraph O
numbers O
removed O
and O
evaluate O
it O
on O
a O
test O
set O
without O
paragraph O
numbers O
, O
resulting O
in O
macro B-MetricName
- I-MetricName
F1 I-MetricName
of O
51.16 B-MetricValue
( O
i.e. O
, O
a O
nearly O
30 B-MetricValue
points O
drop O
) O
. O
gradCou B-MethodName
, O
gradLen B-MethodName
, O
gradVocab B-MethodName
vs. O
paraRem B-MethodName
: O
In O
all O
tasks O
, O
we O
observe O
that O
all O
singleGrad B-MethodName
models O
improve O
in O
expert O
alignment O
performance O
over O
paraRem B-MethodName
by O
a O
small O
but O
consistent O
margin O
. O
This O
demonstrates O
the O
ability O
of O
our O
deconfounding O
component O
to O
help O
the O
model O
better O
identify O
legally O
relevant O
parts O
of O
the O
input O
. O
Notably O
, O
gradVocab B-MethodName
shows O
the O
most O
improvement O
in O
alignment O
over O
paraRem B-MethodName
in O
all O
tasks O
except O
Task O
B O
( O
alleged B-TaskName
article I-TaskName
prediction I-TaskName
) O
, O
where O
gradLen B-MethodName
performs O
best O
. O
During O
development O
on O
task O
B O
, O
we O
observed O
that O
the O
decisiontree O
based O
removal O
of O
predictive O
words O
led O
to O
only O
a O
marginal O
falloff O
in O
tree O
model O
accuracy B-MetricName
, O
even O
after O
multiple O
iterations O
, O
since O
there O
was O
simply O
a O
lot O
of O
topical O
words O
( O
e.g. O
, O
for O
police O
misconduct O
, O
legal O
proceedings O
, O
etc O
. O
) O
to O
take O
over O
as O
some O
of O
them O
were O
removed O
. O
This O
in O
part O
reflects O
the O
different O
nature O
of O
the O
tasks O
and O
shows O
a O
limitation O
of O
our O
tree O
- O
training O
- O
based O
method O
for O
identifying O
spurious O
tokens O
. O
Similar O
to O
paraRem B-MethodName
, O
the O
gradLen B-MethodName
model O
( O
in O
case O
of O
Task O
A O
, O
B O
, O
and O
A|B O
) O
also O
shows O
improvement O
in O
prediction O
performance O
compared O
to O
the O
baseline O
model O
. O
This O
suggests O
that O
deconfounding O
can O
potentially O
prevent O
the O
model O
getting O
stuck O
in O
distractor O
- O
related O
local O
optima O
. O
Alignment O
: O
All O
singleGrad B-MethodName
models O
outperform O
the O
baseline O
with O
regard O
to O
expert O
alignment O
. O
We O
observe O
that O
gradAll B-MethodName
achieves O
the O
highest O
score O
, O
which O
establishes O
some O
degree O
of O
complementarity O
among O
the O
three O
singleGrad B-MethodName
models O
and O
the O
distracting O
signals O
they O
remedy O
. O
A O
paired O
t O
- O
test O
( O
gradAll B-MethodName
vs. O
paraRem B-MethodName
) O
reveals O
p O
- O
values O
above O
typical O
significance O
levels O
for O
the O
validation O
partition O
of O
task O
J O
, O
along O
with O
a O
considerable O
divergence O
in O
the O
general O
score O
level O
for O
the O
two O
tasks O
. O
We O
conjecture O
that O
this O
is O
the O
result O
of O
our O
small O
rationale B-HyperparameterName
sample I-HyperparameterName
size I-HyperparameterName
( O
50 B-HyperparameterValue
from O
each O
partition O
) O
and O
differences O
in O
distribution O
between O
the O
task O
J O
data O
partitions O
, O
which O
have O
been O
split O
along O
the O
timeline O
rather O
than O
random O
. O
We O
also O
see O
a O
higher O
p O
- O
value O
for O
task O
A|B O
, O
which O
is O
intuitive O
since O
it O
is O
the O
most O
difficult O
. O
Its O
dataset O
lacks O
easily O
identifiable O
inadmissible O
cases O
( O
as O
in O
task O
J O
) O
and O
it O
has O
access O
to O
B O
's O
labels O
as O
concurrent O
, O
non O
- O
textual O
input O
. O
To O
gain O
some O
more O
insight O
into O
A|B O
, O
we O
report O
on O
a O
qualitative O
error O
analysis O
of O
the O
model O
rationales O
below O
. O

Qualitative O
Evaluation O
& O
Discussion O

Expert O
Scores O
: O
We O
sample O
40 O
cases O
from O
task O
A|B O
validation O
and O
test O
sets O
( O
see O
App O
. O
Sec O
. O
D O
) O
. O
We O
provide O
the O
expert O
with O
randomized O
visualizations O
of O
IG B-MetricName
scores I-MetricName
at O
the O
token O
level O
derived O
from O
our O
paraRem B-MethodName
and O
gradAll B-MethodName
models O
. O
Following O
( O
Jayaram O
and O
Allaway O
, O
2021 O
) O
, O
the O
expert O
was O
asked O
to O
rate O
these O
on O
a O
five O
- O
point O
Likert O
scale O
( O
range O
-2 O
to O
2 O
) O
on O
two O
metrics O
: O
( O
i O
) O
Sufficiency B-MetricName
: O
Is O
a O
sufficiently O
large O
set O
of O
tokens O
focused O
on O
to O
arrive O
at O
the O
prediction O
? O
; O
and O
( O
ii O
) O
Irrelevance B-MetricName
: O
How O
many O
irrelevant O
tokens O
does O
the O
model O
focus O
on O
? O
We O
phrased O
the O
scale O
such O
that O
, O
for O
both O
parameters O
, O
a O
higher O
rating O
signals O
a O
better O
alignment O
between O
the O
model O
focus O
and O
the O
expert O
's O
assessment O
. O
Table O
3 O
presents O
averages O
of O
the O
raw O
scale O
scores O
. O
We O
observe O
that O
the O
deconfounded B-MethodName
gradAll I-MethodName
model O
scores O
higher O
( O
See O
App O
. O
I O
for O
an O
example O
pair O
of O
IG B-MetricName
visualizations O
) O
. O
Manual O
IG B-MetricName
Inspection O
: O
For O
the O
paraRem B-MethodName
model O
, O
we O
notice O
that O
high O
scoring O
IG B-MetricName
tokens O
are O
sparse O
, O
whereas O
in O
gradAll B-MethodName
, O
focus O
is O
densely O
distributed O
. O
There O
, O
contiguous O
spans O
of O
tokens O
tend O
to O
receive O
higher O
scores O
. O
This O
phenomenon O
is O
likely O
due O
to O
paraRem B-MethodName
being O
drawn O
to O
single O
word O
distractors O
. O
Deconfounding O
helps O
the O
gradAll B-MethodName
model O
to O
spread O
its O
focus O
across O
larger O
segments O
of O
the O
text O
. O
Our O
ECtHR B-DatasetName
expert O
further O
observed O
that O
gradAll B-MethodName
highlighted O
words O
that O
, O
in O
conjunction O
, O
were O
indicative O
of O
the O
outcome O
, O
even O
if O
those O
were O
a O
considerable O
distance O
apart O
. O
At O
the O
same O
time O
, O
however O
, O
it O
seemed O
that O
two O
words O
hinting O
at O
opposite O
outcomes O
in O
a O
single O
sentence O
forced O
the O
system O
to O
focus O
only O
on O
one O
of O
the O
two O
, O
leaving O
the O
other O
one O
unhighlighted O
. O
We O
conjecture O
that O
these O
long O
- O
and O
short O
- O
distance O
phenomena O
are O
a O
result O
of O
the O
hierarchical O
model O
architecture O
necessitated O
by O
the O
long O
documents O
and O
leave O
their O
further O
exploration O
for O
future O
work O
. O

An O
inspection O
of O
high O
scored O
tokens O
in O
paraRem B-MethodName
reveals O
that O
many O
of O
them O
are O
highly O
discriminative O
in O
our O
decision O
tree O
models O
, O
showing O
that O
complex O
neural O
models O
can O
easily O
fall O
for O
distractors O
at O
the O
expense O
of O
missing O
equally O
predictive O
but O
semantically O
more O
complex O
signals O
. O
This O
reinforces O
our O
paradigm O
to O
identify O
discriminative O
tokens O
using O
a O
simpler O
model O
and O
subject O
them O
to O
expert O
scrutiny O
. O
In O
particular O
, O
we O
found O
that O
the O
word O
" O
represented O
" O
forms O
a O
natural O
decoy O
and O
, O
when O
injected O
into O
a O
violation O
- O
outcome O
fact O
statement O
, O
flips O
the O
predicted O
label O
of O
trained O
deep O
neural O
models O
. O
This O
led O
us O
to O
believe O
those O
models O
rely O
more O
on O
individual O
words O
than O
one O
might O
expect O
, O
and O
motivated O
us O
to O
explore O
how O
this O
can O
be O
exploited O
with O
information O
derived O
from O
simple O
models O
. O
Figure O
2 O
shows O
that O
the O
performance O
of O
decision O
trees O
with O
unigram O
features O
( O
at O
iteration O
1 O
without O
removed O
tokens O
) O
can O
even O
come O
close O
to O
BERT O
models O
. O

In O
paraRem B-MethodName
, O
we O
further O
observe O
that O
tokens O
at O
the O
start O
of O
sentences O
receive O
higher O
IG B-MetricName
scores O
. O
We O
believe O
this O
to O
be O
the O
model O
counting O
sentences O
, O
which O
justifies O
deconfounding O
for O
length O
. O
For O
gradAll B-MethodName
, O
we O
observe O
that O
sentence O
beginnings O
still O
receive O
focus O
, O
but O
less O
strongly O
so O
. O
This O
may O
be O
due O
to O
BERT O
recognizing O
sentence O
boundaries O
. O
Further O
alignment O
improvement O
: O
The O
overall O
low O
precision B-MetricName
@ I-MetricName
Oracle I-MetricName
scores O
show O
that O
considerable O
differences O
in O
alignment O
with O
human O
experts O
remain O
. O
We O
conjecture O
that O
the O
model O
is O
shifting O
its O
focus O
, O
at O
least O
in O
part O
, O
to O
other O
spurious O
attributes O
which O
our O
current O
setup O
could O
not O
reveal O
. O
This O
calls O
for O
further O
investigation O
to O
design O
effective O
methods O
to O
identify O
such O
patterns O
. O
However O
, O
we O
expect O
them O
to O
be O
increasingly O
subtle O
and O
difficult O
to O
recognize O
, O
potentially O
even O
for O
legal O
experts O
. O
An O
intuitive O
upper O
bound O
for O
the O
system O
would O
be O
the O
annotation O
agreement O
of O
multiple O
experts O
, O
which O
to O
the O
best O
of O
our O
knowledge O
remains O
unexplored O
in O
the O
current O
state O
of O
the O
art O
. O
Expert O
Pattern O
Identification O
: O
Our O
results O
naturally O
raise O
the O
question O
of O
how O
distractors O
can O
be O
identified O
in O
ECtHR B-DatasetName
fact O
texts O
by O
experts O
. O
Generally O
, O
the O
patterns O
we O
focused O
on O
affect O
the O
relationship O
between O
the O
argumentation O
in O
the O
judgment O
and O
the O
supportive O
facts O
given O
. O
There O
is O
copious O
literature O
on O
the O
court O
's O
inconsistent O
approach O
to O
Voeten O
, O
2020 O
) O
. O
We O
hence O
paid O
attention O
to O
specific O
markers O
in O
the O
fact O
section O
and O
correlated O
them O
to O
existing O
precedents O
and O
argumentation O
patterns O
. O

A O
few O
examples O
: O
The O
court O
may O
decide O
to O
make O
use O
of O
positive O
obligations O
and O
decide O
against O
the O
state O
( O
violation O
) O
by O
highlighting O
failures O
of O
national O
authorities O
, O
or O
may O
decide O
to O
use O
those O
same O
positive O
obligations O
under O
' O
the O
responsible O
authorities O
' O
doctrine O
, O
highlighting O
the O
efforts O
of O
national O
authorities O
to O
bring O
domestic O
legislation O
in O
line O
with O
the O
convention O
, O
thus O
deciding O
that O
there O
has O
been O
no O
violation O
. O
There O
are O
also O
fact O
patterns O
and O
practices O
specific O
to O
particular O
state O
parties O
to O
the O
convention O
( O
e.g. O
, O
prison O
overcrowding O
, O
procedural O
issues O
in O
child O
abduction O
cases O
) O
. O
The O
court O
may O
also O
sometimes O
highlight O
specific O
facts O
of O
a O
case O
with O
the O
view O
to O
' O
document O
' O
its O
resemblance O
to O
, O
or O
divergence O
from O
, O
an O
existing O
precedent O
. O
A O
detailed O
, O
legally O
informed O
case O
study O
on O
predictive O
patterns O
is O
beyond O
the O
scope O
of O
this O
work O
. O

Recommendations O
for O
LJP B-TaskName
Research O

In O
order O
to O
produce O
value O
for O
legal O
practice O
, O
we O
believe O
that O
LJP B-TaskName
/ O
LJF B-TaskName
as O
an O
NLP O
task O
should O
strive O
for O
a O
productive O
combination O
of O
expert O
knowledge O
with O
data O
- O
derived O
insight O
. O
Based O
on O
our O
results O
, O
we O
formulate O
the O
following O
recommendations O
: O
First O
, O
as O
has O
already O
been O
observed O
in O
the O
field O
, O
any O
prediction O
/ O
classification O
should O
happen O
from O
suitable O
source O
text O
that O
does O
not O
encode O
information O
about O
the O
outcome O
but O
contains O
as O
complete O
factual O
information O
as O
possible O
, O
or O
at O
least O
control O
for O
this O
influence O
. O
Second O
, O
straightforward O
predictors O
( O
e.g. O
, O
input O
length O
and O
shallow O
unigram O
models O
) O
should O
be O
used O
to O
identify O
distractors O
and O
confounders O
. O
Third O
, O
claimed O
performance O
levels O
in O
predicting O
case O
outcomes O
should O
be O
contextualized O
by O
information O
about O
the O
distribution O
of O
the O
legal O
issues O
and O
respective O
conditional O
outcome O
probabilities O
in O
the O
corpus O
, O
as O
well O
as O
against O
baseline O
classifiers O
capable O
of O
exploiting O
known O
distractors O
. O
Fourth O
, O
more O
granular O
outcome O
variable O
information O
( O
e.g. O
, O
case O
declared O
inadmissible O
vs. O
case O
dismissed O
on O
the O
merits O
, O
decomposition O
into O
outcomes O
of O
individual O
issues O
) O
will O
allow O
the O
development O
of O
more O
nuanced O
prediction O
/ O
classification O
systems O
. O
Taken O
together O
, O
if O
such O
models O
can O
be O
explained O
and O
integrated O
into O
a O
decision O
support O
system O
for O
suitable O
tasks O
in O
legal O
practice O
, O
experts O
will O
be O
more O
likely O
to O
perceive O
them O
as O
adding O
value O
. O

Conclusion O

Our O
results O
show O
that O
our O
deconfounded O
LJP B-TaskName
models O
are O
consistently O
better O
aligned O
with O
expert O
rationales O
than O
a O
baseline O
optimized O
for O
the O
target O
label O
only O
, O
and O
in O
many O
cases O
can O
even O
achieve O
better O
prediction O
performance O
. O
However O
, O
the O
improvement O
is O
small O
and O
the O
paragraphs O
focused O
on O
by O
all O
our O
models O
are O
still O
quite O
different O
from O
what O
an O
expert O
has O
annotated O
as O
relevant O
, O
as O
indicated O
by O
generally O
low O
precision B-MetricName
@ I-MetricName
Oracle I-MetricName
scores O
( O
< O
50 B-MetricValue
% I-MetricValue
) O
. O
Still O
, O
our O
quantitative O
results O
show O
that O
expert O
- O
informed O
deconfounding O
LJP B-TaskName
works O
in O
principle O
and O
can O
potentially O
go O
a O
long O
way O
to O
train O
more O
robust O
and O
trustworthy O
neural O
LJP B-TaskName
models O
, O
as O
well O
as O
derive O
more O
useful O
legal O
insight O
from O
them O
. O

Limitations O

We O
present O
a O
case O
study O
in O
deconfounding O
legal O
judgment O
prediction O
on O
the O
ECtHR B-DatasetName
, O
and O
all O
results O
are O
to O
be O
understood O
as O
relative O
to O
the O
ECtHR B-DatasetName
, O
its O
jurisprudence O
, O
the O
used O
datasets O
, O
and O
the O
formal O
tasks O
. O
The O
distracting O
attributes O
we O
identify O
include O
confounding O
effects O
of O
the O
court O
's O
document O
production O
, O
where O
the O
decision O
may O
be O
known O
before O
the O
decision O
text O
( O
including O
the O
fact O
section O
) O
is O
finalized O
. O
A O
replication O
of O
this O
study O
in O
other O
LJP B-TaskName
settings O
is O
of O
course O
warranted O
before O
general O
applicability O
can O
be O
claimed O
. O
Our O
analysis O
of O
task O
B O
has O
further O
revealed O
that O
redundant O
vocabulary O
distribution O
can O
challenge O
the O
system O
's O
ability O
to O
point O
out O
individual O
' O
smoking O
gun O
' O
distracting O
tokens O
. O
This O
aspect O
is O
particularly O
complex O
in O
light O
of O
differing O
legal O
systems O
and O
their O
respective O
cultures O
and O
patterns O
of O
drafting O
texts O
that O
may O
form O
the O
basis O
of O
predictive O
or O
, O
more O
generally O
, O
assistive O
systems O
. O
Morphologically O
rich O
languages O
, O
where O
distracting O
signal O
may O
be O
spread O
across O
multiple O
tokens O
, O
may O
make O
this O
challenge O
more O
difficult O
and O
require O
stem O
- O
or O
lemma O
- O
based O
processing O
as O
part O
of O
the O
method O
. O

Our O
deconfounding O
method O
is O
work O
- O
intensive O
and O
assumes O
the O
identifiability O
of O
distracting O
information O
in O
text O
and O
metadata O
by O
an O
expert O
. O
Legal O
expert O
agreement O
about O
what O
parts O
of O
decisions O
are O
relevant O
remains O
underexplored O
, O
and O
the O
division O
of O
genuine O
versus O
spurious O
language O
may O
also O
vary O
in O
between O
multiple O
experts O
. O
While O
we O
are O
convinced O
that O
further O
research O
on O
effective O
deconfounding O
of O
legal O
NLP O
systems O
is O
needed O
if O
these O
systems O
are O
to O
become O
robust O
and O
trustworthy O
, O
the O
time O
- O
intensive O
nature O
of O
collaboratively O
developing O
and O
qualitatively O
evaluating O
such O
models O
with O
legal O
experts O
poses O
a O
considerable O
resource O
challenge O
. O

A O
technical O
difficulty O
in O
working O
with O
legal O
documents O
is O
their O
length O
, O
and O
the O
use O
of O
packet O
- O
based O
hierarchical O
models O
constrains O
the O
maximum O
distance O
across O
which O
tokens O
can O
directly O
attend O
to O
one O
another O
. O
The O
impact O
of O
this O
limitation O
on O
model O
performance O
in O
various O
types O
of O
tasks O
is O
the O
subject O
of O
ongoing O
exploratory O
work O
( O
e.g. O
, O
Dai O
et O
al O
. O
2022 O
) O
. O

Ethics O
Statement O

The O
research O
presented O
here O
works O
exclusively O
with O
publicly O
available O
datasets O
of O
ECtHR B-DatasetName
decisions O
, O
which O
are O
based O
on O
HUDOC O
* O
, O
the O
public O
database O
of O
the O
Court O
. O
While O
these O
decisions O
are O
not O
anonymized O
and O
contain O
the O
real O
names O
of O
individuals O
involved O
, O
our O
work O
does O
not O
engage O
with O
the O
data O
in O
a O
way O
that O
we O
consider O
harmful O
beyond O
this O
availability O
. O

Our O
models O
are O
designed O
to O
be O
used O
with O
pretrained O
language O
models O
and O
hence O
inherit O
any O
bi- O
* O
https O
: O
/ O
/ O
hudoc.echr.coe.int O
ases O
they O
may O
contain O
. O
This O
entails O
an O
obligation O
to O
screen O
incorporated O
models O
and O
to O
test O
any O
developed O
system O
with O
regard O
to O
its O
performance O
across O
groups O
of O
cases O
( O
e.g. O
Chalkidis O
et O
al O
. O
2022b O
) O
, O
and O
to O
remedy O
any O
disparities O
before O
deploying O
it O
as O
a O
prediction O
and O
inference O
tool O
. O
Our O
experiments O
are O
targeted O
at O
controlling O
for O
legally O
irrelevant O
distractors O
in O
the O
input O
, O
which O
is O
in O
line O
with O
this O
responsibility O
. O

The O
task O
of O
legal B-TaskName
judgment I-TaskName
prediction I-TaskName
raises O
ethical O
concerns O
, O
both O
general O
as O
well O
as O
specific O
to O
the O
European O
Court O
of O
Human O
Rights O
. O
( O
Fikfak O
, O
2021 O
) O
emphasizes O
focal O
issues O
with O
regard O
to O
the O
court O
considering O
the O
use O
predictive O
technology O
to O
tackle O
its O
caseload O
, O
including O
system O
bias O
and O
the O
challenges O
of O
designing O
the O
interaction O
between O
judges O
and O
predictive O
systems O
. O
The O
latter O
is O
of O
course O
especially O
sensitive O
given O
experiences O
made O
with O
recidivism O
risk O
prediction O
( O
Collins O
2018 O
) O
and O
possible O
disparate O
effects O
of O
how O
judges O
interact O
with O
scores O
( O
Albright O
2019 O
) O
. O
Our O
research O
group O
is O
committed O
to O
research O
on O
LJP B-TaskName
as O
a O
means O
to O
derive O
insight O
from O
legal O
decision O
data O
towards O
increasing O
accountability O
, O
fairness O
, O
and O
transparency O
in O
the O
use O
of O
technology O
in O
legal O
systems O
. O
The O
premise O
of O
this O
work O
is O
that O
the O
behavior O
of O
legal O
outcome O
prediction O
systems O
is O
to O
be O
scrutinized O
with O
great O
care O
. O
This O
paper O
does O
not O
advocate O
for O
the O
practical O
use O
of O
such O
systems O
, O
but O
rather O
empirically O
explores O
difficulties O
that O
arise O
in O
their O
development O
and O
recommends O
a O
closer O
connection O
between O
technical O
research O
and O
legal O
expertise O
( O
see O
Sec O
. O
5.4 O
) O
. O

All O
models O
of O
this O
project O
were O
developed O
and O
trained O
on O
Google O
Colab O
. O
Our O
models O
adapted O
pretrained O
language O
models O
and O
we O
did O
not O
engage O
in O
any O
training O
of O
such O
large O
models O
from O
scratch O
. O
We O
did O
not O
track O
computation O
hours O
. O

A O
Dataset O
Statistics O

Table O
5 O
demonstrates O
the O
artefacts O
from O
corpus O
construction O
and O
admissibility O
- O
related O
confounding O
information O
in O
the O
training O
set O
of O
Task O
J. O
Figure O
3 O
and O
4 O
display O
the O
distribution O
of O
text O
length O
and O
the O
respondent O
state O
in O
the O
Task O
J O
train O
set O
. O
Figure O
5 O
and O
6 O
show O
the O
statistics O
of O
text O
length O
and O
respondent O
state O
in O
the O
Task O
B O
train O
set O
. O

B O
LexGlue B-DatasetName
Dataset O
Characteristics O

Table O
4 O
describes O
the O
conditional O
probability O
of O
a O
violation O
finding O
by O
the O
court O
given O
the O
allegation O
of O
a O
particular O
article O
as O
well O
as O
the O
probability O
of O
a O
violation O
finding O
regarding O
a O
particular O
article O
even O
though O
it O
was O
not O
alleged O
. O

C O
Rational O
annotation O
Process O
for O
Task O
J O

We O
sampled O
50 O
cases O
( O
25 O
each O
) O
from O
the O
validation O
and O
test O
split O
. O
In O
each O
split O
, O
we O
sample O
two O
cases O
for O
each O
of O
the O
ten O
violated O
articles O
, O
one O
containing O
the O
token O
' O
represented O
' O
and O
one O
without O
, O
along O
with O
five O
inadmissible O
cases O
. O
While O
the O
article O
information O
is O
available O
in O
the O
task O
J O
dataset O
, O
we O
do O
not O
use O
it O
as O
it O
was O
introduced O
as O
a O
binary B-TaskName
violation I-TaskName
classification I-TaskName
task O
. O
The O
rationale O
annotation O
process O
was O
done O
using O
the O
GLOSS O
annotation O
tool O
. O
The O
third O
author O
of O
this O
paper O
, O
who O
is O
an O
ECtHR B-DatasetName
expert O
, O
read O
the O
case O
fact O
statements O
and O
highlighted O
paragraphs O
which O
she O
considered O
indicative O
of O
an O
eventual O
finding O
of O
a O
violation O
for O
any O
convention O
article O
by O
the O
court O
. O
Despite O
our O
sampling O
involving O
randomness O
, O
the O
expert O
was O
already O
familiar O
with O
a O
considerable O
portion O
of O
the O
decisions O
. O
Given O
this O
, O
we O
abstained O
from O
producing O
a O
human O
expert O
outcome O
prediction O
baseline O
. O

D O
Case O
Sampling O
for O
Qualitative O
Evaluation O

For O
the O
qualitative O
evaluation O
of O
Task O
A|B O
, O
we O
sample O
40 O
cases O
( O
20 O
each O
) O
from O
validation O
and O
test O
split O
. O

In O
each O
split O
, O
we O
sample O
two O
cases O
for O
each O
of O
the O
ten O
allegedly O
violated O
articles O
, O
one O
with O
a O
finding O
of O
a O
convention O
violation O
and O
with O
a O
non O
- O
violation O
finding O
. O

E O
Decision O
Tree O
Performance O

Figure O
2 O
shows O
the O
performance O
of O
our O
decision O
tree O
model O
across O
iterations O
for O
different O
tasks O
. O
After O
each O
iteration O
, O
we O
remove O
the O
informative O
to O
- O
kens O
from O
previous O
iterations O
. O
In O
case O
of O
task O
J O
, O
we O
notice O
a O
steep O
fall O
after O
iteration O
5 O
. O
Tasks O
A O
and O
B O
exhibit O
less O
dramatic O
falloff O
of O
macro B-MetricName
- I-MetricName
averaged I-MetricName
F1 I-MetricName
, O
owing O
to O
the O
different O
nature O
of O
the O
tasks O
as O
article O
- O
specific O
violations O
. O
Performance O
on O
Task O
A|B O
even O
shows O
small O
increases O
, O
albeit O
with O
a O
low O
absolute O
score O
. O
The O
large O
standard O
deviations O
bands O
computed O
across O
all O
articles O
show O
considerable O
variation O
. O

F O
Spurious O
Vocabulary O
identified O
by O
Expert O

Following O
is O
the O
spurious O
vocabulary O
we O
obtained O
with O
respect O
to O
each O
task O
. O

• O
Task O
J O
: O
represented O
, O
national O
, O
mr O
, O
summarised O
, O
practising O
, O
lawyer O
, O
agent O
, O
paragraph O
The O
words O
were O
chosen O
as O
relevant O
or O
irrelevant O
by O
using O
the O
daily O
vocabulary O
of O
a O
human O
rights O
lawyer O
working O
at O
the O
ECtHR B-DatasetName
as O
a O
reference O
. O
A O
word O
was O
considered O
legally O
relevant O
if O
, O
taken O
individually O
, O
it O
could O
be O
introduced O
into O
legal O
reasoning O
. O
For O
instance O
, O
the O
word O
" O
religious O
" O
was O
spurious O
because O
taken O
individually O
it O
says O
nothing O
about O
the O
content O
of O
a O
norm O
. O
One O
may O
talk O
about O
religious O
freedom O
, O
but O
the O
legally O
relevant O
word O
there O
is O
freedom O
. O
Article O
9 O
mentions O
religion O
, O
but O
restrictions O
related O
to O
religion O
may O
also O
be O
present O
under O
Article O
8 O
, O
3 O
, O
2 O
, O
5 O
, O
etc O
. O
Under O
the O
same O
Article O
9 O
for O
instance O
, O
the O
court O
decides O
whether O
there O
has O
been O
a O
violation O
depending O
on O
criteria O
such O
as O
tolerance O
, O
pluralism O
, O
etc O
. O
It O
is O
those O
criteria O
that O
are O
relevant O
whereas O
" O
religion O
" O
is O
not O
by O
itself O
relevant O
as O
a O
part O
of O
the O
legal O
reasoning O
. O

G O
LMI O
Calculation O

We O
calculate O
LMI O
for O
each O
pair O
of O
token O
t O
and O
label O
y O
as O
follows O
: O

LM O
I O
( O
t O
, O
y O
) O
= O
p O
( O
t O
, O
y O
) O
× O
P O
M O
I O
( O
t O
, O
y O
) O
( O
4 O
) O

p O
( O
t O
, O
y O
) O
= O
count O
( O
t O
, O
y O
) O

|D| O

( O
5 O
) O

P O
M O
I O
( O
t O
, O
y O
) O
= O
log O
p O
( O
t O
| O
y O
) O
p O
( O
t O
) O
( O
6 O
) O

where O
count O
( O
t O
, O
y O
) O
denotes O
the O
co O
- O
occurrence O
of O
t O
and O
label O
y O
, O
and O
|D| O
is O
the O
number O
of O
unique O
words O
in O
the O
training O
set O
. O

In O
the O
case O
of O
binary O
classification O
( O
task O
J O
) O
and O
one O
- O
vs O
- O
one O
multi O
- O
label O
classification O
( O
task O
A|B O
) O
, O
we O
calculate O
the O
LMI O
score O
for O
a O
token O
as O
the O
absolute O
difference O
between O
LMI O
scores O
for O
both O
positive O
and O
negative O
labels O
, O
as O
both O
the O
labels O
represent O
a O
particular O
class O
. O
In O
one O
- O
vs O
- O
rest O
( O
tasks O
A O
, O
B O
) O
, O
we O
simply O
take O
the O
difference O
between O
LMI O
scores O
for O
both O
positive O
and O
negative O
labels O
( O
rather O
than O
absolute O
difference O
) O
as O
the O
negative O
label O
does O
not O
specifically O
represent O
a O
particular O
class O
. O
Finally O
, O
we O
calculate O
the O
z O
- O
score O
statistic O
of O
the O
effective O
LMI O
score O
for O
each O
token O
to O
identify O
significant O
tokens O
. O

H O
Model O
configuration O
& O
Training O

Spurious O
token O
identification O
: O
We O
train O
a O
series O
of O
decision O
trees O
of O
depth B-HyperparameterName
3 B-HyperparameterValue
to O
assemble O
lists O
of O
predictive O
tokens O
for O
expert O
filtering O
. O
The O
feature O
vector O
consists O
of O
whitespace O
- O
tokenized O
unigrams O
reduced O
by O
the O
LMI O
filtering O
explained O
above O
. O
For O
task O
J O
, O
this O
means O
training O
trees O
that O
predict O
the O
binary O
violation O
label O
. O
For O
task O
A O
and O
B O
we O
employ O
a O
one O
- O
vs O
- O
rest O
classification O
to O
produce O
one O
decision O
tree O
series O
per O
article O
. O
For O
task O
A|B O
we O
provided O
the O
task O
B O
labels O
( O
allegedly O
violated O
articles O
) O
in O
onevs O
- O
one O
fashion O
per O
article O
, O
with O
positive O
instances O
being O
facts O
where O
that O
particular O
article O
was O
deemed O
violated O
, O
and O
negatives O
where O
that O
particular O
article O
was O
merely O
alleged O
but O
not O
deemed O
violated O
. O
LJP B-TaskName
models O
: O
Our O
models O
compute O
BERT B-HyperparameterName
- I-HyperparameterName
based I-HyperparameterName
word I-HyperparameterName
embeddings I-HyperparameterName
of I-HyperparameterName
size I-HyperparameterName
768 B-HyperparameterValue
. O
Our O
word B-HyperparameterName
level I-HyperparameterName
attention I-HyperparameterName
context I-HyperparameterName
vector I-HyperparameterName
size I-HyperparameterName
is O
300 B-HyperparameterValue
. O
The O
sentence B-HyperparameterName
level I-HyperparameterName
GRU I-HyperparameterName
encoder I-HyperparameterName
dimension I-HyperparameterName
is O
200 B-HyperparameterValue
, O
thus O
giving O
a O
bidirectional B-HyperparameterName
embedding I-HyperparameterName
of O
size O
400 B-HyperparameterValue
, O
and O
a O
sentence B-HyperparameterName
level I-HyperparameterName
attention I-HyperparameterName
vector I-HyperparameterName
dimension I-HyperparameterName
of O
200 B-HyperparameterValue
. O
The O
final O
dense O
classifier O
for O
all O
tasks O
has O
100 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
. O
The O
output B-HyperparameterName
dimension I-HyperparameterName
is O
1 B-HyperparameterValue
for O
task O
J O
and O
10 B-HyperparameterValue
for O
the O
other O
tasks O
( O
i.e. O
one O
per O
convention O
article O
) O
. O
For O
task O
A|B O
, O
we O
concatenate O
a O
multi O
- O
hot O
10 O
- O
element O
feature O
vector O
containing O
the O
task O
B O
labels O
to O
the O
output O
of O
the O
feature O
extractor O
before O
it O
is O
passed O
to O
the O
classifier O
. O
All O
discriminators O
( O
country O
, O
length O
, O
and O
vocabulary O
) O
are O
built O
as O
analogous O
classifiers O
with O
a O
hidden B-HyperparameterName
dimension I-HyperparameterName
of O
100 B-HyperparameterValue
and O
output O
layer O
dimensions O
as O
required O
by O
each O
of O
them O
. O
We O
use O
mini B-HyperparameterName
batches I-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
in O
case O
of O
Task O
J O
and O
16 B-HyperparameterValue
for O
all O
other O
tasks O
. O
The O
model O
is O
optimized O
endto O
- O
end O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
. O
The O
dropout B-HyperparameterName
rate I-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
in O
all O
layers O
is O
0.1 B-HyperparameterValue
. O
We O
determine O
the O
best O
learning B-HyperparameterName
rate I-HyperparameterName
using O
a O
grid O
search O
on O
the O
development O
set O
and O
use O
early O
stopping O
based O
on O
the O
development O
set O
F1 B-MetricName
score I-MetricName
. O

I O
Visualization O
of O
IG B-MetricName
score I-MetricName

Figure O
7 O
exhibits O
screenshot O
excerpts O
of O
a O
sample O
case O
text O
provided O
to O
the O
legal O
expert O
for O
qualitative O
evaluation O
. O
The O
yellow O
background O
highlight O
was O
not O
in O
the O
original O
visualization O
and O
has O
been O
supplied O
here O
as O
a O
reference O
. O
We O
add O
it O
here O
as O
an O
example O
of O
focus O
patterns O
shifting O
incurred O
by O
our O
deconfounding O
method O
. O

Acknowledgments O

We O
are O
grateful O
to O
Jaromir O
Savelka O
for O
the O
ability O
to O
use O
the O
Gloss O
annotation O
tool O
and O
for O
providing O
feedback O
on O
the O
draft O
. O
We O
also O
thank O
the O
anonymous O
reviewers O
for O
valuable O
comments O
. O

Entity O
Cloze O
By O
Date O
: O
What O
LMs O
Know O
About O
Unseen O
Entities O

Language O
models O
( O
LMs O
) O
are O
typically O
trained O
once O
on O
a O
large O
- O
scale O
corpus O
and O
used O
for O
years O
without O
being O
updated O
. O
However O
, O
in O
a O
dynamic O
world O
, O
new O
entities O
constantly O
arise O
. O
We O
propose O
a O
framework O
to O
analyze O
what O
LMs O
can O
infer O
about O
new O
entities O
that O
did O
not O
exist O
when O
the O
LMs O
were O
pretrained O
. O
We O
derive O
a O
dataset O
of O
entities O
indexed O
by O
their O
origination O
date O
and O
paired O
with O
their O
English O
Wikipedia O
articles O
, O
from O
which O
we O
can O
find O
sentences O
about O
each O
entity O
. O
We O
evaluate O
LMs O
' O
perplexity O
on O
masked O
spans O
within O
these O
sentences O
. O
We O
show O
that O
models O
more O
informed O
about O
the O
entities O
, O
such O
as O
those O
with O
access O
to O
a O
textual O
definition O
of O
them O
, O
achieve O
lower O
perplexity O
on O
this O
benchmark O
. O
Our O
experimental O
results O
demonstrate O
that O
making O
inferences O
about O
new O
entities O
remains O
difficult O
for O
LMs O
. O
Given O
its O
wide O
coverage O
on O
entity O
knowledge O
and O
temporal O
indexing O
, O
our O
dataset O
can O
be O
used O
to O
evaluate O
LMs O
and O
techniques O
designed O
to O
modify O
or O
extend O
their O
knowledge O
. O
Our O
automatic O
data O
collection O
pipeline O
can O
be O
easily O
used O
to O
continually O
update O
our O
benchmark O
. O

Introduction O

New O
entities O
arise O
every O
day O
: O
new O
movies O
, O
TV O
shows O
, O
and O
products O
are O
created O
, O
new O
events O
occur O
, O
and O
new O
people O
come O
into O
the O
spotlight O
. O
Whatever O
the O
capabilities O
of O
language O
models O
( O
LMs O
) O
to O
represent O
entity O
knowledge O
, O
these O
new O
entities O
can O
not O
possibly O
be O
included O
in O
the O
language O
models O
' O
parametric O
knowledge O
( O
i.e. O
, O
knowledge O
acquired O
during O
pretraining O
) O
, O
as O
they O
did O
not O
exist O
when O
LMs O
were O
trained O
. O
Since O
this O
temporal O
mismatch O
between O
LMs O
and O
real O
- O
world O
knowledge O
affects O
model O
performance O
on O
downstream O
tasks O
Dhingra O
et O
al O
. O
, O
2021 O
; O
Lazaridou O
et O
al O
. O
, O
2021 O
) O
, O
understanding O
what O
LMs O
know O
about O
real O
- O
world O
entities O
is O
an O
important O
task O
. O

The O
existing O
literature O
provides O
various O
benchmarks O
to O
measure O
LMs O
' O
knowledge O
about O
entities O
( O
Petroni O
et O
al O
. O
, O
2019 O
( O
Petroni O
et O
al O
. O
, O
, O
2021Dhingra O
et O
al O
. O
, O
Figure O
1 O
: O
Our O
framework O
( O
ECBD O
) O
collects O
entities O
indexed O
by O
the O
year O
when O
they O
were O
first O
introduced O
in O
Wikipedia O
and O
their O
cloze O
sentences O
, O
unlike O
existing O
cloze O
datasets O
( O
LAMA O
( O
Petroni O
et O
al O
. O
, O
2019 O
) O
) O
which O
broadly O
cover O
entities O
introduced O
prior O
to O
2019 O
. O

2021 O
) O
. O
Those O
benchmarks O
are O
typically O
formulated O
as O
cloze O
- O
style O
tasks O
covering O
a O
limited O
set O
of O
relations O
bounded O
by O
knowledge O
bases O
: O
LAMA B-DatasetName
uses O
around O
40 O
Wikidata O
relations O
and O
entities O
collected O
in O
2017 O
. O
Newer O
cloze O
benchmarks O
( O
Dhingra O
et O
al O
. O
, O
2021 O
; O
Jang O
et O
al O
. O
, O
2021 O
) O
integrate O
temporal O
aspects O
to O
identify O
a O
time O
period O
when O
a O
cloze O
sentence O
is O
valid O
, O
but O
do O
not O
differentiate O
new O
and O
existing O
entities O
. O
These O
knowledge O
probing O
datasets O
fail O
to O
test O
broad O
knowledge O
about O
real O
- O
world O
entities O
or O
evaluate O
how O
LMs O
' O
knowledge O
differs O
on O
entities O
that O
are O
seen O
or O
unseen O
during O
pre O
- O
training O
. O

To O
fill O
this O
gap O
, O
we O
propose O
a O
framework O
to O
evaluate O
LMs O
' O
knowledge O
about O
entities O
classified O
by O
their O
origination O
date O
. O
We O
extract O
a O
set O
of O
Origination O
Date O
Indexed O
Entities O
( O
ODIE O
) O
based O
on O
metadata O
from O
Wikidata O
. O
We O
then O
construct O
cloze O
statements O
by O
masking O
sentences O
in O
those O
entities O
' O
Wikipedia O
articles O
. O
Unlike O
past O
knowledge O
probing O
datasets O
, O
these O
cloze O
sentences O
test O
the O
ability O
of O
a O
model O
to O
make O
a O
wide O
range O
of O
inferences O
related O
to O
entities O
, O
without O
being O
resticted O
to O
a O
pre O
- O
defined O
set O
of O
KB O
relations O
. O
We O
choose O
masked O
spans O
near O
these O
entities O
that O
likely O
contain O
information O
related O
to O
the O
entities O
, O
which O
we O
evaluate O
based O
on O
the O
perplexity O
gap O
between O
the O
raw O
sentence O
and O
the O
sentence O
with O
the O
entity O
replaced O
. O

We O
release O
the O
Entity B-DatasetName
Cloze I-DatasetName
by I-DatasetName
Date I-DatasetName
( O
ECBD B-DatasetName
) O
dataset O
of O
35k O
masked O
sentences O
that O
contain O
mentions O
of O
2.1 O
K O
ODIE O
entities O
, O
1 O
split O
by O
year O
covering O
a O
time O
period O
from O
2017 O
to O
2021 O
, O
together O
with O
8k O
masked O
sentences O
of O
popular O
entities O
from O
any O
time O
period O
. O
In O
our O
experiments O
, O
we O
evaluate O
three O
pre O
- O
trained O
language O
models O
in O
terms O
of O
perplexity O
. O
We O
establish O
that O
injecting O
additional O
information O
such O
as O
a O
text O
definition O
can O
meaningfully O
teach O
the O
model O
to O
make O
better O
guesses O
about O
masked O
spans O
, O
highlighting O
this O
dataset O
's O
utility O
for O
benchmarking O
methods O
of O
knowledge O
injection O
. O

Entity B-DatasetName
Cloze I-DatasetName
by I-DatasetName
Date I-DatasetName

We O
aim O
to O
test O
language O
models O
' O
1 O
) O
broader O
entity O
knowledge O
and O
2 O
) O
ability O
to O
reason O
about O
completely O
unseen O
entities O
( O
i.e. O
, O
unseen O
during O
pretraining O
) O
. O
Thus O
, O
we O
want O
to O
have O
the O
following O
properties O
in O
our O
entity O
cloze O
sentences O
. O
( O
1 O
) O
Date O
indexing O
. O
If O
each O
cloze O
example O
is O
associated O
with O
an O
entity O
and O
indexed O
by O
the O
origination O
date O
of O
that O
entity O
, O
we O
can O
understand O
whether O
a O
model O
may O
have O
seen O
it O
in O
its O
pre O
- O
training O
corpus O
or O
not O
. O
( O
2 O
) O
Diverse O
sentences O
. O
When O
going O
beyond O
KB O
triples O
, O
entity O
knowledge O
can O
take O
many O
forms O
: O
actions O
that O
an O
entity O
can O
take O
, O
other O
entities O
that O
action O
can O
effect O
, O
typical O
ways O
in O
which O
an O
entity O
is O
described O
, O
and O
more O
. O
Thus O
, O
we O
want O
include O
diverse O
sentences O
and O
masked O
spans O
that O
cover O
rich O
relations O
and O
various O
syntactic O
categories O
( O
e.g. O
, O
POS O
and O
nonterminal O
categories O
, O
span O
length O
) O
. O

Task O
Definition O

Each O
entity O
e O
is O
paired O
with O
e O
i O
, O
its O
origination O
year O
. O
Given O
a O
sentence O
s O
containing O
an O
entity O
mention O
span O
m O
e O
and O
a O
masked O
query O
span O
m O
q O
, O
a O
language O
model O
is O
asked O
to O
predict O
the O
gold O
masked O
span O
m O
y O
. O
See O
the O
following O
example O
: O
We O
evaluate O
language O
models O
by O
perplexity O
on O
the O
masked O
span O
m O
q O
( O
see O
Appendix O
D O
for O
a O
discussion O
of O
recall O
as O
another O
metric O
) O
. O

Sentence O
Collection O

Entities O
with O
Origination O
Date O

Sentences O
with O
Entity O
Mentions O

Masked O
Sentences O

Entity O
Mining O

Span O
Selection O

Figure O
2 O
: O
Overview O
of O
the O
data O
collection O
process O
. O

Data O
Collection O

Our O
data O
collection O
protocol O
consists O
of O
three O
stages O
: O
entity O
mining O
, O
sentence O
collection O
and O
span O
selection O
. O
We O
use O
English O
Wikipedia O
( O
the O
September O
1 O
, O
2021 O
dump O
) O
and O
Wikidata O
as O
knowledge O
sources O
. O

ODIE O
Mining O

We O
begin O
by O
gathering O
all O
entities O
on O
Wikidata O
that O
have O
an O
associated O
start O
time O
, O
announcement O
date O
, O
time O
of O
discovery O
or O
invention O
, O
inception O
date O
, O
point O
in O
time O
, O
or O
date O
it O
was O
introduced O
on O
. O
For O
such O
entities O
, O
we O
take O
the O
first O
of O
these O
dates O
to O
create O
our O
temporal O
splits O
, O
assuming O
that O
this O
is O
the O
earliest O
date O
the O
entity O
could O
have O
appeared O
in O
any O
pretraining O
corpus O
. O

To O
compare O
with O
ODIE O
which O
covers O
relatively O
new O
entities O
originated O
in O
2017 O
at O
the O
earliest O
, O
we O
use O
a O
set O
of O
POPULAR B-DatasetName
entities O
ranked O
by O
article O
contributor O
numbers O
and O
incoming O
links O
from O
prior O
work O
( O
Onoe O
et O
al O
. O
, O
2021 O
; O
Geva O
et O
al O
. O
, O
2021 O
) O
. O

Entity O
Sentence O
Collection O

Once O
we O
obtain O
a O
list O
of O
entities O
, O
we O
look O
up O
their O
English O
Wikipedia O
articles O
. O
To O
enrich O
the O
candidate O
sentence O
pool O
and O
exclude O
trivial O
sentences O
from O
stub O
articles O
, O
we O
filter O
entities O
if O
their O
corresponding O
articles O
contain O
less O
than O
500 O
words O
. O
From O
each O
article O
, O
we O
exclude O
the O
first O
paragraph O
of O
the O
article O
, O
to O
be O
used O
as O
an O
entity O
definition O
, O
and O
sample O
sentences O
from O
the O
rest O
of O
the O
paragraphs O
. O
We O
sample O
sentences O
that O
include O
the O
entity O
name O
or O
one O
of O
their O
Wikidata O
aliases O
. O
We O
do O
not O
accept O
entity O
mention O
spans O
located O
in O
quotes O
since O
they O
are O
often O
in O
nested O
named O
entities O
such O
as O
book O
titles O
. O
We O
also O
filter O
out O
any O
sentences O
with O
less O
than O
five O
words O
. O

Span O
Selection O
Next O
, O
we O
determine O
spans O
m O
q O
to O
be O
masked O
on O
a O
sentence O
, O
s O
; O
we O
can O
have O
multiple O
masked O
spans O
per O
sentence O
, O
masked O
separately O
. O
All O
spans O
must O
be O
: O
( O
a O
) O
not O
overlapping O
with O
the O
entity O
mention O
span O
, O
m O
e O
, O
( O
b O
) O
located O
after O
the O
entity O
mention O
span O
, O
m O
e O
, O
and O
( O
c O
) O
starting O
no O
more O
than O
ten O
words O
away O
from O
the O
mention O
span O
, O
to O
improve O
relatedness O
to O
the O
entity O
. O
We O
select O
spans O
after O
the O
entity O
mention O
so O
left O
- O
to O
- O
right O
language O
models O
will O
condition O
on O
the O
entity O
at O
test O
time O
. O

We O
extract O
two O
types O
of O
spans O
: O
NP O
spans O
are O
selected O
from O
any O
suitable O
noun O
phrases O
in O
the O
sentence O
using O
spaCy O
( O
Honnibal O
and O
Montani O
, O
2017 O
) O
. O
These O
spans O
primarily O
represent O
relational O
knowledge O
about O
the O
entity O
, O
analogous O
to O
the O
object O
in O
a O
KB O
triple O
. O
Random O
spans O
are O
arbitrary O
sequences O
of O
words O
sampled O
from O
the O
sentence O
. O
This O
broader O
set O
of O
spans O
may O
cover O
other O
types O
of O
entity O
knowledge O
( O
e.g. O
, O
probable O
actions O
an O
entity O
can O
take O
) O
. O
We O
uniformly O
sample O
span O
length O
between O
1 O
and O
5 O
and O
then O
randomly O
select O
the O
starting O
location O
of O
the O
span O
within O
the O
sentence O
. O
We O
only O
accept O
valid O
spans O
not O
overlapping O
with O
the O
entity O
mention O
. O
We O
extract O
at O
most O
100 O
spans O
per O
entity O
to O
limit O
any O
one O
entity O
's O
contribution O
to O
the O
final O
dataset O
. O

Span O
sensitivity O
to O
entity O
knowledge O

To O
see O
if O
our O
design O
choices O
are O
effective O
, O
we O
perform O
a O
test O
that O
measures O
the O
performance O
drop O
in O
perplexity O
using O
T5 O
when O
we O
replace O
the O
entity O
mention O
with O
a O
generic O
reference O
to O
" O
the O
entity O
. O
" O
We O
use O
entities O
from O
our O
POPULAR B-DatasetName
set O
to O
ensure O
that O
the O
LM O
has O
seen O
them O
during O
pre O
- O
training O
. O
If O
a O
masked O
span O
is O
related O
to O
the O
entity O
, O
the O
perplexity O
of O
that O
span O
should O
increase O
when O
the O
entity O
mention O
is O
omitted O
. O

We O
see O
that O
the O
median B-MetricName
perplexity I-MetricName
of O
a O
span O
increases O
by O
32.2 B-MetricValue
% I-MetricValue
when O
the O
entity O
is O
removed O
, O
indicating O
that O
these O
spans O
are O
indeed O
related O
to O
the O
entity O
. O
Moreover O
, O
removing O
the O
distance O
- O
based O
criterion O
for O
span O
selection O
decreases O
the O
perplexity B-MetricName
change O
to O
25.9 B-MetricValue
% I-MetricValue
. O
These O
results O
indicate O
that O
our O
selected O
spans O
are O
correlated O
with O
the O
entity O
. O
This O
gap O
test O
was O
performed O
only O
for O
analysis O
and O
we O
do O
not O
use O
any O
model O
- O
based O
data O
filtering O
. O
set O
of O
entities O
, O
ranging O
from O
events O
, O
products O
to O
organizations O
. O
One O
notably O
missing O
entity O
category O
is O
people O
; O
it O
is O
hard O
to O
pin O
down O
an O
origination O
year O
because O
of O
the O
significant O
gap O
between O
birth O
year O
and O
the O
year O
someone O
became O
prominent O
. O
Table O
2 O
reports O
statistics O
on O
our O
cloze O
task O
data O
and O
existing O
probe O
dataset O
( O
Petroni O
et O
al O
. O
, O
2019 O
) O
. O
While O
containing O
fewer O
entities O
, O
our O
dataset O
exhibits O
much O
richer O
vocabulary O
( O
19 O
K O
vs. O
2 O
K O
) O
, O
demonstrating O
diverse O
knowledge O
it O
covers O
. O
We O
split O
this O
data O
into O
dev O
and O
test O
sets O
by O
entities O
( O
i.e. O
, O
no O
shared O
entities O
between O
dev O
and O
test O
) O
. O
To O
balance O
out O
the O
data O
sizes O
across O
the O
groups O
, O
we O
sample O
4k O
examples O
for O
each O
year O
group O
, O
yielding O
35k O
examples O
in O
total O
( O
approx O
. O
20k O
for O
dev O
and O
20k O
for O
test O
) O
. O
Earlier O
dates O
contain O
a O
larger O
set O
of O
entities O
( O
599 O
entities O
for O
2017 O
compared O
to O
158 O
entities O
for O
2021 O
) O
as O
entities O
are O
continuously O
updated O
in O
Wikidata O
. O
In O
other O
words O
, O
many O
entities O
originated O
in O
2021 O
have O
not O
been O
yet O
added O
to O
Wikidata O
. O
We O
sample O
the O
same O
number O
of O
NP O
spans O
and O
random O
spans O
. O
Within O
the O
NP O
spans O
, O
35 O
% O
of O
them O
are O
proper O
noun O
phrases O
. O

Dataset O
Statistics O

Experiments O

Setup O
We O
evaluate O
T5 B-MethodName
- I-MethodName
large I-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
BART B-MethodName
- I-MethodName
large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
and O
GPT B-MethodName
- I-MethodName
Neo I-MethodName
( O
Black O
et O
al O
. O
, O
2021 O
) O
on O
our O
dataset O
in O
the O
zero O
- O
shot O
setting O
where O
the O
model O
parameters O
are O
fixed O
. O
In O
addition O
to O
the O
original O
masked O
sentence O
( O
ORIGINAL B-MethodName
) O
, O
we O
feed O
three O
modified O
masked O
sentences O
. O
NO B-MethodName
ENT I-MethodName
replaces O
the O
entity O
mention O
span O
with O
a O
generic O
string O
" O
the O
entity O
. O
" O
RANDOM B-MethodName
DEF I-MethodName
. O
prepends O
a O
definition O
sentence O
of O
a O
randomly O
selected O
entity O
. O
DEFINITION B-MethodName
prepends O
the O
first O
sentence O
of O
the O
entity O
's O
Wikipedia O
article O
to O
the O
cloze O
sentence O
. O

We O
evaluate O
these O
models O
on O
the O
subsets O
split O
by O
year O
as O
well O
as O
a O
set O
of O
popular O
entities O
. O
Note O
that O
the O
entities O
in O
the O
2020 O
and O
2021 O
subsets O
are O
unseen O
for O
T5 B-MethodName
and O
BART B-MethodName
. O
Most O
entities O
in O
the O
2020 O
and O
2021 O
subsets O
are O
unseen O
to O
GPT B-MethodName
- I-MethodName
Neo I-MethodName
, O
but O
its O
training O
data O
( O
the O
Pile O
( O
Gao O
et O
al O
. O
, O
2020 O
) O
) O
does O
include O
the O
March O
2020 O
English O
Wikipedia O
dump O
. O
In O
our O
experiments O
, O
we O
group O
the O
2020 O
and O
2021 O
subsets O
together O
as O
they O
consist O
of O
" O
unseen O
" O
entities O
. O
Similarly O
, O
we O
group O
the O
2017 O
, O
2018 O
, O
and O
2019 O
subsets O
whose O
entities O
are O
" O
seen O
" O
during O
pre O
- O
training O
. O
See O
Appendix O
B O
for O
perplexity O
per O
year O
. O

Evaluation O
Metric O
We O
compute O
tokennormalized O
perplexity B-MetricName
over O
the O
span O
as O
a O
proxy O
for O
entity O
knowledge O
stored O
in O
LMs O
. O
Each O
subset O
has O
different O
distribution O
of O
entity O
types O
( O
e.g. O
, O
2020 O
contains O
many O
COVID O
related O
entities O
and O
a O
lot O
less O
sports O
events O
compared O
to O
other O
years O
) O
, O
and O
some O
frequent O
entities O
might O
contribute O
to O
perplexity B-MetricName
excessively O
. O
To O
mitigate O
biases O
from O
particular O
entities O
, O
we O
first O
average O
negative O
log O
- O
likelihood O
( O
token O
normalized O
) O
over O
entities O
then O
average O
over O
examples O
. O
We O
follow O
the O
target O
sequence O
format O
used O
in O
LMs O
' O
pre O
- O
training O
tasks O
( O
see O
Figure O
3 O
) O
. O

Figure O
3 O
shows O
the O
perplexity O
computation O
. O
For O
left O
- O
to O
- O
right O
language O
models O
like O
GPT B-MethodName
- I-MethodName
Neo I-MethodName
, O
we O
compute O
the O
perplexity O
of O
the O
span O
given O
the O
left O
context O
only O
. O
T5 B-MethodName
and O
BART B-MethodName
, O
as O
seq2seq O
models O
, O
are O
able O
to O
also O
condition O
on O
the O
right O
context O
in O
their O
input O
; O
this O
makes O
perplexity O
values O
between O
these O
model O
classes O
not O
directly O
comparable O
( O
in O
addi- O
tion O
to O
differences O
in O
tokenization O
and O
pre O
- O
training O
tasks O
) O
. O
For O
T5 B-MethodName
and O
BART B-MethodName
, O
we O
condition O
on O
the O
input O
with O
a O
single O
mask O
. O
At O
decoding O
, O
for O
BART B-MethodName
we O
initialize O
the O
decoder O
with O
the O
left O
context O
of O
the O
span O
and O
compute O
perplexity O
on O
the O
true O
span O
filler O
following O
this O
left O
context O
. O
For O
T5 O
, O
we O
compute O
perplexity O
on O
the O
output O
span O
between O
the O
special O
tokens O
< O
extra_id_0 O
> O
and O
< O
extra_id_1 O
> O
. O

Results O
Table O
3 O
reports O
perplexity B-MetricName
( O
lower O
is O
better O
) O
on O
the O
test O
set O
that O
is O
split O
into O
three O
subsets O
: O
POPULAR B-DatasetName
, O
2017POPULAR B-DatasetName
, O
-2019POPULAR B-DatasetName
, O
, O
and O
2020POPULAR B-DatasetName
, O
-2021 O
. O
Note O
that O
absolute O
perplexity B-MetricName
across O
years O
is O
sensitive O
to O
factors O
such O
as O
distribution O
of O
sentences O
or O
entity O
types O
; O
we O
thus O
focus O
on O
relative O
performance O
. O
In O
all O
subsets O
, O
we O
observe O
two O
consistent O
trends O
across O
three O
LMs O
. O
( O
1 O
) O
NO B-MethodName
ENT I-MethodName
always O
degrades O
performance O
compared O
to O
ORIGINAL B-MethodName
. O
This O
result O
confirms O
that O
our O
masked O
spans O
are O
sensitive O
to O
the O
content O
of O
the O
entity O
span O
, O
although O
it O
is O
not O
conclusive O
proof O
of O
entity O
knowledge O
being O
required O
, O
as O
changing O
to O
" O
the O
entity O
" O
modifies O
other O
latent O
stylistic O
attributes O
that O
the O
LMs O
may O
be O
sensitive O
to O
. O

( O
2 O
) O
DEFINITION B-MethodName
always O
boosts O
performance O
over O
ORIGINAL B-MethodName
, O
indicating O
that O
providing O
more O
information O
about O
entities O
helps O
to O
retrieve O
information O
distributed O
over O
LMs O
' O
parameters O
. O
RANDOM B-MethodName
DEF I-MethodName
. O
distracts O
BART B-MethodName
and O
GPT B-MethodName
- O
Neo O
but O
slightly O
improves O
T5 B-MethodName
performance O
even O
though O
the O
additional O
information O
is O
taken O
from O
a O
random O
entity O
. O
This O
could O
be O
due O
to O
the O
model O
using O
different O
positional O
encodings O
as O
a O
result O
of O
having O
a O
definition O
, O
or O
LMs O
may O
select O
information O
if O
it O
is O
useful O
in O
some O
cases O
, O
leading O
the O
small O
gains O
. O

Performance O
on O
unseen O
entities O
Recall O
that O
we O
consider O
2020 O
- O
2021 O
as O
unseen O
entities O
, O
and O
2017 O
- O
2019 O
and O
POPULAR B-DatasetName
as O
seen O
entities O
. O
All O
three O
LMs O
give O
higher O
perplexity O
on O
unseen O
entities O
, O
showing O
that O
the O
spans O
in O
2020 O
- O
2021 O
are O
relatively O
unexpected O
to O
the O
LMs O
. O

We O
further O
investigate O
the O
performance O
delta O
between O
ORIGINAL B-MethodName
and O
DEFINITION B-MethodName
per O
subset O
. O
For O
all O
three O
LMs O
, O
we O
see O
that O
the O
performace O
delta O
is O
relatively O
larger O
on O
2020 O
- O
2021 O
, O
indicating O
definition O
sentences O
are O
more O
useful O
on O
unseen O
entities O
. O

Also O
, O
the O
performance O
delta O
on O
the O
popular O
entity O
set O
is O
notably O
smaller O
than O
2020 O
- O
2021 O
( O
compare O
T5 B-MethodName
numbers O
: O
13.02 O
→ O
11.04 O
for O
POPULAR B-MethodName
versus O
19.43 O
→ O
13.60 O
for O
2020 O
- O
2021 O
) O
. O
This O
implies O
that O
LMs O
contain O
some O
prior O
knowledge O
about O
common O
entities O
they O
have O
observed O
before O
, O
and O
can O
use O
additional O
information O
about O
new O
entities O
or O
less O
frequent O
entities O
. O
How O
to O
inject O
knowledge O
requires O
further O
investigation O
. O

Use O
Cases O

We O
envision O
this O
dataset O
as O
being O
useful O
for O
general O
knowledge O
probing O
, O
as O
the O
real O
- O
world O
knowledge O
covered O
by O
the O
existing O
benchmarks O
is O
gradually O
outdated O
. O
With O
our O
framework O
, O
we O
can O
easily O
update O
datasets O
using O
the O
most O
recent O
knowledge O
sources O
with O
a O
controlled O
manner O
. O
Since O
the O
entity O
knowledge O
in O
our O
dataset O
is O
time O
- O
indexed O
, O
this O
is O
suitable O
for O
evaluating O
knowledge O
editing O
approaches O
( O
Sinitsin O
et O
al O
. O
, O
2020 O
; O
Zhu O
et O
al O
. O
, O
2020 O
; O
Mitchell O
et O
al O
. O
, O
2021 O
; O
Meng O
et O
al O
. O
, O
2022 O
) O
and O
also O
continual O
knowledge O
learning O
approaches O
( O
Jang O
et O
al O
. O
, O
2021 O
) O
. O
Crucially O
, O
ex O
- O
isting O
work O
studies O
whether O
these O
approaches O
can O
inject O
single O
facts O
, O
but O
not O
whether O
they O
can O
enable O
models O
to O
robustly O
support O
a O
broad O
range O
of O
new O
inferences O
about O
entities O
, O
like O
our O
dataset O
allows O
. O

Related O
Work O

Temporal O
mismatch O
/ O
misalignment O
between O
large O
pre O
- O
trained O
LMs O
and O
real O
- O
world O
knowledge O
is O
an O
emerging O
research O
direction O
. O
Lazaridou O
et O
al O
. O
( O
2021 O
) O
show O
that O
the O
corpus O
- O
level O
perplexity O
on O
documents O
from O
beyond O
LMs O
' O
training O
period O
becomes O
increasingly O
poor O
over O
time O
. O
Dhingra O
et O
al O
. O
( O
2021 O
) O
propose O
TEMPORALLAMA B-DatasetName
, O
which O
is O
based O
on O
time O
- O
dependent O
knowledge O
base O
triples O
( O
i.e. O
, O
valid O
subject O
, O
relation O
, O
and O
object O
combinations O
given O
time O
) O
. O
SITUATEDQA B-DatasetName
( O
Zhang O
and O
Choi O
, O
2021 O
) O
includes O
time O
- O
dependent O
QA O
examples O
. O
While O
these O
datasets O
primarily O
test O
temporal O
information O
about O
entities O
in O
the O
pre O
- O
training O
data O
, O
ECBD B-DatasetName
focuses O
on O
new O
entities O
which O
did O
not O
exist O
during O
pretraining O
. O
TemporalWiki B-DatasetName
( O
Jang O
et O
al O
. O
, O
2022 O
) O
annotates O
new O
facts O
/ O
entities O
based O
on O
the O
differences O
between O
Wikidata O
/ O
English O
Wikipedia O
dumps O
, O
but O
does O
not O
necessarily O
reflect O
real O
- O
world O
changes O
during O
the O
time O
period O
( O
e.g. O
, O
an O
ancient O
queen O
can O
be O
added O
to O
Wikidata O
in O
2022 O
) O
. O
ECBD B-DatasetName
selects O
entities O
based O
on O
their O
origination O
date O
to O
align O
them O
with O
the O
real O
- O
world O
timeline O
. O

Another O
line O
of O
work O
has O
looked O
at O
diachronic O
embeddings O
: O
( O
Wijaya O
and O
Yeniterzi O
, O
2011 O
; O
Kim O
et O
al O
. O
, O
2014 O
; O
Hamilton O
et O
al O
. O
, O
2016 O
; O
Bamler O
and O
Mandt O
, O
2017 O
) O
, O
which O
can O
model O
changing O
meanings O
of O
words O
over O
time O
. O
Our O
setting O
focuses O
on O
introducing O
new O
concepts O
rather O
than O
rewriting O
existing O
ones O
, O
but O
data O
similar O
to O
ECBD B-DatasetName
could O
be O
collected O
for O
new O
usages O
of O
existing O
words O
. O

Although O
our O
dataset O
follows O
the O
widely O
- O
used O
cloze O
format O
, O
our O
focus O
is O
orthogonal O
to O
datasets O
like O
the O
Children B-DatasetName
's I-DatasetName
Book I-DatasetName
Test I-DatasetName
( O
Hill O
et O
al O
. O
, O
2016 O
) O
and O
LAMBADA B-DatasetName
( O
Paperno O
et O
al O
. O
, O
2016 O
) O
, O
which O
come O
from O
fiction O
and O
do O
not O
cover O
real O
- O
world O
entities O
. O

Conclusion O

In O
this O
paper O
, O
we O
present O
a O
dataset O
to O
understand O
language O
models O
' O
broad O
inferences O
about O
entities O
across O
time O
. O
We O
collect O
43k O
cloze O
- O
style O
sentences O
associated O
with O
a O
time O
- O
indexed O
set O
of O
entities O
. O
We O
also O
perform O
analysis O
on O
our O
data O
set O
and O
show O
that O
handling O
completely O
unseen O
entities O
remains O
challenging O
for O
the O
current O
LMs O
. O

A O
Examples O
of O
ECBD B-DatasetName
Sentences O

See O
Table O
4 O
for O
examples O
of O
masked O
sentences O
in O
the O
ECBD B-DatasetName
data O
. O

B O
Perplexity O
per O
year O

See O
Table O
5 O
for O
a O
more O
fine O
- O
grained O
view O
of O
the O
results O
in O
Table O
3 O
. O

C O
Perplexity O
per O
span O
type O

See O
Table O
6 O
for O
a O
breakdown O
of O
the O
perplexity O
that O
T5 B-MethodName
achieves O
on O
different O
types O
of O
spans O
, O
showing O
that O
random O
spans O
are O
generally O
higher O
perplexity O
than O
NP O
spans O
but O
that O
adding O
definitions O
can O
help O
both O
. O

D O
Recall O
@ O
10 O

LMs O
can O
be O
evaluated O
on O
recall O
@ O
10 O
, O
i.e. O
, O
a O
binary O
score O
indicating O
if O
model O
's O
top O
ten O
predictions O
contains O
the O
gold O
masked O
span O
m O
y O
. O
For O
T5 B-MethodName
, O
we O
first O
generate O
sequences O
using O
beam O
search O
( O
we O
choose O
beam O
size O
= O
100 O
in O
our O
experiments O
) O
. O
Then O
we O
take O
the O
top O
ten O
unique O
sequences O
and O
extract O
the O
text O
spans O
between O
< O
extra_id_0 O
> O
and O
< O
extra_id_1 O
> O
as O
predictions O
. O
Table O
7 O
reports O
recall O
@ O
10 O
on O
each O
subset O
. O
Table O
8 O
list O
recall O
@ O
10 O
per O
span O
type O
for O
each O
subset O
. O

We O
only O
explore O
recall O
on O
T5 B-MethodName
, O
since O
it O
is O
not O
obvious O
how O
to O
compute O
it O
for O
the O
other O
two O
models O
. O
For O
BART B-MethodName
, O
we O
can O
extract O
the O
predicted O
span O
by O
aligning O
the O
model O
's O
prediction O
with O
the O
gold O
context O
, O
assuming O
that O
it O
starts O
to O
copy O
from O
the O
input O
right O
context O
at O
some O
point O
. O
However O
, O
in O
some O
cases O
, O
we O
found O
that O
the O
generated O
right O
context O
does O
not O
match O
with O
the O
gold O
right O
context O
; O
it O
's O
unclear O
how O
to O
be O
handle O
this O
. O
For O
GPT B-MethodName
- I-MethodName
Neo I-MethodName
, O
since O
it O
is O
a O
leftto O
- O
right O
LM O
, O
extracting O
the O
predicted O
span O
would O
require O
conditioning O
on O
the O
span O
length O
, O
which O
is O
information O
that O
T5 O
does O
not O
have O
access O
to O
. O
As O
a O
result O
, O
we O
do O
not O
report O
recall O
@ O
10 O
for O
these O
models O
. O

E O
Data O
Licensing O

The O
Wikipedia O
text O
we O
used O
is O
licensed O
under O
CC O
BY O
- O
SA O
. O
Our O
use O
of O
Wikipedia O
, O
constructing O
a O
dataset O
which O
we O
will O
make O
publicly O
available O
under O
the O
same O
license O
, O
is O
consistent O
with O
the O
terms O
of O
the O
license O
. O

F O
Computational O
Resources O

All O
experiments O
were O
conducted O
using O
an O
NVIDIA O
Quadro O
RTX O
8000 O
. O
We O
only O
evaluate O
existing O
mod- O

Acknowledgments O

This O
work O
was O
partially O
supported O
by O
NSF O
Grant O
IIS-1814522 O
and O
by O
the O
Air O
Force O
Research O
Laboratory O
( O
AFRL O
) O
, O
Google O
Research O
Award O
, O
DARPA O
for O
the O
KAIROS O
program O
under O
agreement O
number O
FA8750 O
- O
19 O
- O
2 O
- O
1003 O
. O
The O
views O
and O
conclusions O
contained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
official O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
DARPA O
, O
or O
the O
U.S. O
Government O
. O
The O
U.S. O
Government O
is O
authorized O
to O
reproduce O
and O
distribute O
reprints O
for O
governmental O
purposes O
notwithstanding O
any O
copyright O
annotation O
therein O
. O

els O
on O
our O
datasets O
and O
did O
not O
do O
any O
finetuning O
. O
One O
evaluation O
experiment O
typically O
takes O
15 O
minutes O
to O
complete O
. O
For O
T5 B-MethodName
experiments O
, O
we O
use O
Hugging O
Face O
's O
Transformer O
package O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O

Robust O
Representation O
Learning O
with O
Reliable O
Pseudo O
- O
labels O
Generation O
via O
Self O
- O
Adaptive O
Optimal O
Transport O
for O
Short B-TaskName
Text I-TaskName
Clustering I-TaskName

Short B-TaskName
text I-TaskName
clustering I-TaskName
is O
challenging O
since O
it O
takes O
imbalanced O
and O
noisy O
data O
as O
inputs O
. O
Existing O
approaches O
can O
not O
solve O
this O
problem O
well O
, O
since O
( O
1 O
) O
they O
are O
prone O
to O
obtain O
degenerate O
solutions O
especially O
on O
heavy O
imbalanced O
datasets O
, O
and O
( O
2 O
) O
they O
are O
vulnerable O
to O
noises O
. O
To O
tackle O
the O
above O
issues O
, O
we O
propose O
a O
Robust B-MethodName
Short I-MethodName
Text I-MethodName
Clustering I-MethodName
( O
RSTC B-MethodName
) O
model O
to O
improve O
robustness O
against O
imbalanced O
and O
noisy O
data O
. O
RSTC B-MethodName
includes O
two O
modules O
, O
i.e. O
, O
pseudo O
- O
label O
generation O
module O
and O
robust O
representation O
learning O
module O
. O
The O
former O
generates O
pseudo O
- O
labels O
to O
provide O
supervision O
for O
the O
later O
, O
which O
contributes O
to O
more O
robust O
representations O
and O
correctly O
separated O
clusters O
. O
To O
provide O
robustness O
against O
the O
imbalance O
in O
data O
, O
we O
propose O
self O
- O
adaptive O
optimal O
transport O
in O
the O
pseudo O
- O
label O
generation O
module O
. O
To O
improve O
robustness O
against O
the O
noise O
in O
data O
, O
we O
further O
introduce O
both O
class O
- O
wise O
and O
instance O
- O
wise O
contrastive O
learning O
in O
the O
robust O
representation O
learning O
module O
. O
Our O
empirical O
studies O
on O
eight O
short B-TaskName
text I-TaskName
clustering I-TaskName
datasets O
demonstrate O
that O
RSTC B-MethodName
significantly O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
models O
. O
The O
code O
is O

Introduction O

Text B-TaskName
clustering I-TaskName
, O
one O
of O
the O
most O
fundamental O
tasks O
in O
text O
mining O
, O
aims O
to O
group O
text O
instances O
into O
clusters O
in O
an O
unsupervised O
manner O
. O
It O
has O
been O
proven O
to O
be O
beneficial O
in O
many O
applications O
, O
such O
as O
, O
recommendation O
system O
( O
Liu O
et O
al O
. O
, O
2021 O
( O
Liu O
et O
al O
. O
, O
, O
2022a O
, O
opinion O
mining O
( O
Stieglitz O
et O
al O
. O
, O
2018 O
) O
, O
stance O
detection O
, O
etc O
. O
With O
the O
advent O
of O
digital O
era O
, O
more O
and O
more O
people O
enjoy O
sharing O
and O
discovering O
various O
of O
contents O
on O
the O
web O
, O
where O
short O
text O
is O
an O
import O
form O
of O
information O
carrier O
. O
Therefore O
, O
it O
is O
helpful O
to O
utilize O
short B-TaskName
text I-TaskName
clustering I-TaskName
for O
mining O
valuable O
insights O
on O
the O
web O
. O

However O
, O
short B-TaskName
text I-TaskName
clustering I-TaskName
is O
not O
a O
trivial O
task O
. O
On O
the O
one O
hand O
, O
short O
text O
has O
many O
categories O
and O
the O
category O
distributions O
are O
diversifying O
, O
where O
the O
heavy O
imbalanced O
data O
is O
common O
. O
The O
heavy O
imbalanced O
data O
is O
prone O
to O
lead O
to O
degenerate O
solutions O
where O
the O
tail O
clusters O
( O
i.e. O
, O
the O
clusters O
with O
a O
small O
proportion O
of O
instances O
) O
disappear O
. O
Specifically O
, O
the O
recent O
deep O
joint O
clustering O
methods O
for O
short O
text O
clustering O
, O
( O
Hadifar O
et O
al O
. O
, O
2019 O
) O
and O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
, O
adopt O
the O
clustering O
objective O
proposed O
in O
( O
Xie O
et O
al O
. O
, O
2016 O
) O
, O
which O
may O
obtain O
a O
trivial O
solution O
where O
all O
the O
text O
instances O
fall O
into O
the O
same O
cluster O
( O
Yang O
et O
al O
. O
, O
2017 O
; O
Ji O
et O
al O
. O
, O
2019 O
) O
. O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
introduces O
instancewise O
contrastive O
learning O
to O
train O
discriminative O
representations O
, O
which O
avoids O
the O
trivial O
solution O
to O
some O
extent O
. O
However O
, O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
still O
tends O
to O
generate O
degenerate O
solutions O
, O
especially O
on O
the O
heavy O
imbalanced O
datasets O
. O

On O
the O
other O
hand O
, O
short O
text O
is O
typically O
characterized O
by O
noises O
, O
which O
may O
lead O
to O
meaningless O
or O
vague O
representations O
and O
thus O
hurt O
clustering O
accuracy O
and O
stability O
. O
Existing O
short B-TaskName
text I-TaskName
clustering I-TaskName
methods O
cope O
with O
the O
noise O
problem O
in O
three O
ways O
, O
i.e. O
, O
( O
1 O
) O
text O
preprocessing O
, O
( O
2 O
) O
outliers O
postprocessing O
, O
and O
( O
3 O
) O
model O
robustness O
. O
Specifically O
, O
earlier O
methods O
( O
Xu O
et O
al O
. O
, O
2017 O
; O
Hadifar O
et O
al O
. O
, O
2019 O
) O
apply O
preprocessing O
procedures O
on O
the O
text O
( O
HaCohen O
- O
Kerner O
et O
al O
. O
, O
2020 O
) O
for O
reducing O
the O
negative O
impact O
of O
noises O
. O
The O
recent O
method O
( O
Rakib O
et O
al O
. O
, O
2020 O
) O
proposes O
to O
postprocess O
outliers O
by O
repeatedly O
reassigning O
outliers O
to O
clusters O
for O
enhancing O
the O
clustering O
performance O
. O
However O
, O
both O
preprocessing O
and O
postprocessing O
methods O
do O
not O
provide O
model O
robustness O
against O
the O
noise O
in O
data O
. O
The O
more O
recently O
short O
text O
clustering O
method O
SCCL B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
proposes O
to O
utilize O
the O
instance O
- O
wise O
contrastive O
learning O
to O
support O
clustering O
, O
which O
is O
useful O
for O
dealing O
with O
the O
noises O
in O
the O
perspective O
of O
model O
robustness O
. O
However O
, O
the O
learned O
representations O
of O
SCCL B-MethodName
lack O
discrim O
- O
inability O
due O
to O
the O
lack O
of O
supervision O
information O
, O
causing O
insufficiently O
robust O
representations O
. O

In O
summary O
, O
there O
are O
two O
main O
challenges O
, O
i.e. O
, O
CH1 O
: O
How O
to O
provide O
model O
robustness O
to O
the O
imbalance O
in O
data O
, O
and O
avoid O
the O
clustering O
degeneracy O
? O
CH2 O
: O
How O
to O
improve O
model O
robustness O
against O
the O
noise O
in O
data O
, O
and O
enhance O
the O
clustering O
performance O
? O

To O
address O
the O
aforementioned O
issues O
, O
in O
this O
paper O
, O
we O
propose O
RSTC B-MethodName
, O
an O
end O
- O
to O
- O
end O
model O
for O
short B-TaskName
text I-TaskName
clustering I-TaskName
. O
In O
order O
to O
improve O
model O
robustness O
to O
the O
imbalance O
in O
data O
( O
solving O
CH1 O
) O
and O
the O
noise O
in O
data O
( O
solving O
CH2 O
) O
, O
we O
utilize O
two O
modules O
in O
RSTC O
, O
i.e. O
, O
pseudo O
- O
label O
generation O
module O
and O
robust O
representation O
learning O
module O
. O
The O
pseudo O
- O
label O
generation O
module O
generates O
pseudo O
- O
labels O
for O
the O
original O
texts O
. O
The O
robust O
representation O
learning O
module O
uses O
the O
generated O
pseudo O
- O
labels O
as O
supervision O
to O
facilitate O
intra O
- O
cluster O
compactness O
and O
inter O
- O
cluster O
separability O
, O
thus O
attaining O
more O
robust O
representations O
and O
more O
correctly O
separated O
clusters O
. O
The O
better O
cluster O
predictions O
in O
turn O
can O
be O
conductive O
to O
generate O
more O
reliable O
pseudo O
- O
labels O
. O
The O
iterative O
training O
process O
forms O
a O
virtuous O
circle O
, O
that O
is O
, O
the O
learned O
representations O
and O
cluster O
predictions O
will O
constantly O
boost O
each O
other O
, O
as O
more O
reliable O
pseudo O
- O
labels O
are O
discovered O
during O
iterations O
. O

The O
key O
idea O
to O
solve O
CH1 O
is O
to O
enforce O
a O
constraint O
on O
pseudo O
- O
labels O
, O
i.e. O
, O
the O
distribution O
of O
the O
generated O
pseudo O
- O
labels O
should O
match O
the O
estimated O
class O
distribution O
. O
The O
estimated O
class O
distribution O
is O
dynamically O
updated O
and O
expected O
to O
get O
closer O
to O
the O
ground O
truth O
progressively O
. O
Meanwhile O
, O
the O
estimated O
class O
distribution O
are O
encouraged O
to O
be O
a O
uniform O
distribution O
for O
avoiding O
clustering O
degeneracy O
. O
We O
formalize O
the O
idea O
as O
a O
new O
paradigm O
of O
optimal O
transport O
( O
Peyré O
et O
al O
. O
, O
2019 O
) O
and O
the O
optimization O
objective O
can O
be O
tractably O
solved O
by O
the O
Sinkhorn O
- O
Knopp O
( O
Cuturi O
, O
2013 O
) O
style O
algorithm O
, O
which O
needs O
only O
a O
few O
computational O
overheads O
. O
For O
addressing O
CH2 O
, O
we O
further O
introduce O
class O
- O
wise O
contrastive O
learning O
and O
instancewise O
contrastive O
learning O
in O
the O
robust O
representation O
learning O
module O
. O
The O
class O
- O
wise O
contrastive O
learning O
aims O
to O
use O
the O
pseudo O
- O
labels O
as O
supervision O
for O
achieving O
smaller O
intra O
- O
cluster O
distance O
and O
larger O
inter O
- O
cluster O
distance O
. O
While O
the O
instancewise O
contrastive O
learning O
tends O
to O
disperse O
the O
representations O
of O
different O
instances O
apart O
for O
the O
separation O
of O
overlapped O
clusters O
. O
These O
two O
modules O
cooperate O
with O
each O
other O
to O
provide O
better O
short B-TaskName
text I-TaskName
clustering I-TaskName
performance O
. O

We O
summarize O
our O
main O
contributions O
as O
follows O
: O
( O
1 O
) O
We O
propose O
an O
end O
- O
to O
- O
end O
model O
, O
i.e. O
, O
RSTC B-MethodName
, O
for O
short B-TaskName
text I-TaskName
clustering I-TaskName
, O
the O
key O
idea O
is O
to O
discover O
the O
pseudo O
- O
labels O
to O
provide O
supervision O
for O
robust O
representation O
learning O
, O
hence O
enhancing O
the O
clustering O
performance O
. O
( O
2 O
) O
To O
our O
best O
knowledge O
, O
we O
are O
the O
first O
to O
propose O
self O
- O
adaptive O
optimal O
transport O
for O
discovering O
the O
pseudo O
- O
label O
information O
, O
which O
provides O
robustness O
against O
the O
imbalance O
in O
data O
. O

( O
3 O
) O
We O
propose O
the O
combination O
of O
class O
- O
wise O
contrastive O
learning O
and O
instance O
- O
wise O
contrastive O
learning O
for O
robustness O
against O
the O
noise O
in O
data O
. O
( O
4 O
( O
Scott O
and O
Matwin O
, O
1998 O
; O
Salton O
and O
McGill O
, O
1983 O
) O
often O
obtain O
very O
sparse O
representations O
that O
lack O
discriminations O
. O
The O
deep O
learning O
method O
( O
Xu O
et O
al O
. O
, O
2017 O
) O
leverages O
pre O
- O
trained O
word O
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
deep O
neural O
network O
to O
enrich O
the O
representations O
. O
However O
, O
the O
learned O
representations O
may O
not O
appropriate O
for O
clustering O
. O
The O
deep O
joint O
clustering O
methods O
Hadifar O
et O
al O
. O
( O
2019 O
) O
; O
Zhang O
et O
al O
. O
( O
2021 O
) O
integrate O
clustering O
with O
deep O
representation O
learning O
to O
learn O
the O
representations O
that O
are O
appropriate O
for O
clustering O
. O
Moreover O
, O
Zhang O
et O
al O
. O
( O
2021 O
) O
utilizes O
the O
pre O
- O
trained O
SBERT O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
and O
contrastive O
learning O
to O
learn O
discriminative O
representations O
, O
which O
is O
conductive O
to O
deal O
with O
the O
noises O
. O
However O
, O
the O
adopted O
clustering O
objectives O
are O
prone O
to O
obtain O
degenerate O
solutions O
( O
Yang O
et O
al O
. O
, O
2017 O
; O
Ji O
et O
al O
. O
, O
2019 O
) O
, O
especially O
on O
heavy O
imbalance O
data O
. O

Among O
the O
above O
methods O
, O
only O
Zhang O
et O
al O
. O
( O
2021 O
) O
provides O
model O
robustness O
to O
the O
noise O
in O
data O
. O
However O
, O
its O
robustness O
is O
still O
insufficient O
due O
to O
the O
lack O
of O
supervision O
information O
. O
Besides O
, O
Zhang O
et O
al O
. O
( O
2021 O
) O
can O
not O
deal O
with O
various O
imbalanced O
data O
due O
to O
the O
degeneracy O
problem O
. O
As O
a O
contrast O
, O
in O
this O
work O
, O
we O
adopt O
pseudo O
- O
label O
technology O
to O
provide O
reliable O
supervision O
to O
learn O
robust O
representations O
for O
coping O
with O
imbalanced O
and O
noisy O
data O
. O

Pseudo O
- O
labels O
for O
Unsupervised O
Learning O

Pseudo O
- O
labels O
can O
be O
helpful O
to O
learn O
more O
discriminative O
representations O
in O
unsupervised O
learning O
( O
Hu O
et O
al O
. O
, O
2021 O
) O
. O
Caron O
et O
al O
. O
( O
2018 O
) O
shows O
that O
k O
- O
means O
clustering O
can O
be O
utilized O
to O
generate O
pseudo O
- O
labels O
for O
learning O
visual O
representations O
. O
However O
, O
it O
does O
not O
have O
a O
unified O
, O
well O
- O
defined O
objective O
to O
optimize O
( O
i.e. O
, O
there O
are O
two O
objectives O
: O
k O
- O
means O
loss O
minimization O
and O
cross O
- O
entropy O
loss O
minimization O
) O
, O
which O
means O
that O
it O
is O
difficult O
to O
characterize O
its O
convergence O
properties O
. O
Asano O
et O
al O
. O
( O
2020 O
) O
proposes O
SeLa O
to O
optimize O
the O
same O
objective O
( O
i.e. O
, O
cross O
- O
entropy O
loss O
minimization O
) O
for O
both O
pseudo O
- O
label O
generation O
and O
representation O
learning O
, O
which O
can O
guarantee O
its O
convergence O
. O
Besides O
, O
SeLa O
transforms O
pseudo O
- O
label O
generation O
problem O
into O
an O
optimal O
transport O
problem O
. O
Caron O
et O
al O
. O
( O
2020 O
) O
proposes O
SwAV O
which O
combines O
SeLa O
with O
contrastive O
learning O
to O
learn O
visual O
representations O
in O
an O
online O
fashion O
. O
However O
, O
both O
SeLa O
and O
SwAV O
add O
the O
constraint O
that O
the O
distribution O
of O
generated O
pseudo O
- O
labels O
should O
match O
the O
uniform O
distribution O
, O
to O
avoid O
clustering O
degeneracy O
. O
With O
the O
constraint O
, O
it O
is O
hard O
for O
them O
to O
cope O
with O
imbalanced O
data O
. O
As O
a O
contrast O
, O
in O
this O
work O
, O
we O
propose O
self O
- O
adaptive O
optimal O
transport O
to O
simultaneously O
estimate O
the O
real O
class O
distribution O
and O
generate O
pseudo O
- O
labels O
. O
Our O
method O
enforce O
the O
distribution O
of O
the O
generated O
pseudo O
- O
labels O
to O
match O
the O
estimated O
class O
distribution O
, O
and O
thus O
can O
avoid O
clustering O
degeneracy O
and O
adapt O
to O
various O
imbalanced O
data O
. O

Methodology O

An O
Overview O
of O
RSTC B-MethodName

The O
goal O
of O
RSTC B-MethodName
is O
to O
discover O
and O
utilize O
the O
pseudo O
- O
labels O
to O
provide O
supervision O
for O
robust O
representation O
learning O
. O
RSTC B-MethodName
consists O
of O
pseudolabel O
generation O
module O
and O
robust O
representation O
learning O
module O
, O
as O
illustrated O
in O
Fig O
. O
1 O
. O
The O
pseudo O
- O
label O
generation O
module O
aims O
to O
generate O
reliable O
pseudo O
- O
labels O
for O
the O
robust O
representation O
learning O
module O
. O
To O
achieve O
this O
aim O
, O
we O
first O
obtain O
cluster O
predictions O
by O
the O
cluster O
assignment O
step O
, O
then O
we O
excavate O
pseudo O
- O
label O
information O
from O
the O
predictions O
by O
the O
self O
- O
adaptive O
optimal O
transport O
( O
SAOT O
) O
step O
. O
The O
robust O
representation O
learning O
module O
aims O
to O
use O
the O
generated O
pseudolabels O
as O
supervision O
to O
train O
robust O
representations O
. O
To O
achieve O
this O
goal O
, O
we O
introduce O
class O
- O
wise O
and O
instance O
- O
wise O
contrastive O
learning O
. O
In O
this O
way O
, O
RSTC B-MethodName
can O
provide O
robustness O
to O
imbalanced O
and O
noisy O
data O
, O
thus O
enhancing O
the O
clustering O
performance O
. O

Pseudo O
- O
label O
Generation O
Module O

We O
first O
introduce O
the O
pseudo O
- O
label O
generation O
module O
. O
Although O
the O
deep O
joint O
clustering O
methods O
( O
Xie O
et O
al O
. O
, O
2016 O
; O
Hadifar O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
are O
popular O
these O
days O
, O
their O
clustering O
performance O
is O
limited O
due O
to O
the O
following O
reasons O
. O
Firstly O
, O
lacking O
supervision O
information O
prevents O
the O
deep O
joint O
clustering O
methods O
from O
learning O
more O
discriminative O
representations O
( O
Hu O
et O
al O
. O
, O
2021 O
) O
. O
Secondly O
, O
they O
are O
prone O
to O
obtain O
degenerate O
solutions O
( O
Yang O
et O
al O
. O
, O
2017 O
; O
Ji O
et O
al O
. O
, O
2019 O
) O
, O
especially O
on O
heavy O
imbalanced O
datasets O
. O
Therefore O
, O
to O
provide O
reliable O
supervision O
information O
for O
various O
imbalanced O
data O
, O
we O
propose O
SAOT O
in O
the O
pseudo O
- O
label O
generation O
module O
to O
generate O
pseudo O
- O
labels O
for O
the O
robust O
representation O
learning O
module O
. O
The O
overview O
of O
pseudo O
- O
label O
generation O
module O
is O
shown O
in O
Fig O
. O
1 O
( O
a O
) O
, O
which O
mainly O
has O
two O
steps O
: O
Step O
1 O
: O
cluster O
assignment O
, O
and O
Step O
2 O
: O
SAOT O
. O

Step O
1 O
: O
cluster O
assignment O
. O
Cluster O
assignment O
aims O
to O
obtain O
cluster O
predictions O
of O
the O
original O
texts O
. O
Specifically O
, O
we O
adopt O
SBERT O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
as O
the O
encoding O
network O
Φ O
to O
encode O
the O
original O
text O
X O
as O
Φ O
( O
X O
) O
= O
E O
∈ O
R O
N O
×D O
1 O
where O
N B-HyperparameterName
denotes O
batch O
size O
and O
D B-HyperparameterName
1 I-HyperparameterName
is O
the O
dimension O
of O
the O
representations O
. O
We O
utilize O
the O
fully O
connected O
layers O
as O
the O
clustering O
network O
G O
p O
to O
predict O
the O
cluster O
assignment O
probability O
( O
predictions O
) O
, O
i.e. O
, O
G O
p O
( O
E O
) O
= O
P O
∈ O
R O
N O
×C O
, O
where O
C O
is O
the O
category O
number O
. O
The O
encoding O
network O
and O
the O
clustering O
network O
are O
fixed O
in O
this O
module O
. O

Step O
2 O
: O
SAOT O
. O
SAOT O
aims O
to O
exploit O
the O
cluster O
predictions O
to O
discover O
reliable O
pseudo O
- O
label O
. O
Asano O
et O
al O
. O
( O
2020 O
) O
extends O
standard O
cross O
- O
entropy O
minimization O
to O
an O
optimal O
transport O
( O
OT O
) O
problem O
to O
generate O
pseudo O
- O
labels O
for O
learning O
image O
representations O
. O
This O
OT O
problem O
can O
be O
regarded O
as O
seeking O
the O
solution O
of O
transporting O
the O
sample O
distribution O
to O
the O
class O
distribution O
. O
However O
, O
the O
class O
distribution O
is O
unknown O
. O
Although O
Asano O
et O
al O
. O
( O
2020 O
) O
sets O
it O
to O
a O
uniform O
distribution O
to O
avoid O
degenerate O
solutions O
, O
the O
mismatched O
class O
distribution O
will O
lead O
to O
unreliable O
pseudo O
- O
labels O
. O
Therefore O
, O
it O
is O
essential O
to O
estimate O
real O
class O
distribution O
for O
addressing O
this O
issue O
. O
The O
recent O
research O
studies O
the O
class O
distribution O
estimation O
, O
but O
it O
tends O
to O
cause O
clustering O
degeneracy O
on O
heavy O
imbalanced O
data O
, O
which O
we O
will O
further O
discuss O
in O
Appendix O
A. O
Hence O
, O
to O
discover O
reliable O
pseudo O
- O
labels O
on O
various O
imbalanced O
data O
, O
we O
propose O
SAOT O
. O
We O
will O
provide O
the O
details O
of O
SAOT O
below O
. O
We O
expect O
to O
minimize O
the O
cross O
entropy O
loss O
to O
generate O
the O
pseudo O
- O
labels O
by O
solving O
a O
discrete O
OT O
problem O
. O
Specifically O
, O
we O
denote O
the O
pseudo O
- O
labels O
as O
Q O
∈ O
R O
N O
×C O
. O
Let O
π O
= O
1 O
N O
Q O
be O
the O
transport O
matrix O
between O
samples O
and O
classes O
, O
M O
= O
− O
log O
P O
be O
the O
cost O
matrix O
to O
move O
probability O
mass O
from O
samples O
to O
classes O
. O
The O
reason O
that O
we O
use O
1 O
N O
between O
π O
and O
Q O
is O
the O
transport O
matrix O
should O
be O
a O
joint O
probability O
( O
Cuturi O
, O
2013 O
) O
, O
i.e. O
, O
the O
sun O
of O
all O
values O
in O
the O
π O
should O
be O
1 O
, O
while O
the O
sum O
of O
each O
raw O
in O
Q O
is O
1 O
. O
We O
have O
, O

Weights O
Sharing O
Weights O
Sharing O
Projecting O
( O
! O
) O
" O
, O
# O
" O
, O
# O
Augmented O
Pairs O
( O
" O
) O
, O
( O
# O
) O
Clustering O
( O
& O
) O
Encoding O
( O
Φ O
) O
Original O
Texts O
Encoding O
( O
Φ O
) O
Clustering O
( O
& O
) O
ℒ O
& O
ℒ O
' O
" O
, O
# O
Representations O

Q O
* O
= O
argmin O
Q O
⟨Q O
, O
− O
log O
P O
⟩ O
= O
N O
argmin O
π O
⟨π O
, O
M O
⟩. O
Thus O
, O
the O
OT O
problem O
is O
as O
fol- O
lows O
: O
min O
π O
⟨π O
, O
M O
⟩ O
+ O
ϵH O
( O
π O
) O
s.t O
. O
π1 O
= O
a O
, O
π O
T O
1 O
= O
b O
, O
π O
≥ O
0 O
, O
( O
1 O

where O
ϵ B-HyperparameterName
is O
a O
balance O
hyper O
parameter O
, O
H O
( O
π O
) O
= O
⟨π O
, O
log O
π O
− O
1⟩ O
is O
the O
entropy O
regularization O
( O
Cuturi O
, O
2013 O
) O
, O
a O
= O
1 O
N O
1 O
is O
the O
sample O
distribution O
, O
and O
b O
is O
an O
unknown O
class O
distribution O
. O
To O
avoid O
clustering O
degeneracy O
and O
obtain O
reliable O
transport O
matrix O
with O
randomly O
initialized O
b O
, O
we O
introduce O
a O
penalty O
function O
about O
b O
to O
the O
OT O
objective O
and O
update O
b O
during O
the O
process O
of O
solving O
the O
transport O
matrix O
. O
We O
formulate O
the O
SAOT O
optimization O
problem O
as O
: O

min O
π O
, O
b O
⟨π O
, O
M O
⟩ O
+ O
ϵ O
1 O
H O
( O
π O
) O
+ O
ϵ O
2 O
( O
Ψ O
( O
b O
) O
) O
T O
1 O
s.t O
. O
π1 O
= O
a O
, O
π O
T O
1 O
= O
b O
, O
π O
≥ O
0 O
, O
b O
T O
1 O
= O
1 O
, O
( O
2 O
) O

where O
ϵ B-HyperparameterName
1 I-HyperparameterName
and O
ϵ B-HyperparameterName
2 I-HyperparameterName
are O
balance O
hyper O
- O
parameters O
, O

Ψ O
( O
b O
) O
= O
− O
log O
b−log O
( O
1−b O
) O

is O
the O
penalty O
function O
about O
b. O
The O
penalty O
function O
not O
only O
limits O
b O
j O
( O
a O
value O
of O
b O
) O
ranges O
from O
0 O
to O
1 O
, O
but O
also O
avoids O
clustering O
degeneracy O
by O
encouraging O
b O
to O
be O
a O
uniform O
distribution O
. O
The O
encouragement O
is O
achieved O
by O
increasing O
the O
punishment O
for O
b O
j O
that O
is O
close O
to O
0 O
or O
1 O
. O
Besides O
, O
the O
level O
of O
the O
encouragement O
can O
be O
adjusted O
by O
ϵ B-HyperparameterName
2 I-HyperparameterName
. O
Specifically O
, O
there O
are O
two O
critical O
terms O
in O
Equation O
( O
2 O
) O
for O
exploring O
b O
, O
i.e. O
, O
( O
1 O
) O
the O
cost O
matrix O
M O
and O
( O
2 O
) O
the O
penalty O
function O
Ψ O
( O
b O
) O
, O
and O
we O
use O
ϵ B-HyperparameterName
2 I-HyperparameterName
to O
balance O
these O
two O
terms O
. O
For O
balanced O
data O
, O
both O
M O
and O
Ψ O
( O
b O
) O
encourage O
b O
to O
be O
a O
uniform O
distribution O
. O
For O
imbalanced O
data O
, O
M O
encourages O
the O
head O
clusters O
( O
i.e. O
, O
the O
clusters O
with O
a O
large O
proportion O
of O
instances O
) O
to O
have O
larger O
b O
j O
and O
the O
tail O
clusters O
( O
i.e. O
, O
the O
clusters O
with O
a O
small O
proportion O
of O
instances O
) O
to O
have O
smaller O
b O
j O
. O
When O
b O
j O
of O
a O
tail O
cluster O
approaches O
0 O
, O
this O
tail O
cluster O
tends O
to O
disappear O
( O
clustering O
degeneracy O
) O
. O
Whereas O
Ψ O
( O
b O
) O
still O
encourages O
b O
to O
be O
a O
uniform O
distribution O
for O
avoiding O
the O
degeneracy O
. O
With O
a O
decent O
trade O
- O
off O
parameter O
ϵ B-HyperparameterName
2 I-HyperparameterName
, O
SAOT O
can O
explore O
appropriate O
b O
and O
obtain O
reliable O
π O
for O
various O
imbalanced O
data O
. O
We O
provide O
the O
optimization O
details O
in O
Appendix O
B. O
After O
obtaining O
π O
, O
we O
can O
get O
pseudo O
- O
labels O
by O
argmax O
operation O
, O
i.e O
, O

Q O
ij O
= O
 O
 O
 O
1 O
, O
if O
j O
= O
argmax O
j O
′ O
π O
ij O
′ O
0 O
, O
otherwise O
. O
( O
3 O
) O

It O
should O
be O
noted O
that O
, O
for O
convenience O
, O
we O
let O
π O
= O
1 O
N O
Q O
before O
. O
However O
, O
π O
is O
essentially O
a O
join O
probability O
matrix O
and O
π O
ij O
can O
be O
decimals O
, O
while O
each O
row O
of O
Q O
is O
a O
one O
- O
hot O
vector O
. O

Through O
the O
steps O
of O
cluster O
assignment O
and O
selfadaptive O
optimal O
transport O
, O
we O
can O
generate O
reliable O
pseudo O
- O
labels O
on O
various O
imbalanced O
data O
for O
the O
robust O
representation O
learning O
module O
. O

Robust O
Representation O
Learning O
module O

We O
then O
introduce O
the O
robust O
representation O
learning O
module O
. O
To O
begin O
with O
, O
motivated O
by O
( O
Wenzel O
et O
al O
. O
, O
2022 O
) O
, O
we O
propose O
to O
adopt O
instance O
augmentations O
to O
improve O
the O
model O
robustness O
against O
various O
noises O
. O
Furthermore O
, O
inspired O
by O
( O
Chen O
et O
al O
. O
, O
2020 O
) O
, O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
and O
( O
Dong O
et O
al O
. O
, O
2022 O
) O
, O
we O
adopt O
both O
class O
- O
wise O
and O
instance O
- O
wise O
contrastive O
learning O
to O
utilize O
the O
pseudo O
- O
labels O
and O
the O
augmented O
instance O
pairs O
for O
robust O
representation O
learning O
, O
as O
shown O
in O
Fig O
. O
1 O
( O
b O
) O
. O
The O
class O
- O
wise O
contrastive O
learning O
uses O
pseudo O
- O
labels O
as O
the O
supervision O
to O
pull O
the O
representations O
from O
the O
same O
cluster O
together O
and O
push O
away O
different O
clusters O
. O
While O
the O
instance O
- O
wise O
contrastive O
learning O
disperses O
different O
instances O
apart O
, O
which O
is O
supposed O
to O
separate O
the O
overlapped O
clusters O
. O

Next O
, O
we O
provide O
the O
details O
of O
the O
robust O
representation O
learning O
module O
. O
We O
utilize O
contextual O
augmenter O
( O
Kobayashi O
, O
2018 O
; O
Ma O
, O
2019 O
) O
to O
generate O
augmented O
pairs O
of O
the O
original O
texts O
as O
X O
( O
1 O
) O
and O
X O
( O
2 O
) O
. O
Like O
the O
cluster O
assignment O
step O
in O
the O
pseudo O
- O
labels O
generation O
module O
, O
we O
can O
obtain O
the O
representations O
of O
augmented O
pairs O
X O
( O
1 O
) O
and O

X O
( O
2 O
) O
as O
E O
( O
1 O
) O
∈ O
R O
N O
×D O
1 O
and O
E O
( O
2 O
) O
∈ O
R O
N O
×D O
1 O
, O
re- O
spectively O
. O

We O
can O
obtain O
the O
predictions O
of O
them O
as O
P O
( O
1 O
) O
∈ O
R O
N O
×C O
and O
P O
( O
2 O
) O
∈ O
R O
N O
×C O
, O
respectively O
. O
We O
use O
the O
fully O
connected O
layers O
as O
the O
projecting O
network O
G O
z O
to O
map O
the O
representations O
to O
the O
space O
where O
instance O
- O
wise O
contrastive O
loss O
is O
applied O
, O
i.e. O
, O
G O
z O
( O
E O
( O
1 O
) O
) O
= O
Z O
( O
1 O
) O
∈ O
R O
N O
×D O
2 O
and O
G O
z O
( O
E O
( O
2 O
) O
) O
= O
Z O
( O
2 O
) O
∈ O
R O
N O
×D O
2 O
, O
where O
D O
2 O
is O
the O
dimension O
of O
the O
projected O
representations O
. O
The O
encoding O
network O
and O
the O
clustering O
network O
share O
weights O
with O
the O
pseudo O
- O
label O
generation O
module O
. O

The O
class O
- O
wise O
contrastive O
learning O
enforces O
consistency O
between O
cluster O
predictions O
of O
positive O
pairs O
. O
Specifically O
, O
the O
two O
augmentations O
from O
the O
same O
original O
text O
are O
regarded O
as O
a O
positive O
pair O
and O
the O
contrastive O
task O
is O
defined O
on O
pairs O
of O
augmented O
texts O
. O
Moreover O
, O
the O
pseudo O
- O
label O
of O
an O
original O
text O
is O
considered O
as O
the O
target O
of O
corresponding O
two O
augmented O
texts O
. O
We O
use O
the O
augmented O
texts O
with O
the O
targets O
as O
supervised O
data O
for O
cross O
- O
entropy O
minimization O
to O
achieve O
the O
consistency O
. O
The O
class O
- O
wise O
contrastive O
loss O
is O
defined O
as O
below O
: O

L O
C O
= O
1 O
N O
⟨Q O
, O
− O
log O
P O
( O
1 O
) O
⟩ O
+ O
1 O
N O
⟨Q O
, O
− O
log O
P O
( O
2 O
) O
⟩ O
. O

( O
4 O
) O
The O
instance O
- O
wise O
contrastive O
learning O
enforces O
consistency O
between O
projected O
representations O
of O
positive O
pairs O
while O
maximizing O
the O
distance O
between O
negative O
pairs O
. O
Specifically O
, O
for O
a O
batch O
, O
there O
are O
2N O
augmented O
texts O
, O
their O
projected O
representations O
are O
Z O
= O
[ O
Z O
( O
1 O
) O
, O
Z O
( O
2 O
) O
] O
T O
, O
given O
a O
positive O
pair O
with O
two O
texts O
which O
are O
augmented O
from O
the O
same O
original O
text O
, O
the O
other O
2 O
( O
N O
− O
1 O
) O
augmented O
texts O
are O
treated O
as O
negative O
samples O
. O
The O
loss O
for O
a O
positive O
pair O
( O
i O
, O
j O
) O
is O
defined O
as O
: O

l O
( O
i O
, O
j O
) O
= O
− O
log O
exp O
( O
sim O
( O
Z O
i O
, O
Z O
j O
) O
/ O
τ O
) O
2N O
k=1 O
1 O
k̸ O
= O
i O
exp O
( O
sim O
( O
Z O
i O
, O
Z O
k O
) O
/ O
τ O
) O
, O
( O
5 O
) O

where O
sim O
( O
u O
, O
v O
) O
denotes O
cosine O
similarity O
between O
u O
and O
v O
, O
τ B-HyperparameterName
denotes O
the O
temperature O
parameter O
, O
and O
1 O
is O
an O
indicator O
. O
The O
instance O
- O
wise O
contrastive O
loss O
is O
computed O
across O
all O
positive O
pairs O
in O
a O
batch O
, O
including O
both O
( O
i O
, O
j O
) O
and O
( O
j O
, O
i O
) O
. O
That O
is O
, O

L O
I O
= O
1 O
2N O
N O
i=1 O
( O
l O
( O
i O
, O
2i O
) O
+ O
l O
( O
2i O
, O
i O
) O
) O
. O
( O
6 O
) O

By O
combining O
the O
pseudo O
- O
supervised O
class O
- O
wise O
contrastive O
learning O
and O
the O
instance O
- O
wise O
contrastive O
learning O
, O
we O
can O
obtain O
robust O
representations O
and O
correctly O
separated O
clusters O
. O

Putting O
Together O

The O
total O
loss O
of O
RSTC B-MethodName
could O
be O
obtained O
by O
combining O
the O
pseudo O
- O
supervised O
class O
- O
wise O
contrastive O
loss O
and O
the O
instance O
- O
wise O
contrastive O
loss O
. O
That O
is O
, O
the O
loss O
of O
RSTC B-MethodName
is O
given O
as O
: O

L O
= O
L O
C O
+ O
λ B-HyperparameterName
I I-HyperparameterName
L O
I O
, O
( O
7 O
) O

where O
λ B-HyperparameterName
I I-HyperparameterName
is O
a O
hyper O
- O
parameter O
to O
balance O
the O
two O
losses O
. O
By O
doing O
this O
, O
RSTC B-MethodName
not O
only O
provides O
robustness O
to O
the O
imbalance O
in O
data O
, O
but O
also O
improve O
robustness O
against O
the O
noise O
in O
data O
. O

The O
whole O
model O
with O
two O
modules O
forms O
a O
closed O
loop O
and O
self O
evolution O
, O
which O
indicates O
that O
the O
learned O
representations O
( O
more O
robust O
) O
and O
cluster O
predictions O
( O
more O
accurate O
) O
elevate O
each O
other O
progressively O
, O
as O
more O
reliable O
pseudo O
- O
labels O
are O
discovered O
during O
the O
iterations O
. O
Specifically O
, O
we O
firstly O
initialize O
the O
pseudo O
- O
labels O
Q O
by O
performing O
k O
- O
means O
on O
text O
representations O
. O
Next O
, O
we O
train O
the O
robust O
representation O
learning O
module O
by O
batch O
with O
the O
supervision O
of O
pseudo O
- O
labels O
. O
Meanwhile O
, O
we O
update O
Q O
throughout O
the O
whole O
training O
process O
in O
a O
logarithmic O
distribution O
, O
following O
( O
Asano O
et O
al O
. O
, O
2020 O
) O
. O
Finally O
, O
we O
can O
obtain O
the O
cluster O
assignments O
by O
the O
column O
index O
of O
the O
largest O
entry O
in O
each O
row O
of O
P O
. O
The O
training O
stops O
if O
the O
change O
of O
cluster O
assignments O
between O
two O
consecutive O
updates O
for O
P O
is O
less O
than O
a O
threshold O
δ B-HyperparameterName
or O
the O
maximum B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
iterations I-HyperparameterName
is O
reached O
. O

Experiment O

In O
this O
section O
, O
we O
conduct O
experiments O
on O
several O
real O
- O
world O
datasets O
to O
answer O
the O
following O
questions O
: O
( O
1 O
) O
RQ1 O
: O
How O
does O
our O
approach O
perform O
compared O
with O
the O
state O
- O
of O
- O
the O
- O
art O
short B-TaskName
text I-TaskName
clustering I-TaskName
methods O
? O
( O
2 O
) O
RQ2 O
: O
How O
do O
the O
SAOT O
, O
and O
the O
two O
contrastive O
losses O
contribute O
to O
the O
performance O
improvement O
? O
( O
3 O
) O
RQ3 O
: O
How O
does O
the O
performance O
of O
RSTC B-MethodName
vary O
with O
different O
values O
of O
the O
hyper O
- O
parameters O
? O

Datasets O

We O
conduct O
extensive O
experiments O
on O
eight O
popularly O
used O
real O
- O
world O
datasets O
, O
i.e. O
, O
AgNews B-DatasetName
, O
StackOverflow B-DatasetName
, O
Biomedical B-DatasetName
, O
SearchSnippets B-DatasetName
, O
GoogleNews B-DatasetName
- I-DatasetName
TS I-DatasetName
, O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
, O
GoogleNews B-DatasetName
- I-DatasetName
S I-DatasetName
and O
Tweet B-DatasetName
. O
Among O
them O
, O
AgNews B-DatasetName
, O
Stack B-DatasetName
- I-DatasetName
Overflow I-DatasetName
and O
Biomedical B-DatasetName
are O
balanced O
datasets O
, O
SearchSnippets B-DatasetName
is O
a O
light O
imbalanced O
dataset O
, O
GoogleNews B-DatasetName
, O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
, O
GoogleNews B-DatasetName
- I-DatasetName
S I-DatasetName
and O
Tweet B-DatasetName
are O
heavy O
imbalanced O
datasets O
. O
Following O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
, O
we O
take O
unpreprocessed O
data O
as O
input O
to O
demonstrate O
that O
our O
model O
is O
robust O
to O
noise O
, O
for O
a O
fair O
comparison O
. O
More O
details O
about O
the O
datasets O
are O
shown O
in O
Appendix O
C.1 O
. O

Experiment O
Settings O

We O
build O
our O
model O
with O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2019 O
) O
and O
train O
it O
using O
the O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
. O
We O
study O
the O
effect O
of O
hyper O
- O
parameters O
ϵ B-HyperparameterName
1 I-HyperparameterName
and O
ϵ B-HyperparameterName
2 I-HyperparameterName
on O
SAOT O
by O
varying O
them O
in O
{ O
0.05 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.2 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
} O
and O
{ O
0 B-HyperparameterValue
, O
0.001 B-HyperparameterValue
, O
0.01 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
} O
, O
respectively O
. O
Besides O
, O
we O
study O
the O
effect O
of O
the O
hyper O
- O
parameter O
λ B-HyperparameterName
I I-HyperparameterName
by O
varying O
it O
in O
{ O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
20 B-HyperparameterValue
, O
50 B-HyperparameterValue
, O
100 B-HyperparameterValue
} O
. O
The O
more O
details O
are O
provided O
in O
Appendix O
C.2 O
. O
Following O
previous O
work O
( O
Xu O
et O
al O
. O
, O
2017 O
; O
Hadifar O
et O
al O
. O
, O
2019 O
; O
Rakib O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
, O
we O
set O
the O
cluster O
numbers O
to O
the O
ground O
- O
truth O
category O
numbers O
, O
and O
we O
adopt O
Accuracy B-MetricName
( O
ACC B-MetricName
) O
and O
Normalized B-MetricName
Mutual I-MetricName
Information I-MetricName
( O
NMI B-MetricName
) O
to O
evaluate O
different O
approaches O
. O
The O
specific O
definitions O
of O
the O
evaluation O
methods O
are O
shown O
in O
Appendix O
C.3 O
. O
For O
all O
the O
experiments O
, O
we O
repeat O
five O
times O
and O
report O
the O
average O
results O
. O

Baselines O

We O
compare O
our O
proposed O
approach O
with O
the O
following O
short B-TaskName
text I-TaskName
clustering I-TaskName
methods O
. O
BOW B-MethodName
( O
Scott O
and O
Matwin O
, O
1998 O
) O
& O
TF B-MethodName
- I-MethodName
IDF I-MethodName
( O
Salton O
and O
McGill O
, O
1983 O
) O
applies O
k O
- O
means O
on O
the O
TF O
- O
IDF O
representations O
and O
BOW B-MethodName
representations O
respectively O
. O
STC B-MethodName
2 I-MethodName
-LPI I-MethodName
( O
Xu O
et O
al O
. O
, O
2017 O
) O
first O
uses O
word2vec O
to O
train O
word O
embeddings O
on O
the O
in O
- O
domain O
corpus O
, O
and O
then O
uses O
a O
convolutional O
neural O
network O
to O
obtain O
the O
text O
representations O
where O
k O
- O
means O
is O
applied O
for O
clustering O
. O
Self B-MethodName
- I-MethodName
Train I-MethodName
( O
Hadifar O
et O
al O
. O
, O
2019 O
) O
follows O
( O
Xie O
et O
al O
. O
, O
2016 O
) O
uses O
an O
autoencoder O
to O
get O
the O
representations O
, O
and O
finetunes O
the O
encoding O
network O
with O
the O
same O
clustering O
objective O
. O
The O
difference O
are O
that O
it O
uses O
the O
word O
embeddings O
provided O
by O
( O
Xu O
et O
al O
. O
, O
2017 O
) O
with O
SIF O
( O
Arora O
et O
al O
. O
, O
2017 O
) O
to O
enhance O
the O
pretrained O
word O
embeddings O
, O
and O
obtains O
the O
final O
cluster O
assignments O
via O
k O
- O
means O
. O
K B-MethodName
- I-MethodName
means_IC I-MethodName
( O
Rakib O
et O
al O
. O
, O
2020 O
) O
first O
applies O
k O
- O
means O
on O
the O
TF O
- O
IDF O
representations O
and O
then O
enhances O
clustering O
by O
the O
iterative O
classification O
algorithm O
. O
SCCL B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
is O
the O
more O
recent O
short B-TaskName
text I-TaskName
clustering I-TaskName
model O
which O
utilizes O
SBERT O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
as O
the O
backbone O
and O
introduces O
instance O
- O
wise O
contrastive O
learning O
to O
support O
clustering O
. O
Besides O
, O
SCCL B-MethodName
uses O
the O
clustering O
objective O
proposed O
in O
( O
Xie O
et O
al O
. O
, O
2016 O
) O
for O
deep O
joint O
clustering O
and O
obtains O
the O
final O
cluster O
assignments O
by O
k O
- O
means O
. O

Clustering O
Performance O
( O
RQ1 O
) O

Results O
and O
discussion O
The O
comparison O
results O
on O
eight O
datasets O
are O
shown O
in O
Table O
1 O
. O
SBERT B-MethodName
( I-MethodName
kmeans I-MethodName
) I-MethodName
denotes O
the O
pre O
- O
trained O
SBERT O
model O
with O
k O
- O
means O
clustering O
, O
which O
is O
the O
initial O
state O
of O
our O
RSTC B-MethodName
. O

From O
the O
results O
, O
we O
can O
find O
that O
: O
( O
1 O
) O
Only O
adopting O
traditional O
text O
representations O
( O
BOW B-MethodName
and O
( O
Yang O
et O
al O
. O
, O
2017 O
; O
Ji O
et O
al O
. O
, O
2019 O
1 O
. O
From O
it O
, O
we O
can O
observe O
that O
they O
all O
can O
not O
achieve O
satisfactory O
results O
due O
to O
their O
limitations O
. O
Specifically O
, O
( O
1 O
) O
RSTC B-MethodName
- I-MethodName
OT I-MethodName
will O
be O
guided O
by O
the O
mismatched O
distribution O
constraint O
to O
generate O
unreliable O
pseudo O
- O
labels O
. O
( O
2 O
) O
RSTC B-MethodName
- I-MethodName
C I-MethodName
is O
good O
at O
aggregating O
instances O
, O
but O
it O
has O
difficulties O
to O
address O
the O
situation O
when O
different O
categories O
are O
overlapped O
with O
each O
other O
in O
the O
representation O
space O
at O
the O
beginning O
of O
the O
learning O
progress O
, O
which O
may O
lead O
to O
a O
false O
division O
. O
shows O
that O
choosing O
the O
proper O
hyper O
- O
parameters O
for O
different O
imbalance O
levels O
of O
datasets O
is O
important O
, O
especially O
on O
the O
heavy O
imbalanced O
dataset O
GoogleNews B-DatasetName
- I-DatasetName
T. I-DatasetName
Empirically O
, O
we O
choose O
ϵ B-HyperparameterName
1 I-HyperparameterName
= O
0.1 B-HyperparameterValue
on O
all O
datasets O
, O
ϵ B-HyperparameterName
2 I-HyperparameterName
= O
0.1 B-HyperparameterValue
on O
the O
balanced O
datasets O
, O
ϵ B-HyperparameterName
2 I-HyperparameterName
= O
0.01 B-HyperparameterValue
on O
the O
light O
imbalanced O
datasets O
, O
and O
ϵ B-HyperparameterName
2 I-HyperparameterName
= O
0.001 B-HyperparameterValue
on O
the O
heavy O
imbalanced O
datasets O
. O
Then O
we O
perform O
experiments O
by O
varying O
λ B-HyperparameterName
I I-HyperparameterName
in O
{ O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
20 B-HyperparameterValue
, O
50 B-HyperparameterValue
, O
100 B-HyperparameterValue
} O
. O
The O
results O
on O
three O
datasets O
are O
shown O
in O
Fig O
. O
4 O
. O
From O
them O
, O
we O
can O
see O
that O
the O
performance O
improves O
when O
λ B-HyperparameterName
I I-HyperparameterName
increases O
, O
then O
keeps O
a O
relatively O
stable O
level O
after O
λ B-HyperparameterName
I I-HyperparameterName
reaches O
1 O
and O
finally O
decreases O
when O
λ B-HyperparameterName
I I-HyperparameterName
becomes O
too O
large O
. O
We O
can O
conclude O
that O
when O
λ B-HyperparameterName
I I-HyperparameterName
is O
too O
small O
, O
the O
ability O
of O
instance O
- O
wise O
contrastive O
learning O
can O
not O
be O
fully O
exploited O
. O
When O
λ B-HyperparameterName
I I-HyperparameterName
is O
too O
large O
, O
the O
ability O
of O
class O
- O
wise O
contrastive O
learning O
will O
be O
suppressed O
, O
which O
also O
reduces O
the O
clustering O
performance O
. O
Empirically O
, O
we O
choose O
λ B-HyperparameterName
I I-HyperparameterName
= O
10 B-HyperparameterValue
for O
all O
datasets O
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
a O
robust B-MethodName
short I-MethodName
text I-MethodName
clustering I-MethodName
( O
RSTC B-MethodName
) O
model O
, O
which O
includes O
pseudo O
- O
label O
generation O
module O
and O
robust O
representation O
learning O
module O
. O
The O
former O
generates O
pseudo O
- O
labels O
as O
the O
supervision O
for O
the O
latter O
. O
We O
innovatively O
propose O
SAOT O
in O
the O
pseudo O
- O
label O
generation O
mod O
- O
ule O
to O
provide O
robustness O
against O
the O
imbalance O
in O
data O
. O
We O
further O
propose O
to O
combine O
classwise O
contrastive O
learning O
with O
instance O
- O
wise O
contrastive O
learning O
in O
the O
robust O
representation O
learning O
module O
to O
provide O
robustness O
against O
the O
noise O
in O
data O
. O
Extensive O
experiments O
conducted O
on O
eight O
real O
- O
world O
datasets O
demonstrate O
the O
superior O
performance O
of O
our O
proposed O
RSTC B-MethodName
. O

Limitations O

Like O
existing O
short B-TaskName
text I-TaskName
clustering I-TaskName
methods O
, O
we O
assume O
the O
real O
cluster O
number O
is O
known O
. O
In O
the O
future O
, O
we O
would O
like O
to O
explore O
a O
short B-TaskName
text I-TaskName
clustering I-TaskName
method O
with O
an O
unknown O
number O
of O
clusters O
. O
Moreover O
, O
the O
time O
complexity O
of O
self O
- O
adaptive O
optimal O
transport O
is O
O O
( O
n O
2 O
) O
, O
we O
are O
going O
to O
seek O
a O
new O
computation O
to O
reduce O
the O
complexity O
. O

Proceedings O
of O
the O
2021 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
NAACL O
- O
HLT O
2021 O
, O
Online O
, O
June O
6 O
- O
11 O
, O
2021 O
, O
pages O
5419 O
- O
5430 O
. O
Association O
for O
Computational O
Linguistics O
. O

Xiang O
Zhang O
, O
Junbo O
Zhao O
, O
and O
Yann O
LeCun O
. O
2015 O
. O
Character O
- O
level O
convolutional O
networks O
for O
text O
classification O
. O
Advances O
in O
neural O
information O
processing O
systems O
, O
28 O
. O

A O
Different O
Class O
Distribution O
Estimation O
Methods O

We O
have O
tried O
three O
class O
distribution O
estimation O
methods O
, O
including O
: O
( O
1 O
This O
method O
replaces O
the O
penalty O
function O
in O
our O
method O
with O
the O
common O
entropy O
regularization O
Ψ O
( O
b O
) O
= O
KL O
( O
b O
∥b O
) O
, O
whereb O
is O
the O
last O
updated O
b O
, O
and O
the O
current O
b O
can O
be O
updated O
the O
same O
way O
our O
method O
does O
. O
Note O
that O
, O
the O
parameters O
of O
M2 B-MethodName
are O
following O
, O
the O
parameters O
of O
M3 B-MethodName
are O
the O
same O
as O
M1 B-MethodName
( O
ours B-MethodName
) O
. O

For O
comprehensive O
comparison O
, O
we O
conduct O
the O
experiments O
on O
one O
imbalanced O
dataset O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
and O
one O
balanced O
dataset O
Stack B-DatasetName
- I-DatasetName
Overflow I-DatasetName
with O
randomly O
initialized O
b O
for O
visualizing O
how O
the O
accuracy O
and O
the O
number O
of O
predicted O
clusters O
are O
changing O
over O
iterations O
. O
Moreover O
, O
except O
the O
update O
of O
b O
, O
everything O
else O
about O
the O
experiments O
is O
the O
same O
for O
three O
methods O
. O
The O
results O
are O
shown O
in O
Fig O
. O
5 O
( O
a O
) O
- O
( O
d O
) O
. O
From O
them O
, O
we O
can O
find O
that O
: O
( O
1 O
) O
For O
the O
imbalanced O
dataset O
, O
M1 B-MethodName
( O
ours B-MethodName
) O
achieves O
the O
best O
accuracy B-MetricName
and O
converges O
to O
the O
real O
category O
number O
, O
while O
other O
methods O
have O
clustering O
degeneracy O
problem O
. O
( O
2 O
) O
For O
the O
balanced O
dataset O
, O
M2 B-MethodName
achieves O
best O
accuracy B-MetricName
more O
quickly O
while O
M1 B-MethodName
( O
ours B-MethodName
) O
catches O
up O
in O
the O
end O
, O
and O
all O
methods O
obtain O
real O
category O
number O
. O
Although O
M3 B-MethodName
can O
obtain O
good O
accuracy B-MetricName
on O
the O
imbalanced O
dataset O
, O
it O
has O
the O
worst O
accuracy B-MetricName
on O
the O
balanced O
dataset O
. O
In O
addition O
, O
although O
M2 B-MethodName
achieves O
good O
accuracy B-MetricName
on O
the O
balanced O
dataset O
, O
it O
has O
the O
worst O
accuracy B-MetricName
on O
the O
imbalanced O
dataset O
. O
Only O
M1 B-MethodName
( O
ours B-MethodName
) O
achieves O
Figure O
5 O
: O
The O
accuracy B-MetricName
and O
the O
number O
of O
predicted O
clusters O
at O
different O
iterations O
on O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
( O
first O
row O
) O
and O
StackOverflow B-DatasetName
( O
second O
row O
) O
. O
Note O
that O
because O
the O
samples O
in O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
are O
too O
short O
, O
which O
makes O
it O
difficult O
to O
generate O
relatively O
reliable O
pseudolabels O
, O
we O
pre O
- O
train O
the O
representations O
with O
L O
I O
for O
three O
methods O
in O
the O
first O
600 O
steps O
. O
Due O
to O
the O
same O
pretraining O
process O
, O
we O
omit O
the O
curves O
in O
the O
first O
600 O
steps O
on O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
. O

fairly O
good O
performance O
on O
both O
datasets O
, O
which O
indicates O
that O
our O
method O
are O
robust O
to O
various O
imbalance O
levels O
of O
datasets O
. O
The O
experiments O
prove O
the O
effectiveness O
of O
our O
class O
distribution O
estimation O
method O
. O

B O
SAOT O

As O
mentioned O
in O
Section O
3.2 O
, O
the O
SAOT O
problem O
is O
formulated O
as O
: O

min O
π O
, O
b O
⟨π O
, O
M O
⟩ O
+ O
ϵ O
1 O
H O
( O
π O
) O
+ O
ϵ O
2 O
( O
Ψ O
( O
b O
) O
) O
T O
1 O
, O
s.t O
. O
π1 O
= O
a O
, O
π O
T O
1 O
= O
b O
, O
π O
≥ O
0 O
, O
b O
T O
1 O
= O
1 O
. O
( O
8 O
) O

where O
ϵ B-HyperparameterName
1 I-HyperparameterName
and O
ϵ B-HyperparameterName
2 I-HyperparameterName
are O
balance O
hyper O
- O
parameters O
, O

Ψ O
( O
b O
) O
= O
− O
log O
b O
− O
log O
( O
1 O
− O
b O
) O

is O
the O
penalty O
function O
about O
b. O
We O
adopt O
the O
Lagrangian O
multiplier O
algorithm O
to O
optimize O
the O
problem O
: O

min O
π O
, O
b O
⟨π O
, O
M O
⟩ O
+ O
ϵ B-HyperparameterName
1 I-HyperparameterName
H O
( O
π O
) O
+ O
ϵ B-HyperparameterName
2 I-HyperparameterName
( O
Ψ O
( O
b O
) O
) O
T O
1 O
− O
f O
T O
( O
π1 O
− O
a O
) O
− O
g O
T O
( O
π O
T O
1 O
− O
b O
) O
− O
h O
( O
b O
T O
1 O
− O
1 O
) O
, O
( O
9 O
) O

where O
f O
, O
g O
, O
and O
h O
are O
all O
Lagrangian O
multipliers O
. O
Taking O
the O
differentiation O
of O
Equation O
( O
9 O
) O
on O
the O
variable O
π O
, O
we O
can O
obtain O
: O

π O
ij O
= O
exp O
( O
f O
i O
+ O
g O
j O
− O
M O
ij O
ϵ O
1 O
) O
> O
0 O
. O
( O
10 O

We O
first O
fix O
b O
, O
due O
to O
the O
fact O
that O
π1 O
= O
a O
and O
π O
T O
1 O
= O
b O
, O
we O
can O
get O
: O

exp O
( O
f O
i O
ϵ B-HyperparameterName
1 I-HyperparameterName
) O
= O
a O
i O
C O
j O
exp O
( O
g O
j O
−M O
ij O
ϵ B-HyperparameterName
1 I-HyperparameterName
) O
, O
( O
11 O
) O

exp O
( O
g O
j O
ϵ B-HyperparameterName
1 I-HyperparameterName
) O
= O
b O
j O
N O
i O
exp O
( O
f O
i O
−M O
ij O
ϵ B-HyperparameterName
1 I-HyperparameterName
) O
. O
( O
12 O
) O

Then O
we O
fix O
f O
and O
g O
, O
and O
update O
b O
by O
: O

min O
b O
ϵ B-HyperparameterName
2 I-HyperparameterName
( O
Ψ O
( O
b O
) O
) O
T O
1 O
+ O
g O
T O
b O
− O
h O
( O
b O
T O
1 O
− O
1 O
) O
. O
( O
13 O

Taking O
the O
differentiation O
of O
Equation O
( O
13 O
) O
on O
the O
variable O
b O
, O
we O
can O
obtain O
: O

( O
g O
j O
− O
h O
) O
b O
2 O
j O
− O
( O
( O
g O
j O
− O
h O
) O
+ O
2ϵ B-HyperparameterName
2 I-HyperparameterName
) O
b O
j O
+ O
ϵ B-HyperparameterName
2 I-HyperparameterName
= O
0 O
. O
( O
14 O
) O

It O
is O
easy O
to O
get O
the O
discriminant O
of O
Equation O
( O
14 O
) O

∆ O
j O
= O
( O
g O
j O
− O
h O
) O
2 O
+ O
4ϵ B-HyperparameterName
2 I-HyperparameterName
2 O
> O
0 O
, O
b O
j O
( O
h O
) O
= O
( O
g O
j O
− O
h O
+ O
2ϵ B-HyperparameterName
2 I-HyperparameterName
) O
± O
∆ O
j O
2 O
( O
g O
j O
− O
h O
) O
. O
( O
15 O
) O

Note O
that O
, O

b O
j O
( O
h O
) O
= O
( O
( O
g O
j O
− O
h O
) O
+ O
2ϵ B-HyperparameterName
2 I-HyperparameterName
) O
+ O
∆ O
j O
2 O
( O
g O
j O
− O
h O
) O
≥ O
1 O
. O
( O
16 O
) O

Thus O
, O
we O
choose O
the O
following O
b O
j O
( O
h O
) O
: O

b O
j O
( O
h O
) O
= O
( O
( O
g O
j O
− O
h O
) O
+ O
2ϵ B-HyperparameterName
2 I-HyperparameterName
) O
− O
∆ O
j O
2 O
( O
g O
j O
− O
h O
) O
. O
( O
17 O
) O

Taking O
Equation O
( O
17 O
) O
back O
to O
the O
original O
constraint O
b O
T O
1 O
= O
1 O
, O
the O
formula O
is O
defined O
as O
below O
: O

( O
b O
( O
h O
) O
) O
T O
1 O
− O
1 O
= O
0 O
, O
( O
18 O

where O
h O
is O
the O
root O
of O
Equation O
( O
18 O
) O
, O
and O
we O
can O
use O
Newton O
's O
method O
to O
work O
out O
it O
. O
Specifically O
, O
we O
first O
define O
that O
f O
( O
h O
) O
= O
( O
b O
( O
h O
) O
) O
T O
1 O
− O
1 O
, O
then O
h O
can O
be O
updated O
by O
: O

h O
← O
h O
− O
f O
( O
h O
) O
f O
′ O
( O
h O
) O
, O
( O
19 O
) O

where O
the O
iteration B-HyperparameterName
number I-HyperparameterName
is O
set O
to O
10 B-HyperparameterValue
. O
Then O
we O
can O
obtain O
b O
by O
Equation O
( O
17 O
) O
. O
In O
short O
, O
through O
iteratively O
updating O
Equation O
( O
11 O
) O
, O
( O
12 O
) O
, O
( O
19 O
) O
, O
and O
( O
17 O
) O
, O
we O
can O
obtain O
the O
transport O
matrix O
π O
on O
Equation O
( O
10 O
) O
. O
We O
show O
the O
iteration O
optimization O
scheme O
of O
SAOT O
in O
Algorithm O
1 O
. O

Algorithm O
1 O
The O
optimization O
scheme O
of O
SAOT O
Input O
: O
The O
cost O
distance O
matrix O
: O
M O
. O
Output O
: O
The O
transport O
matrix O
: O
π O
. O
Procedure O
: O

1 O
: O
Initialize O
f O
and O
g O
randomly O
; O
2 O
: O
Initialize O
b O
randomly O
and O
perform O
normalization O
so O
that O
b O
T O
1 O
= O
1 O
; O
3 O
: O
Initialize O
h O
= O
1 O
. O
4 O
: O
for O
i O
= O
1 O
to O
T O
do O

C O
Experiment O

C.1 O
Datasets O

We O
conduct O
extensive O
experiments O
on O
eight O
popularly O
used O
real O
- O
world O
datasets O
. O
The O
details O
of O
each O
dataset O
are O
as O
follows O
. O

AgNews B-DatasetName
( O
Rakib O
et O
al O
. O
, O
2020 O
) O
is O
a O
subset O
of O
AG O
's O
news O
corpus O
collected O
by O
( O
Zhang O
et O
al O
. O
, O
2015 O
) O
which O
consists O
of O
8,000 O
news O
titles O
in O
4 O
topic O
categories O
. O
StackOverflow B-DatasetName
( O
Xu O
et O
al O
. O
, O
2017 O
) O
consists O
of O
20,000 O
question O
titles O
associated O
with O
20 O
different O
tags O
, O
which O
is O
randomly O
selected O
from O
the O
challenge O
data O
published O
in O
Kaggle.com O
1 O
. O
Biomedical B-DatasetName
( O
Xu O
et O
al O
. O
, O
2017 O
) O
is O
composed O
of O
20,000 O
paper O
titles O
from O
20 O
different O
topics O
and O
it O
is O
selected O
from O
the O
challenge O
data O
published O
in O
BioASQ O
's O
official O
website O
2 O
. O
SearchSnippets B-DatasetName
( O
Phan O
et O
al O
. O
, O
2008 O
) O
contains O
12,340 O
snippets O
from O
8 O
different O
classes O
, O
which O
is O
selected O
from O
the O
results O
of O
web O
search O
transaction O
. O
GoogleNews B-DatasetName
( O
Yin O
and O
Wang O
, O
2016 O
) O
consists O
of O
the O
titles O
and O
snippets O
of O
11,109 O
news O
articles O
about O
152 O
events O
( O
Yin O
and O
Wang O
, O
2014 O
) O
which O
is O
divided O
into O
three O
datasets O
: O
the O
full O
dataset O
is O
GoogleNews B-DatasetName
- I-DatasetName
TS I-DatasetName
, O
while O
GoogleNews B-DatasetName
- I-DatasetName
T I-DatasetName
only O
contains O
titles O
and O
GoogleNews B-DatasetName
- I-DatasetName
S I-DatasetName
only O
has O
snippets O
. O
Tweet B-DatasetName
( O
Yin O
and O
Wang O
, O
2016 O
) O

C.2 O
Experiment O
Settings O

We O
choose O
distilbert O
- O
base O
- O
nli O
- O
stsb O
- O
mean O
- O
tokens O
in O
Sentence O
Transformer O
library O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
to O
encode O
the O
text O
, O
and O
the O
maximum B-HyperparameterName
input I-HyperparameterName
length I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
5×10 B-HyperparameterValue
−6 I-HyperparameterValue
for O
optimizing O
the O
encoding O
network O
, O
and O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
for O
optimizing O
both O
the O
projecting O
network O
and O
clustering O
network O
. O
The O
dimensions O
of O
the O
text O
representations O
and O
the O
projected O
representations O
are O
set O
to O
D B-HyperparameterName
1 I-HyperparameterName
= O
768 B-HyperparameterValue
and O
D B-HyperparameterName
2 I-HyperparameterName
= O
128 B-HyperparameterValue
, O
respectively O
. O
The O
batch O
size O
is O
set O
to O
N B-HyperparameterName
= O
200 B-HyperparameterValue
. O
The O
temperature O
parameter O
is O
set O
to O
τ B-HyperparameterName
= O
1 B-HyperparameterValue
. O
The O
threshold O
δ B-HyperparameterName
is O
set O
to O
0.01 B-HyperparameterValue
. O
The O
datasets O
specific O
tuning O
is O
avoided O
as O
much O
as O
possible O
. O
For O
BOW B-MethodName
and O
TF B-MethodName
- I-MethodName
IDF I-MethodName
, O
we O
achieved O
the O
code O
with O
scikit O
- O
learn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
. O
For O
all O
the O
other O
baselines O
, O
i.e. O
, O
STC B-MethodName
2 I-MethodName
-LPI I-MethodName
4 O
, O
Self B-MethodName
- I-MethodName
Train I-MethodName
5 O
, O
K B-MethodName
- I-MethodName
means_IC I-MethodName
6 O
, O
and O
SCCL B-MethodName
7 O
( O
MIT-0 O
license O
) O
, O
we O
used O
their O
released O
code O
. O

Besides O
, O
we O
substitute O
the O
accuracy B-MetricName
evaluation O
code O
of O
K B-MethodName
- I-MethodName
means_IC I-MethodName
with O
the O
evaluation O
method O
described O
in O
our O
paper O
. O

In O
addition O
, O
as O
STC B-MethodName
2 I-MethodName
-LPI I-MethodName
and O
Self B-MethodName
- I-MethodName
Train I-MethodName
use O
the O
word O
embeddings O
pre O
- O
trained O
with O
in O
- O
domain O
corpus O
, O
and O
there O
are O
only O
three O
datasets O
' O
pre O
- O
trained O
word O
embeddings O
provided O
, O
therefore O
we O
do O
not O
report O
the O
results O
of O
other O
five O
datasets O
for O
them O
. O

C.3 O
Evaluation O
Metrics O

We O
report O
two O
widely O
used O
evaluation O
metrics O
of O
text O
clustering O
, O
i.e. O
, O
accuracy B-MetricName
( O
ACC B-MetricName
) O
and O
normalized B-MetricName
mutual I-MetricName
information I-MetricName
( O
NMI B-MetricName
) O
, O
following O
( O
Xu O
et O
al O
. O
, O
2017 O
; O
Hadifar O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
. O
Accuracy O
is O
defined O
as O
: O

ACC B-MetricName
= O
N O
i=1 O
1 O
y O
i O
= O
map O
( O
ŷ O
i O
) O
N O
, O
( O
20 O
) O

where O
y O
i O
andŷ O
i O
are O
the O
ground O
truth O
label O
and O
the O
predicted O
label O
for O
a O
given O
text O
x O
i O
respectively O
, O
map O
( O
) O
maps O
each O
predicted O
label O
to O
the O
corresponding O
target O
label O
by O
Hungarian O
algorithm O
( O
Papadimitriou O
and O
Steiglitz O
, O
1998 O
) O
. O
Normalized B-MetricName
mutual I-MetricName
information I-MetricName
is O
defined O
as O
: O

N B-MetricName
M I-MetricName
I I-MetricName
( O
Y O
, O
Ŷ O
) O
= O
I O
( O
Y O
, O
Ŷ O
) O
H O
( O
Y O
) O
H O
( O
Ŷ O
) O
, O
( O
21 O
) O

where O
Y O
andŶ O
are O
the O
ground O
truth O
labels O
and O
the O
predicted O
labels O
respectively O
, O
I O
( O
) O
is O
the O
mutual O
information O
, O
and O
H O
( O
) O
is O
the O
entropy O
. O

C.4 O
Visualization O

To O
better O
show O
the O
clustering O
degeneracy O
problem O
, O
we O
visualize O
how O
the O
number O
of O
predicted O
clusters O
( O
we O
call O
it O
clusters O
later O
) O
are O
changing O
over O
iterations O
on O
SCCL B-MethodName
and O
RSTC B-MethodName
. O
The O
results O
are O
shown O
in O
Fig O
. O
6 O
. O
From O
it O
, O
we O
verify O
that O
SCCL B-MethodName
has O
relatively O
serious O
clustering O
degeneracy O
problem O
while O
RSTC B-MethodName
solves O
it O
to O
some O
extent O
. O
Specifically O
, O
the O
clusters O
of O
SCCL B-MethodName
is O
much O
less O
than O
the O
real O
category O
number O
. O
Moreover O
, O
the O
degeneracy O
has O
a O
negative O
effect O
on O
the O
final O
k O
- O
means O
clustering O
performance O
because O
it O
makes O
the O
representations O
getting O
worse O
. O
Whereas O
the O
clusters O
of O
RSTC B-MethodName
almost O
convergent O
to O
real O
category O
number O
, O
which O
assures O
the O
high O
accuracy B-MetricName
of O
RSTC B-MethodName
. O
The O
visualization O
results O
illustrate O
the O
validity O
of O
our O
model O
. O

C.5 O
Computational O
Budget O

The O
number O
of O
parameters O
in O
our O
model O
is O
68M O
. O

Our O
training O
for O
each O
dataset O
takes O
about O
10 O
- O
30 O
minutes O
, O
using O
a O
GeForce O
RTX O
3090 O
GPU O
. O

Acknowledgements O

This O
work O
was O
supported O
in O
part O
by O
the O
Leading O
Expert O
of O
" O
Ten O
Thousands O
Talent O
Program O
" O
of O
Zhejiang O
Province O
( O
No.2021R52001 O
) O
and O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No.72192823 O
) O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O

We O
use O
the O
datasets O
the O
same O
way O
as O
existing O
work O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
The O
datasets O
we O
use O
only O
have O
the O
text O
instances O
and O
their O
category O
IDs O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Appendix O
C.1 O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Appendix O
C.5 O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Norm B-MethodName
- I-MethodName
based I-MethodName
Noisy I-MethodName
Corpora I-MethodName
Filtering I-MethodName
and I-MethodName
Refurbishing I-MethodName
in I-MethodName
Neural I-MethodName
Machine I-MethodName
Translation I-MethodName

Recent O
advances O
in O
neural B-TaskName
machine I-TaskName
translation I-TaskName
depend O
on O
massive O
parallel O
corpora O
, O
which O
are O
collected O
from O
any O
open O
source O
without O
much O
guarantee O
of O
quality O
. O
It O
stresses O
the O
need O
for O
noisy B-TaskName
corpora I-TaskName
filtering I-TaskName
, O
but O
existing O
methods O
are O
insufficient O
to O
solve O
this O
issue O
. O
They O
spend O
much O
time O
ensembling O
multiple O
scorers O
trained O
on O
clean O
bitexts O
, O
unavailable O
for O
low O
- O
resource O
languages O
in O
practice O
. O
In O
this O
paper O
, O
we O
propose O
a O
norm B-MethodName
- I-MethodName
based I-MethodName
noisy I-MethodName
corpora I-MethodName
filtering I-MethodName
and I-MethodName
refurbishing I-MethodName
method O
with O
no O
external O
data O
and O
costly O
scorers O
. O
The O
noisy O
and O
clean O
samples O
are O
separated O
based O
on O
how O
much O
information B-MetricName
from O
the O
source O
and O
target O
sides O
the O
model O
requires O
to O
fit O
the O
given O
translation O
. O
For O
the O
unparallel O
sentence O
, O
the O
target O
- O
side O
history O
translation O
is O
much O
more O
important O
than O
the O
source O
context O
, O
contrary O
to O
the O
parallel O
ones O
. O
The O
amount O
of O
these O
two O
information O
flows O
can O
be O
measured O
by O
norms B-MetricName
of O
source- O
/ O
target O
- O
side O
context O
vectors O
. O
Moreover O
, O
we O
propose O
to O
reuse O
the O
discovered O
noisy O
data O
by O
generating O
pseudo O
labels O
via O
online O
knowledge O
distillation O
. O
Extensive O
experiments O
show O
that O
our O
proposed O
filtering O
method O
performs O
comparably O
with O
state O
- O
ofthe O
- O
art O
noisy O
corpora O
filtering O
techniques O
but O
is O
more O
efficient O
and O
easier O
to O
operate O
. O
Noisy B-MethodName
sample I-MethodName
refurbishing I-MethodName
further O
enhances O
the O
performance O
by O
making O
the O
most O
of O
the O
given O
data O
1 O
. O

Introduction O

Neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
NMT B-TaskName
) O
has O
achieved O
significant O
progress O
with O
help O
from O
large O
parallel O
corpora O
for O
training O
( O
Tiedemann O
, O
2012 O
; O
. O
These O
data O
are O
typically O
extracted O
from O
the O
web O
without O
much O
control O
over O
the O
quality O
, O
which O
presents O
misalignment O
, O
wrong O
languages O
, O
too O
many O
numbers O
or O
URLs O
, O
etc O
. O
In O
this O
case O
, O
noisy O
corpora O
filtering O
holds O
a O
critical O
research O
area O
to O
prevent O
noisy O
bitexts O
from O
degrading O
the O
generalization O
performance O
of O
NMT B-TaskName
( O
Khayrallah O
and O
Koehn O
, O
2018 O
) O
. O

wang O
yong O
@ O
@ O
zhi O
[ O
37 O
@ O
@ O
69 O
30 O
@ O
@ O
57 O
18 O
@ O
@ O
07 O
] O
is O
the O
chief O
designer O
in O
china O
's O
manned O
spaceflight O
project O
. O

Src O
: O

Tgt O
: O
Source O
- O
side O
Our O
metric O
Target O
- O
side O
[ O
37 O
@ O
@ O
69 O
30 O
@ O
@ O
57 O
18 O
@ O
@ O
07 O
] O
in O
China O
Figure O
1 O
: O
An O
example O
of O
the O
required O
source- O
/ O
target O
- O
side O
information O
when O
the O
model O
fits O
an O
unparallel O
Zh⇒En O
sentence O
pair O
( O
words O
in O
red O
are O
real O
noisy O
segments O
) O
. O
The O
amount O
of O
information O
on O
each O
side O
is O
counted O
by O
norms O
of O
corresponding O
context O
vectors O
, O
positively O
correlated O
to O
the O
darkness O
of O
color O
blocks O
. O

Much O
effort O
has O
been O
devoted O
to O
this O
field O
with O
the O
promotion O
of O
a O
WMT O
shared O
task O
for O
parallel O
corpus O
filtering O
. O
However O
, O
prior O
work O
is O
difficult O
to O
operate O
in O
practice O
due O
to O
two O
drawbacks O
. O
( O
1 O
) O
High O
time O
and O
computational O
cost O
. O
Their O
good O
performance O
relies O
on O
the O
ensembling O
of O
multiple O
largescale O
scorers O
, O
which O
involves O
costly O
pre O
- O
training O
and O
fine O
- O
tuning O
( O
Esplà O
- O
Gomis O
et O
al O
. O
, O
2020 O
; O
Lu O
et O
al O
. O
, O
2020 O
) O
. O
( O
2 O
) O
Dependence O
on O
clean O
bitexts O
. O
The O
training O
of O
above O
scorers O
needs O
clean O
parallel O
sentence O
pairs O
as O
positive O
samples O
, O
which O
are O
scarce O
for O
lowresource O
languages O
in O
real O
- O
world O
applications O
. O

This O
paper O
introduces O
a O
norm B-MethodName
- I-MethodName
based I-MethodName
noisy I-MethodName
sample I-MethodName
filtering I-MethodName
and I-MethodName
refurbishing I-MethodName
method O
, O
which O
avoids O
extra O
clean O
bitexts O
and O
heavy O
scorers O
. O
We O
distinguish O
unparallel O
sentence O
pairs O
from O
others O
based O
on O
observed O
model O
behaviors O
during O
the O
training O
of O
NMT O
. O
Generally O
, O
the O
model O
captures O
two O
aspects O
of O
information O
to O
predict O
the O
given O
translation O
, O
the O
source O
- O
side O
context O
information O
from O
the O
encoder O
and O
the O
target O
- O
side O
one O
from O
history O
translations O
in O
the O
decoder O
. O
For O
unparallel O
sentence O
pairs O
, O
provided O
translations O
are O
partially O
or O
entirely O
unrelated O
to O
the O
source O
sentence O
. O
In O
this O
case O
, O
the O
NMT B-TaskName
model O
behaves O
as O
a O
language O
model O
, O
which O
requires O
excessive O
target O
- O
side O
information O
to O
fit O
noises O
. O
Thus O
, O
we O
use O
the O
information O
ratio O
of O
the O
source O
to O
the O
target O
side O
as O
the O
criterion O
to O
filter O
noises O
. O

Specifically O
, O
we O
observe O
that O
a O
greater O
vector O
norm O
implies O
richer O
context O
information O
captured O
by O
the O
model O
. O
Thus O
, O
we O
calculate O
the O
amount B-MetricName
of I-MetricName
each I-MetricName
information I-MetricName
flow I-MetricName
by O
norms O
of O
corresponding O
context O
vectors O
. O
This O
metric O
is O
easy O
to O
obtain O
in O
the O
training O
process O
and O
sufficient O
to O
model O
how O
much O
information B-MetricName
is O
encoded O
on O
each O
side O
. O
We O
take O
Figure O
1 O
as O
an O
example O
. O
When O
the O
model O
predicts O
the O
content O
word O
" O
china O
" O
, O
the O
norm O
of O
the O
source O
- O
side O
context O
information O
is O
more O
significant O
than O
that O
of O
the O
target O
side O
. O
The O
opposite O
situation O
is O
in O
generating O
the O
function O
word O
" O
in O
" O
. O
However O
, O
the O
quantity O
of O
target O
- O
side O
information O
appears O
to O
be O
exceptionally O
great O
for O
the O
noisy O
fraction O
, O
which O
presents O
a O
deeper O
color O
than O
others O
, O
leading O
to O
a O
lower O
score O
than O
correct O
translations O
under O
our O
estimation O
. O

We O
further O
propose O
to O
refurbish B-MethodName
discovered I-MethodName
noisy I-MethodName
samples I-MethodName
by O
generating O
pseudo O
labels O
via O
online B-MethodName
knowledge I-MethodName
distillation I-MethodName
. O
By O
doing O
this O
, O
the O
source O
sentence O
in O
Figure O
1 O
is O
regarded O
as O
the O
monolingual O
data O
to O
complement O
limited O
clean O
bitexts O
. O
Throughout O
the O
whole O
, O
we O
incorporate O
filtering O
and O
refurbishing O
into O
the O
training O
of O
NMT B-MethodName
rather O
than O
separating O
data O
filtering O
and O
training O
, O
thus O
considerably O
improving O
computational O
efficiency O
. O

We O
validate O
the O
effectiveness O
of O
our O
approaches O
on O
Transformer B-MethodName
- I-MethodName
based I-MethodName
NMT I-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
including O
the O
WMT2020 B-DatasetName
shared I-DatasetName
task I-DatasetName
for O
parallel O
corpus O
filtering O
( O
Km⇒En O
and O
Ps⇒En O
) O
and O
our O
inhouse O
web O
- O
crawled O
datasets O
( O
He O
, O
I O
d O
, O
Pt O
, O
Ko O
, O
and O
Es⇒Zh O
) O
. O
Empirical O
results O
show O
that O
our O
proposed O
method O
performs O
comparably O
with O
SOTA O
noisy B-TaskName
corpora I-TaskName
filtering I-TaskName
approaches O
. O
Refurbishing O
noisy O
samples O
further O
substantially O
boosts O
the O
performance O
. O
Detailed O
analyses O
show O
that O
our O
metric O
can O
reflect O
the O
alignment O
extent O
at O
word O
and O
sentence O
levels O
. O

The O
contributions O
of O
this O
paper O
are O
three O
- O
fold O
: O

• O
We O
propose O
to O
use O
norms B-MetricName
of O
source O
- O
and O
targetside O
context O
vectors O
to O
represent O
the O
amount B-MetricName
of I-MetricName
information I-MetricName
flowing I-MetricName
from O
each O
side O
. O
We O
find O
that O
the O
model O
needs O
excessive O
target O
- O
side O
information O
to O
fit O
the O
unparallel O
sentence O
pair O
, O
which O
is O
the O
basis O
of O
the O
following O
work O
. O

• O
We O
propose O
a O
norm B-MethodName
- I-MethodName
based I-MethodName
noisy I-MethodName
corpora I-MethodName
filtering I-MethodName
method O
by O
calculating O
the O
information B-MetricName
ratio I-MetricName
from I-MetricName
the I-MetricName
source I-MetricName
to I-MetricName
the I-MetricName
target I-MetricName
side I-MetricName
. O
It O
is O
experimentally O
efficient O
and O
effective O
under O
the O
condition O
of O
no O
extra O
data O
and O
costly O
scorers O
. O
The O
model O
first O
sees O
history O
translations O
and O
then O
sourceside O
contexts O
by O
attention O
mechanism O
and O
obtains O
two O
context O
vectors O
t O
l O
j O
and O
s O
l O
j O
. O
In O
the O
enlarged O
view O
of O
MHA O
, O
the O
arrows O
in O
the O
circles O
represent O
the O
corresponding O
vectors O
. O
The O
sizes O
of O
circles O
illustrate O
the O
values O
of O
attention O
weights O
or O
the O
vectors O
' O
norms O
. O

• O
We O
propose O
to O
refurbish B-MethodName
discovered I-MethodName
noisy I-MethodName
samples I-MethodName
by O
producing O
pseudo O
labels O
via O
online O
knowledge O
distillation O
, O
which O
makes O
the O
most O
of O
the O
corpora O
and O
further O
boosts O
the O
performance O
. O

Background O

In O
this O
section O
, O
we O
first O
briefly O
introduce O
a O
mainstream O
NMT B-MethodName
framework O
, O
Transformer B-MethodName
, O
with O
a O
focus O
on O
how O
to O
capture O
source O
- O
and O
target O
- O
side O
contexts O
. O
We O
then O
present O
how O
the O
vector O
norm O
serves O
as O
an O
indicator O
of O
diverse O
features O
, O
which O
motivates O
us O
to O
count O
how O
much O
information B-MetricName
is O
encoded O
in O
context O
vectors O
of O
each O
side O
based O
on O
vector B-MetricName
norms I-MetricName
. O

Transformer B-MethodName
- I-MethodName
based I-MethodName
NMT I-MethodName

The O
Transformer B-MethodName
is O
an O
encoder O
- O
decoder O
framework O
which O
alternately O
looks O
over O
source O
- O
and O
target O
- O
side O
contexts O
to O
make O
prediction O
. O
The O
encoder O
with O
L B-HyperparameterName
layers B-HyperparameterName
transforms O
an O
input O
x O
= O
{ O
x O
1 O
, O
x O
2 O
, O
... O
x O
n O
} O
to O
a O
sequence O
of O
hidden O
states O
h O
where O
y O
< O
j O
is O
a O
partial O
translation O
. O
c O
L O
j O
denotes O
the O
j O
- O
th O
hidden O
state O
in O
the O
L B-HyperparameterName
- O
th O
decoder O
layer O
. O
We O
take O
the O
l O
- O
th O
decoder O
layer O
as O
an O
example O
in O
Figure O
2 O
. O
The O
model O
first O
attends O
to O
the O
history O
translation O
c O
l−1 O
< O
j O
by O
multi O
- O
head O
attention O
( O
MHA O
) O
and O
obtains O
the O
target O
context O
vector O
t O
l O
j O
. O

L O
= O
h O
L O
1 O
, O
h O
L O
2 O
, O
... O
h O
L O
n O
, O

t O
l O
j O
= O
MHA O
( O
c O
l−1 O
< O
j O
, O
c O
l−1 O
j O
) O
( O
2 O
) O

where O
MHA O
enables O
dynamically O
selecting O
relevant O
tokens O
by O
assigning O
different O
attention O
weights O
. O
t O
l O
j O
is O
then O
transformed O
to O
z O
l O
j O
by O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
and O
residual O
network O
( O
He O
et O
al O
. O
, O
2016 O
) O
. O
The O
model O
later O
looks O
at O
the O
sourceside O
context O
to O
obtain O
the O
source O
context O
vector O
s O
l O
j O
: O

s O
l O
j O
= O
MHA O
( O
h O
L O
, O
z O
l O
j O
) O
( O
3 O
) O

Two O
- O
side O
information O
is O
mixed O
up O
to O
calculate O
the O
next O
- O
layer O
hidden O
state O
c O
l O
j O
. O

o O
l O
j O
= O
LayerNorm O
( O
s O
l O
j O
+ O
z O
l O
j O
) O
c O
l O
j O
= O
LayerNorm O
( O
o O
l O
j O
+ O
FFN O
( O
o O
l O
j O
) O
) O
( O
4 O
) O

where O
FFN O
denotes O
a O
feedforward O
neural O
network O
. O

Norm O
- O
based O
Word O
Importance O
Measurement O

As O
the O
key O
element O
in O
the O
NMT B-MethodName
model O
, O
word O
representations O
capture O
rich O
semantic O
features O
. O
Schakel O
and O
Wilson O
( O
2015 O
) O
report O
that O
the O
L B-MetricName
2 I-MetricName
norm I-MetricName
of O
word O
vectors O
learned O
in O
the O
word2vec O
model O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
is O
informative O
, O
where O
words O
with O
low O
frequency O
or O
diverse O
contexts O
are O
more O
likely O
to O
be O
assigned O
higher O
norms O
. O
Liu O
et O
al O
. O
( O
2020 O
) O
state O
that O
the O
norm B-MetricName
of O
word O
embeddings O
in O
the O
NMT B-MethodName
model O
is O
also O
a O
good O
proxy O
of O
word B-MetricName
importance I-MetricName
. O

Here O
, O
we O
extend O
to O
hidden O
states O
attended O
by O
the O
attention O
mechanism O
( O
h O
L O
produced O
by O
the O
encoder O
' O
HFRGLQJ6WHSj O
and O
c O
L−1 O
j O
in O
the O
decoder O
) O
. O
As O
shown O
in O
Figure O
3 O
, O
norms O
of O
h O
L O
shift O
downwards O
with O
the O
frequency O
increasing O
. O
Specifically O
, O
norms O
of O
content O
words O
are O
relatively O
higher O
than O
function O
words O
. O
It O
suggests O
that O
the O
rare O
and O
informative O
words O
obtain O
a O
high O
norm B-MetricName
of O
h O
L O
, O
which O
stays O
consistent O
with O
results O
of O
c O
L−1 O
j O
as O
given O
in O
Appendix O
A. O
Thus O
, O
norms B-MetricName
of O
hidden O
states O
that O
the O
attention O
mechanism O
looks O
at O
can O
indicate O
word B-MetricName
importance I-MetricName
. O

As O
context O
vectors O
are O
formed O
as O
a O
weighted O
sum O
of O
those O
hidden O
states O
via O
attention O
mechanism O
, O
the O
derived O
context O
vectors O
' O
norms O
would O
grow O
in O
line O
with O
the O
model O
's O
increasing O
attention O
towards O
informative O
words O
with O
higher O
norms O
. O
These O
observations O
motivate O
us O
to O
evaluate O
how O
much O
information O
is O
encoded O
in O
context O
vectors O
from O
this O
point O
. O

Methodology O

We O
aim O
to O
detect O
and O
refurbish O
noisy O
sentence O
pairs O
by O
observing O
how O
the O
model O
predicts O
each O
token O
. O
A O
sentence O
pair O
is O
potentially O
misaligned O
if O
the O
model O
depends O
heavily O
on O
history O
predictions O
rather O
than O
the O
source O
sentence O
to O
fit O
given O
translations O
. O
To O
this O
end O
, O
we O
first O
introduce O
a O
norm O
- O
based O
measurement O
to O
count O
the O
amount B-MetricName
of I-MetricName
information I-MetricName
extracted O
from O
the O
source O
and O
target O
side O
( O
Section O
3.1 O
) O
. O
Then O
, O
we O
show O
how O
to O
use O
this O
metric O
to O
filter O
noisy O
samples O
( O
Section O
3.2 O
) O
, O
which O
are O
further O
refurbished O
by O
producing O
pseudo O
labels O
via O
online B-MethodName
knowledge I-MethodName
distillation I-MethodName
( O
Section O
3.3 O
) O
. O

Norm O
- O
based O
Source O
- O
and O
Target O
- O
side O
Information O
Measurement O

As O
shown O
in O
Figure O
2 O
, O
the O
model O
repeatedly O
collects O
information O
from O
the O
source O
sentence O
( O
h O
L O
) O
and O
history O
translation O
( O
c O
l−1 O
< O
j O
) O
to O
calculate O
context O
vectors O
by O
attention O
mechanisms O
. O
Specifically O
, O
it O
computes O
a O
weighted O
sum O
of O
hidden O
states O
, O
the O
norm B-MetricName
of O
which O
indicates O
word B-MetricName
importance I-MetricName
as O
stated O
in O
section O
2.2 O
. O
If O
the O
model O
pays O
more O
attention O
to O
content O
words O
with O
greater O
norms O
, O
the O
norm O
of O
obtained O
context O
vector O
would O
correspondingly O
increase O
, O
and O
vice O
versa O
. O
Thus O
, O
we O
can O
use O
the O
norm B-MetricName
of O
the O
source O
and O
target O
context O
vector O
( O
∥s O
l O
j O
∥ O
2 O
and O
∥t O
l O
j O
∥ O
2 O
) O
to O
count O
the O
amount B-MetricName
of I-MetricName
information I-MetricName
extracted O
from O
two O
sides O
. O

However O
, O
directly O
comparing O
∥t O
l O
j O
∥ O
2 O
at O
different O
steps O
is O
unfair O
, O
for O
more O
history O
translations O
are O
available O
for O
the O
model O
in O
the O
later O
steps O
. O
In O
this O
case O
, O
the O
decoder O
has O
access O
to O
more O
content O
words O
and O
gets O
a O
high O
- O
norm O
context O
vector O
. O
As O
shown O
in O
Figure O
4 O
, O
∥t O
L O
j O
∥ O
2 O
rapidly O
increases O
at O
first O
, O
and O
then O
the O
growth O
slows O
down O
. O
The O
overall O
trend O
is O
similar O
to O
the O
function O
y O
= O
3 O
√ O
x. O
Thus O
, O
we O
normalize O
the O
norm O
of O
the O
target O
context O
vector O
with O
3 O
√ O
j. O
Here O
, O
we O
extract O
context O
vectors O
from O
the O
L B-HyperparameterName
- O
th O
layer O
and O
design O
the O
metric O
as O
: O

γ O
j O
= O
∥s O
L O
j O
∥ O
2 O
∥t O
L O
j O
∥ O
2 O
/ O
3 O
√ O
j O
( O
5 O
) O

which O
is O
positively O
related O
to O
how O
much O
the O
source O
sentence O
is O
relied O
on O
to O
make O
predictions O
. O
Different O
values O
of O
γ B-HyperparameterName
j I-HyperparameterName
indicate O
different O
cases O
: O

• O
If O
γ B-MetricName
j I-MetricName
is O
big O
, O
the O
model O
mainly O
depends O
on O
the O
source O
sentence O
x O
to O
predict O
y O
j O
, O
which O
may O
be O
nouns O
, O
verbs O
, O
or O
other O
content O
words O
. O

• O
If O
γ B-MetricName
j I-MetricName
is O
medium O
, O
the O
partial O
translation O
has O
a O
larger O
impact O
on O
the O
prediction O
of O
y O
j O
, O
which O
is O
slightly O
related O
to O
x. O
Here O
, O
y O
j O
may O
be O
prepositions O
, O
determiners O
, O
or O
other O
function O
words O
. O

• O
If O
γ B-MetricName
j I-MetricName
is O
small O
, O
the O
model O
relies O
on O
the O
language O
model O
to O
produce O
the O
unrelated O
translations O
, O
which O
are O
exactly O
our O
targeted O
noisy O
samples O
. O

Norm B-MethodName
- I-MethodName
based I-MethodName
Corpus I-MethodName
Filtering I-MethodName

Based O
on O
the O
metric O
γ B-MetricName
j I-MetricName
calculated O
at O
each O
step O
, O
we O
measure O
how O
much O
the O
target O
sentence O
y O
is O
aligned O
with O
the O
input O
x O
as O
follows O
: O

R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
= O
1 O
m O
m O
j=1 O
γ O
j O
( O
6 O
) O

When O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
is O
smaller O
than O
a O
threshold B-HyperparameterName
k B-HyperparameterName
, O
the O
target O
sentence O
may O
be O
desperately O
inadequate O
or O
even O
wholly O
unrelated O
to O
the O
source O
sentence O
. O
To O
eliminate O
the O
impact O
of O
these O
noisy O
samples O
, O
we O
erase O
their O
loss O
during O
training O
by O
the O
norm O
- O
based O
sentence O
- O
level O
objective O
: O

L O
= O
I O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
> O
k B-HyperparameterName
• O
L O
NLL O
( O
7 O
) O

where O
the O
indicative O
function O
I O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
> O
k B-HyperparameterName
is O
equal O
to O
1 O
if O
R B-HyperparameterName
( I-HyperparameterName
x I-HyperparameterName
, I-HyperparameterName
y I-HyperparameterName
) I-HyperparameterName
> O
k B-HyperparameterName
, O
else O
0 O
. O
k B-HyperparameterName
is O
a O
hyperparameter O
which O
is O
used O
to O
adjust O
the O
filtering O
ratio O
. O
L O
NLL O
is O
the O
loss O
of O
the O
NMT B-TaskName
calculated O
by O
the O
negative O
log O
- O
likelihood O
in O
Equation O
( O
1 O
) O
. O
Considering O
the O
early O
NMT B-TaskName
model O
is O
not O
welltrained O
to O
gather O
information O
from O
the O
source O
and O
target O
sides O
, O
we O
first O
warm O
up O
the O
model O
on O
the O
entire O
dataset O
for O
T B-HyperparameterName
steps B-HyperparameterName
and O
then O
filter O
the O
noisy O
sentence O
pairs O
based O
on O
observed O
model O
behaviors O
. O
In O
the O
middle O
and O
later O
stages O
, O
I O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
> O
k B-HyperparameterName
would O
stable O
at O
a O
particular O
value O
. O

Noisy B-MethodName
Label I-MethodName
Refurbishing I-MethodName

The O
detected O
unparallel O
sentence O
pairs O
hamper O
the O
training O
of O
the O
NMT B-TaskName
system O
. O
But O
they O
can O
split O
into O
individual O
monolingual O
sentences O
, O
which O
remain O
to O
be O
fully O
re O
- O
utilized O
. O
From O
this O
perspective O
, O
we O
propose O
to O
refurbish O
these O
noisy O
samples O
by O
generating O
pseudo O
labels O
via O
knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
; O
Kim O
and O
Rush O
, O
2016 O
) O
. O

The O
biggest O
issue O
of O
integrating O
knowledge O
distillation O
in O
our O
scenario O
is O
how O
to O
acquire O
a O
strong O
teacher O
model O
, O
which O
decides O
the O
performance O
of O
the O
student O
model O
( O
Gou O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
the O
absence O
of O
a O
large O
- O
scale O
clean O
corpus O
makes O
it O
hard O
to O
train O
an O
offline O
competitive O
teacher O
model O
. O
Alternatively O
, O
we O
employ O
online B-MethodName
self I-MethodName
- I-MethodName
distillation I-MethodName
( O
Wei O
et O
al O
. O
, O
2019 O
) O
to O
let O
the O
history O
model O
generate O
the O
translation O
for O
noisy O
samples O
. O

Concretely O
, O
we O
use O
the O
checkpoint O
with O
the O
best O
performance O
on O
the O
validation O
set O
as O
the O
teacher O
. O
Then O
, O
the O
current O
model O
learns O
to O
match O
the O
teacher O
model O
's O
prediction O
q O
( O
•|x O
) O
on O
the O
noisy O
data O
. O
The O
word O
- O
level O
self O
- O
distillation O
loss O
can O
be O
defined O
as O
: O

L O
SD O
= O
− O
m O
j=1 O
|V| O
i=1 O
q O
( O
y O
j O
= O
i|y O
< O
j O
, O
x O
; O
θ O
T O
) O
× O
log O
p O
( O
y O
j O
= O
i|y O
< O
j O
, O
x O
; O
θ O
) O
( O
8 O
) O

where O
θ O
T O
and O
θ O
parameterize O
the O
teacher O
and O
student O
model O
separately O
. O
V O
is O
the O
target O
vocabulary B-HyperparameterName
set O
. O
In O
this O
way O
, O
we O
make O
full O
use O
of O
the O
corpus O
by O
precisely O
figuring O
out O
the O
clean O
data O
and O
replacing O
the O
remaining O
noises O
with O
pseudo O
labels O
: O

L O
= O
I O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
> O
k B-HyperparameterName
• O
L O
NLL O
+ O
I O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
≤k B-HyperparameterName
• O
L O
SD O
( O
9 O
) O

Experiments O

We O
conduct O
experiments O
on O
two O
types O
of O
datasets O
: O

( O
1 O
) O
WMT B-DatasetName
2020 I-DatasetName
shared O
task O
on O
parallel O
corpus O
fil- O
tering O
and O
alignment O
for O
low O
- O
resource O
conditions O
2 O
: O
Khmer⇒English O
( O
Km⇒En O
) O
and O
Pashto⇒English O
( O
Ps⇒En O
) O
, O
and O
( O
2 O
) O
our O
in O
- O
house O
web O
crawled O
corpora O
: O
Hebrew O
( O
He O
) O
, O
Indonesian O
( O
I O
d O
) O
, O
Korean O
( O
Ko O
) O
, O
Portuguese O
( O
Pt O
) O
, O
and O
Spanish O
( O
Es O
) O
to O
Chinese O
( O
Zh O
) O
. O

Dataset O

WMT20 B-TaskName
corpus I-TaskName
filtering I-TaskName
task O
asks O
participants O
to O
select O
different O
- O
scale O
subsets O
of O
high O
- O
quality O
sentence O
pairs O
from O
the O
noisy O
data O
. O
The O
quality O
of O
selected O
subsets O
is O
measured O
by O
the O
performance O
of O
an O
NMT B-TaskName
system O
trained O
on O
this O
data O
. O
This O
task O
provides O
three O
kinds O
of O
data O
: O
( O
1 O
) O
4.17 B-HyperparameterValue
M I-HyperparameterValue
Km⇒En B-DatasetName
and O
1.02 B-HyperparameterValue
M I-HyperparameterValue
Ps⇒En B-MetricName
noisy O
sentence O
pairs O
which O
participants O
have O
to O
score O
for O
filtering O
, O
and O
( O
2 O
) O
clean O
parallel O
and O
monolingual O
data O
to O
train O
quality O
estimation O
models O
that O
help O
the O
filtering O
task O
, O
and O
( O
3 O
) O
development O
and O
test O
sets O
used O
to O
evaluate O
translation O
systems O
trained O
on O
filtered O
data O
. O
Note O
that O
we O
do O
not O
use O
the O
second O
part O
of O
the O
data O
and O
only O
experiment O
with O
( O
1 O
) O
and O
( O
3 O
) O
. O
We O
strictly O
follow O
Koehn O
et O
al O
. O
( O
2020 O
) O
to O
preprocess O
the O
raw O
data O
. O
For O
the O
in O
- O
house O
corpora O
, O
we O
apply O
sentence O
pieces O
on O
tokenized O
text O
. O
We O
construct O
the O
vocabulary B-HyperparameterName
with O
the O
size O
of O
20k B-HyperparameterValue
tokens O
where O
the O
source O
and O
target O
languages O
are O
separately O
encoded O
. O

The O
raw O
corpora O
are O
firstly O
filtered O
by O
heuristic O
rules O
to O
remove O
extremely O
noisy O
sentence O
pairs O
. O
We O
implement O
rule O
- O
based O
filtering O
as O
in O
Lu O
et O
al O
. O
( O
2020 O
) O
, O
the O
details O
of O
which O
are O
listed O
in O
Appendix O
B O
. O

A O
cursory O
review O
of O
the O
above O
corpora O
is O
given O
in O
Figure O
5 O
. O
We O
categorize O
unparallel O
sentence O
pairs O
into O
three O
types O
based O
on O
the O
level O
of O
misalignment O
: O
( O
I O
) O
words O
, O
( O
II O
) O
phrases O
, O
and O
( O
III O
) O
the O
whole O
sentence O
. O
We O
randomly O
sample O
200 B-HyperparameterValue
sentence B-HyperparameterName
pairs I-HyperparameterName
from O
each O
preprocessed O
dataset O
and O
manually O
annotate O
them O
with O
predefined O
labels O
based O
on O
their O
noise O
degrees O
. O

We O
find O
a O
high O
noise B-MetricName
rate I-MetricName
in O
the O
WMT20 B-DatasetName
corpora I-DatasetName
, O
while O
noise O
types O
in O
in O
- O
house O
datasets O
are O
diverse O
. O
These O
two O
kinds O
of O
datasets O
pose O
different O
challenges O
for O
our O
methods O
to O
filter O
noises O
accurately O
. O

Settings O

We O
strictly O
follow O
model O
configurations O
and O
evaluation O
settings O
provided O
by O
WMT20 O
organizers O
. O
The O
evaluation O
is O
done O
on O
subsets O
of O
two O
predefined O
sizes O
, O
5 B-HyperparameterValue
M I-HyperparameterValue
and O
7 B-HyperparameterValue
M I-HyperparameterValue
English O
words O
. O
The O
most O
striking O
difference O
between O
participants O
and O
us O
is O
that O
we O
simultaneously O
perform O
data O
filtering O
and O
model O
training O
rather O
than O
" O
filter O
first O
and O
next O
train O
" O
. O

For O
our O
in O
- O
house O
datasets O
, O
we O
experiment O
with O
Transformer B-MethodName
Base I-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
More O
details O
about O
experimental O
settings O
for O
WMT20 B-DatasetName
and O
in O
- O
house O
datasets O
are O
given O
in O
Appendix O
C O
. O

The O
choice O
of O
the O
threshold B-HyperparameterName
k B-HyperparameterName
is O
key O
to O
our O
methods O
. O
In O
practice O
, O
we O
rank O
200 B-HyperparameterValue
samples O
manually O
labeled O
with O
noise O
extents O
in O
Section O
4.1 O
by O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
. O
k B-HyperparameterName
is O
set O
based O
on O
different O
scenarios O
. O
If O
given O
the O
remaining O
data O
size O
, O
i.e. O
, O
WMT20 B-DatasetName
predefines O
the O
size O
of O
selected O
data O
, O
we O
select O
the O
corresponding O
k B-HyperparameterName
in O
200 B-HyperparameterValue
annotated O
samples O
by O
the O
ratio O
of O
remaining O
data O
. O
In O
the O
WMT20 B-DatasetName
scenario O
, O
k B-HyperparameterName
is O
2.75 B-HyperparameterValue
and O
2.45 B-HyperparameterValue
for O
the O
5 B-HyperparameterValue
M I-HyperparameterValue
and O
7 B-HyperparameterValue
M I-HyperparameterValue
words O
setting O
in O
Km⇒En B-DatasetName
. O
For O
Ps⇒En B-DatasetName
, O
k B-HyperparameterName
is O
2.3 B-HyperparameterValue
and O
1.4 B-HyperparameterValue
for O
those O
two O
settings O
. O
In O
the O
case O
of O
no O
required O
size O
for O
the O
data O
left O
, O
we O
set O
k B-HyperparameterName
based O
on O
the O
noise B-MetricName
rate I-MetricName
of O
annotated O
samples O
and O
filter O
the O
noisiest O
samples O
ranking O
at O
the O
bottom O
. O
For O
various O
noise O
rates O
in O
in O
- O
house O
datasets O
, O
k B-HyperparameterName
is O
1.8 B-HyperparameterValue
for O
He B-DatasetName
, I-DatasetName
Pt I-DatasetName
, I-DatasetName
Ko⇒Zh I-DatasetName
, O
1.65 B-HyperparameterValue
for O
Id⇒Zh B-DatasetName
, O
and O
1.9 B-HyperparameterValue
for O
Es⇒Zh B-DatasetName
. O
We O
find O
that O
the O
model O
capacity O
affects O
the O
value O
of O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
, O
making O
k B-HyperparameterName
differ O
greatly O
for O
experiments O
on O
WMT20 B-DatasetName
and O
in O
- O
house O
datasets O
( O
model B-HyperparameterName
parameters I-HyperparameterName
47 B-HyperparameterValue
M I-HyperparameterValue
vs. O
68 B-HyperparameterValue
M I-HyperparameterValue
) O
. O
Besides O
, O
it O
is O
easy O
to O
see O
that O
different O
language O
pairs O
have O
similar O
ranges O
of O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
under O
one O
experimental O
setting O
. O

Main O
Results O

To O
thoroughly O
compare O
with O
participants O
in O
the O
WMT20 B-DatasetName
corpus O
filtering O
task O
, O
we O
report O
the O
performance O
of O
two O
models O
ranking O
the O
top O
( O
Esplà O
- O
Gomis O
et O
al O
. O
, O
2020 O
; O
Lu O
et O
al O
. O
, O
2020 O
) O
and O
the O
official O
baseline O
LASER B-MethodName
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
The O
best O
showings O
leverage O
the O
clean O
external O
parallel O
and O
monolingual O
data O
to O
score O
each O
language O
pair O
, O
whereas O
we O
do O
not O
use O
this O
part O
of O
the O
data O
. O

Table O
1 O
presents O
the O
performance O
of O
the O
NMT B-TaskName
model O
trained O
on O
participants O
' O
selected O
subsets O
with O
varying O
scales O
. O
Our O
proposed O
method O
yields O
comparable O
results O
with O
the O
best O
results O
in O
this O
competi- O
We O
find O
that O
further O
benefits O
from O
our O
methods O
vary O
across O
different O
datasets O
, O
which O
are O
minor O
in O
He⇒Zh O
and O
Es⇒Zh O
but O
extremely O
significant O
in O
Id⇒Zh O
and O
Pt⇒Zh O
. O
There O
are O
two O
main O
reasons O
for O
that O
: O
the O
scale O
of O
the O
dataset O
and O
the O
noise O
rate O
. O
A O
large O
- O
scale O
dataset O
in O
He⇒Zh O
makes O
it O
robust O
to O
a O
high O
percentage O
of O
noises O
( O
Jayanthi O
and O
Pratapa O
, O
2021 O
) O
. O
On O
the O
other O
hand O
, O
as O
seen O
in O
Figure O
5 O
, O
the O
type O
III O
noise O
, O
which O
presents O
the O
most O
misaligned O
sentence O
pairs O
, O
only O
accounts O
for O
7 O
% O
in O
Es⇒Zh O
, O
which O
leads O
to O
low O
demand O
for O
noisy O
data O
filtering O
. O

Notably O
, O
our O
method O
has O
a O
specific O
scope O
of O
applications O
. O
We O
do O
not O
suggest O
using O
noisy O
label O
refurbishing O
when O
the O
noise B-MetricName
rate I-MetricName
3 O
exceeds O
30 B-MetricValue
% I-MetricValue
, O
i.e. O
, O
WMT20 B-DatasetName
datasets O
, O
for O
massive O
noises O
lead O
to O
a O
weak O
baseline O
model O
. O
However O
, O
our O
norm B-MethodName
- I-MethodName
based I-MethodName
corpora I-MethodName
filtering I-MethodName
method O
still O
works O
in O
these O
cases O
. O
( O
Zhang O
and O
Zong O
, O
2016 O
) O
. O
" O
sample O
" O
and O
" O
beam O
search O
" O
are O
two O
inference O
ways O
to O
get O
the O
synthetic O
data O
. O

Variations O
of O
Knowledge O
Distillation O
. O
As O
aforesaid O
, O
we O
use O
the O
best O
checkpoint O
on O
the O
validation O
set O
as O
the O
teacher O
model O
to O
distill O
located O
noisy O
samples O
only O
. O
It O
raises O
the O
question O
whether O
we O
have O
better O
options O
for O
the O
teacher O
model O
or O
whether O
we O
can O
conduct O
a O
wide O
range O
of O
knowledge O
distillation O
. O

For O
comparison O
, O
we O
try O
more O
variations O
of O
knowledge O
distillation O
and O
present O
results O
in O
Table O
3 O
. O
We O
find O
that O
the O
best O
checkpoint O
is O
more O
competent O
than O
the O
last O
one O
, O
which O
is O
largely O
similar O
to O
the O
current O
model O
with O
limited O
complementary O
knowledge O
to O
the O
student O
. O
For O
selective O
distillation O
, O
we O
see O
that O
distilling O
the O
whole O
dataset O
is O
not O
a O
good O
choice O
. O
The O
amount O
of O
teacher O
knowledge O
is O
not O
" O
more O
is O
better O
" O
( O
Wang O
et O
al O
. O
, O
2021 O
) O
. O
It O
may O
induce O
more O
noise O
, O
especially O
for O
a O
weak O
teacher O
model O
. O
Among O
them O
, O
the O
bottom O
50 O
% O
of O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
are O
in O
a O
higher O
demand O
for O
distillation O
. O
Those O
samples O
with O

+ O
H O
= O
K O
5 O
[ O
\ O
, O
G O
= O
K O
2ND\ O
7\SH O
, O
7\SH O
, O
,7\SH O
, O
, O
, O
Figure O
7 O
: O

R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
of O
manually O
annotated O
samples O
with O
varying O
noise O
degrees O
in O
section O
4.1 O
. O
Type O
I O
, O
II O
, O
and O
III O
represent O
word- O
, O
phrase- O
, O
and O
sentence O
- O
level O
misalignment O
, O
respectively O
. O
Type O
III O
and O
partial O
type O
II O
are O
our O
target O
noisy O
samples O
that O
need O
to O
be O
filtered O
. O
The O
dashed O
red O
line O
is O
the O
threshold B-HyperparameterName
k B-HyperparameterName
we O
set O
. O

higher O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
are O
likely O
with O
clean O
labels O
where O
the O
teacher O
model O
is O
useless O
. O
The O
results O
show O
the O
necessity O
of O
carefully O
selecting O
distilled O
samples O
in O
the O
presence O
of O
noise O
. O
Furthermore O
, O
our O
method O
is O
related O
to O
Forward B-MethodName
Translation I-MethodName
( O
FT B-MethodName
) O
( O
Zhang O
and O
Zong O
, O
2016 O
) O
for O
exploiting O
the O
monolingual O
data O
. O
They O
use O
the O
earlier O
trained O
model O
as O
the O
teacher O
to O
translate O
source O
sentences O
to O
target O
translation O
, O
and O
the O
obtained O
synthetic O
corpora O
are O
fed O
to O
the O
student O
model O
trained O
later O
. O
To O
study O
the O
usefulness O
of O
FT B-MethodName
in O
our O
scenario O
, O
we O
regard O
our O
proposed O
noisy O
data O
filtering O
method O
as O
the O
teacher O
. O
Then O
, O
we O
split O
misaligned O
samples O
, O
where O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
≤ O
k B-HyperparameterName
( O
1.01 O
M O
in O
He⇒Zh O
) O
, O
as O
monolingual O
source O
sentences O
. O
The O
following O
steps O
are O
in O
line O
with O
FT B-MethodName
. O
From O
the O
last O
two O
lines O
in O
Table O
3 O
, O
the O
synthetic O
data O
is O
of O
poor O
quality O
if O
generated O
by O
sampling O
for O
a O
weak O
teacher O
model O
. O
Beam O
search O
ensures O
good O
translations O
and O
performs O
better O
but O
is O
computationally O
expensive O
. O
Unlike O
sentence O
- O
level O
distillation O
, O
the O
word O
- O
level O
distillation O
in O
this O
paper O
allows O
the O
transfer O
of O
local O
word O
distributions O
. O
It O
eliminates O
the O
error O
propagated O
from O
the O
teacher O
model O
, O
which O
is O
more O
suitable O
in O
the O
noise O
scenario O
. O

Analysis O

We O
conduct O
extensive O
analyses O
to O
evaluate O
the O
ability O
of O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
to O
pinpoint O
unparallel O
sentence O
pairs O
. O
We O
first O
examine O
whether O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
can O
reflect O
the O
overall O
degree O
of O
misalignment O
. O
From O
a O
more O
finegrained O
view O
, O
we O
explore O
the O
correlation O
between O
the O
score O
γ B-MetricName
j I-MetricName
at O
step O
j O
and O
linguistic O
properties O
. O

Correlation O
with O
the O
Misalignment O
Degree O

As O
previous O
, O
we O
filter O
sentence O
pairs O
where O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
is O
lower O
than O
the O
threshold B-HyperparameterName
k. B-HyperparameterName
To O
explore O
whether O
filtered O
samples O
are O
indeed O
corrupted O
, O
we O
calculate O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
of O
annotated O
samples O
in O
section O
4.1 O
, O
which O
are O
categorized O
into O
four O
classes O
based O
on O
the O
degree O
of O
misalignment O
. O
As O
shown O
in O
Figure O
7 O
, O
R B-HyperparameterName
( I-HyperparameterName
x I-HyperparameterName
, I-HyperparameterName
y I-HyperparameterName
) I-HyperparameterName
is O
negatively O
correlated O
with O
the O
extent O
of O
noises O
. O
Type O
III O
and O
part O
of O
type O
II O
noisy O
samples O
are O
assigned O
with O
relatively O
lower O
R B-HyperparameterName
( I-HyperparameterName
x I-HyperparameterName
, I-HyperparameterName
y I-HyperparameterName
) I-HyperparameterName
where O
too O
much O
target O
- O
side O
information O
is O
required O
to O
predict O
the O
translation O
. O
It O
indicates O
that O
our O
proposed O
measurement O
is O
reflective O
of O
misalignment O
and O
sufficient O
to O
filter O
unparallel O
sentence O
pairs O
for O
NMT B-TaskName
. O

+ O
H O
= O
K O
j O
, O
G O
= O
K O
1RXQ O
$ O
GM O
9HUE O
3UHS O
3XQF O
' O
HWH O

Correlation O
with O
Linguistic O
Properties O

As O
seen O
in O
Equation O
( O
6 O
) O
, O
the O
sentence O
- O
level O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
is O
averaged O
over O
γ B-MetricName
j I-MetricName
at O
step O
j O
, O
which O
depicts O
whether O
each O
target O
word O
corresponds O
to O
any O
source O
words O
. O
This O
section O
studies O
the O
relation O
between O
γ B-MetricName
j I-MetricName
and O
two O
properties O
, O
syntactic O
roles O
and O
fertility O
. O
Chinese O
sentences O
are O
POS O
tagged O
by O
jieba O
4 O
. O
Fertility O
reveals O
how O
many O
source O
tokens O
a O
target O
token O
is O
aligned O
to O
, O
which O
is O
obtained O
by O
fast O
align O
( O
Dyer O
et O
al O
. O
, O
2013 O
) O
to O
extract O
bilingual O
alignment O
. O
Results O
are O
reported O
on O
the O
validation O
set O
. O

As O
shown O
in O
Figure O
8 O
, O
content O
words O
are O
in O
great O
need O
of O
the O
source O
context O
, O
thus O
leading O
to O
a O
higher O
γ B-MetricName
j I-MetricName
. O
However O
, O
content O
- O
free O
words O
, O
like O
punctuation O
and O
determiner O
, O
mainly O
rely O
on O
the O
target O
- O
side O
information O
, O
where O
γ B-MetricName
j I-MetricName
is O
significantly O
below O
average O
. O
Furthermore O
, O
the O
value O
of O
γ B-MetricName
j I-MetricName
is O
positively O
related O
to O
the O
fertility O
of O
the O
target O
word O
. O
As O
illustrated O
in O
Figure O
9 O
, O
the O
prediction O
of O
the O
target O
token O
aligning O
to O
more O
source O
words O
relies O
more O
on O
the O
source O
sentence O
, O
thus O
leading O
to O
a O
higher O
γ B-MetricName
j I-MetricName
. O
These O
findings O
fully O
show O
the O
rationality O
of O
our O
proposed O
metrics O
. O
4 O
https O
: O
/ O
/ O
pypi.org O
/ O
project O
/ O
jieba O
/ O

Related O
Work O

Many O
web O
- O
crawled O
data O
for O
training O
the O
NMT B-MethodName
system O
are O
so O
noisy O
that O
we O
should O
select O
the O
highquality O
subset O
. O
In O
this O
section O
, O
we O
first O
review O
recent O
advances O
in O
noisy B-TaskName
corpora I-TaskName
filtering I-TaskName
for O
NMT B-TaskName
. O
As O
we O
treat O
the O
discovered O
noisy O
data O
as O
unlabeled O
monolingual O
data O
to O
distill O
in O
this O
paper O
, O
another O
related O
work O
is O
knowledge O
distillation O
in O
NMT B-TaskName
. O

Parallel O
Corpus O
Filtering O

There O
is O
a O
rich O
body O
of O
work O
on O
filtering O
out O
noises O
in O
parallel O
data O
. O
Xu O
and O
Koehn O
( O
2017 O
) O
construct O
the O
noisy O
synthetic O
data O
( O
inadequate O
and O
non O
- O
fluent O
translations O
) O
and O
train O
a O
classifier O
to O
distinguish O
good O
from O
the O
bad O
. O
Açarçiçek O
et O
al O
. O
( O
2020 O
) O
follow O
this O
idea O
with O
a O
classifier O
based O
on O
a O
multilingual O
version O
of O
the O
RoBERTa O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
. O
Many O
other O
studies O
employ O
different O
bilingual O
and O
monolingual O
language O
models O
to O
score O
the O
sentence O
pairs O
( O
Lu O
et O
al O
. O
, O
2020 O
; O
Esplà O
- O
Gomis O
et O
al O
. O
, O
2020 O
) O
, O
which O
are O
data O
- O
hungry O
and O
time O
- O
consuming O
in O
practice O
. O
Unlike O
them O
, O
our O
method O
does O
not O
use O
external O
data O
and O
yields O
comparable O
results O
. O
Also O
, O
we O
perform O
data O
filtering O
and O
model O
training O
in O
one O
stage O
to O
reduce O
time O
and O
computation O
costs O
. O

Knowledge O
Distillation O
in O
NMT O

Knowledge O
distillation O
transfers O
the O
knowledge O
from O
the O
teacher O
to O
the O
student O
model O
. O
It O
is O
widely O
studied O
in O
NMT B-TaskName
to O
obtain O
a O
lightweight O
and O
effective O
model O
. O
Kim O
and O
Rush O
( O
2016 O
) O
use O
an O
offline O
large O
- O
capacity O
NMT B-TaskName
system O
as O
the O
fixed O
teacher O
model O
, O
which O
is O
also O
extended O
to O
multilingual O
NMT B-TaskName
( O
Tan O
et O
al O
. O
, O
2019 O
) O
. O
Instead O
, O
the O
same O
- O
capacity O
teacher O
model O
is O
used O
in O
Zhang O
and O
Zong O
( O
2016 O
) O
; O
Sennrich O
et O
al O
. O
( O
2016 O
) O
. O
Such O
approaches O
need O
massive O
clean O
data O
for O
training O
an O
accurate O
teacher O
model O
, O
which O
is O
impractical O
in O
the O
limited O
noisy O
corpora O
. O
Another O
line O
of O
work O
applies O
self O
- O
distillation O
to O
NMT O
using O
the O
current O
or O
history O
model O
as O
the O
teacher O
model O
( O
Wei O
et O
al O
. O
, O
2019 O
; O
Hahn O
and O
Choi O
, O
2019 O
) O
, O
updating O
the O
distilled O
knowledge O
as O
a O
better O
model O
comes O
. O
Here O
, O
we O
focus O
on O
a O
new O
scenario O
where O
self O
- O
distillation O
is O
employed O
to O
relabel O
the O
noisy O
samples O
in O
the O
training O
of O
the O
noisy O
corpora O
. O

Conclusion O

This O
paper O
presents O
a O
novel O
norm B-MethodName
- I-MethodName
based I-MethodName
noisy I-MethodName
corpora I-MethodName
filtering I-MethodName
and I-MethodName
refurbishing I-MethodName
method O
. O
We O
propose O
to O
use O
the O
information B-MetricName
ratio I-MetricName
from O
the O
source O
to O
the O
target O
side O
to O
distinguish O
unparallel O
sentence O
pairs O
. O
The O
amounts O
of O
these O
two O
information O
flows O
are O
calculated O
by O
norms B-MetricName
of O
context O
vectors O
of O
each O
side O
. O
Unlike O
parallel O
sentence O
pairs O
, O
the O
excessive O
targetside O
information O
is O
needed O
for O
the O
model O
to O
fit O
unparallel O
ones O
, O
which O
present O
relatively O
lower O
scores O
. O
We O
incorporate O
the O
noisy O
corpora O
filtering O
into O
the O
training O
of O
NMT B-TaskName
without O
any O
extra O
clean O
data O
or O
costly O
pre O
- O
trained O
scorers O
. O
Extensive O
experiments O
show O
that O
our O
method O
performs O
comparably O
with O
SOTA O
results O
with O
significant O
advantages O
in O
time O
and O
computational O
costs O
. O
We O
further O
refurbish O
the O
discovered O
noisy O
data O
by O
producing O
pseudo O
labels O
via O
online O
knowledge O
distillation O
, O
which O
obtains O
further O
performance O
gains O
. O

Limitations O

Our O
methods O
have O
a O
specific O
scope O
of O
applications O
due O
to O
the O
methodology O
design O
. O
As O
highlighted O
in O
Section O
3 O
, O
the O
basis O
of O
our O
approach O
is O
the O
difference O
in O
how O
the O
model O
processes O
unparallel O
and O
parallel O
sentence O
pairs O
. O
Thus O
, O
it O
can O
not O
work O
well O
when O
a O
large O
extent O
of O
noise O
makes O
the O
NMT B-TaskName
model O
hard O
to O
converge O
. O
We O
take O
Nepali O
- O
English O
in O
WMT B-DatasetName
2019 I-DatasetName
shared O
task O
on O
parallel O
corpus O
filtering O
and O
alignment O
5 O
as O
an O
example O
. O
By O
sampling O
inspection O
, O
we O
find O
that O
89.4 B-MetricValue
% I-MetricValue
of O
the O
dataset O
is O
wholly O
misaligned O
after O
pre O
- O
processing O
. O
It O
is O
unstable O
to O
directly O
train O
an O
NMT B-TaskName
model O
with O
the O
whole O
dataset O
, O
which O
results O
in O
our O
worse O
performance O
than O
the O
best O
showing O
( O
2.2 B-MetricValue
BLEU B-MetricName
vs. O
3.2 B-MetricValue
BLEU B-MetricName
) O
. O
In O
this O
case O
, O
extra O
clean O
data O
or O
resources O
of O
similar O
languages O
are O
necessary O
to O
build O
a O
competent O
scorer O
. O

A O
Norm B-MetricName
- O
based O
Word O
Importance B-MetricName
Measurement O

We O
present O
the O
relation O
between O
the O
norm O
of O
c O
L−1 O
i O
and O
token O
frequency O
in O
Figure O
10 O
. O
We O
can O
see O
that O
the O
norm O
of O
c O
L−1 O
i O
decreases O
with O
a O
high O
word O
frequency O
. O
Moreover O
, O
the O
distribution O
of O
blue O
dots O
( O
function O
words O
) O
is O
lower O
than O
that O
of O
green O
dots O
( O
content O
words O
) O
. O
It O
indicates O
that O
words O
with O
more O
diverse O
semantics O
receive O
higher O
norms O
. O
The O
observations O
in O
Figure O
3 O
and O
10 O
show O
that O
we O
can O
infer O
how O
much O
information B-MetricName
is O
encoded O
in O
word O
representations O
from O
the O
perspective O
of O
norms O
. O

H O
H O
H O
7RNHQ O
) O
UHTXHQF\ O
versus O
token O
frequency O
of O
English O
words O
in O
the O
LDC O
Chinese O
- O
to O
- O
English O
vocabulary O
labeled O
content O
words O
( O
green O
dots O
) O
and O
function O
words O
( O
blue O
dots O
) O
. O
This O
produces O
a O
downward O
trendline O
. O

B O
Rule O
- O
based O
Pre O
- O
filtering O

Following O
Lu O
et O
al O
. O
( O
2020 O
) O
, O
we O
apply O
a O
series O
of O
heuristic O
rules O
to O
filter O
the O
low O
- O
quality O
sentence O
pairs O
, O
which O
includes O
: O

• O
The O
length B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
sentence I-HyperparameterName
. O
The O
too O
short O
( O
≤ O
2 B-HyperparameterValue
words O
) O
or O
too O
long O
( O
> O
50 B-HyperparameterValue
words O
) O
sentences O
will O
be O
removed O
. O

• O
The O
length B-HyperparameterName
ratio I-HyperparameterName
of O
the O
source O
sentence O
to O
the O
target O
sentence O
. O
The O
ratio B-HyperparameterName
is O
set O
between O
0.2 B-HyperparameterValue
to O
5 B-HyperparameterValue
for O
all O
language O
pairs O
. O

• O
The O
proportion O
of O
valid O
tokens O
. O
A O
valid O
token O
should O
include O
the O
letters O
in O
the O
corresponding O
language O
. O
The O
sentence O
is O
dropped O
if O
the O
validtoken B-HyperparameterName
ratio I-HyperparameterName
is O
less O
than O
0.2 B-HyperparameterValue
. O

• O
Language O
filtering O
. O
We O
detect O
the O
language O
of O
a O
sentence O
by O
using O
a O
language O
detection O
tool O
fasttext O
. O
It O
helps O
remove O
the O
sentence O
pairs O
if O
either O
the O
source O
or O
the O
target O
sentence O
does O
not O
belong O
to O
the O
required O
language O
. O

• O
URLs O
or O
numbers O
. O
We O
remove O
the O
sentence O
which O
contains O
URLs O
or O
more O
than O
25 B-HyperparameterValue
% I-HyperparameterValue
numerical B-HyperparameterName
tokens I-HyperparameterName
. O

The O
size O
of O
data O
after O
ruled O
- O
based O
pre O
- O
filtering O
is O
given O
in O
Table O
4 O
. O
We O
find O
that O
WMT20 B-DatasetName
datasets O
are O
very O
noisy O
, O
where O
around O
72.66 O
% O
of O
Km B-DatasetName
- I-DatasetName
En I-DatasetName
and O
51.96 O
% O
of O
Ps B-DatasetName
- I-DatasetName
En I-DatasetName
are O
filtered O
out O
. O
The O
noise B-MetricName
rate I-MetricName
in O
He B-DatasetName
- I-DatasetName
Zh I-DatasetName
is O
relatively O
lower O
. O
As O
shown O
in O
Table O
1 O
, O
pre O
- O
filtering O
is O
necessary O
to O
relieve O
stress O
for O
the O
following O
filtering O
method O
. O

C O
Experimental O
Settings O

For O
WMT20 B-DatasetName
datasets O
, O
we O
strictly O
follow O
the O
model O
configuration O
and O
evaluation O
settings O
provided O
by O
the O
WMT20 O
organizers O
( O
Koehn O
et O
al O
. O
, O
2020 O
) O
. O
It O
includes O
five O
stacked O
encoder O
layers O
and O
five O
stacked O
decoder O
layers O
. O
During O
training O
, O
we O
use O
Adam O
optimizer O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.998 B-HyperparameterValue
, O
an O
inverse B-HyperparameterName
sqrt I-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
4,000 B-HyperparameterValue
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
steps I-HyperparameterName
, O
and O
dropout B-HyperparameterName
is O
0.4 B-HyperparameterValue
. O
All O
experiments O
last O
for O
100 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
single O
GPU O
, O
where O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
4000 B-HyperparameterValue
tokens O
. O
We O
accumulate O
the O
gradient O
of O
parameters O
and O
update O
every O
4 B-HyperparameterValue
steps O
. O
Scores O
on O
test O
sets O
are O
reported O
by O
case O
- O
insensitive O
Sacrebleu O
( O
Post O
, O
2018 O
) O
. O

For O
our O
in O
- O
house O
datasets O
, O
we O
experiment O
with O
Transformer B-MethodName
Base I-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
During O
training O
, O
we O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
of O
value O
ϵ B-HyperparameterName
ls O
= O
0.1 B-HyperparameterValue
and O
employ O
the O
Adam O
( O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.998 B-HyperparameterValue
) O
for O
parameter O
optimization O
with O
a O
scheduled O
learning B-HyperparameterName
rate I-HyperparameterName
of O
4,000 B-HyperparameterValue
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
steps I-HyperparameterName
. O
The O
training O
lasts O
150k O
for O
He⇒Zh B-DatasetName
and O
Id⇒Zh B-DatasetName
, O
100k O
for O
the O
other O
three O
. O
We O
average O
the O
last O
ten O
checkpoints O
and O
use O
beam O
search O
( O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
, O
length B-HyperparameterName
penalty I-HyperparameterName
1.2 B-HyperparameterValue
) O
for O
inference O
. O
We O
measure O
case B-MetricName
- I-MetricName
insensitive I-MetricName
BLEU I-MetricName
calculated O
by O
multi-bleu.perl O
. O

Our O
method O
introduces O
two O
hyper O
- O
parameters O
. O
The O
first O
one O
is O
the O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
step I-HyperparameterName
T B-HyperparameterName
to O
ensure O
the O
stability O
of O
our O
metric O
, O
and O
the O
second O
is O
the O
threshold B-HyperparameterValue
k B-HyperparameterValue
to O
determine O
the O
amount O
of O
the O
filtered O
data O
. O
For O
the O
former O
, O
we O
observe O
that O
our O
proposed O
metric O
reaches O
a O
stable O
range O
after O
several O
thousand O
steps O
and O
thus O
set O
T B-HyperparameterName
= O
3k B-HyperparameterValue
. O
As O
aforesaid O
in O
Section O
4.2 O
, O
we O
determine O
the O
threshold B-HyperparameterName
k B-HyperparameterName
based O
on O
the O
percentage O
of O
the O
remaining O
data O
if O
given O
the O
required O
size O
of O
selected O
subsets O
. O
We O
take O
Ps B-DatasetName
- I-DatasetName
En I-DatasetName
as O
an O
example O
. O
The O
dataset O
after O
rule O
- O
based O
filtering O
contains O
7.78 O
M O
English O
words O
. O
Thus O
, O
we O
should O
remove O
10 O
% O
and O
35 O
% O
of O
the O
data O
to O
obtain O
the O
5 O
M O
and O
7 O
M O
words O
settings O
. O
We O
determine O
the O
threshold B-HyperparameterName
k B-HyperparameterName
as O
the O
lower O
decile O
in O
the O
R B-MetricName
( I-MetricName
x I-MetricName
, I-MetricName
y I-MetricName
) I-MetricName
of O
200 O
annotated O
samples O
for O
10 O
% O
removal O
, O
similar O
to O
35 O
% O
. O

Acknowledgements O

This O
work O
is O
supported O
by O
the O
Natural O
Science O
Foundation O
of O
China O
under O
Grant O
62122088 O
and O
U1836221 O
. O

Introduction O

Results O

Models O

Results O
and O
Discussion O

Related O
Work O

A.7 O
Annotations O

Annotations O

Acknowledgements O

From O
the O
One O
, O
Judge O
of O
the O
Whole O
: O
Typed O
Entailment O
Graph O
Construction O
with O
Predicate O
Generation O

Entailment O
Graphs O
( O
EGs O
) O
have O
been O
constructed O
based O
on O
extracted O
corpora O
as O
a O
strong O
and O
explainable O
form O
to O
indicate O
contextindependent O
entailment O
relations O
in O
natural O
languages O
. O
However O
, O
EGs O
built O
by O
previous O
methods O
often O
suffer O
from O
the O
severe O
sparsity O
issues O
, O
due O
to O
limited O
corpora O
available O
and O
the O
longtail O
phenomenon O
of O
predicate O
distributions O
. O
In O
this O
paper O
, O
we O
propose O
a O
multi O
- O
stage O
method O
, O
Typed B-MethodName
Predicate I-MethodName
- I-MethodName
Entailment I-MethodName
Graph I-MethodName
Generator I-MethodName
( O
TP B-MethodName
- I-MethodName
EGG I-MethodName
) O
, O
to O
tackle O
this O
problem O
. O
Given O
several O
seed O
predicates O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
builds O
the O
graphs O
by O
generating O
new O
predicates O
and O
detecting O
entailment O
relations O
among O
them O
. O
The O
generative O
nature O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
helps O
us O
leverage O
the O
recent O
advances O
from O
large O
pretrained O
language O
models O
( O
PLMs O
) O
, O
while O
avoiding O
the O
reliance O
on O
carefully O
prepared O
corpora O
. O
Experiments O
on O
benchmark O
datasets O
show O
that O
TP B-MethodName
- I-MethodName
EGG I-MethodName
can O
generate O
high O
- O
quality O
and O
scale O
- O
controllable O
entailment O
graphs O
, O
achieving O
significant O
in O
- O
domain O
improvement O
over O
state O
- O
of O
- O
the O
- O
art O
EGs O
and O
boosting O
the O
performance O
of O
down O
- O
stream O
inference O
tasks O
1 O
. O

Introduction O

The O
entailment O
relation O
between O
textual O
predicates O
plays O
a O
critical O
role O
in O
natural O
language O
inference O
and O
natural O
language O
understanding O
tasks O
, O
including O
question O
answering O
( O
Pathak O
et O
al O
. O
, O
2021 O
; O
McKenna O
et O
al O
. O
, O
2021 O
) O
and O
knowledge O
graph O
completion O
( O
Yoshikawa O
et O
al O
. O
, O
2019 O
; O
Hosseini O
et O
al O
. O
, O
2019 O
. O
To O
detect O
entailment O
relations O
, O
previous O
works O
pay O
attention O
to O
the O
Recognizing B-TaskName
Textual I-TaskName
Entailment I-TaskName
( O
RTE B-TaskName
) O
task O
, O
which O
takes O
a O
pair O
of O
sentences O
as O
input O
and O
predicts O
whether O
one O
sentence O
entails O
the O
other O
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
He O
et O
al O
. O
, O
2021b O
; O
Pilault O
et O
al O
. O
, O
2020 O
) O
. O
Current O
RTE B-TaskName
models O
perform O
well O
on O
RTE B-TaskName
benchmarks O
, O
but O
most O
of O
them O
are O
lacking O
in O
explainability O
, O
as O
they O
make O
use O
of O
the O
black O
- O
box O
Language O
Models O
( O
LM O
) O
without O
providing O
any O
explainable O
clues O
. O

Recent O
works O
focus O
on O
learning O
the O
Entailment O
Graph O
( O
EG O
) O
structure O
, O
which O
organizes O
typed O
predicates O
in O
directional O
graphs O
with O
entailment O
relations O
as O
the O
edges O
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2018 I-MethodName
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
, I-MethodName
2019McKenna I-MethodName
et O
al O
. O
, O
2021 O
) O
, O
as O
shown O
in O
Figure O
1 O
. O
With O
the O
explicit O
graph O
structure O
containing O
predicates O
and O
their O
entailment O
relations O
, O
similar O
to O
Knowledge O
Graphs O
( O
KGs O
) O
, O
using O
EGs O
becomes O
an O
explainable O
and O
context O
- O
independent O
way O
to O
represent O
the O
knowledge O
required O
in O
natural O
language O
inference O
and O
other O
NLP O
tasks O
. O

Most O
existing O
EGs O
are O
constructed O
with O
the O
Distributional O
Inclusion O
Hypothesis O
( O
DIH O
) O
, O
which O
suggests O
that O
all O
typical O
context O
features O
of O
a O
predicate O
v O
can O
also O
occur O
with O
another O
predicate O
w O
if O
v O
entails O
w O
( O
Geffet O
and O
Dagan O
, O
2005 O
) O
. O
Constructing O
EGs O
with O
DIH O
requires O
distributional O
cooccurrences O
of O
contextual O
features O
from O
large O
corpora O
to O
calculate O
the O
semantic O
similarity O
between O
predicates O
( O
Szpektor O
and O
Dagan O
, O
2008 O
; O
Schoenmackers O
et O
al O
. O
, O
2010 O
) O
. O
However O
, O
the O
EGs O
constructed O
from O
large O
corpora O
often O
suffer O
from O
two O
different O
kinds O
of O
sparsity O
issues O
: O
the O
predicate O
sparsity O
and O
the O
edge O
sparsity O
. O
Existing O
corpora O
used O
for O
EG O
construction O
are O
mainly O
collected O
from O

generative O
LMs O

Figure O
2 O
: O
An O
illustration O
of O
our O
TP B-MethodName
- I-MethodName
EGG I-MethodName
. O
Given O
three O
seed O
predicates O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
generates O
a O
graph O
with O
8 O
predicates O
and O
15 O
entailment O
relations O
. O
The O
circles O
represents O
different O
predicates O
, O
while O
the O
rounded O
rectangles O
is O
sentences O
in O
natural O
language O
. O
Seed O
predicates O
is O
in O
green O
, O
and O
newly O
generated O
predicates O
is O
in O
blue O
. O
specific O
resources O
( O
Zhang O
and O
Weld O
, O
2013 O
) O
, O
such O
as O
news O
articles O
. O
As O
a O
result O
, O
entailment O
relations O
could O
not O
be O
learned O
between O
those O
predicates O
that O
do O
not O
appear O
in O
the O
corpora O
, O
which O
leads O
to O
the O
predicate O
sparsity O
issue O
. O
Meanwhile O
, O
if O
two O
predicates O
scarcely O
appear O
around O
similar O
contexts O
in O
the O
given O
corpora O
, O
the O
DIH O
could O
not O
indicate O
the O
potential O
entailment O
relationship O
between O
them O
. O
It O
leads O
to O
the O
edge O
sparsity O
of O
EGs O
as O
the O
corresponding O
edges O
may O
be O
missing O
due O
the O
limited O
coverage O
of O
the O
corpora O
. O

To O
tackle O
the O
sparsity O
issues O
, O
previous O
works O
pay O
attention O
to O
learning O
global O
graph O
structures O
to O
mine O
latent O
entailment O
relations O
and O
alleviate O
the O
edge O
sparsity O
( O
Berant O
et O
al O
. O
, O
2011 O
( O
Berant O
et O
al O
. O
, O
, O
2015Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2018 I-MethodName
; O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2022 I-MethodName
) O
, O
but O
predicate O
sparsity O
is O
still O
holding O
back O
the O
improvement O
of O
EGs O
. O
Solving O
predicate O
sparsity O
by O
simply O
scaling O
up O
the O
distributional O
feature O
extraction O
is O
impracticable O
, O
due O
to O
the O
long O
- O
tail O
phenomenon O
of O
predicate O
distribution O
( O
McKenna O
and O
Steedman O
, O
2022 O
) O
. O

The O
shortcomings O
of O
extractive O
methods O
come O
in O
quest O
for O
non O
- O
extraction O
way O
to O
overcome O
. O
Recent O
progress O
in O
deep O
generative O
LMs O
, O
including O
GPT-3 O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
and O
T5 O
( O
Raffel O
et O
al O
. O
, O
2022 O
) O
, O
makes O
it O
possible O
to O
produce O
predicates O
and O
entailment O
relations O
by O
generative O
methods O
. O
Inspired O
by O
the O
Commonsense O
Transformer O
( O
Bosselut O
et O
al O
. O
, O
2019 O
) O
, O
we O
propose O
a O
novel O
generative O
multi O
- O
stage O
EG O
construction O
method O
, O
called O
Typed B-MethodName
Predicate I-MethodName
- I-MethodName
Entailment I-MethodName
Graph I-MethodName
Generator I-MethodName
( O
TP B-MethodName
- I-MethodName
EGG I-MethodName
) O
. O
As O
shown O
in O
Figure O
2 O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
takes O
several O
seed O
predicates O
as O
input O
of O
the O
LM O
- O
based O
predicate O
generator O
to O
depict O
the O
domain O
of O
predicates O
and O
generate O
more O
in O
- O
domain O
predicates O
. O
With O
generated O
predicates O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
uses O
a O
novel O
transitivity O
- O
ensured O
edge O
se O
- O
lector O
by O
representing O
predicates O
as O
spheres O
in O
the O
vector O
space O
, O
to O
pick O
out O
the O
potential O
entailment O
relations O
among O
generated O
predicates O
. O
Then O
TP B-MethodName
- I-MethodName
EGG I-MethodName
calculates O
the O
corresponding O
edge O
weights O
by O
the O
LM O
- O
based O
edge O
calculator O
. O
Our O
key O
insight O
is O
that O
by O
re O
- O
modeling O
the O
predicate O
extraction O
process O
as O
a O
generation O
process O
, O
we O
can O
leverage O
the O
underlying O
knowledge O
about O
natural O
language O
inference O
inside O
the O
LMs O
to O
avoid O
the O
data O
sparsity O
issues O
of O
extractive O
methods O
. O
By O
choosing O
appropriate O
seed O
predicates O
and O
setting O
the O
parameters O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
, O
one O
can O
generate O
EGs O
containing O
knowledge O
from O
a O
specific O
domain O
in O
arbitrary O
scales O
to O
fit O
the O
downstream O
requirement O
, O
without O
limitations O
from O
the O
uncontrollable O
distribution O
in O
domain O
- O
independent O
corpora O
. O
Since O
almost O
all O
the O
EG O
construction O
modules O
in O
TP B-MethodName
- I-MethodName
EGG I-MethodName
is O
controlled O
by O
pre O
- O
trained O
LMs O
, O
the O
output O
EGs O
can O
be O
seen O
as O
explicit O
representations O
of O
the O
knowledge O
in O
LMs O
and O
used O
in O
downstream O
tasks O
, O
such O
as O
RTE B-TaskName
in O
our O
experiments O
. O

In O
a O
word O
, O
our O
contributions O
can O
be O
summarized O
as O
follows O
: O
( O
1 O

Related O
Work O

Previous O
EG O
construction O
methods O
construct O
feature O
representations O
for O
typed O
predicates O
, O
weighted O
by O
counts O
or O
Pointwise O
Mutual O
Information O
( O
Berant O
et O
al O
. O
, O
2015 O
) O
, O
and O
compute O
the O
distribution O
similarity O
guided O
by O
DIH O
. O
For O
a O
predicate O
pair O
, O
different O
similarities O
are O
calculated O
, O
such O
as O
cosine O
similarity O
, O
Lin O
( O
Lin O
, O
1998 O
) O
, O
Weed O
( O
Weeds O
and O
Weir O
, O
2003 O
) O
, O
and O
Balanced O
Inclusion O
( O
Szpektor O
and O
Dagan O
, O
2008 O
) O
. O
Markov O
chain O
of O
predicate O
- O
argument O
transition O
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2019 I-MethodName
) O
and O
temporal O
information O
from O
extracted O
corpora O
( O
Guillou O
et O
al O
. O
, O
2020 O
) O
are O
also O
used O
in O
EGs O
construction O
. O
These O
methods O
independently O
calculate O
the O
entailment O
relations O
for O
each O
pair O
, O
called O
local O
methods O
. O
Besides O
, O
global O
constraints O
are O
used O
to O
detect O
new O
entailment O
relations O
beyond O
local O
relations O
. O
The O
transitivity O
in O
EGs O
, O
which O
means O
a O
entails O
b O
and O
b O
entails O
c O
indicate O
a O
entails O
c O
for O
three O
predicates O
a O
, O
b O
and O
c O
, O
is O
the O
most O
widely O
used O
in O
previous O
works O
as O
hard O
constraints O
( O
Berant O
et O
al O
. O
, O
2011 O
( O
Berant O
et O
al O
. O
, O
, O
2015 O
or O
soft O
loss O
functions O
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2018 I-MethodName
; O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2022 I-MethodName
) O
. O
The O
weight O
similarity O
constraints O
between O
different O
typed O
EGs O
and O
similar O
predicates O
are O
also O
taken O
into O
consideration O
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2018 I-MethodName
) O
. O

As O
one O
of O
the O
most O
important O
areas O
of O
NLP O
, O
text O
generation O
, O
or O
Natural O
Language O
Generation O
( O
NLG O
) O
, O
has O
also O
been O
advanced O
by O
the O
surgent O
development O
of O
pre O
- O
trained O
LMs O
. O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
uses O
encoder O
- O
decoder O
transformer O
architecture O
to O
re O
- O
correct O
the O
corrupted O
data O
in O
pre O
- O
training O
phase O
; O
GPT-3 O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
uses O
transformer O
decoder O
to O
achieve O
in O
- O
context O
learning O
with O
massive O
multi O
- O
task O
unsupervised O
data O
. O
T5 O
( O
Raffel O
et O
al O
. O
, O
2022 O
) O
unifies O
different O
tasks O
into O
natural O
language O
prefixes O
and O
solves O
them O
by O
text O
generation O
. O

Pre O
- O
trained O
LMs O
are O
also O
applied O
in O
recent O
EG O
methods O
. O
CNCE B-MethodName
initializes O
the O
contextualized O
embeddings O
of O
entity O
- O
relation O
triplets O
by O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
uses O
random O
walk O
to O
get O
the O
entailment O
probability O
; O
EGT2 B-MethodName
( O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2022 I-MethodName
) O
fine O
- O
tunes O
a O
patternadapted O
LM O
on O
the O
predicate O
sentences O
and O
recalculates O
high O
- O
quality O
edge O
weights O
for O
global O
constraints O
; O
McKenna O
and O
Steedman O
( O
2022 O
) O
applies O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
as O
predicate O
encoder O
and O
matches O
missing O
predicates O
in O
EGs O
with O
K O
- O
Nearest O
Neighbor O
algorithm O
to O
alleviate O
the O
predicate O
sparsity O
. O
As O
far O
as O
we O
are O
concerned O
, O
our O
method O
is O
the O
first O
attempt O
to O
use O
generative O
LM O
in O
EG O
construction O
and O
directly O
generate O
EGs O
without O
the O
distributional O
features O
from O
large O
corpora O
. O

Our O
Approach O

EGs O
store O
predicates O
as O
nodes O
and O
entailment O
relations O
between O
them O
as O
edges O
in O
graph O
structures O
. O
Following O
previous O
EG O
methods O
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2018 I-MethodName
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
, I-MethodName
2019Chen I-MethodName
et B-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2022 I-MethodName
) O
, O
we O
use O
the O
neo O
- O
Davisonian O
semantic O
form O
of O
binary O
relation O
( O
Parsons O
, O
1990 O
) O
to O
indicate O
typed O
predicates O
, O
whose O
types O
are O
defined O
by O
the O
combination O
of O
argument O
types O
. O
Predicate O
p O
connecting O
two O
arguments O
a O
1 O
, O
a O
2 O
with O
types O
t O
1 O
, O
t O
2 O
can O
be O
represented O
as O
p O
= O
( O
w O
1 O
.i O
1 O
, O
w O
2 O
.i O
2 O
, O
t O
1 O
, O
t O
2 O
) O
, O
where O
w O
j O
is O
the O
center O
relation O
tokens O
( O
and O
perhaps O
prepositions O
) O
about O
a O
j O
, O
and O
i O
j O
is O
corresponding O
argument O
order O
of O
a O
j O
in O
w O
j O
. O
For O
example O
, O
the O
event O
" O
The O
government O
is O
elected O
in O
1910 O
and O
adored O
by O
natives O
" O
contains O
two O
predicates O
( O
elect.2 O
, O
elect.in.2 O
, O
government O
, O
time O
) O
and O
( O
adore.1 O
, O
adore.2 O
, O
person O
, O
government O
) O
. O
We O
denote O
P O
as O
the O
collection O
of O
all O
typed O
predicates O
, O
T O
as O
the O
collection O
of O
all O
argument O
types O
, O
and O
τ O
1 O
, O
τ O
2 O
: O
P O
→ O
T O
as O
type O
indicator O
functions O
, O
where O
τ O
1 O
( O
p O
) O
= O
t O
1 O
and O
τ O
2 O
( O
p O
) O
= O
t O
2 O
for O
any O
predicate O
p O
= O
( O
w O
1 O
.i O
1 O
, O
w O
2 O
.i O
2 O
, O
t O
1 O
, O
t O
2 O
) O
. O

We O
formally O
define O
that O
a O
typed O
entailment O
graph O
G O
( O
t O
1 O
, O
t O
2 O
) O
= O
< O
P O
( O
t O
1 O
, O
t O
2 O
) O
, O
E O
( O
t O
1 O
, O
t O
2 O
) O
> O
includes O
the O
collection O
of O
typed O
predicates O
P O
( O
t O
1 O
, O
t O
2 O
) O
= O
{ O
p| O
( O
τ O
1 O
( O
p O
) O
, O
τ O
2 O
( O
p O
) O
) O
∈ O
{ O
( O
t O
1 O
, O
t O
2 O
) O
, O
( O
t O
2 O
, O
t O
1 O
) O
} O
} O
, O
and O
the O
directional O
weighted O
edge O
set O
E O
( O
t O
1 O
, O
t O
2 O
) O
, O
which O
can O
be O
represented O
as O
an O
adjacent O
matrix O
W O
( O
t O
1 O
, O
t O
2 O
) O
∈ O
[ O
0 O
, O
1 O
] O
|P O
( O
t O
1 O
, O
t O
2 O
) O
|×|P O
( O
t O
1 O
, O
t O
2 O
) O
| O
. O
For O
those O
G O
( O
t O
1 O
, O
t O
2 O
) O
whose O
t O
1 O
̸ O
= O
t O
2 O
, O
the O
order O
of O
types O
t1 O
, O
t2 O
is O
naturally O
determined O
. O
When O
t O
1 O
= O
t O
2 O
= O
t O
, O
argument O
types O
are O
ordered O
such O
that O
G O
( O
t O
, O
t O
) O
can O
determine O
the O
order O
of O
types O
like O
" O
Thing O
A O
" O
and O
" O
Thing O
B O
" O
to O
distinguish O
predicates O
like O
" O
Thing O
A O
eat O
Thing O
B O
" O
and O
" O
Thing O
B O
eat O
Thing O
A O
" O
. O
This O
order O
obviously O
affect O
the O
meaning O
of O
predicates O
, O
as O
" O
Thing O
A O
eats O
Thing O
B O
" O
entails O
" O
Thing O
B O
is O
eaten O
by O
Thing O
A O
" O
, O
but O
" O
Thing O
eats O
Thing O
" O
is O
doubtful O
to O
entail O
" O
Thing O
is O
eaten O
by O
Thing O
" O
. O

Predicate O
Generation O

In O
order O
to O
avoid O
the O
predicate O
sparsity O
issue O
in O
a O
given O
corpus O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
uses O
a O
predicate O
generator O
G O
to O
generate O
novel O
in O
- O
domain O
predicates O
. O
G O
takes O
a O
set O
of O
seed O
predicates O
P O
seed O
⊂ O
P O
( O
t O
1 O
, O
t O
2 O
) O
as O
input O
and O
outputs O
a O
set O
of O
generated O
predicates O
P O
G O
, O
where O
P O
seed O
are O
expected O
to O
contain O
the O
domain O
knowledge O
of O
required O
EGs O
and O
P O
G O
should O
be O
semantically O
related O
to O
P O
seed O
in O
varying O
degrees O
. O

Our O
G O
is O
designed O
to O
be O
based O
on O
generative O
LMs O
, O
thus O
the O
input O
predicates O
p O
∈ O
P O
seed O
should O
be O
converted O
into O
natural O
language O
forms O
to O
fit O
in O
the O
LMs O
. O
We O
use O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( I-MethodName
2022 I-MethodName
) I-MethodName
's O
sentence O
generator O
S O
to O
convert O
predicate O
p O
into O
its O
corresponding O
sentence O
S O
( O
p O
) O
. O
For O
example O
, O
p O
= O
( O
elect.2 O
, O
elect.in.2 O
, O
government O
, O
time O
) O
will O
be O
converted O
into O
Government O
A O
is O
elected O
in O
Time O
B. O
With O
converted O
sentences O
, O
generator O
G O
uses O
a O
generative O
LM O
, O
T5 O
- O
large O
( O
Raffel O
et O
al O
. O
, O
2022 O
) O
in O
our O
experiments O
, O
to O
generate O
new O
sentences O
and O
then O
re O
- O
converts O
them O
into O
generated O
predicates O
by O
a O
sentence O
- O
predicate O
mapping O
function O
S O
−1 O
( O
details O
in O
Appendix O
C O
) O
. O
Starting O
from O
the O
seed O
sentences O
S O
0 O
= O
{ O
S O
( O
p O
) O
|p O
∈ O
P O
seed O
} O
, O
the O
generative O
LM O
outputs O
sentences O
S O
1 O
for O
the O
next O
step O
, O
and O
S O
1 O
is O
used O
to O
generate O
S O
2 O
and O
so O
on O
, O
while O
S O
−1 O
is O
used O
to O
re O
- O
convert O
S O
i O
to O
P O
i O
= O
S O
−1 O
( O
S O
i O
) O
for O
every O
step O
. O
The O
generation O
process O
continues O
until O
the O
union O
of O
seed O
predicates O
and O
generated O
predicates O

P O
′ O
i O
= O
P O
seed O
∪ O
P O
1 O
... O
∪ O
P O
i O
is O
equal O
to O
P O
′ O
i−1 O
or O
its O
size O
|P O
′ O
i O
| O
exceeds O
a O
pre O
- O
defined O
scale O
parameter O
K O
p O
. O

To O
use O
T5 O
- O
large O
as O
the O
generation O
component O
, O
we O
need O
to O
design O
an O
input O
template O
to O
generate O
new O
sentences O
. O
For O
sentence O
s O
∈ O
S O
i O
, O
the O
input O
template O
will O
be O
constructed O
like O
: O

s O
, O
which O
entails O
that O
t O
1 O
A O
< O
extra_id_0 O
> O
t O
2 O
B. O
s O
, O
which O
entails O
that O
t O
2 O
B O
< O
extra_id_0 O
> O
t O
1 O
A. O
where O
< O
extra_id_0 O
> O
is O
the O
special O
token O
representing O
the O
generating O
location O
of O
the O
T5 O
- O
large O
output O
. O
The O
max O
length O
of O
stripped O
output O
sequence O
s O
′ O
is O
limited O
to O
5 O
, O
and O
the O
new O
predicate O
p O
′ O
is O
produced O
by O
S O
−1 O
( O
" O
t O
1 O
A O
s O
′ O
t O
2 O
B. O
" O
) O
or O
S O
−1 O
( O
" O
t O
2 O
B O
s O
′ O
t O
1 O
A. O
" O
) O
correspondingly O
. O
For O
each O
s O
, O
T5 O
- O
large O
uses O
beam O
- O
search O
algorithm O
with O
beam O
size O
K O
beam O
to O
find O
top O
- O
K O
sent O
output O
sequences O
s O
′ O
with O
highest O
probabilities O
. O

To O
ensure O
the O
quality O
of O
generated O
predicates O
and O
filter O
noisy O
ones O
, O
only O
those O
predicates O
which O
are O
generated O
by O
T5 O
- O
large O
from O
at O
least O
two O
different O
predicates O
in O
P O
′ O
i−1 O
could O
be O
included O
in O
P O
i O
. O
Algorithm O
1 O
depicts O
how O
predicate O
generator O
G O
works O
( O
more O
details O
and O
exmaples O
in O
Appendix O
D O
) O
. O

Edge O
Selection O

After O
generating O
new O
predicates O
P O
( O
t O
1 O
, O
t O
2 O
) O
= O
P O
G O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
constructs O
G O
( O
t O
1 O
, O
t O
2 O
) O
by O
generating O
weighted O
edge O
set O
E O
( O
t O
1 O
, O
t O
2 O
) O
. O
As O
TP B-MethodName
- I-MethodName
EGG I-MethodName
does O
not O
use O
large O
corpora O
to O
calculate O
distributional O
features O
regarding O
context O
coherence O
, O
we O
need O
to O
determine O
which O
predicate O
pairs O
could O
be O
potential O
entailment O
relations O
for O
later O
calculation O
. O
Regarding O
ALL O
pairs O
as O
candidates O
is O
a O
simple O
solution O
, O
but O
when O
P O
( O
t O
1 O
, O
t O
2 O
) O
scales O
up O
, O
calculating O
all O
|P O
| O
2 O
pairs O
Algorithm O
1 O
The O
predicate O
generator O
G O
. O

Require O
: O
A O
set O
of O
seed O
predicates O
P O
seed O
, O
sentence O
generator O
S O
, O
parameter O
K O
beam O
, O
Ksent O
, O
Kp O
Ensure O
: O
A O
set O
of O
generated O
predicates O
PG O
1 O
: O

Ponce O
= O
{ O
} O
2 O
: O
i O
= O
0 O
, O
P0 O
= O
P O
′ O
0 O
= O
P O
seed O
3 O
: O
while O
|P O
′ O
i O
| O
≤ O
Kp O
do O
4 O
: O
Si O
= O
{ O
S O
( O
p O
) O
|p O
∈ O
Pi O
} O
5 O
: O
Pi+1 O
= O
{ O
} O
6 O
: O

for O
s O
∈ O
Si O
do O
7 O
: O

S O
g O
= O
T O
5 O
( O
s O
, O
K O
beam O
, O
Ksent O
) O
8 O
: O

P O
g O
= O
Set O
( O
S O
−1 O
( O
s O
g O
) O
|s O
g O
∈ O
S O
g O
) O
9 O
: O

P O
g O
= O
P O
g O
− O
P O
′ O
i O
10 O
: O

Pi+1.update O
( O
P O
g O
∩ O
Ponce O
) O
11 O
: O

Ponce O
= O
Ponce O
XOR O
Pg O
12 O
: O

end O
for O
13 O
: O

Pi+1 O
= O
Pi+1 O
− O
P O
′ O
i O
14 O
: O

P O
′ O
i+1 O
= O
P O
′ O
i O
∪ O
Pi+1 O
15 O
: O
if O
P O
′ O
i+1 O
= O
P O
′ O
i O
then O
16 O
: O
return O
PG O
= O
P O
′ O
i O
17 O
: O

end O
if O
18 O
: O

i O
= O
i O
+ O
1 O
19 O
: O
end O
while O
20 O
: O
return O
PG O
= O
P O
′ O
i O
will O
be O
unacceptably O
expensive O
as O
we O
intend O
to O
adopt O
an O
LM O
- O
based O
edge O
weight O
calculator O
, O
which O
only O
takes O
one O
pair O
as O
input O
at O
a O
time O
. O
Therefore O
, O
we O
require O
an O
effective O
edge O
selector O
M O
to O
select O
potential O
pairs O
E O
′ O
⊂ O
P O
( O
t O
1 O
, O
t O
2 O
) O
×P O
( O
t O
1 O
, O
t O
2 O
) O
with O
acceptable O
computational O
overhead O
, O
where O
|E O
′ O
| O
should O
be O
equal O
to O
a O
given O
parameter O
K O
edge O
. O

Calculating O
embeddings O
for O
each O
predicate O
and O
quickly O
getting O
similarities O
between O
all O
pairs O
in O
P O
( O
t O
1 O
, O
t O
2 O
) O
perform O
worse O
than O
pair O
- O
wise O
LMs O
with O
cross O
attention O
in O
general O
, O
but O
are O
good O
enough O
as O
the O
edge O
selector O
to O
maintain O
high O
- O
quality O
pairs O
in O
high O
ranking O
. O
Inspired O
by O
Ristoski O
et O
al O
. O
( O
2017 O
) O
, O
we O
represent O
predicate O
p O
as O
a O
sphere O
in O
the O
vector O
space O
. O
TP B-MethodName
- I-MethodName
EGG I-MethodName
uses O
BERT B-MethodName
- I-MethodName
base I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
to O
calculate O
embedding O
vector O
v O
p O
for O
every O
predicate O
p O
based O
on O
S O
( O
p O
) O
, O
and O
represents O
p O
as O
a O
sphere O
⊙ O
p O
in O
a O
vector O
space O
with O
center O
c O
p O
and O
radius O
r O
p O
: O

v O
p O
= O
BERT B-MethodName
( O
S O
( O
p O
) O
) O
∈ O
R O
dv O
, O
c O
p O
= O
f O
c O
( O
v O
p O
) O
∈ O
R O
dc O
, O
r O
p O
= O
f O
+ O
( O
f O
r O
( O
v O
p O
) O
) O
∈ O
R O
+ O
. O
( O
1 O
) O

where O
f O
c O
, O
f O
r O
are O
two O
- O
layer O
trainable O
neural O
networks O
, O
d O
v O
, O
d O
r O
are O
corresponding O
vector O
dimensions O
, O
f O
+ O
( O
x O
) O
∈ O
{ O
exp O
( O
x O
) O
, O
x O
2 O
} O
ensures O
the O
positive O
radius O
. O
By O
representing O
p O
as O
a O
sphere O
, O
we O
expect O
that O
when O
p O
entails O
q O
, O
⊙ O
q O
should O
enclose O
⊙ O
p O
, O
as O
all O
points O
in O
⊙ O
p O
are O
also O
included O
in O
⊙ O
q O
. O
Under O
such O
assumption O
, O
the O
transitivity O
referred O
in O
Section O
2 O
is O
naturally O
satisfied O
as O
⊙ O
a O
⊂ O
⊙ O
b O
⊂ O
⊙ O
c O
. O
The O
overlapping O
ratio O
between O
spheres O
can O
be O
seen O
as O
the O
entail O
- O
ment O
probability O
P O
r O
( O
p O
→ O
q O
) O
, O
and O
we O
simplify O
the O
calculation O
of O
sphere O
overlapping O
to O
diameter O
overlapping O
along O
the O
straight O
line O
between O
two O
centers O
: O

d O
pq O
= O
||c O
p O
− O
c O
q O
|| O
2 O
P O
r O
( O
p O
→ O
q O
) O
= O
 O
 O
 O
 O
 O
0 O
, O
r O
q O
≤ O
d O
pq O
− O
r O
p O
1 O
, O
r O
q O
≥ O
d O
pq O
+ O
r O
p O
rp+rq−dpq O
2rp O

, O
otherwise O

( O
2 O
) O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( I-MethodName
2022 I-MethodName
) I-MethodName
defines O
soft O
transitivity O
as O
P O
r O
( O
a O
→ O
b O
) O
P O
r O
( O
b O
→ O
c O
) O
≤ O
P O
r O
( O
a O
→ O
c O
) O
for O
all O
predicate O
pairs O
above O
a O
threshold O
. O
Similar O
in O
spirit O
, O
our O
simplified O
sphere O
- O
based O
probability O
holds O
transitivity O
in O
part O
: O

Theorem O
1 O
Given O
a O
threshold O
ϵ O
∈ O
( O
0 O
, O
1 O
) O
, O
∀a O
, O
b O
, O
c O
where O
P O
r O
( O
a O
→ O
b O
) O
> O
ϵ O
and O
P O
r O
( O
b O
→ O
c O
) O
> O
ϵ O
, O
we O
have O

P O
r O
( O
a O
→ O
c O
) O
> O
ϵ O
− O
( O
1 O
− O
ϵ O
) O
r O
b O

ra O
. O
We O
give O
its O
proof O
in O
Appendix O
A. O
Noted O
that O
while O
ϵ O
is O
close O
to O
1 O
, O
the O
right O
part O
ϵ O
− O
( O
1 O
− O
ϵ O
) O
r O
b O
ra O
will O
be O
nearly O
equal O
to O
ϵ. O
As O
we O
use O
this O
probability O
in O
edge O
selection O
, O
higher O
P O
r O
( O
a O
→ O
b O
) O
and O
P O
r O
( O
b O
→ O
c O
) O
will O
naturally O
ensure O
the O
appearance O
of O
( O
a O
, O
c O
) O
in O
final O
entailment O
relations O
, O
without O
the O
disturbance O
from O
low O
- O
confident O
edges O
. O
As O
P O
r O
( O
p O
→ O
q O
) O
is O
constant O
when O
r O
q O
≤ O
d O
pq O
− O
r O
p O
or O
r O
q O
≥ O
d O
pq O
+ O
r O
p O
, O
its O
gradient O
becomes O
zero O
which O
makes O
it O
untrainable O
. O
Therefore O
, O
we O
smooth O
it O
with O
order O
- O
preserving O
Sigmoid O
function O
and O
interpolation O
, O
and O
finally O
get O
the O
selected O
edge O
set O
for O
G O
( O
t O
1 O
, O
t O
2 O
) O
: O

M O
( O
p O
, O
q O
) O
= O
σ O
( O
2r O
q O
− O
2d O
pq O
r O
p O
) O
, O
E O
( O
t O
1 O
, O
t O
2 O
) O
= O
{ O
topK O
edge O
( O
M O
( O
p O
, O
q O
) O
) O
|p O
, O
q O
∈ O
V O
( O
t O
1 O
, O
t O
2 O
) O
} O
( O
3 O
) O

where O
σ O
is O
Sigmoid O
function O
σ O
( O
x O
) O
= O
1 O
/ O
( O
1 O
+ O
e O
x O
) O
. O
A O
geometrical O
illustration O
presenting O
how O
the O
selector O
M O
works O
can O
be O
found O
in O
Appendix O
B O
. O

Edge O
Weight O
Calculation O

With O
the O
selected O
edge O
set O
E O
( O
t O
1 O
, O
t O
2 O
) O
⊂ O
P O
( O
t O
1 O
, O
t O
2 O
) O
× O
P O
( O
t O
1 O
, O
t O
2 O
) O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
calculates O
the O
edge O
weight O
W O
p O
, O
q O
for O
each O
predicate O
pairs O
( O
p O
, O
q O
) O
individually O
in O
the O
adjacent O
matrix O
W O
( O
t O
1 O
, O
t O
2 O
) O
. O
Inspired O
by O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( I-MethodName
2022 I-MethodName
) I-MethodName
, O
as O
the O
distributional O
features O
of O
generated O
predicates O
are O
unavailable O
for O
TP B-MethodName
- I-MethodName
EGG I-MethodName
, O
we O
re O
- O
implement O
their O
local O
entailment O
calculator O
W O
to O
obtain O
the O
entailment O
edge O
weight O
W O
p O
, O
q O
. O
W O
is O
based O
on O
DeBERTa B-MethodName
( O
He O
et O
al O
. O
, O
2020 O
( O
He O
et O
al O
. O
, O
, O
2021a O
) O
and O
fine O
- O
tuned O
to O
adapt O
to O
the O
sentence O
patterns O
generated O
by O
S. O
The O
entailment O
- O
oriented O
LM O
will O
produce O
three O
scores O
, O
corresponding O
to O
entailment O
( O
E O
) O
, O
neutral O
( O
N O
) O
and O
contradiction O
( O
C O
) O
respectively O
, O
for O
each O
sentence O
pair O
. O
The O
score O
of O
entailment O
class O
is O
used O
as O
the O
entailment O
edge O
weight O
in O
our O
EGs O
: O

W O
p O
, O
q O
= O
W O
( O
p O
, O
q O
) O
= O
exp O
( O
LM O
( O
E|p O
, O
q O
) O
) O
r∈ O
{ O
E O
, O
N O
, O
C O
} O
exp O
( O
LM O
( O
r|p O
, O
q O
) O
) O
( O
4 O
) O

where O
LM O
( O
r|p O
, O
q O
) O
is O
the O
score O
of O
class O
r. O
After O
calculating O
all O
predicate O
pairs O
( O
p O
, O
q O
) O
∈ O
E O
( O
t O
1 O
, O
t O
2 O
) O
by O
the O
LM O
- O
based O
calculator O
W O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
completes O
the O
adjacent O
matrix O
W O
( O
t O
1 O
, O
t O
2 O
) O
, O
and O
consequently O
constructs O
G O
( O
t O
1 O
, O
t O
2 O
) O
, O
as O
shown O
in O
Figure O
2 O
. O

Experimental O
Setup O

Datasets O
. O
Following O
previous O
works O
( O
Hosseini O
et O
al O
. O
, O
2018 O
( O
Hosseini O
et O
al O
. O
, O
, O
2019Chen O
et O
al O
. O
, O
2022 O
) O
1 O
. O
More O
details O
can O
be O
found O
in O
Appendix O
F O
. O

Metrics O
. O
Following O
previous O
works O
, O
we O
evaluate O
TP B-MethodName
- I-MethodName
EGG I-MethodName
on O
the O
test O
datasets O
by O
calculating O
the O
area B-MetricName
under I-MetricName
the I-MetricName
curves I-MetricName
( O
AUC B-MetricName
) O
of O
Precision B-MetricName
- I-MetricName
Recall I-MetricName
Curve I-MetricName
( O
PRC B-MetricName
) O
for O
precision O
> O
0.5 O
and O
traditional O
ROC B-MetricName
curve I-MetricName
. O
2 O
The O
evaluated O
EGs O
are O
used O
to O
match O
the O
predicate O
pairs O
in O
datasets O
and O
return O
the O
entailment O
scores O
. O
Noted O
that O
our O
generated O
predicates O
might O
be O
semantically O
same O
with O
required O
ones O
but O
have O
different O
forms O
, O
like O
( O
use.2 O
, O
use.in.2 O
, O
thing O
, O
event O
) O
and O
( O
be.1 O
, O
be.used.in.2 O
, O
thing O
, O
event O
) O
are O
both O
reasonable O
for O
" O
Thing O
A O
is O
used O
in O
Event O
B O
" O
while O
our O
S O
−1 O
generates O
the O
first O
one O
. O
Hence O
we O
relax O
the O
predicate O
matching O
standard O
in O
evaluation O
from O
exactly O
matching O
to O
sentence O
matching O
, O
i.e. O
, O
S O
( O
p O
) O
= O
S O
( O
p O
′ O
) O
rather O
than O
p O
= O
p O
′ O
. O
This O
modification O
has O
nearly O
no O
effect O
on O
previous O
extraction O
- O
based O
EGs O
, O
but O
can O
better O
evaluate O
generative O
methods O
.. O
Implementation O
Details O
. O
In O
experiments O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
uses O
BERT O
- O
base O
in O
M O
and O
T5 O
- O
large O
in O
G O
implemented O
by O
the O
Hugging O
Face O
transformer O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
3 O
, O
and O
DeBERTa O
reimplementation O
from O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( I-MethodName
2022 I-MethodName
) I-MethodName
to O
finetune O
on O
MNLI B-DatasetName
and O
adapt O
to O
sentence O
pattern O
in O
W. O
Taking O
both O
EG O
performance O
and O
computational O
overhead O
into O
account O
, O
we O
set O
K B-HyperparameterName
p I-HyperparameterName
= O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
3 I-HyperparameterValue
, O
K B-HyperparameterName
edge I-HyperparameterName
= O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
7 I-HyperparameterValue
, O
K B-HyperparameterName
beam I-HyperparameterName
= O
50 B-HyperparameterValue
, O
K B-HyperparameterName
sent I-HyperparameterName
= O
50 B-HyperparameterValue
, O
d B-HyperparameterName
r I-HyperparameterName
= O
16 B-HyperparameterValue
, O
d B-HyperparameterName
v I-HyperparameterName
= O
768 B-HyperparameterValue
. O
Discussion O
about O
K B-HyperparameterName
p I-HyperparameterName
and O
K B-HyperparameterName
edge I-HyperparameterName
can O
be O
found O
in O
Appendix O
E O
. O

For O
EG O
generation O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
uses O
the O
predicates O
in O
validation O
set O
of O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
- I-DatasetName
r I-DatasetName
and O
SherLIiC B-DatasetName
Dataset O
respectively O
as O
the O
seed O
predicate O
P O
seed O
. O
With O
different O
P O
seed O
, O
we O
also O
only O
use O
corresponding O
validation O
set O
as O
the O
training O
data O
for O
all O
later O
modules O
to O
keep O
the O
EGs O
in O
- O
domain O
, O
called O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
and O
TP B-MethodName
- I-MethodName
EGG I-MethodName
SherLIiC I-MethodName
respectively O
. O

Only O
positive O
pairs O
are O
used O
to O
generate O
the O
training O
inputs O
and O
outputs O
to O
fine O
- O
tune O
T5 O
- O
large O
in O
the O
predicate O
generator O
G O
with O
learning B-HyperparameterName
rate I-HyperparameterName
α B-HyperparameterName
G I-HyperparameterName
= O
10 B-HyperparameterValue
−3 I-HyperparameterValue
. O
We O
use O
f O
+ O
( O
x O
) O
= O
exp O
( O
x O
) O
for O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
and O
f O
+ O
( O
x O
) O
= O
x O
2 O
for O
TP B-MethodName
- I-MethodName
EGG I-MethodName
SherLIiC I-MethodName
. O
The O
edge O
selector O
M O
is O
also O
trained O
by O
the O
validation O
predicate O
pairs O
, O
but O
the O
positive O
examples O
are O
repeat O
5 O
times O
( O
for O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
- I-DatasetName
r I-DatasetName
) O
or O
2 O
times O
( O
for O
SherLIiC B-DatasetName
) O
to O
alleviate O
the O
label O
imbalance O
in O
training O
. O
BERT O
- O
base O
parameters O
are O
trained O
with O
learning O
rate O
α B-HyperparameterName
M,1 I-HyperparameterName
= O
10 B-HyperparameterValue
−5 I-HyperparameterValue
, O
while O
other O
parameters O
, O
including O
f O
c O
and O
f O
r O
, O
are O
trained O
with O
learning O
rate O
α B-HyperparameterName
M,2 I-HyperparameterName
= O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
. O
The O
edge O
weight O
calculator O
W O
is O
trained O
by O
the O
same O
method O
in O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( I-MethodName
2022 I-MethodName
) I-MethodName
. O

All O
modules O
are O
trained O
by O
AdamW O
optimizer B-HyperparameterName
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
with O
cross O
entropy O
loss O
function O
, O
and O
controlled O
by O
early O
- O
stop O
mechanism O
, O
which O
stops O
the O
training O
when O
performances O
( O
loss O
for O
G O
and O
F O
1 O
for O
others O
) O
on O
validation O
set O
do O
not O
reach O
the O
highest O
in O
the O
last O
10 O
epoches O
. O
It O
takes O
about O
5 O
- O
6 O
hours O
to O
train O
all O
modules O
in O
TP B-MethodName
- I-MethodName
EGG I-MethodName
, O
and O
about O
2 O
- O
3 O
hours O
to O
generate O
a O
typed O
EG O
on O
GeForce O
RTX O
3090 O
. O
The O
three O
modules O
, O
G O
, O
M O
and O
W O
, O
contain O
738 O
M O
, O
109 O
M O
and O
139 O
M O
parameters O
respectively O
. O

To O
be O
comparable O
with O
previous O
works O
( O
Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2018 I-MethodName
) O
, O
we O
apply O
their O
lemma O
- O
based O
heuristic O
on O
all O
datasets O
except O
SherLIiC B-DatasetName
, O
and O
their O
average O
backup O
strategy O
on O
all O
datasets O
. O

Compared O
Methods O
We O
compare O
TP B-MethodName
- I-MethodName
EGG I-MethodName
with O
the O
best O
local O
distributional O
feature O
, O
Balanced O
Inclusion O
or O
called O
BInc B-MethodName
( O
Szpektor O
and O
Dagan O
, O
2008 O
) O
, O
and O
existing O
state O
- O
of O
- O
the O
- O
art O
local O
and O
global O
EG O
construction O
methods O
, O
including O
Hosseini O
et O
al O
. O
( O
2018Hosseini B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
( I-MethodName
, I-MethodName
2019 I-MethodName
, O
CNCE B-MethodName
and O
EGT2 B-MethodName
( O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2022 I-MethodName
) O
. O
Downstream O
Task O
. O
Despite O
of O
evaluating O
on O
EG O
construction O
benchmarks O
, O
we O
adapt O
an O
LM O
- O
based O
three O
- O
way O
RTE B-TaskName
framework O
into O
the O
EG O
evaluation O
testbed O
. O
For O
premise O
pm O
and O
hypothesis O
h O
, O
RTE B-TaskName
models O
take O
their O
concatenation O
[ O
pm O
; O
h O
] O
as O
inputs O
, O
and O
return O
three O
probability O
scores O
of O
three O
classes O
. O
In O
order O
to O
incorporate O
the O
knowledge O
in O
EGs O
into O
RTE B-TaskName
models O
, O
we O
design O
the O
following O
architecture O
available O
to O
any O
LM O
- O
based O
RTE B-TaskName
model O
: O
given O
pm O
and O
h O
, O
we O
extract O
binary O
predicates O
from O
them O
, O
and O
try O
to O
match O
the O
predicates O
in O
our O
EGs O
. O
Each O
matched O
predicates O
a O
in O
premise O
pm O
will O
be O
replaced O
by O
its O
K O
nbr O
neighbors O
b O
with O
highest O
weight O
W O
ab O
. O
For O
h O
, O
the O
neighbors O
b O
are O
with O
highest O
weight O
W O
ba O
. O
Replaced O
sentences O
pm O
1 O
, O
... O
, O
pm O
j O
and O
h O
1 O
, O
... O
, O
h O
k O
for O
pm O
and O
h O
will O
be O
concatenated O
to O
represent O
the O
information O
from O
EGs O
in O
calculation O
: O

( O
s O
E1 O
, O
s O
N1 O
, O
s O
C1 O
) O
= O
Softmax O
( O
LM O
1 O
( O
[ O
pm O
; O
h O
] O
) O
) O
, O

( O
s O
E2 O
, O
s O
N2 O
, O
s O
C2 O
) O
= O
Softmax O
( O
LM O
2 O
( O
[ O
pm O
; O
pm O
1 O
; O
... O
; O
pm O
j O
; O
h O
; O
h O
1 O
; O
... O
; O
h O
k O
] O
) O
) O
, O

s O
i O
= O
( O
s O
i1 O
+ O
s O
i2 O
) O
/ O
2 O
, O
i O
∈ O
{ O
E O
, O
N O
, O
C O
} O
. O
( O
5 O
) O

where O
LM O
1 O
and O
LM O
2 O
represent O
two O
different O
LMs O
followed O
by O
a O
linear O
layer O
respectively O
. O
As O
the O
additional O
calculation O
unfairly O
requires O
more O
parameters O
, O
we O
also O
consider O
the O
models O
with O
equal O
parameters O
but O
do O
not O
use O
the O
EGs O
, O
referred O
as O
NO O
- O
EG O
setting O
, O
by O
inputting O
[ O
pm O
; O
h O
] O
into O
LM O
2 O
directly O
. O
We O
use O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
SciTail B-DatasetName
( O
Khot O
et O
al O
. O
, O
2018 O
) O
as O
our O
RTE B-TaskName
benchmark O
datasets O
. O
We O
use O
BERT B-MethodName
- I-MethodName
base I-MethodName
and O
DeBERTa B-MethodName
- I-MethodName
base I-MethodName
as O
the O
backbone O
, O
learning O
rate O
α B-HyperparameterName
RT I-HyperparameterName
E I-HyperparameterName
= O
10 B-HyperparameterValue
−5 I-HyperparameterValue
, O
K B-HyperparameterName
nbr I-HyperparameterName
= O
5 B-HyperparameterValue
for O
SNLI B-DatasetName
and O
K B-HyperparameterName
nbr I-HyperparameterName
= O
3 B-HyperparameterValue
for O
SciTail B-DatasetName
. O

Results O
and O
Analysis O

Main O
Results O

The O
performance O
of O
different O
EGs O
on O
benchmark O
datasets O
are O
shown O
in O
using O
extracted O
features O
from O
large O
corpora O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
achieves O
significant O
improvement O
or O
at O
least O
reaches O
comparable O
performance O
with O
baselines O
for O
in O
- O
domain O
evaluations O
( O
L B-DatasetName
/ I-DatasetName
H I-DatasetName
and O
L B-DatasetName
/ I-DatasetName
H I-DatasetName
- I-DatasetName
r I-DatasetName
for O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
and O
SherLIiC B-DatasetName
for O
TP B-MethodName
- I-MethodName
EGG I-MethodName
SherLIiC I-MethodName
) O
. O
Interestingly O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
always O
performs O
better O
on O
the O
AUC B-MetricName
of I-MetricName
PRC I-MetricName
, O
which O
indicates O
the O
strong O
ability O
of O
our O
generative O
methods O
to O
maintain O
impressive O
recall O
with O
high O
precision O
as O
shown O
in O
the O
curves O
. O
On O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
- I-DatasetName
r I-DatasetName
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
significantly O
outperforms O
all O
other O
extraction O
- O
based O
methods O
on O
pre B-MetricName
- I-MetricName
cision I-MetricName
> I-MetricName
0.5 I-MetricName
, O
showing O
that O
with O
higher O
classification O
threshold O
, O
extraction O
- O
based O
methods O
fail O
to O
detect O
the O
entailment O
relations O
between O
rare O
predicates O
due O
to O
the O
sparsity O
issues O
, O
while O
generation O
- O
based O
TP B-MethodName
- I-MethodName
EGG I-MethodName
successfully O
finds O
these O
relations O
by O
generating O
more O
predicates O
and O
correctly O
assigns O
high O
probabilities O
between O
them O
. O
Noted O
that O
our O
TP B-MethodName
- I-MethodName
EGG I-MethodName
is O
a O
local O
method O
, O
although O
certain O
global O
properties O
are O
ensured O
by O
our O
edge O
selector O
M. O
We O
try O
to O
apply O
a O
state O
- O
of O
- O
the O
- O
art O
global O
method O
, O
EGT2 B-MethodName
- I-MethodName
L I-MethodName
1 I-MethodName
( O
Chen B-MethodName
et I-MethodName
al I-MethodName
. I-MethodName
, I-MethodName
2022 I-MethodName
) O
on O
our O
local O
EGs O
4 O
. O
As O
shown O
in O
the O
bottom O
of O
Table O
2 O
, O
the O
global O
method O
further O
improves O
the O
performance O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
, O
demonstrating O
the O
potential O
of O
our O
local O
EGs O
to O
continuously O
reducing O
the O
data O
sparsity O
with O
global O
EG O
learning O
methods O
. O

Although O
we O
have O
observed O
the O
significant O
improvement O
of O
evaluation O
metrics O
by O
TP B-MethodName
- I-MethodName
EGG I-MethodName
, O
it O
is O
not O
clear O
enough O
to O
determine O
TP B-MethodName
- I-MethodName
EGG I-MethodName
can O
alleviate O
the O
predicate O
sparsity O
to O
what O
extent O
. O
Therefore O
, O
we O
count O
the O
predicate O
pairs O
in O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
testset O
that O
exactly O
appeared O
as O
edges O
in O
EGs O
. O
We O
find O
that O
6,873 O
pairs O
appear O
in O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
, O
meanwhile O
875 O
in O
EGT2 B-MethodName
- I-MethodName
L I-MethodName
3 I-MethodName
. O
The O
far O
more O
appearance O
of O
in O
- O
domain O
predicates O
indicates O
the O
alleviation O
of O
predicate O
sparsity O
. O
Previous O
works O
have O
claimed O
that O
LMs O
for O
entailments O
might O
be O
strong O
in O
undirectional O
paraphrasing O
, O
but O
weak O
in O
directional O
entailment O
recognizing O
( O
Cabezudo O
et O
al O
. O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2022 O
True O
, O
and O
therefore O
symmetric O
models O
will O
have O
AUC B-MetricName
< O
0.5 B-MetricValue
. O
TP B-MethodName
- I-MethodName
EGG I-MethodName
performs O
better O
than O
baselines O
on O
the O
directional O
portion O
, O
and O
the O
AUC B-MetricName
far O
higher O
than O
0.5 B-MetricValue
indicates O
its O
directional O
entailment O
ability O
. O
Global O
models O
perform O
better O
here O
, O
which O
is O
reasonable O
as O
global O
constraints O
are O
strongly O
related O
to O
the O
directional O
reasoning O
. O

Learning O
with O
Multiple O
Domains O

Although O
TP B-MethodName
- I-MethodName
EGG I-MethodName
performs O
well O
on O
in O
- O
domain O
evaluation O
, O
the O
out O
- O
domain O
scenario O
is O
still O
hard O
, O
as O
the O
knowledge O
required O
for O
out O
- O
domain O
evaluation O
is O
inaccessible O
in O
all O
training O
and O
generation O
steps O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
. O
Next O
, O
we O
study O
the O
effect O
of O
using O
merged O
validation O
sets O
of O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
- I-DatasetName
r I-DatasetName
and O
SherLIiC B-DatasetName
Dataset O
at O
different O
modules O
. O
The O
performance O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
trained O
with O
the O
merged O
data O
, O
referred O
as O
L+S O
, O
are O
shown O
in O
Table O
5 O
. O
While O
using O
merged O
data O
as O
P O
seed O
and O
also O
as O
training O
data O
for O
other O
modules O
( O
❶ O
) O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
reaches O
impressive O
performances O
on O
both O
datasets O
, O
which O
is O
not O
surprising O
, O
as O
both O
datasets O
are O
in O
- O
domain O
in O
this O
situation O
. O

Using O
merged O
dataset O
to O
train O
G O
, O
M O
and O
W O
boosts O
out O
- O
domain O
performance O
with O
in O
- O
domain O
performance O
loss O
( O
comparing O
❷ O
and O
❹ O
, O
❺ O
and O
❼ O
) O
. O
However O
, O
adding O
some O
out O
- O
domain O
predicates O
into O
P O
seed O
is O
surprisingly O
beneficial O
to O
the O
in O
- O
domain O
evaluation O
while O
improving O
out O
- O
domain O
generalization O
( O
comparing O
❷ O
and O
❸ O
, O
❺ O
and O
❻ O
) O
. O
We O
attribute O
it O
to O
the O
diversity O
of O
generated O
predicates O
led O
by O
the O
newly O
incorporated O
seed O
predicates O
, O
which O
might O
not O
be O
generated O
with O
the O
in O
- O
domain O
seed O
predicates O
. O
The O
out O
- O
domain O
predicates O
help O
TP B-MethodName
- I-MethodName
EGG I-MethodName
to O
find O
new O
predicates O
related O
to O
in O
- O
domain O
predicates O
as O
Algorithm O
1 O
might O
tend O
to O
generate O
predicates O
from O
at O
least O
two O
predicates O
across O
two O
domains O
. O
Therefore O
, O
the O
predicate O
coverage O
over O
evaluation O
datasets O
can O
be O
increased O
. O

Results O
on O
RTE B-TaskName

In O
downstream O
task O
evaluation O
, O
we O
use O
EGs O
generated O
by O
different O
methods O
to O
enhance O
LM O
- O
based O
RTE B-TaskName
models O
, O
and O
report O
the O
results O
in O
Table O
6 O
. O
Compared O
with O
CNCE B-MethodName
and O
EGT2 B-MethodName
, O
our O
TP B-MethodName
- I-MethodName
EGG I-MethodName
achieves O
better O
performance O
on O
two O
RTE B-TaskName
datasets O
with O
both O
BERT B-MethodName
base I-MethodName
and O
DeBERTa B-MethodName
base I-MethodName
backbones O
. O
The O
performances O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
on I-MethodName
DeBERTa I-MethodName
base I-MethodName
are O
significantly O
better O
than O
NO B-MethodName
- I-MethodName
EG I-MethodName
( O
p B-MetricName
< O
0.05 B-MetricValue
) O
. O
Noted O
that O
TP B-MethodName
- I-MethodName
EGG I-MethodName
offers O
pm O
j O
, O
h O
k O
for O
4,600 O
sentences O
in O
SNLI B-DatasetName
testset O
, O
which O
is O
5,596 O
for O
EGT2 B-MethodName
- I-MethodName
L I-MethodName
3 I-MethodName
. O
Even O
with O
lower O
coverage O
over O
predicates O
in O
the O
dataset O
, O
TP B-MethodName
- I-MethodName
EGG I-MethodName
supports O
RTE B-TaskName
models O
with O
more O
highquality O
entailment O
relations O
to O
generate O
pm O
j O
, O
h O
k O
and O
improve O
the O
performance O
. O
On O
the O
other O
hand O
, O
the O
noisy O
entailment O
relations O
in O
CNCE B-MethodName
and O
EGT2 B-MethodName
perhaps O
misguide O
RTE B-TaskName
models O
, O
thus O
lead O
to O
even O
worse O
results O
than O
NO B-MethodName
- I-MethodName
EG I-MethodName
in O
some O
cases O
. O

Ablation O
Study O

We O
run O
the O
ablation O
experiments O
which O
directly O
use O
the O
original O
version O
of O
LMs O
in O
G O
, O
M O
and O
W O
without O
fine O
- O
tuning O
on O
EG O
benchmark O
datasets O
. O
For O
M O
, O
as O
non O
- O
LM O
parameters O
are O
involved O
, O
we O
replace O
it O
with O
randomly O
selecting O
K O
edge O
edges O
. O
As O
shown O
in O
Table O
7 O
, O
without B-MethodName
fine I-MethodName
- I-MethodName
tuning I-MethodName
G I-MethodName
or I-MethodName
W I-MethodName
, O
the O
performance B-MetricName
on O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
- I-DatasetName
r I-DatasetName
suffers O
a O
significant O
drop O
( O
about O
0.1 B-MetricValue
) O
, O
indicating O
the O
importance O
of O
fine O
- O
tuned O
modules O
for O
EG O
generation O
. O
The O
performance O
on O
SherLIiC B-DatasetName
also O
decreases O
severely O
without B-MethodName
fine I-MethodName
- I-MethodName
tuning I-MethodName
G I-MethodName
, O
as O
the O
fine O
- O
tuning O
step O
can O
improve O
the O
quality O
of O
generated O
predicates O
and O
cover O

Limitations O

First O
, O
as O
we O
do O
not O
rely O
on O
specific O
corpora O
and O
avoid O
the O
shortcomings O
of O
extractive O
methods O
, O
we O
also O
lose O
their O
advantages O
. O
The O
typed O
EGs O
generated O
by O
our O
TP B-MethodName
- I-MethodName
EGG I-MethodName
is O
strongly O
related O
to O
the O
seed O
predicates O
and O
training O
data O
of O
generation O
modules O
, O
while O
extractive O
EGs O
can O
generate O
domainindependent O
EGs O
from O
large O
corpora O
and O
do O
not O
require O
supervised O
training O
data O
to O
a O
considerable O
degree O
. O
Second O
, O
the O
edge O
calculator O
W O
is O
time O
- O
consuming O
even O
we O
can O
control O
the O
scales O
of O
output O
EGs O
, O
as O
the O
edge O
num O
|E O
( O
t O
1 O
, O
t O
2 O
) O
| O
will O
be O
relatively O
large O
for O
TP B-MethodName
- I-MethodName
EGG I-MethodName
to O
generate O
powerful O
EGs O
. O
Furthermore O
, O
how O
to O
effectively O
select O
seed O
predicates O
still O
remains O
a O
difficult O
problem O
which O
has O
not O
been O
discussed O
thoroughly O
in O
this O
work O
by O
using O
the O
validation O
datasets O
. O
We O
assume O
that O
this O
problem O
could O
be O
solved O
by O
carefully O
confirming O
how O
the O
seed O
predicates O
represent O
corresponding O
domain O
knowledge O
and O
we O
leave O
it O
to O
future O
work O
. O

Ethics O
Statement O

We O
re O
- O
annotate O
the O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
Dataset O
which O
is O
a O
publicly O
available O
dataset O
for O
entailment O
graph O
evaluation O
. O
Annotators O
receive O
a O
competitive O
pay O
of O
about O
100 O
yuan O
per O
hour O
under O
the O
agreement O
of O
the O
institute O
, O
which O
is O
more O
than O
4 O
times O
the O
local O
minimum O
wage O
. O
The O
annotation O
complies O
with O
the O
ACL O
Code O
of O
Ethics O
. O
The O
sentences O
used O
in O
annotation O
are O
generated O
from O
the O
original O
dataset O
and O
we O
do O
not O
incorporate O
external O
content O
into O
the O
sentences O
. O
However O
, O
there O
may O
still O
be O
sentences O
containing O
potentially O
improper O
content O
, O
which O
do O
not O
reflect O
the O
views O
or O
stances O
of O
the O
authors O
. O
The O
re O
- O
annotation O
results O
are O
confirmed O
by O
the O
majority O
voting O
of O
annotators O
, O
and O
may O
still O
contain O
natural O
errors O
. O
Further O
usage O
of O
the O
re O
- O
annotated O
dataset O
should O
be O
aware O
of O
the O
limitation O
and O
the O
authors O
are O
not O
responsible O
for O
any O
issues O
in O
further O
usage O
of O
this O
dataset O
. O
A O
The O
Proof O
of O
Theorem O
1 O

Theorem O
1 O
Given O
a O
threshold O
ϵ O
∈ O
( O
0 O
, O
1 O
) O
, O
∀a O
, O
b O
, O
c O
where O
P O
r O
( O
a O
→ O
b O
) O
> O
ϵ O
and O
P O
r O
( O
b O
→ O
c O
) O
> O
ϵ O
, O
we O
have O
P O
r O
( O
a O
→ O
c O
) O
> O
ϵ O
− O
( O
1 O
− O
ϵ O
) O
r O
b O
ra O
. O

As O
P O
r O
( O
p O
→ O
q O
) O
= O
rp+rq−dpq O
2rp O

holds O
when O
d O
pq O
− O
r O
p O
< O
r O
q O
< O
d O
pq O
+ O
r O
p O
, O
and O
P O
r O
( O
p O
→ O
q O
) O
= O
1 O
≤ O
rp+rq−dpq O
2rp O
holds O
when O
r O
q O
≥ O
d O
pq O
+ O
r O
p O
, O
we O
have O
: O

r O
a O
+ O
r O
b O
− O
d O
ab O
2r O
a O
≥ O
P O
r O
( O
a O
→ O
b O
) O
> O
ϵ O
→ O
d O
ab O
< O
r O
b O
+ O
( O
1 O
− O
2ϵ O
) O
r O
a O
. O
( O
6 O
) O

Similarly O
, O
for O
b O
, O
c O
: O

d O
bc O
< O
r O
c O
+ O
( O
1 O
− O
2ϵ O
) O
r O
b O
. O
( O
7 O
) O

For O
the O
case O
P O
r O
( O
a O
→ O
c O
) O
= O
1 O
, O
obviously O
the O
theorem O
holds O
for O
ϵ O
∈ O
( O
0 O
, O
1 O
) O
; O

For O
the O
case O
P O
r O
( O
a O
→ O
c O
) O
= O
0 O
or O
P O
r O
( O
a O
→ O
c O
) O
= O
rp+rq−dpq O
2rp O

, O
we O
have O
P O
r O
( O
a O
→ O
c O
) O
≥ O
rp+rq−dpq O
2rp O
as O
r O
p O
+ O
r O
q O
− O
d O
pq O
< O
0 O
under O
P O
r O
( O
a O
→ O
c O
) O
= O
0 O
, O
and O
therefore O
: O

P O
r O
( O
a O
→ O
b O
) O
≥ O
r O
a O
+ O
r O
c O
− O
d O
ac O
2r O
a O
≥ O
r O
a O
+ O
r O
c O
− O
( O
d O
ab O
+ O
d O
bc O
) O
2r O
a O
( O
d O
ac O
≤ O
d O
ab O
+ O
d O
bc O
) O
> O
r O
a O
+ O
r O
c O
− O
( O
r O
b O
+ O
( O
1 O
− O
2ϵ O
) O
r O
a O
+ O
r O
c O
+ O
( O
1 O
− O
2ϵ O
) O
r O
b O
) O
2r O
a O
= O
ϵr O
a O
+ O
( O
ϵ O
− O
1 O
) O
r O
b O
r O
a O
= O
ϵ O
+ O
( O
ϵ O
− O
1 O
) O
r O
b O
r O
a O
. O
( O
8 O
) O
Q.E.D O
. O

B O
Geometrical O
Illustration O
of O
Edge O

Selector O
M O

To O
understand O
how O
the O
edge O
selector O
M O
works O
more O
intuitively O
, O
we O
pick O
four O
predicate O
sentences O
from O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
Dataset O
and O
visualize O
their O
corresponding O
spheres O
⊙ O
p O
in O
Figure O
4 O
: O
p O
0 O
: O
Living O
Thing O
A O
is O
imported O
from O
Location O
B. O
p O
1 O
: O
Living O
Thing O
A O
is O
native O
to O
Location O
B. O
p O
2 O
: O
Living O
Thing O
A O
is O
found O
in O
Location O
B. O
p O
3 O
: O
Living O
Thing O
A O
is O
concentrated O
in O
Location O
B O
. O

The O
centers O
c O
p O
and O
radius O
r O
p O
are O
generated O
by O
M O
from O
our O
final O
TP B-MethodName
- I-MethodName
EGG I-MethodName
model O
, O
while O
the O
dimension O
of O
c O
p O
are O
reduced O
to O
maintain O
the O
distances O
between O
them O
. O
Three O
entailment O
relations O
, O
p O
0 O
→ O
p O
1 O
, O
p O
1 O
→ O
p O
2 O
and O
p O
3 O
→ O
p O
2 O
, O
are O
annotated O
in O
the O
dataset O
, O
and O
p O
0 O
→ O
p O
3 O
is O
also O
plausible O
. O
In O
Figure O
4 O
, O
the O
hypothesis O
spheres O
obviously O
enclose O
premise O
spheres O
, O
and O
the O
more O
generic O
a O
predicate O
is O
, O
the O
bigger O
its O
sphere O
becomes O
, O
which O
is O
consistent O
with O
our O
expectation O
about O
M. O
With O
high O
directional O
overlapping O
, O
all O
of O
the O
four O
entailment O
relations O
will O
correctly O
appear O
in O
later O
weight O
calculation O
while O
low O
- O
confident O
inverse O
edges O
will O
be O
filtered O
out O
. O

C O
The O
Sentence O
- O
Predicate O
Mapping O
Function O
S O
−1 O

The O
sentence O
- O
predicate O
mapping O
function O
S O
−1 O
used O
in O
predicate O
generation O
is O
described O
in O
Algorithm O
2 O
. O
Noted O
that O
S O
−1 O
is O
a O
simplified O
approximation O
of O
the O
reverse O
function O
of O
sentence O
generator O
S O
while O
different O
predicates O
might O
generate O
the O
same O
sentence O
by O
S. O
Therefore O
, O
S O
−1 O
does O
not O
cover O
all O
possible O
predicates O
and O
sentences O
. O
appear O
in O
final O
predicate O
set O
P O
′ O
2 O
. O

E O
Discussion O
about O
Graph O
Scales O

As O
referred O
in O
Section O
4 O
, O
we O
set O
the O
number O
of O
predicates O
K B-HyperparameterName
p I-HyperparameterName
= O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
3 I-HyperparameterValue
and O
edges O
K B-HyperparameterName
edge I-HyperparameterName
= O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
7 I-HyperparameterValue
, O
which O
determine O
the O
final O
scale O
of O
generated O
EGs O
. O
We O
report O
the O
performance O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
on O
the O
evaluation O
datasets O
with O
different O
K B-HyperparameterName
p I-HyperparameterName
and O
K B-HyperparameterName
edge I-HyperparameterName
in O
Figure O
5 O
. O
Changing O
K B-HyperparameterName
p I-HyperparameterName
from O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
3 I-HyperparameterValue
to O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
4 I-HyperparameterValue
, O
the O
overall O
performance O
is O
the O
best O
while O
K B-HyperparameterName
p I-HyperparameterName
= O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
3 I-HyperparameterValue
. O
We O
assume O
that O
lower O
K B-HyperparameterName
p I-HyperparameterName
might O
limit O
the O
coverage O
of O
predicate O
set O
, O
while O
higher O
K B-HyperparameterName
p I-HyperparameterName
makes O
the O
EGs O
more O
sparse O
and O
miss O
potential O
entailment O
relations O
. O
Noted O
that O
the O
computational O
overhead O
and O
space O
occupation O
is O
almost O
proportional O
to O
K B-HyperparameterName
edge I-HyperparameterName
, O
setting O
K B-HyperparameterName
edge I-HyperparameterName
= O
+ B-HyperparameterValue
∞ I-HyperparameterValue
to O
regard O
ALL O
pairs O
as O
candidates O
is O
impractical O
( O
the O
largest O
EG O
in O
TP B-MethodName
- I-MethodName
EGG I-MethodName
L I-MethodName
/ I-MethodName
H−r I-MethodName
will O
contain O
7 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
7 I-HyperparameterValue
edges O
) O
. O
We O
find O
that O
K B-HyperparameterName
edge I-HyperparameterName
= O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
7 I-HyperparameterValue
is O
able O
to O
reach O
the O
overall O
performances O
comparable O
with O
K B-HyperparameterName
edge I-HyperparameterName
= O
+ B-HyperparameterValue
∞ I-HyperparameterValue
under O
our O
settings O
, O
while O
further O
decreasing O
K O
edge O
will O
significantly O
cut O
down O
the O
performances O
. O
To O
balance O
between O
the O
overall O
performance O
and O
computational O
overhead O
, O
we O
finally O
set O
K B-HyperparameterName
p I-HyperparameterName
= O
5 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
3 I-HyperparameterValue
and O
K B-HyperparameterName
edge I-HyperparameterName
= O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
7 I-HyperparameterValue
. O

F O
Details O
about O
Datasets O

Levy O
and O
Dagan O
( O
2016 O
) O
uses O
questions O
and O
candidate O
answers O
with O
textual O
predicates O
to O
collect O
the O
entailment O
relations O
, O
and O
proposes O
a O
widely O
used O
EG O
evaluation O
dataset O
which O
is O
later O
re O
- O
annotated O
by O
Holt O
( O
2018 O
) O
, O
called O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
Dataset O
. O
For O
example O
, O
if O
the O
annotator O
figures O
out O
that O
" O
The O
government O
is O
adored O
by O
natives O
" O
can O
be O
used O
to O
answer O
" O
Who O
recognize O
the O
government O
? O
" O
, O
the O
dataset O
will O
indicate O
that O
" O
adore O
" O
entails O
" O
recognize O
" O
between O
type O
person O
and O
government O
. O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
Dataset O
contains O
18,407 O
predicate O
pairs O
( O
14,491 O
negative O
and O
3,916 O
positive O
) O
. O
We O
use O
the O
30 O
% O
/ O
70 O
% O
splitting O
of O
validation O
/ O
test O
set O
as O
Hosseini O
et O
al O
. O
( O
2018 O
) O
in O
our O
experiments O
. O
However O
, O
because O
the O
QA O
annotation O
form O
incorporates O
additional O
information O
about O
entities O
related O
to O
the O
predicates O
, O
some O
consistent O
predicates O
pairs O
are O
annotated O
with O
different O
labels O
, O
and O
the O
transitivity O
is O
disobeyed O
between O
some O
predicate O
pairs O
. O
The O
inconsistent O
pairs O
are O
those O
( O
a O
, O
b O
) O
which O
( O
a O
, O
b O
, O
T O
rue O
) O
and O
( O
a O
, O
b O
, O
F O
alse O
) O
both O
appear O
in O
the O
dataset O
. O
The O
transitivity O
- O
disobeying O
pairs O
are O
those O
( O
a O
, O
b O
) O
, O
( O
b O
, O
c O
) O
and O
( O
a O
, O
c O
) O
which O
( O
a O
, O
b O
, O
T O
rue O
) O
, O
( O
b O
, O
c O
, O
T O
rue O
) O
and O
( O
a O
, O
c O
, O
F O
alse O
) O
all O
appear O
. O
We O
find O
that O
there O
are O
89 O
inconsistent O
pairs O
and O
159 O
transitivity O
- O
disobeying O
pairs O
in O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
Dataset O
, O
and O
re O
- O
annotate O
these O
248 O
pairs O
by O
five O
annotators O
with O
Fleiss O
' O
κ O
= O
0.43 O
. O
After O
re O
- O
annotating O
, O
we O
get O
the O
final O
Levy B-DatasetName
/ I-DatasetName
Holt I-DatasetName
- I-DatasetName
r I-DatasetName
Dataset O
with O
14,490 O
negative O
and O
3,777 O
positive O
pairs O
. O
Berant O
et O
al O
. O
( O
2011 O
) O
proposes O
an O
annotated O
entailment O
relation O
dataset O
, O
containing O
3,427 O
positive O
and O
35,585 O
negative O
examples O
, O
called O
Berant B-DatasetName
Dataset O
. O
Schmitt O
and O
Schütze O
( O
2019 O
) O
extracts O
verbal O
relations O
from O
ClueWeb09 O
( O
Gabrilovich O
et O
al O
. O
, O
2013 O
) O
based O
on O
Freebase O
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
entities O
, O
and O
splits O
the O
extracted O
relations O
into O
typed O
one O
based O
on O
their O
most O
frequent O
Freebase O
types O
, O
which O
is O
naturally O
compatible O
to O
typed O
EG O
settings O
. O
We O
use O
their O
manually O
- O
labeled O
1,325 O
positive O
and O
2,660 O
negative O
examples O
in O
our O
EG O
benchmark O
, O
called O
SherLIiC B-DatasetName
Dataset O
. O
The O
dataset O
is O
split O
into O
25 O
% O
( O
validation O
) O
and O
75 O
% O
( O
test O
) O
in O
our O
experiments O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Section O
3 O
and O
4 O
B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Section O
3 O
, O
4 O
and O
" O
Ethics O
Statement O
" O
B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Section O
4 O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Section O
4 O
C O
Did O
you O
run O
computational O
experiments O
? O
Section O
4 O
and O
5 O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Section O
4 O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Acknowledgements O

This O
work O
is O
supported O
in O
part O
by O
NSFC O
( O
62161160339 O
) O
. O
We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
helpful O
comments O
and O
suggestions O
. O

Algorithm O
2 O

The O
mapping O
function O
S O
−1 O
. O

Require O
: O
A O
generated O
sentence O
s. O
Ensure O
: O
A O
predicate O
p O
, O
or O
N O
U O
LL O
indicating O
that O
s O
is O
not O
a O
valid O
predicate O
sentence O
. O
1 O
: O
Split O
the O
sentences O
into O
tokens O
l O
and O
strip O
t1 O
A O
, O
t2 O
B O
2 O
: O
prefix= O
" O
" O
3 O
: O
if O
|l| O
= O
0 O
then O
4 O
: O

return O
NULL O
5 O
: O
end O
if O
6 O
: O
if O
not O
or O
n't O
in O
l O
[ O
0 O
] O
then O
7 O
: O

prefix O
= O
N O
EG O
/ O
/ O
representing O
the O
negation O
8 O
: O
end O
if O
9 O
: O
Remove O
the O
modal O
verbs O
in O
l O
10 O
: O
if O
l O
begins O
with O
have O
been O
or O
has O
been O
then O
11 O
: O
l O
= O
l O
[ O
1 O
: O
] O
12 O
: O
end O
if O
13 O
: O
if O
|l| O
> O
1 O
and O
l O
[ O
: O
2 O
] O
is O
have+P.P. O
then O
14 O
: O

l O
= O
l O
[ O
1 O
: O
] O
15 O
: O
end O
if O
16 O
: O
if O
|l| O
> O
2 O
and O
the O
present O
tense O
of O
l O
[ O
: O
2 O
] O
is O
have O
to O
then O
17 O
: O
l O
= O
l O
[ O
2 O
: O
] O
18 O
: O
end O
if O
19 O
: O
if O
|l| O
= O
0 O
then O
20 O
: O

return O
NULL O
21 O
: O
end O
if O
22 O
: O
i O
head O
= O
0 O
, O
i O
tail O
= O
|l| O
− O
1 O
23 O
: O
while O
i O
head O
≤ O
i O
tail O
and O
l O
[ O
i O
head O
] O
is O
not O
a O
verb O
do O
24 O
: O

i O
head O
= O
i O
head O
+ O
1 O
25 O
: O
end O
while O
26 O
: O
while O
i O
head O
≤ O
i O
tail O
and O
l O
[ O
i O
tail O
] O
is O
not O
a O
verb O
or O
a O
preposition O
do O
27 O
: O

i O
tail O
= O
i O
tail O
− O
1 O
28 O
: O
end O
while O
29 O
: O
if O
i O
head O
> O
i O
tail O
then O
30 O
: O

return O
NULL O
31 O
: O
end O
if O
32 O
: O
l O
′ O
= O
l O
[ O
i O
head O
: O
i O
tail O
+ O
1 O
] O
/ O
/ O
cut O
the O
token O
between O
i O
head O
and O
i O
tail O
33 O
: O
if O
l O
′ O
[ O
0 O
: O
2 O
] O
is O
a O
verb O
like O
be O
doing O
then O
34 O
: O

is O
an O
adjective O
or O
a O
noun O
, O
and O
l O
′ O
[ O
−1 O
] O
is O
a O
preposition O
then O
46 O
: O

D O
An O
Example O
of O
Generating O
Predicates O
from O
Seed O
Predicates O

We O
show O
an O
example O
process O
of O
generating O
new O
predicates O
by O
the O
generator O
G O
of O
TP B-MethodName
- I-MethodName
EGG I-MethodName
in O
Table O
8 O
. O

The O
predicates O
repeating O
in O
current O
generation O
or O
appearing O
in O
previous O
stages O
, O
and O
sentences O
that O
can O
not O
be O
resolved O
by O
S O
−1 O
are O
omitted O
. O

Predicates O
generated O
from O
at O
least O
two O
different O
s O
are O
in O
red O
, O
and O
predicates O
appeared O
in O
generation O
of O
previous O
steps O
are O
in O
blue O
. O
According O
to O
Algorithm O
1 O
, O
only O
seed O
predicates O
and O
colored O
predicates O
will O

Continual O
Contrastive O
Finetuning O
Improves O
Low B-TaskName
- I-TaskName
Resource I-TaskName
Relation I-TaskName
Extraction I-TaskName

Relation B-TaskName
extraction I-TaskName
( O
RE B-TaskName
) O
, O
which O
has O
relied O
on O
structurally O
annotated O
corpora O
for O
model O
training O
, O
has O
been O
particularly O
challenging O
in O
lowresource O
scenarios O
and O
domains O
. O
Recent O
literature O
has O
tackled O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
by O
selfsupervised O
learning O
, O
where O
the O
solution O
involves O
pretraining O
the O
entity O
pair O
embedding O
by O
RE B-TaskName
- O
based O
objective O
and O
finetuning O
on O
labeled O
data O
by O
classification O
- O
based O
objective O
. O
However O
, O
a O
critical O
challenge O
to O
this O
approach O
is O
the O
gap O
in O
objectives O
, O
which O
prevents O
the O
RE B-TaskName
model O
from O
fully O
utilizing O
the O
knowledge O
in O
pretrained O
representations O
. O
In O
this O
paper O
, O
we O
aim O
at O
bridging O
the O
gap O
and O
propose O
to O
pretrain O
and O
finetune O
the O
RE O
model O
using O
consistent O
objectives O
of O
contrastive O
learning O
. O
Since O
in O
this O
kind O
of O
representation O
learning O
paradigm O
, O
one O
relation O
may O
easily O
form O
multiple O
clusters O
in O
the O
representation O
space O
, O
we O
further O
propose O
a O
multi B-MethodName
- I-MethodName
center I-MethodName
contrastive I-MethodName
loss I-MethodName
that O
allows O
one O
relation O
to O
form O
multiple O
clusters O
to O
better O
align O
with O
pretraining O
. O
Experiments O
on O
two O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
datasets O
, O
BioRED B-DatasetName
and O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
, O
demonstrate O
the O
effectiveness O
of O
our O
method O
. O
Particularly O
, O
when O
using O
1 O
% O
end O
- O
task O
training O
data O
, O
our O
method O
outperforms O
PLMbased B-MethodName
RE B-TaskName
classifier O
by O
10.5 B-MetricValue
% I-MetricValue
and O
6.1 B-MetricValue
% I-MetricValue
on O
the O
two O
datasets O
, O
respectively O
. O

Introduction O

Relation B-TaskName
extraction I-TaskName
( O
RE B-TaskName
) O
is O
a O
fundamental O
task O
in O
NLP O
. O
It O
aims O
to O
identify O
the O
relations O
among O
entities O
in O
a O
given O
text O
from O
a O
predefined O
set O
of O
relations O
. O
While O
much O
effort O
has O
been O
devoted O
to O
RE B-TaskName
in O
supervised O
settings O
( O
Zhang O
et O
al O
. O
, O
2017 O
( O
Zhang O
et O
al O
. O
, O
, O
2018Nan O
et O
al O
. O
, O
2020 O
) O
, O
RE B-TaskName
is O
extremely O
challenging O
in O
high O
- O
stakes O
domains O
such O
as O
biology O
and O
medicine O
, O
where O
annotated O
data O
are O
comparatively O
scarce O
due O
to O
overly O
high O
annotation O
costs O
. O
Therefore O
, O
there O
is O
a O
practical O
and O
urgent O
need O
for O
developing O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
models O
without O
the O
reliance O
on O
large O
- O
scale O
end O
- O
task O
annotations O
. O

To O
realize O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
, O
previous O
work O
has O
focused O
on O
pretraining O
entity O
pair O
embedding O
on O
large O
corpora O
using O
RE B-TaskName
- O
based O
pretraining O
objectives O
. O
Particularly O
, O
Baldini O
Soares O
et O
al O
. O
( O
2019 O
) O
propose O
a O
self B-MethodName
- I-MethodName
supervised I-MethodName
matching I-MethodName
- I-MethodName
theblanks I-MethodName
( O
MTB B-MethodName
) O
objective O
that O
encourages O
embeddings O
of O
the O
same O
entity O
pairs O
in O
different O
sentences O
to O
be O
similar O
. O
Later O
work O
( O
Peng O
et O
al O
. O
, O
2020 O
; O
Qin O
et O
al O
. O
, O
2021 O
) O
extends O
this O
idea O
with O
distant O
supervision O
( O
Mintz O
et O
al O
. O
, O
2009 O
) O
and O
improves O
representation O
learning O
using O
contrastive O
learning O
( O
Hadsell O
et O
al O
. O
, O
2006 O
; O
Oord O
et O
al O
. O
, O
2018 O
; O
. O
To O
adapt O
to O
training O
on O
RE O
annotations O
, O
these O
works O
finetune O
pretrained O
entity O
pair O
embedding O
on O
labeled O
data O
using O
classification O
- O
based O
objectives O
. O
Although O
this O
paradigm O
produces O
better O
results O
compared O
to O
RE B-TaskName
models O
initialized O
with O
pretrained O
language O
models O
( O
PLMs O
) O
, O
it O
creates O
a O
significant O
divergence O
between O
pretraining O
and O
finetuning O
objectives O
, O
thus O
preventing O
the O
model O
from O
fully O
exploiting O
knowledge O
in O
pretraining O
. O

In O
this O
paper O
, O
we O
aim O
to O
bridge O
this O
gap O
in O
RE B-TaskName
pretraining O
and O
finetuning O
. O
Our O
key O
idea O
is O
to O
use O
similar O
objectives O
in O
pretraining O
and O
finetuning O
. O
First O
, O
we O
propose O
to O
continually O
finetune O
pretrained O
embedding O
by O
contrastive O
learning O
, O
which O
encourages O
the O
entity O
pair O
embeddings O
corresponding O
to O
the O
same O
relation O
to O
be O
similar O
. O
However O
, O
as O
pretraining O
and O
finetuning O
are O
conducted O
on O
different O
tasks O
, O
entity O
pairs O
of O
the O
same O
relation O
can O
form O
multiple O
different O
clusters O
in O
the O
pretrained O
embedding O
, O
where O
standard O
supervised O
contrastive O
loss O
( O
Khosla O
et O
al O
. O
, O
2020 O
) O
may O
distort O
the O
representation O
because O
of O
its O
underlying O
onecluster O
assumption O
( O
Graf O
et O
al O
. O
, O
2021 O
) O
. O
Therefore O
, O
we O
further O
propose O
a O
multi B-MethodName
- I-MethodName
center I-MethodName
contrastive I-MethodName
loss I-MethodName
( O
MCCL B-MethodName
) O
, O
which O
encourages O
an O
entity O
pair O
to O
be O
similar O
to O
only O
a O
subset O
of O
entity O
pairs O
of O
the O
same O
relation O
, O
allowing O
one O
relation O
to O
form O
multiple O
clusters O
. O
Second O
, O
we O
propose O
to O
use O
classwise O
k B-MethodName
- I-MethodName
nearest I-MethodName
neighbors I-MethodName
( O
kNN B-MethodName
; O
Khandelwal O
et O
al O
. O
2020Khandelwal O
et O
al O
. O
, O
2021 O
in O
inference O
, O
where O
predictions O
are O
made O
based O
on O
most O
similar O
instances O
. O

We O
focus O
our O
work O
on O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
( O
Jia O
et O
al O
. O
, O
2019 O
; O
Yao O
et O
al O
. O
, O
2019 O
) O
, O
which O
consists O
of O
both O
intra O
- O
and O
cross O
- O
sentence O
relations O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
work O
represents O
the O
first O
effort O
to O
explore O
self O
- O
supervised O
pretraining O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Unlike O
prior O
studies O
( O
Peng O
et O
al O
. O
, O
2020 O
; O
Qin O
et O
al O
. O
, O
2021 O
) O
, O
we O
do O
not O
use O
distant O
supervision O
. O
Instead O
, O
we O
pretrain O
entity O
pair O
embedding O
with O
an O
improved B-MethodName
MTB I-MethodName
objective O
on O
unlabeled O
corpora O
, O
where O
we O
use O
contrastive O
learning O
to O
learn O
representations O
that O
suit O
downstream O
RE B-TaskName
. O
We O
then O
finetune O
the O
pretrained O
model O
on O
labeled O
data O
with O
MCCL B-MethodName
. O
Experiments O
on O
two O
datasets O
, O
BioRED B-DatasetName
( O
Luo O
et O
al O
. O
, O
2022 O
) O
in O
the O
biomedical O
domain O
and O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
( O
Tan O
et O
al O
. O
, O
2022b O
) O
in O
the O
general O
domain O
, O
demonstrate O
that O
our O
pretraining O
and O
finetuning O
objectives O
significantly O
outperform O
baseline O
methods O
in O
low O
- O
resource O
settings O
. O
Particularly O
, O
in O
the O
low O
- O
resource O
setting O
of O
using O
1 O
% O
of O
labeled O
data O
, O
our O
method O
outperforms O
PLM B-MethodName
- I-MethodName
based I-MethodName
classifiers O
by O
10.5 B-MetricValue
% I-MetricValue
and O
6.1 B-MetricValue
% I-MetricValue
on O
BioRED B-DatasetName
and O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
, O
respectively O
. O
Based O
on O
our O
pretrained O
representations O
, O
MCCL B-MethodName
outperforms O
classification O
- O
based O
finetuning O
by O
6.0 B-MetricValue
% I-MetricValue
and O
4.1 B-MetricValue
% I-MetricValue
, O
respectively O
. O
We O
also O
find O
observe O
that O
as O
more O
data O
becomes O
available O
, O
the O
performance O
gap O
between O
MCCL B-MethodName
and O
classification O
- O
based O
finetuning O
diminishes O
. O

Our O
technical O
contributions O
are O
three O
- O
fold O
. O
First O
, O
we O
propose O
to O
pretrain O
the O
PLMs O
based O
on O
our O
improved O
MTB O
objective O
and O
show O
that O
it O
significantly O
improves O
PLM O
performance O
in O
lowresource B-TaskName
document I-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Second O
, O
we O
present O
a O
technique O
that O
bridges O
the O
gap O
of O
learning O
objectives O
between O
RE B-TaskName
pretraining O
and O
finetuning O
with O
continual O
contrastive O
finetuning O
and O
kNNbased O
inference O
, O
helping O
the O
RE B-TaskName
model O
leverage O
pretraining O
knowledge O
. O
Third O
, O
we O
design O
a O
novel O
MCCL B-MethodName
finetuning O
objective O
, O
allowing O
one O
relation O
to O
form O
multiple O
different O
clusters O
, O
thus O
further O
reducing O
the O
distributional O
gap O
between O
pretraining O
and O
finetuning O
. O

Related O
Work O

Document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Existing O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
models O
can O
be O
classified O
into O
graph O
- O
based O
and O
sequence O
- O
based O
models O
. O
Graph O
- O
based O
models O
construct O
document O
graphs O
spanning O
across O
sentence O
boundaries O
and O
use O
graph O
encoders O
such O
as O
the O
graph O
convolution O
network O
( O
GCN O
; O
Kipf O
and O
Welling O
2017 O
) O
to O
aggregate O
information O
. O
Particularly O
, O
build O
document O
graphs O
using O
words O
as O
nodes O
with O
innerand O
inter O
- O
sentence O
dependencies O
( O
e.g. O
, O
syntactic O
dependencies O
, O
coreference O
, O
etc O
. O
) O
as O
edges O
. O
Later O
work O
extends O
this O
idea O
by O
applying O
different O
network O
structures O
( O
Peng O
et O
al O
. O
, O
2017 O
; O
Jia O
et O
al O
. O
, O
2019 O
) O
or O
introducing O
other O
node O
types O
and O
edges O
( O
Christopoulou O
et O
al O
. O
, O
2019 O
; O
Nan O
et O
al O
. O
, O
2020 O
; O
Zeng O
et O
al O
. O
, O
2020 O
) O
. O
On O
the O
other O
hand O
, O
sequencebased O
methods O
Zhang O
et O
al O
. O
, O
2021 O
; O
Tan O
et O
al O
. O
, O
2022a O
) O
use O
PLMs O
to O
learn O
crosssentence O
dependencies O
without O
using O
graph O
structures O
. O
Particularly O
, O
propose O
to O
enrich O
relation O
mention O
representation O
by O
localized O
context O
pooling O
. O
Zhang O
et O
al O
. O
( O
2021 O
) O
propose O
to O
model O
the O
inter O
- O
dependencies O
between O
relation O
mentions O
by O
semantic O
segmentation O
( O
Ronneberger O
et O
al O
. O
, O
2015 O
) O
. O
In O
this O
work O
, O
we O
study O
a O
general O
method O
of O
self B-MethodName
- I-MethodName
supervised I-MethodName
RE I-MethodName
. O
Therefore O
, O
our O
method O
is O
independent O
of O
the O
model O
architecture O
and O
can O
be O
adapted O
to O
different O
RE O
models O
. O

Low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
. O
Labeled O
RE B-TaskName
data O
may O
be O
scarce O
in O
real O
- O
world O
applications O
, O
especially O
in O
low O
- O
resource O
and O
high O
- O
stakes O
domains O
such O
as O
finance O
and O
biomedicine O
. O
Much O
effort O
has O
been O
devoted O
to O
training O
RE B-TaskName
models O
in O
low O
- O
resource O
settings O
. O
Some O
work O
tackles O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
by O
indirect O
supervision O
, O
which O
solves O
RE B-TaskName
by O
other O
tasks O
such O
as O
machine O
reading O
comprehension O
( O
Levy O
et O
al O
. O
, O
2017 O
) O
, O
textual O
entailment O
( O
Sainz O
et O
al O
. O
, O
2021 O
) O
, O
and O
abstractive O
summarization O
. O
However O
, O
indirect O
supervision O
may O
not O
be O
practical O
in O
high O
- O
stake O
domains O
, O
where O
annotated O
data O
for O
other O
tasks O
are O
also O
scarce O
. O
Other O
efforts O
( O
Baldini O
Soares O
et O
al O
. O
, O
2019 O
; O
Peng O
et O
al O
. O
, O
2020 O
; O
Qin O
et O
al O
. O
, O
2021 O
) O
improve O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
by O
pretraining O
on O
large O
corpora O
with O
RE B-TaskName
- O
based O
objectives O
. O
Specifically O
, O
Baldini O
Soares O
et O
al O
. O
( O
2019 O
) O
propose O
an O
MTB B-MethodName
objective O
that O
encourages O
embeddings O
of O
the O
same O
entity O
pairs O
in O
different O
sentences O
to O
be O
similar O
. O
Peng O
et O
al O
. O
( O
2020 O
) O
propose O
to O
pretrain O
on O
distantly O
labeled O
corpora O
, O
where O
they O
make O
embeddings O
of O
entity O
pairs O
with O
the O
same O
distant O
label O
to O
be O
similar O
. O
They O
also O
introduce O
a O
contrastive O
learning O
based O
training O
objective O
to O
improve O
representation O
learning O
. O
Qin O
et O
al O
. O
( O
2021 O
) O
further O
introduce O
an O
entity O
discrimination O
task O
and O
pretrain O
the O
RE B-TaskName
model O
on O
distantly O
labeled O
document O
corpora O
. O
In O
this O
paper O
, O
we O
study O
self B-MethodName
- I-MethodName
supervised I-MethodName
pretraining I-MethodName
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
We O
study O
how O
to O
reduce O
the O
gap O
between O
pretraining O
and O
finetuning O
, O
which O
is O
critical O
to O
bridge O
the O
training O
signals O
obtained O
in O
these O
two O
stages O
but O
has O
been O
overlooked O
in O
prior O
work O
. O

Method O

In O
this O
work O
, O
we O
study O
a O
self O
- O
supervised O
approach O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Given O
a O
document O
d O
and O
a O
set O
of O
entities O
{ O
e O
i O
} O
N O
i=1 O
, O
where O
each O
entity O
e O
i O
has O
one O
or O
multiple O
entity O
mentions O
in O
the O
document O
, O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
aims O
at O
predicting O
the O
relations O
of O
all O
entity O
pairs O
( O
e O
s O
, O
e O
o O
) O
s O
, O
o O
∈ O
{ O
1 O
, O
... O
, O
N O
} O
from O
a O
predefined O
set O
of O
relationships O
R O
( O
including O
an O
NA O
class O
indicating O
no O
relation O
exists O
) O
, O
where O
e O
s O
and O
e O
o O
are O
the O
subject O
and O
object O
entities O
, O
respectively O
. O
In O
the O
self B-MethodName
- I-MethodName
supervised I-MethodName
RE I-MethodName
setting O
, O
we O
have O
a O
large O
unlabeled O
document O
corpus O
for O
pretraining O
and O
a O
labeled O
RE O
dataset O
for O
finetuning O
. O
The O
document O
corpus O
has O
been O
annotated O
with O
entity O
mentions O
and O
the O
associated O
entity O
types O
but O
no O
relations O
. O
Our O
goal O
is O
to O
train O
a O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
classifier O
, O
especially O
in O
the O
low O
- O
resource O
setting O
. O

Our O
training O
pipeline O
consists O
of O
two O
phases O
: O
pretraining O
and O
finetuning O
. O
In O
pretraining O
, O
we O
use O
the O
( O
unlabeled O
) O
document O
corpus O
to O
pretrain O
the O
entity O
pair O
embedding O
based O
on O
our O
improved B-MethodName
matching I-MethodName
- I-MethodName
the I-MethodName
- I-MethodName
blanks I-MethodName
training I-MethodName
objective I-MethodName
( O
MTB B-MethodName
; O
Baldini O
Soares O
et O
al O
. O
2019 O
) O
, O
where O
the O
LM O
learns O
to O
decide O
whether O
two O
entity O
pair O
embeddings O
correspond O
to O
the O
entity O
pairs O
or O
not O
, O
and O
the O
learning O
of O
representation O
is O
enhanced O
with O
contrastive O
learning O
. O
In O
finetuning O
, O
we O
continue O
to O
train O
the O
pretrained O
model O
on O
relation O
- O
labeled O
data O
using O
a O
multi B-MethodName
- I-MethodName
center I-MethodName
contrastive I-MethodName
loss I-MethodName
( O
MCCL B-MethodName
) O
, O
which O
achieves O
better O
performance O
than O
the O
traditional O
classifier O
paradigm O
due O
to O
its O
better O
- O
aligned O
learning O
objective O
with O
pretraining O
. O
After O
training O
, O
we O
use O
classwise O
k B-MethodName
- I-MethodName
nearest I-MethodName
neighbor I-MethodName
( O
kNN B-MethodName
) O
inference O
that O
suits O
well O
the O
contrastively O
finetuned O
model O
. O

The O
rest O
of O
this O
section O
is O
organized O
as O
follows O
: O
we O
introduce O
the O
model O
architecture O
used O
in O
both O
pretraining O
and O
finetuning O
in O
Section O
3.1 O
, O
the O
pretraining O
process O
in O
Section O
3.2 O
, O
finetuning O
in O
Section O
3.3 O
, O
and O
inference O
in O
Section O
3.4 O
. O

Model O
Architecture O

Encoder O
. O
Given O
a O
document O
d O
= O
[ O
x O
1 O
, O
x O
2 O
, O
... O
, O
x O
l O
] O
, O
we O
first O
mark O
the O
spans O
of O
the O
entity O
mentions O
by O
adding O
special O
entity O
markers O
[ O
E O
] O
and O
[ O
/ O
E O
] O
to O
the O
start O
and O
the O
end O
of O
each O
mention O
. O
Then O
we O
encode O
the O
document O
with O
a O
PLM O
to O
get O
the O
contextual O
embedding O
of O
textual O
tokens O
: O

H O
= O
[ O
h O
1 O
, O
h O
2 O
, O
... O
, O
h O
l O
] O
= O
PLM O
( O
[ O
x O
1 O
, O
x O
2 O
, O
... O
, O
x O
l O
] O
) O
. O

We O
take O
the O
contextual O
embedding O
of O
[ O
E O
] O
at O
the O
last O
layer O
of O
the O
PLM O
as O
the O
embedding O
of O
entity O
mentions O
. O
We O
accumulate O
the O
embedding O
of O
mentions O
corresponding O
to O
the O
same O
entity O
by O
Log O
- O
SumExp O
pooling O
( O
Jia O
et O
al O
. O
, O
2019 O
) O
to O
get O
the O
entity O
embedding O
h O
e O
i O
. O
Entity O
pair O
embedding O
. O
Given O
an O
entity O
pair O
t O
= O
( O
e O
s O
, O
e O
o O
) O
in O
document O
d O
, O
where O
e O
s O
and O
e O
o O
are O
the O
subject O
and O
object O
entities O
, O
respectively O
, O
we O
calculate O
the O
entity O
pair O
embedding O
by O
: O
( O
es O
, O
eo O
) O
. O

z O
t O
= O
W O
linear O
h O
es O
, O
h O
eo O
, O
c O

Here O
h O
es O
, O
h O
eo O
∈ O
R O
d O
are O
embeddings O
of O
subject O
and O
object O
entities O
, O
c O
es O
, O
eo O
∈ O
R O
d O
is O
the O
localized O
context O
encoding O
for O
( O
e O
s O
, O
e O
o O
) O
, O
W O
linear O
∈ O
R O
3d×d O
is O
a O
linear O
projector O
. O
The O
localized O
context O
encoding O
is O
introduced O
by O
to O
derive O
the O
context O
embedding O
conditioned O
on O
an O
entity O
pair O
, O
which O
finds O
the O
context O
that O
both O
the O
subject O
and O
object O
entities O
attend O
to O
. O
Specifically O
, O
denote O
the O
multi O
- O
head O
attention O
in O
the O
last O
layer O
of O
PLM O
as O
A O
∈ O
R O
m×l×l O
, O
where O
m O
is O
the O
number O
of O
attention O
heads O
, O
l O
is O
the O
input O
length O
, O
we O
first O
take O
the O
attention O
scores O
from O
[ O
E O
] O
as O
the O
attention O
from O
each O
entity O
mention O
, O
then O
accumulate O
the O
attention O
of O
this O
entity O
mention O
by O
mean O
pooling O
to O
get O
the O
entitylevel O
attention O
A O
( O
e O
i O
) O
∈ O
R O
m×l O
. O
Finally O
, O
we O
compute O
c O
( O
es O
, O
eo O
) O
by O
: O
( O
es O
, O
eo O
) O
. O

A O
( O
es O
, O
eo O
) O
= O
A O
( O
es O
) O
⊙ O
A O
( O
eo O
) O
, O
q O
( O
es O
, O
eo O
) O
= O
m O
i=1 O
A O
( O
es O
, O
eo O
) O
i O
, O
a O
( O
es O
, O
eo O
) O
= O
q O
( O
es O
, O
eo O
) O
/ O
1 O
⊺ O
q O
( O
es O
, O
eo O
) O
, O
c O
( O
es O
, O
eo O
) O
= O
H O
⊺ O
a O

We O
introduce O
in O
the O
rest O
of O
the O
section O
how O
to O
pretrain O
and O
finetune O
the O
RE B-TaskName
model O
based O
on O
the O
entity O
pair O
embedding O
z O
( O
es O
, O
eo O
) O
. O

Pretraining O

We O
pretrain O
the O
LM O
on O
the O
document O
corpus O
using O
the O
MTB B-MethodName
objective O
. O
MTB B-MethodName
is O
based O
on O
a O
simple O
assumption O
that O
, O
in O
contrast O
to O
different O
entity O
pairs O
, O
it O
is O
more O
frequent O
for O
the O
same O
entity O
pair O
to O
be O
connected O
with O
the O
same O
relation O
. O
The O
MTB B-MethodName
objective O
transforms O
the O
similarity O
learning O
problem O
into O
a O
pairwise O
binary O
classification O
problem O
: O
given O
two O
relation O
- O
describing O
utterances O
where O
entity O
mentions O
are O
masked O
, O
the O
model O
classifies O
whether O
the O
entity O
pairs O
are O
the O
same O
or O
not O
. O
This O
pretraining O
objective O
has O
shown O
effectiveness O
in O
several O
sentence O
- O
level O
RE O
datasets O
( O
Zhang O
et O
al O
. O
, O
2017 O
; O
Hendrickx O
et O
al O
. O
, O
2010 O
; O
Han O
et O
al O
. O
, O
2018 O
) O
. O

However O
, O
when O
it O
comes O
to O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
, O
Qin O
et O
al O
. O
( O
2021 O
) O
have O
observed O
no O
improvement O
led O
by O
the O
vanilla B-MethodName
MTB I-MethodName
pretraining I-MethodName
. O
Therefore O
, O
we O
replace O
the O
pairwise O
binary O
classification O
with O
contrastive O
learning O
, O
which O
is O
adopted O
in O
later O
RE O
pretraining O
works O
( O
Peng O
et O
al O
. O
, O
2020 O
; O
Qin O
et O
al O
. O
, O
2021 O
) O
and O
can O
effectively O
learn O
from O
more O
positive O
and O
negative O
examples O
. O
Details O
of O
training O
objectives O
are O
elaborated O
in O
the O
rest O
of O
the O
section O
. O
We O
introduce O
the O
details O
of O
data O
preprocessing O
of O
the O
pretraining O
corpus O
in O
Appendix O
A O
. O

Training O
objective O
. O
The O
overall O
goal O
of O
pretraining O
is O
to O
make O
the O
embedding O
of O
the O
same O
entity O
pair O
from O
different O
documents O
more O
similar O
than O
different O
entity O
pairs O
. O
For O
clarity O
, O
we O
call O
two O
same O
entity O
pairs O
from O
different O
documents O
as O
a O
positive O
pair O
, O
and O
two O
different O
entity O
pairs O
as O
a O
negative O
pair O
. O
We O
use O
the O
InfoNCE O
loss O
( O
Oord O
et O
al O
. O
, O
2018 O
) O
to O
model O
this O
objective O
. O
Given O
the O
documents O
in O
batch O
, O
P O
as O
the O
set O
of O
all O
positive O
pairs O
, O
and O
N O
t O
denote O
the O
set O
of O
entity O
pairs O
different O
to O
t O
, O
the O
contrastive O
MTB B-MethodName
loss O
is O
1 O
: O

L O
rel O
= O
− O
1 O
|P| O
t O
i O
, O
t O
j O
∈P O
log O
e O
sim O
( O
z O
t O
i O
, O
z O
t O
j O
) O
/ O
τ O
Z O
t O
i O
, O
( O
1 O
) O

Z O
t O
i O
= O
e O
sim O
( O
z O
t O
i O
, O
z O
t O
j O
) O
/ O
τ B-HyperparameterName
+ O
t O
k O
∈Nt O
i O
e O
sim O
( O
z O
t O
i O
, O
z O
t O
k O
) O
/ O
τ B-HyperparameterName
, O

where O
sim O
( O
z O
t O
i O
, O
z O
t O
j O
) O
denotes O
the O
similarity O
between O
the O
embeddings O
of O
t O
i O
and O
t O
j O
, O
and O
τ B-HyperparameterName
is O
a O
temperature O
hyperprameter O
. O
Following O
Chen O
et O
al O
. O
( O
2020 O
) O
, O
we O
use O
cosine O
similarity O
as O
the O
similarity O
metric O
. O
Similar O
to O
SimCSE O
, O
we O
further O
add O
a O
self O
- O
supervised O
contrastive O
loss O
that O
requires O
the O
same O
entity O
pair O
embedding O
augmented O
by O
different O
dropout O
masks O
to O
be O
similar O
, O
thus O
encouraging O
the O
model O
to O
learn O
more O
instance O
- O
discriminative O
features O
that O
lead O
to O
less O
collapsed O
representations O
. O
Specifically O
, O
denote O
the O
two O
entity O
pair O
embeddings O
of O
t O
derived O
by O
different O
dropout O
masks O
as O
z O
t O
andẑ O
t O
, O
respectively O
, O
the O
set O
of O
all O
entity O
pairs O
in O
the O
batch O
as O
T O
, O
and O
the O
set O
of O
entity O
pairs O
in O
positive O
pairs O
as O
T O
P O
, O
the O
self O
- O
supervised O
loss O
is O
: O

L O
self O
= O
− O
1 O
|T O
P O
| O
t O
i O
∈T O
P O
log O
e O
sim O
( O
z O
t O
i O
, O
ẑ O
t O
i O
) O
/ O
τ B-HyperparameterName
Z O
t O
i O
, O
Z O
t O
i O
= O
e O
sim O
( O
z O
t O
i O
, O
ẑ O
t O
i O
) O
/ O
τ B-HyperparameterName
+ O
t O
k O
∈T O
\ O
{ O
t O
i O
} O
e O
sim O
( O
z O
t O
i O
, O
ẑ O
t O
k O
) O
/ O
τ B-HyperparameterName
. O

Finally O
, O
we O
use O
a O
masked O
language O
model O
loss O
L O
mlm O
to O
adapt O
the O
LM O
to O
the O
document O
corpus O
. O
The O
overall O
pretraining O
objective O
is O
: O

L O
pretrain O
= O
L O
rel O
+ O
L O
self O
+ O
L O
mlm O
. O

For O
faster O
convergence O
, O
we O
initialize O
our O
model O
with O
a O
PLM O
that O
is O
pretrained O
on O
a O
larger O
corpus O
, O
and O
continually O
pretrain O
the O
PLM O
on O
the O
document O
corpus O
with O
our O
new O
pretraining O
objectives O
. O
We O
use O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
for O
the O
general O
domain O
and O
PubmedBERT O
( O
Gu O
et O
al O
. O
, O
2021 O
) O
for O
the O
biomedical O
domain O
. O

Finetuning O

After O
pretraining O
, O
we O
finetune O
the O
LM O
on O
labeled O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
datasets O
. O
In O
previous O
studies O
( O
Baldini O
Soares O
et O
al O
. O
, O
2019 O
; O
Peng O
et O
al O
. O
, O
2020 O
; O
Qin O
et O
al O
. O
, O
2021 O
) O
, O
pretraining O
and O
finetuning O
are O
conducted O
in O
processes O
with O
different O
learning O
objectives O
. O
Specifically O
, O
after O
using O
the O
pretrained O
weights O
to O
initialize O
a O
RE B-TaskName
classifier O
, O
the O
model O
is O
finetuned O
with O
a O
classification O
- O
based O
training O
objective O
. O
Based O
on O
our O
model O
architecture O
, O
a O
straightforward O
finetuning O
method O
is O
to O
add O
a O
softmax O
classifier O
upon O
the O
entity O
pair O
embedding O
, O
for O
which O
a O
cross O
- O
entropy O
loss O
for O
a O
batch O
of O
entity O
pairs O
T O
is O
formulated O
as O
: O

P O
t O
i O
r O
= O
softmax O
( O
W O
r O
z O
t O
i O
+ O
b O
r O
) O
, O
L O
ce O
= O
− O
1 O
|T O
| O
t O
i O
∈T O
log O
( O
P O
t O
i O
yt O
i O
) O
, O

where O
y O
t O
is O
the O
ground O
- O
truth O
label O
for O
entity O
pair O
t O
, O
W O
r O
, O
b O
r O
are O
the O
weight O
and O
bias O
of O
the O
classifier O
. O
Though O
this O
approach O
has O
shown O
improvements O
, O
it O
may O
produce O
sub O
- O
optimal O
outcomes O
from O
MTB B-MethodName
pretraining I-MethodName
since O
it O
implicitly O
assumes O
that O
entity O
pairs O
corresponding O
to O
the O
same O
relation O
are O
in O
the O
same O
cluster O
, O
while O
MTB B-MethodName
pretraining I-MethodName
may O
learn O
multiple O
clusters O
for O
a O
relation O
. O
For O
example O
, O
the O
entity O
pairs O
( O
Honda O
Corp. O
, O
Japan O
) O
and O
( O
Mount O
Fuji O
, O
Japan O
) O
, O
although O
likely O
to O
be O
expressed O
with O
Therefore O
, O
to O
accommodate O
this O
multi O
- O
cluster O
assumption O
, O
we O
need O
to O
finetune O
the O
representations O
with O
a O
training O
objective O
that O
suits O
multiple O
clusters O
for O
each O
relation O
. O
Beside O
using O
the O
softmax O
classifier O
with O
cross O
- O
entropy O
loss O
, O
we O
also O
consider O
supervised O
contrastive O
loss O
( O
SupCon O
; O
Khosla O
et O
al O
. O
2020 O
; O
Gunel O
et O
al O
. O
2021 O
) O
. O
SupCon O
has O
a O
similar O
loss O
form O
to O
InfoNCE O
in O
Eq O
. O
( O
1 O
) O
, O
except O
that O
it O
uses O
instances O
of O
the O
same O
/ O
different O
relations O
as O
positive O
/ O
negative O
pairs O
. O
However O
, O
previous O
work O
( O
Graf O
et O
al O
. O
, O
2021 O
) O
has O
shown O
that O
both O
softmax O
and O
SupCon O
are O
minimized O
when O
the O
representations O
of O
each O
class O
collapse O
to O
the O
vertex O
of O
a O
regular O
simplex O
. O
In O
our O
case O
, O
this O
means O
the O
entity O
pair O
embeddings O
corresponding O
to O
the O
same O
relation O
in O
pretraining O
collapses O
to O
a O
single O
point O
, O
which O
creates O
a O
distributional O
gap O
between O
pretraining O
and O
finetuning O
. O

Training O
objective O
. O
We O
thereby O
propose O
the O
MCCL B-MethodName
objective O
. O
Given O
entity O
pairs O
T O
and O
sets O
of O
entity O
pairs O
grouped O
by O
their O
relations O
{ O
T O
r O
} O
r∈R O
, O
our O
loss O
is O
formulated O
as O
: O

w O
( O
t O
i O
, O
t O
j O
) O
r O
= O
e O
sim O
( O
z O
t O
i O
, O
z O
t O
j O
) O
/ O
τ O
1 O
t O
k O
∈Tr\ O
{ O
t O
i O
} O
e O
sim O
( O
z O
t O
i O
, O
z O
t O
k O
) O
/ O
τ O
1 O
, O
s O
t O
i O
r O
= O
t O
j O
∈Tr\ O
{ O
t O
i O
} O
w O
( O
t O
i O
, O
t O
j O
) O
r O

sim O
( O
z O
t O
i O
, O
z O
t O
j O
) O
, O

P O
t O
i O
r O
= O
softmax O
( O
( O
s O
t O
i O
r O
+ O
b O
r O
) O
/ O
τ B-HyperparameterName
2 I-HyperparameterName
) O
, O
L O
mccl O
= O
− O
1 O
|T O
| O
t O
i O
∈T O
log O
( O
P O
t O
i O
yt O
i O
) O
, O

where O
τ B-HyperparameterName
1 I-HyperparameterName
and O
τ B-HyperparameterName
2 I-HyperparameterName
are O
temperature O
hyperparameters O
, O
b O
r O
∈ O
R O
is O
the O
classwise O
bias O
. O
The O
loss O
calculation O
can O
be O
split O
into O
two O
steps O
. O
First O
, O
we O
calculate O
the O
similarity O
between O
t O
i O
and O
relation O
r O
, O
which O
is O
a O
weighted O
average O
of O
the O
similarity O
between O
t O
i O
and O
t O
j O
∈ O
T O
r O
such O
that O
a O
more O
similar O
t O
j O
has O
a O
larger O
weight O
. O
Next O
, O
we O
use O
the O
cross B-MethodName
- I-MethodName
entropy I-MethodName
loss I-MethodName
to O
make O
the O
similarity O
of O
ground O
- O
truth O
relation O
larger O
than O
others O
. O
In O
this O
way O
, O
MCCL B-MethodName
only O
optimizes O
t O
i O
to O
be O
similar O
to O
a O
few O
closest O
entity O
pairs O
of O
the O
ground O
- O
truth O
relation O
, O
and O
thus O
encourages O
multiple O
clusters O
in O
entity O
pair O
embedding O
. O
Note O
that O
MCCL B-MethodName
can O
be O
easily O
extended O
to O
support O
multilabel O
classification O
scenarios O
, O
for O
which O
details O
are O
given O
in O
Appendix O
B O
. O

Proxies O
. O
We O
use O
batched O
training O
for O
finetuning O
, O
where O
entity O
pairs O
in O
the O
current O
batch O
are O
used O
to O
calculate O
MCCL B-MethodName
. O
However O
, O
it O
is O
possible O
that O
a O
subset O
of O
relations O
in O
R O
, O
especially O
the O
long O
- O
tail O
relations O
, O
are O
rare O
or O
missing O
in O
the O
current O
batch O
. O
When O
T O
r O
\ O
{ O
t O
i O
} O
is O
empty O
, O
s O
t O
i O
r O
and O
MCCL B-MethodName
become O
undefined O
. O
To O
tackle O
this O
problem O
, O
we O
propose O
the O
use O
of O
proxies O
( O
Movshovitz O
- O
Attias O
et O
al O
. O
, O
2017 O
; O
Zhu O
et O
al O
. O
, O
2022 O
) O
. O
We O
add O
one O
proxy O
vector O
p O
r O
for O
each O
relation O
r O
, O
which O
is O
a O
trainable O
parameter O
and O
associated O
with O
an O
embedding O
z O
p O
r O
. O
We O
incorporate O
the O
proxies O
into O
MCCL B-MethodName
by O
changing O
T O
r O
to O
T O
′ O
r O
= O
T O
r O
∪ O
{ O
p O
r O
} O
, O
ensuring O
that O
T O
′ O
r O
\ O
{ O
t O
i O
} O
is O
never O
empty O
in O
training O
and O
preventing O
MCCL B-MethodName
from O
becoming O
undefined O
. O
The O
proxies O
are O
randomly O
initialized O
and O
updated O
during O
training O
by O
backward O
propagation O
. O

Inference O

We O
use O
the O
classwise B-MethodName
kNN I-MethodName
( O
Christobel O
and O
Sivaprakasam O
, O
2013 O
) O
for O
inference O
, O
which O
predicts O
relations O
based O
on O
similarly O
represented O
instances O
and O
thus O
aligns O
with O
our O
contrastive O
finetuning O
objective O
. O
Given O
a O
new O
entity O
pair O
to O
predict O
, O
we O
first O
find O
k O
most O
similar O
instances O
2 O
in O
the O
training O
data O
of O
each O
relation O
( O
including O
NA O
) O
, O
then O
calculate O
the O
average O
cosine O
similarity O
of O
each O
relation O
s O
avg O
r O
. O
Finally O
, O
the O
model O
returns O
the O
relation O
with O
the O
maximum O
s O
avg O
r O
+ O
b O
r O
for O
single O
- O
label O
prediction O
, O
and O
all O
relations O
with O
higher O
s O
avg O
r O
+ O
b O
r O
than O
NA O
for O
multilabel O
prediction O
. O
We O
use O
classwise B-MethodName
kNN I-MethodName
because O
it O
is O
more O
suitable O
for O
RE O
datasets O
, O
where O
the O
label O
distribution O
is O
usually O
long O
- O
tailed O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

Experiments O

We O
evaluate O
our O
proposed O
method O
with O
a O
focus O
on O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
( O
Sections O
4.1 O
- O
4.3 O
) O
, O
and O
present O
detailed O
analyses O
( O
Section O
4.4 O
) O
and O
visualization O
( O
Section O
4.5 O
) O
to O
justify O
method O
design O
choices O
. O

Datasets O

We O
conduct O
experiments O
with O
two O
documentlevel O
RE B-TaskName
datasets O
. O
The O
BioRED B-DatasetName
dataset O
( O
Luo O
et O
al O
. O
, O
2022 O
) O
is O
a O
manually O
labeled O
single O
- O
label O
RE O
dataset O
in O
the O
biomedical O
domain O
. O
The O
entity O
pairs O
are O
classified O
into O
9 O
types O
( O
including O
an O
NA O
type O
indicating O
no O
relation O
) O
. O
It O
has O
a O
training O
set O
consisting O
of O
400 O
documents O
, O
which O
we O
use O
in O
finetuning O
. O
For O
pretraining O
, O
we O
use O
the O
PubTator O
Central O
corpus O
, O
which O
annotates O
the O
PubMed O
corpus O
with O
entity O
mentions O
and O
their O
named O
entity O
types O
. O
The O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
dataset O
( O
Tan O
et O
al O
. O
, O
2022b O
) O
is O
a O
multi O
- O
label O
largescale O
dataset O
of O
the O
general O
domain O
. O
It O
is O
a O
relabeled O
version O
of O
the O
DocRED B-DatasetName
dataset O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
. O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
addresses O
the O
incomplete O
annotation O
issue O
of O
DocRED B-DatasetName
, O
where O
a O
large O
percentage O
of O
entity O
pairs O
are O
mislabeled O
as O
NA O
. O
The O
entity O
pairs O
in O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
are O
classified O
into O
97 O
types O
( O
incl O
. O
NA O
) O
. O
It O
has O
a O
training O
set O
consisting O
of O
3,053 O
documents O
, O
which O
we O
use O
in O
finetuning O
. O
For O
pretraining O
, O
we O
use O
the O
distantly O
labeled O
training O
set O
provided O
by O
DocRED B-DatasetName
, O
which O
consists O
of O
101,873 O
documents O
. O
We O
remove O
the O
relation O
labels O
and O
use O
our O
improved B-MethodName
MTB I-MethodName
to O
pretrain O
the O
model O
. O

Experimental O
Setup O

Model O
configurations O
. O
We O
implement O
our O
models O
using O
Hugging O
Face O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
We O
use O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
in O
optimization O
with O
a O
weight B-HyperparameterName
decay I-HyperparameterName
of O
0.01 B-HyperparameterValue
. O
During O
pretraining O
, O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
, O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-6 B-HyperparameterValue
, O
a O
temperature B-HyperparameterName
of O
0.05 B-HyperparameterValue
, O
and O
epochs B-HyperparameterName
of O
3 B-HyperparameterValue
and O
10 B-HyperparameterValue
for O
BioRED B-DatasetName
and O
DocRED B-DatasetName
, O
respectively O
. O
During O
finetuning O
, O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
, O
and O
epochs B-HyperparameterName
of O
100 B-HyperparameterValue
and O
30 B-HyperparameterValue
for O
BioRED B-DatasetName
and O
DocRED B-DatasetName
, O
respectively O
. O
The O
temperatures B-HyperparameterName
in O
MCCL B-MethodName
are O
set O
to O
τ B-HyperparameterName
1 I-HyperparameterName
= O
τ B-HyperparameterName
2 I-HyperparameterName
= O
0.2 B-HyperparameterValue
for O
BioRED B-DatasetName
and O
τ B-HyperparameterName
1 I-HyperparameterName
= O
0.01 B-HyperparameterValue
, O
τ B-HyperparameterName
2 I-HyperparameterName
= O
0.03 B-HyperparameterValue
for O
DocRED B-DatasetName
. O
We O
search O
k B-HyperparameterName
from O
{ O
1 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
20 B-HyperparameterValue
} O
for O
classwise B-MethodName
kNN I-MethodName
using O
the O
development O
set O
3 O
. O
We O
run O
experiments O
with O
Nvidia O
V100 O
GPUs O
. O

Evaluation O
settings O
. O
In O
this O
work O
, O
in O
addition O
to O
the O
standard O
full O
- O
shot O
training O
, O
we O
consider O
lowresource O
settings O
. O
To O
create O
each O
of O
the O
settings O
, O
we O
randomly O
sample O
a O
fixed O
proportion O
p O
% O
of O
the O
entity O
pairs O
from O
the O
training O
set O
as O
our O
training O
data O
, O
and O
use O
the O
original O
test O
set O
for O
evaluation O
. O
We O
use O
the O
same O
evaluation O
metrics O
as O
the O
original O
papers O
. O
We O
use O
micro B-MetricName
- I-MetricName
F I-MetricName
1 I-MetricName
for O
BioRED B-DatasetName
, O
and O
micro B-MetricName
- I-MetricName
F I-MetricName
1 I-MetricName
and O
micro B-MetricName
- I-MetricName
F I-MetricName
1 I-MetricName
-Ign I-MetricName
for O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
. O
The O
micro B-MetricName
- I-MetricName
F I-MetricName
1 I-MetricName
-Ign I-MetricName
removes O
the O
relational O
facts O
in O
the O
test O
set O
that O
have O
appeared O
in O
training O
. O

Compared O
methods O
. O
We O
experiment O
with O
the O
following O
finetuning O
objectives O
: O
( O
1 O
) O
Lazy B-MethodName
learning I-MethodName
, O
which O
directly O
uses O
the O
pretrained O
embedding O
and O
training O
data O
to O
perform O
kNN B-MethodName
without O
finetuning O
; O
( O
2 O
) O
Cross B-MethodName
- I-MethodName
entropy I-MethodName
loss I-MethodName
( O
CE B-MethodName
) O
, O
which O
adds O
a O
softmax O
classifier O
on O
top O
of O
PLM O
and O
uses O
crossentropy O
loss O
to O
finetune O
the O
model O
; O
( O
3 O
) O
Supervised B-MethodName
contrastive I-MethodName
loss I-MethodName
( O
SupCon B-MethodName
) O
; O
and O
( O
4 O
) O
Multicenter B-MethodName
contrastive I-MethodName
loss I-MethodName
( O
MCCL B-MethodName
) O
. O
In O
inference O
, O
classwise B-MethodName
kNN I-MethodName
is O
used O
for O
all O
methods O
except O
for O
CE B-MethodName
. O
Note O
that O
as O
SupCon B-MethodName
does O
not O
apply O
to O
multilabel O
scenarios O
, O
we O
only O
evaluate O
it O
on O
BioRED B-DatasetName
. O
For O
each O
objective O
, O
we O
also O
evaluate O
the O
PLM O
before O
and O
after O
MTB B-MethodName
pretraining I-MethodName
. O
We O
use O
different O
PLMs O
as O
the O
backbone O
of O
the O
model O
, O
namely O
PubmedBERT B-MethodName
BASE I-MethodName
for O
BioRED B-DatasetName
and O
BERT B-MethodName
BASE I-MethodName
for O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
, O
which O
are O
pretrained O
on O
the O
biomedical O
and O
general O
domains O
, O
respectively O
. O

Main O
Results O

The O
results O
on O
the O
test O
sets O
of O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
and O
BioRED B-DatasetName
are O
shown O
in O
Table O
2 O
and O
Table O
3 O
Considering O
other O
training O
objectives O
, O
we O
observe O
that O
lazy B-MethodName
learning I-MethodName
produces O
meaningful O
results O
. O
On O
both O
datasets O
, O
the O
results O
of O
lazy B-MethodName
learning I-MethodName
based I-MethodName
on I-MethodName
MTB I-MethodName
with O
10 O
% O
of O
data O
are O
comparable O
to O
finetuning O
with O
1 O
% O
of O
data O
. O
This O
shows O
that O
the O
entity O
pair O
embedding O
pretrained O
on O
unlabeled O
corpora O
contains O
knowledge O
that O
can O
be O
transferred O
to O
unseen O
relations O
. O
We O
also O
observe O
that O
SupCon B-MethodName
using I-MethodName
kNN I-MethodName
- I-MethodName
based I-MethodName
inference I-MethodName
underperforms O
both O
CE B-MethodName
and O
MCCL B-MethodName
on O
BioRED B-DatasetName
, O
showing O
that O
its O
one O
- O
cluster O
assumption O
hurts O
the O
knowledge O
transfer O
. O

F B-MetricName
1 I-MetricName
F B-MetricName
1 I-MetricName
-Ign I-MetricName
F B-MetricName
1 I-MetricName
F B-MetricName
1 I-MetricName
-Ign I-MetricName
F B-MetricName
1 I-MetricName
F B-MetricName
1 I-MetricName
-Ign I-MetricName
F B-MetricName
1 I-MetricName
F B-MetricName
1 I-MetricName
- O

Ablation O
Study O

Pretraining O
objectives O
. O
We O
analyze O
the O
effectiveness O
of O
our O
proposed O
pretraining O
losses O
in O
Section O
3.2 O
. O
To O
do O
so O
, O
we O
pretrain O
the O
model O
with O
one O
loss O
removed O
at O
a O
time O
while O
keeping O
the O
finetuning O
setup O
on O
BioRED B-DatasetName
fixed O
with O
the O
MCCL B-MethodName
. O

The O
results O
are O
shown O
in O
Table O
4 O
. O
Overall O
, O
we O
observe O
that O
all O
losses O
are O
effective O
. O
If O
we O
remove O
all O
proposed O
techniques O
and O
use O
the O
vanilla B-MethodName
MTB I-MethodName
pretraining I-MethodName
objective O
of O
binary O
pairwise O
classification O
, O
the O
results O
are O
only O
slightly O
better O
or O
even O
worse O
. O
Among O
the O
techniques O
, O
removing B-MethodName
L I-MethodName
rel I-MethodName
leads O
to O
the O
largest O
performance O
drop O
, O
showing O
that O
MTB B-MethodName
- I-MethodName
based I-MethodName
pretraining I-MethodName
is O
critical O
to O
improve O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
. O
Removing B-MethodName
L I-MethodName
self I-MethodName
also O
leads O
to O
a O
large O
performance O
drop O
. O
It O
is O
because O
L B-MethodName
self I-MethodName
encourages O
the O
model O
to O
learn O
more O
discriminative O
features O
that O
lead O
to O
less O
collapsed O
representations O
. O
Our O
finding O
aligns O
with O
recent O
studies O
in O
computer O
vision O
( O
Islam O
et O
al O
. O
, O
2021 O
; O
, O
showing O
that O
reducing O
collapsed O
representations O
with O
self O
- O
supervised O
contrastive O
learning O
improves O
the O
transferability O
to O
downstream O
tasks O
. O

Performance O
w.r.t O
. O
different O
temperatures B-HyperparameterName
. O
We O
discuss O
the O
impact O
of O
two O
temperatures B-HyperparameterName
in O
MCCL B-MethodName
. O

In O
MCCL B-MethodName
, O
τ B-HyperparameterName
1 I-HyperparameterName
controls O
the O
weighting O
of O
instances O
. O

With O
a O
very O
small O
τ B-HyperparameterName
1 I-HyperparameterName
, O
each O
instance O
will O
only O
form O
a O
cluster O
with O
its O
nearest O
neighbor O
in O
the O
batch O
, O
while O
with O
very O
large O
τ B-HyperparameterName
1 I-HyperparameterName
, O
instances O
of O
the O
same O
relation O
will O
collapse O
to O
the O
same O
cluster O
. O
τ B-HyperparameterName
2 I-HyperparameterName
controls O
the O
importance O
of O
hard O
instances O
, O
which O
is O
also O
used O
in O
other O
contrastive O
losses O
( O
e.g. O
, O
τ B-HyperparameterName
in O
Eq O
. O
( O
1 O
) O
) O
. O
Wang O
and O
Liu O
( O
2021 O
) O
observe O
that O
small O
τ B-HyperparameterName
2 I-HyperparameterName
makes O
the O
model O
focus O
more O
on O
hard O
instances O
, O
while O
Khosla O
et O
al O
. O
( O
2020 O
) O
observe O
that O
too O
small O
τ B-HyperparameterName
2 I-HyperparameterName
leads O
to O
numerical O
instability O
. O
We O
show O
the O
results O
of O
using O
different O
temperatures O
in O
Figure O
1 O
, O
where O
we O
keep O
one O
temperature B-HyperparameterName
fixed O
and O
change O
the O
other O
. O
For O
τ B-HyperparameterName
1 I-HyperparameterName
, O
we O
find O
that O
using O
large O
temperature B-HyperparameterName
harms O
the O
performance O
, O
showing O
that O
our O
multi O
- O
cluster O
assumption O
improves O
low O
- O
resource O
RE O
. O
For O
τ B-HyperparameterName
2 I-HyperparameterName
, O
we O
observe O
that O
both O
small O
and O
large O
values O
impair O
the O
performance O
, O
which O
is O
aligned O
with O
prior O
observations O
. O

Performance O
w.r.t O
. O
different O
amount O
of O
data O
. O

The O
main O
results O
show O
that O
MCCL B-MethodName
outperforms O
CE B-MethodName
in O
the O
low O
- O
resource O
setting O
, O
while O
slightly O
underperforming O
CE B-MethodName
when O
full O
training O
data O
is O
used O
. O
We O
further O
evaluate O
MCCL B-MethodName
and O
CE B-MethodName
using O
different O
amounts O
of O
end O
- O
task O
data O
. O
We O
experiment O
on O
BioRED B-DatasetName
and O
use O
the O
entity O
pair O
embedding O
pretrained O
with O
MTB B-MethodName
. O
Results O
are O
shown O
in O
Figure O
2 O
. O
We O
observe O
that O
MCCL B-MethodName
consistently O
outperforms O
CE B-MethodName
by O
a O
large O
margin O
when O
less O
than O
20 O
% O
of O
training O
data O
is O
used O
, O
while O
it O
performs O
similarly O
or O
worse O
than O
CE O
after O
that O
. O
It O
again O
demonstrates O
the O
effectiveness O
of O
MCCL B-MethodName
in O
low B-TaskName
- I-TaskName
resource I-TaskName
RE I-TaskName
. O
However O
, O
as O
the O
pretraining O
and O
finetuning O
are O
based O
on O
different O
tasks O
, O
fully O
adapting O
the O
model O
to O
downstream O
data O
by O
CE B-MethodName
results O
in O
similar O
or O
better O
performance O
in O
data O
- O
sufficient O
scenarios O
. O
Lazy B-MethodName
CE B-MethodName
SupCon B-MethodName
MCCL B-MethodName

Visualization O

Conclusion O

In O
this O
paper O
, O
we O
study O
self O
- O
supervised O
learning O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Our O
method O
conducts O
an O
improved B-MethodName
MTB I-MethodName
pretraining I-MethodName
objective O
that O
acquires O
cheap O
supervision O
signals O
from O
large O
corpora O
without O
relation O
labels O
. O
To O
bridge O
the O
gap O
between O
pretraining O
and O
end O
- O
task O
finetuning O
, O
we O
propose O
a O
continual O
contrastive O
finetuning O
objective O
, O
in O
contrast O
to O
prior O
studies O
that O
typically O
use O
classification O
- O
based O
finetuning O
, O
and O
use O
kNNbased B-MethodName
inference O
. O
As O
pretrained O
representation O
may O
form O
multi O
- O
cluster O
representation O
, O
we O
further O
propose O
a O
multi O
- O
center O
contrastive O
loss O
that O
aligns O
well O
with O
the O
nature O
of O
the O
pretrained O
representation O
. O
Extensive O
experiments O
on O
two O
documentlevel B-TaskName
RE I-TaskName
datasets O
demonstrate O
the O
effectiveness O
of O
these O
key O
techniques O
in O
our O
method O
. O
Future O
work O
is O
adapting O
our O
method O
to O
other O
tasks O
in O
information O
extraction O
, O
such O
as O
n O
- O
ary O
relation O
extraction O
, O
named O
entity O
recognition O
, O
typing O
, O
and O
linking O
. O

Limitations O

The O
main O
limitation O
of O
MCCL B-MethodName
is O
the O
requirement O
of O
a O
sufficiently O
large O
batch B-HyperparameterName
size I-HyperparameterName
in O
training O
( O
32 B-HyperparameterValue
documents O
in O
our O
experiments O
) O
, O
leading O
to O
a O
need O
for O
large O
GPU O
memory O
. O
This O
is O
because O
MCCL B-MethodName
uses O
in O
- O
batch O
entity O
pairs O
for O
contrastive O
learning O
, O
and O
a O
small O
batch B-HyperparameterName
size I-HyperparameterName
does O
not O
provide O
enough O
instances O
to O
form O
multiple O
clusters O
. O
In O
addition O
, O
we O
need O
to O
store O
the O
entity O
pair O
embedding O
of O
the O
whole O
training O
set O
for O
kNN B-MethodName
- I-MethodName
based I-MethodName
inference I-MethodName
, O
which O
is O
less O
memory O
- O
efficient O
than O
CE B-MethodName
. O

and O
PMI O
= O
N O
( O
es O
, O
eo O
) O
N O
( O
es O
) O
×N O
( O
eo O
) O
, O
to O
measure O
the O
popularity O
of O
entity O
pairs O
The O
frequency O
measures O
how O
often O
e O
s O
and O
e O
o O
co O
- O
occur O
. O
The O
PMI O
measures O
whether O
e O
s O
and O
e O
o O
have O
a O
strong O
association O
. O
In O
pretraining O
, O
we O
first O
discard O
the O
entity O
pairs O
with O
frequency O
< O
N O
threshold O
, O
and O
then O
use O
the O
positive O
pairs O
constituted O
by O
the O
top O
K O
entity O
pairs O
measured O
by O
their O
PMIs O
. O
We O
set O
the O
frequency B-HyperparameterName
threshold I-HyperparameterName
to O
be O
16 B-HyperparameterValue
and O
3 B-HyperparameterValue
for O
BioRED B-DatasetName
and O
DocRED B-DatasetName
, O
respectively O
, O
and O
use O
the O
top O
5,000 O
entity O
pairs O
in O
pretraining O
. O

Besides O
, O
as O
MTB B-MethodName
is O
fully O
self O
- O
supervised O
, O
the O
information O
of O
whether O
two O
relations O
mentions O
correspond O
to O
the O
same O
relation O
type O
is O
not O
available O
, O
but O
it O
is O
assumed O
that O
at O
least O
entity O
pairs O
with O
different O
subject O
or O
object O
types O
are O
likely O
to O
be O
of O
different O
relation O
types O
and O
can O
therefore O
be O
used O
as O
negative O
pairs O
. O
Such O
use O
of O
entity O
types O
to O
filter O
the O
pairs O
has O
indeed O
been O
shown O
a O
strong O
feature O
for O
RE B-TaskName
( O
Zhong O
and O
Chen O
, O
2021 O
; O
. O
We O
only O
use O
two O
entity O
pairs O
with O
different O
subject O
or O
object O
entity O
types O
as O
negatives O
. O
While O
the O
entity O
type O
based O
filtering O
may O
also O
discard O
some O
hard O
negatives O
, O
our O
experiment O
( O
see O
Section O
C O
) O
shows O
improved O
results O
, O
meaning O
that O
its O
benefits O
outweigh O
the O
disadvantages O
. O

B O
Adaptation O
to O
Multi B-TaskName
- I-TaskName
label I-TaskName
RE I-TaskName

It O
is O
noteworthy O
that O
in O
some O
RE B-TaskName
tasks O
, O
such O
as O
DocRED B-DatasetName
, O
one O
entity O
pair O
may O
have O
multiple O
relation O
labels O
, O
in O
which O
case O
the O
cross O
- O
entropy O
loss O
does O
not O
apply O
. O
Therefore O
, O
for O
multi O
- O
label O
scenarios O
, O
we O
substitute O
cross O
- O
entropy O
loss O
( O
also O
the O
softmax O
in O
MCCL B-MethodName
) O
with O
the O
adaptive O
thresholding O
loss O
proposed O
by O
. O
Specifically O
, O
denote O
the O
logits O
as O
l O
( O
the O
input O
to O
softmax O
in O
crossentropy O
loss O
) O
, O
the O
set O
of O
positive O
relations O
as O
P O
( O
except O
NA O
) O
, O
and O
the O
set O
of O
the O
remaining O
relations O
except O
for O
NA O
as O
N O
, O
the O
adaptive O
thresholding O
loss O
is O
formulated O
as O
: O

L O
1 O
= O
− O
r∈P O
log O
e O
lr O
r O
′ O
∈P∪ O
{ O
NA O
} O
e O
l O
r O
′ O
, O
L O
2 O
= O
− O
log O
e O
lNA O
r O
′ O
∈N O
∪ O
{ O
NA O
} O
e O
l O
r O
′ O
, O
L O
at O
= O
L O
1 O
+ O
L O
2 O
. O

This O
loss O
encourages O
the O
logits O
of O
positive O
relations O
to O
be O
higher O
than O
NA O
, O
and O
the O
logits O
of O
other O
relations O
to O
be O
lower O
than O
NA O
. O
In O
prediction O
, O
the O
model O
returns O
the O
relations O
with O
higher O
logits O
than O
NA O
as O
predictions O
, O
or O
return O
NA O
if O
none O
of O
such O
relations O
exist O
. O

C O
More O
Experiments O

Performance O
w.r.t O
. O
number O
of O
proxies O
. O
We O
evaluate O
MCCL B-MethodName
with O
different O
number O
of O
proxies O
. O
When O
no O
proxy O
is O
used O
, O
we O
ignore O
the O
relations O
that O
do O
not O
appear O
in O
the O
current O
batch O
. O

The O
F B-MetricName
1 I-MetricName
on O
both O
BioRED B-DatasetName
and O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
in O
the O
1 O
% O
low O
- O
resource O
setting O
is O
shown O
in O
Figure O
4 O
, O
indicating O
that O
adding O
proxies O
improves O
F B-MetricName
1 I-MetricName
significantly O
on O
both O
datasets O
. O
Using O
one O
proxy O
for O
each O
relation O
achieves O
an O
increase O
of O
6.0 B-MetricValue
% I-MetricValue
in O
F B-MetricName
1 I-MetricName
on O
BioRED B-DatasetName
, O
and O
a O
larger O
increase O
of O
10.2 B-MetricValue
% I-MetricValue
in O
F B-MetricName
1 I-MetricName
on O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
. O
Such O
a O
difference O
of O
increment O
is O
due O
to O
the O
fact O
that O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
is O
more O
longtailed O
, O
where O
97 O
% O
of O
instances O
are O
NA O
compared O
to O
80 O
% O
in O
BioRED B-DatasetName
. O
We O
also O
observe O
that O
adding O
more O
proxies O
achieves O
similar O
or O
even O
worse O
results O
. O
These O
results O
make O
sense O
as O
the O
proxies O
are O
mainly O
in O
the O
place O
of O
long O
- O
tail O
relations O
that O
do O
not O
appear O
in O
the O
batch O
, O
and O
these O
relations O
contain O
too O
few O
instances O
to O
form O
multiple O
clusters O
. O

Coarse O
- O
to O
- O
fine O
evaluation O
. O
To O
give O
another O
illustration O
of O
showing O
that O
MCCL B-MethodName
learns O
multiple O
clusters O
, O
we O
experiment O
with O
it O
on O
1 O
% O
of O
BioRED B-DatasetName
in O
a O
coarse O
- O
to O
- O
fine O
setting O
. O
Specifically O
, O
we O
merge O
all O
relations O
except O
NA O
into O
one O
relation O
in O
finetuning O
, O
and O
apply O
kNN B-MethodName
inference I-MethodName
using O
the O
original O
labels O
. O
We O
find O
that O
MCCL B-MethodName
achieves O
an O
F1 B-MetricName
of O
30.3 B-MetricValue
% I-MetricValue
, O
which O
is O
even O
better O
than O
CE B-MethodName
with O
all O
relations O
provided O
. O
However O
, O
if O
we O
remove O
the O
instance O
weights O
in O
MCCL B-MethodName
to O
degrade O
it O
to O
onecluster O
, O
the O
F B-MetricName
1 I-MetricName
constantly O
degrades O
in O
finetuning O
. O
It O
shows O
that O
multi O
- O
cluster O
assumption O
helps O
preserve O
the O
fine O
- O
grained O
relation O
information O
in O
pretrained O
representation O
. O
Other O
ablation O
studies O
. O
We O
analyze O
the O
effectiveness O
of O
entity O
type O
filtering O
in O
Section O
A. O
Results O
are O
shown O
in O
Table O
5 O
. O
Removing O
entity O
type O
filtering O
degrades O
performance O
significantly O
. O
It O
shows O
that O
entity O
type O
filtering O
can O
remove O
a O
lot O
of O
false O
negatives O
in O
pretraining O
and O
greatly O
improves O
the O
pretrained O
model O
. O
Besides O
, O
as O
the O
main O
results O
have O
demonstrated O
the O
effectiveness O
of O
MCCL B-MethodName
in O
finetuning O
, O
we O
wonder O
whether O
MCCL B-MethodName
can O
also O
lead O
to O
improved O
pretraining O
. O
To O
do O
so O
, O
we O
replace O
the O
InfoNCE O
loss O
in O
Eq O
. O
( O
1 O
) O
by O
MCCL B-MethodName
and O
regard O
different O
entity O
pairs O
as O
different O
classes O
. O
The O
results O
are O
comparable O
or O
slightly O
worse O
in O
contrast O
to O
using O
L O
rel O
, O
showing O
that O
the O
multi O
- O
cluster O
assumption O
of O
MCCL B-MethodName
does O
not O
necessarily O
help O
pretraining O
. O
In O
section O
4 O
B2 O
. O
Did O
you O
discuss O
the O
license O
or O
terms O
for O
use O
and O
/ O
or O
distribution O
of O
any O
artifacts O
? O

In O
section O
4 O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O

In O
section O
4 O
B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Not O
applicable O
. O
Left O
blank O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
In O
section O
4 O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
In O
section O
4 O
C O
Did O
you O
run O
computational O
experiments O
? O

In O
section O
4 O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
In O
section O
4 O

Appendices O
A O
Data O
Preparation O

We O
acquire O
positive O
and O
negative O
pairs O
from O
the O
document O
corpus O
. O
We O
regard O
two O
entity O
pairs O
( O
e O
s O
1 O
, O
e O
o O
1 O
) O
, O
( O
e O
s O
2 O
, O
e O
o O
2 O
) O
in O
different O
documents O
as O
a O
positive O
pair O
if O
they O
share O
the O
same O
subject O
and O
object O
entities O
, O
respectively O
( O
i.e. O
, O
e O
s O
1 O
= O
e O
s O
2 O
, O
e O
o O
1 O
= O
e O
o O
2 O
) O
, O
and O
otherwise O
negative O
. O

However O
, O
for O
a O
large O
corpus O
, O
the O
number O
of O
such O
positive O
pairs O
is O
enormous O
. O
For O
instance O
, O
in O
biomedical O
RE O
pretraining O
, O
we O
extract O
37 O
billion O
positive O
pairs O
in O
total O
. O
Using O
all O
these O
pairs O
in O
pretraining O
is O
computationally O
infeasible O
. O
Therefore O
, O
we O
select O
positive O
pairs O
as O
follows O
. O
Denote O
the O
number O
of O
documents O
mentioning O
an O
entity O
e O
or O
an O
entity O
pair O
( O
e O
s O
, O
e O
o O
) O
as O
N O
( O
e O
) O
and O
N O
( O
e O
s O
, O
e O
o O
) O
, O
respectively O
, O
we O
use O
two O
metrics O
, O
frequency O
= O
N O
( O
e O
s O
, O
e O
o O
) O

Introduction O

Task O
Selection O

Probing O
Baselines O

Experiment O
setup O

Main O
Results O
and O
Discussion O

Quantitative O
results O
. O
Table O
2 O
summarizes O
the O
quantitative O
results O
for O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
. O
We O
include O
additional O
results O
and O
discussion O
for O
text B-MethodName
- I-MethodName
davinci-003 I-MethodName
, O
PaLM B-MethodName
and O
Flan B-MethodName
- I-MethodName
PaLM I-MethodName
in O
Appendix O
A.3 O
. O
LLMs O
can O
achieve O
surprisingly O
high O
performance O
when O
provided O
with O
invalid O
reasoning O
steps O
for O
the O
demonstrations O
( O
1 O
) O
. O
In O
particular O
, O
under O
Inter B-MetricName
. I-MetricName
Recall I-MetricName
/ O
Inter B-MetricName
. I-MetricName
F1 I-MetricName
, O
i.e. O
, O
intrinsic B-MetricName
evaluation I-MetricName
, O
which O
is O
arguably O
a O
more O
faithful O
measurement O
of O
the O
rationale O
quality O
( O
§ O
3.3 O
) O
, O
all O
LLMs O
we O
tested O
can O
retain O
over O
90 B-MetricValue
% I-MetricValue
of O
the O
performance O
achieved O
under O
CoT B-MethodName
prompting I-MethodName
. O

Relevance O
matters O
more O
than O
coherence O
for O
bridging O
objects O
. O
Providing O
incoherent O
bridging O
objects O
( O
2 O
) O
achieves O
better O
performance O
than O
providing O
irrelevant O
bridging O
objects O
( O
3 O
) O
, O
especially O
on O
the O
more O
challenging O
GSM8 B-DatasetName
K I-DatasetName
dataset O
( O
39.2 B-MetricValue
v.s. O
26.2 B-MetricValue
Inter B-MetricName
. I-MetricName
F1 I-MetricName
) O
. O
which O
indicates O
that O
it O
is O
important O
for O
the O
bridging O
objects O
to O
be O
relevant O
, O
but O
not O
as O
important O
to O
have O
them O
in O
the O
right O
order O
to O
guide O
the O
LLM O
along O
the O
reasoning O
process O
. O
We O
quantitatively O
measure O
the O
coverage O
of O
bridging O
objects O
from O
the O
query O
for O
the O
generated O
rationales O
, O
and O
find O
that O
the O
settings O
with O
no O
relevance O
for O
bridging O
objects O
( O
3 O
, O
7 O
) O
do O
have O
significantly O
lower O
coverage B-MetricName
( O
below O
60 B-MetricValue
% I-MetricValue
) O
than O
other O
settings O
( O
around O
80 B-MetricValue
% I-MetricValue
) O
. O

We O
use O
Recall B-MetricName
/ O
F1 B-MetricName
of O
the O
bridging O
objects O
as O
the O
metrics O
for O
intrinsic O
evaluation O
of O
the O
generated O
rationales O
. O
While O
the O
metrics O
do O
n't O
take O
into O
account O
the O
quality O
of O
the O
language O
templates O
, O
we O
examine O
the O
predicted O
rationales O
for O
20 O
random O
examples O
under O
each O
setting O
we O
tested O
except O
standard O
prompting O
( O
which O
does O
not O
generate O
any O
rationale O
) O
, O
and O
find O
that O
for O
all O
the O
examples O
, O
whenever O
the O
LLM O
reaches O
a O
correct O
bridging O
object O
, O
the O
corresponding O
language O
template O
within O
the O
step O
is O
also O
correct O
. O
This O
suggests O
that O
overall O
, O
the O
correctness O
of O
bridging O
objects O
is O
a O
very O
good O
indicator O
of O
the O
quality O
of O
the O
reasoning O
steps O
. O

We O
present O
a O
data O
- O
and O
training O
- O
efficient O
approach O
to O
build O
a O
multilingual O
VLP O
model O
mCLIP B-MethodName
, O
by O
aligning O
the O
pretrained O
monolingual O
VLP O
model O
CLIP O
and O
a O
multilingual O
text O
encoder O
XLM O
- O
R O
to O
the O
same O
multimodal O
multilingual O
space O
. O
Despite O
the O
strong O
multimodal O
and O
multilingual O
abilities O
inherited O
from O
both O
models O
, O
the O
proposed O
mCLIP O
also O
inherits O
the O
societal O
impacts O
including O
some O
negative O
ones O
of O
the O
original O
CLIP O
and O
XLM O
- O
R O
, O
e.g. O
, O
societal O
biases O
( O
Radford O
et O
al O
. O
, O
2021 O
) O
and O
misuse O
of O
language O
models O
( O
Tamkin O
et O
al O
. O
, O
2021 O
) O
. O
The O
implicit O
biases O
are O
expected O
to O
be O
removed O
by O
debiasing O
either O
the O
dataset O
or O
the O
model O
( O
Meade O
et O
al O
. O
, O
2022 O
; O
Zhou O
et O
al O
. O
, O
2022 O
) O
. O
Besides O
, O
our O
proposed O
method O
makes O
it O
simpler O
to O
retrieve O
malicious O
or O
offensive O
content O
( O
Welbl O
et O
al O
. O
, O
2021 O
) O
Then O
we O
perform O
the O
TriKD O
in O
Section O
3.2 O
. O
We O
use O
the O
LAMB O
optimizer B-HyperparameterName
( O
You O
et O
al O
. O
, O
2020 O
) O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
linearly O
warmed O
up O
to O
0.01 B-HyperparameterValue
within O
the O
first O
500 B-HyperparameterValue
steps B-HyperparameterName
and O
then O
decayed O
to O
0 B-HyperparameterValue
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
16,384 B-HyperparameterValue
. O
The O
temperature B-HyperparameterName
for O
the O
ITC O
loss O
is O
initialized O
as O
0.07 B-HyperparameterValue
and O
then O
learned O
by O
gradient O
descent O
, O
while O
the O
temperature B-HyperparameterName
of O
TTC O
loss O
is O
fixed O
as O
0.07 B-HyperparameterValue
( O
Jain O
et O
al O
. O
, O
2021 O
) O
. O
The O
models O
are O
pretrained O
for O
15 B-HyperparameterValue
epochs B-HyperparameterName
when O
the O
smaller O
dataset O
CC3 B-DatasetName
M I-DatasetName
is O
used O
, O
while O
for O
3 B-HyperparameterValue
epochs B-HyperparameterName
when O
CC12 B-DatasetName
M I-DatasetName
is O
used O
. O

For O
all O
experiments O
in O
all O
pretraining O
and O
finetuning O
stages O
, O
we O
use O
the O
inverse O
square O
root O
learning B-HyperparameterName
rate I-HyperparameterName
scheduler O
and O
conduct O
experiments O
on O
8 O
NVIDIA O
V100 O
GPUs O
. O
We O
use O
the O
same O
dropout B-HyperparameterName
method O
as O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
. O
The O
dropout B-HyperparameterName
ratios I-HyperparameterName
are O
set O
as O
0.3 B-HyperparameterValue
. O
The O
detailed O
hyperparameters O
of O
different O
stages O
are O
listed O
in O
Table O
9 O
. O

This O
project O
was O
supported O
by O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62106138 O
) O
and O
Shanghai O
Sailing O
Program O
( O
No O
. O
21YF1412100 O
) O
. O
Jia O
Pan O
and O
Wenping O
Wang O
are O
partially O
supported O
by O
Centre O
for O
Transformative O
Garment O
Production O
. O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedbacks O
on O
this O
work O
. O

