Do O
Trajectories O
Encode O
Verb O
Meaning O
? O

Distributional O
models O
learn O
representations O
of O
words O
from O
text O
, O
but O
are O
criticized O
for O
their O
lack O
of O
grounding O
, O
or O
the O
linking O
of O
text O
to O
the O
nonlinguistic O
world O
. O
Grounded O
language O
models O
have O
had O
success O
in O
learning O
to O
connect O
concrete O
categories O
like O
nouns O
and O
adjectives O
to O
the O
world O
via O
images O
and O
videos O
, O
but O
can O
struggle O
to O
isolate O
the O
meaning O
of O
the O
verbs O
themselves O
from O
the O
context O
in O
which O
they O
typically O
occur O
. O
In O
this O
paper O
, O
we O
investigate O
the O
extent O
to O
which O
trajectories O
( O
i.e. O
the O
position O
and O
rotation O
of O
objects O
over O
time O
) O
naturally O
encode O
verb O
semantics O
. O
We O
build O
a O
procedurally O
generated O
agent O
- O
object O
- O
interaction O
dataset O
, O
obtain O
human O
annotations O
for O
the O
verbs O
that O
occur O
in O
this O
data O
, O
and O
compare O
several O
methods O
for O
representation O
learning O
given O
the O
trajectories O
. O
We O
find O
that O
trajectories O
correlate O
as O
- O
is O
with O
some O
verbs O
( O
e.g. O
, O
fall O
) O
, O
and O
that O
additional O
abstraction O
via O
self O
- O
supervised O
pretraining O
can O
further O
capture O
nuanced O
differences O
in O
verb O
meaning O
( O
e.g. O
, O
roll O
vs. O
slide O
) O
. O

Introduction O

While O
large O
distributional O
language O
models O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
GPT O
( O
Radford O
, O
2020 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
have O
had O
empirical O
success O
in O
deriving O
representations O
of O
words O
and O
sentences O
from O
large O
text O
corpora O
, O
most O
of O
these O
models O
lack O
grounding O
, O
or O
a O
connection O
between O
the O
words O
and O
their O
real O
- O
world O
referents O
. O
Grounding B-TaskName
, O
in O
addition O
to O
being O
necessary O
for O
multimodal O
tasks O
like O
video O
recognition O
, O
has O
been O
argued O
to O
lie O
at O
the O
core O
of O
language O
understanding O
( O
Bender O
and O
Koller O
, O
2020 O
) O
. O
Work O
on O
grounded O
language O
learning O
associates O
language O
with O
the O
non O
- O
linguistic O
world O
, O
typically O
by O
learning O
from O
large O
- O
scale O
image O
( O
Bruni O
et O
al O
. O
, O
2011 O
) O
or O
video O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
datasets O
. O

Much O
prior O
work O
on O
language B-TaskName
grounding I-TaskName
has O
focused O
on O
concrete O
nouns O
( O
objects O
) O
and O
adjectives O
( O
attributes O
) O
, O
which O
are O
captured O
well O
by O
patterns O
of O
pixels O
. O
Verbs O
, O
however O
, O
have O
received O
less O
attention O
, O
despite O
being O
essential O
for O
building O
models O
that O
can O
interact O
in O
realistic O
3D O
environments O
( O
Shridhar O
et O
al O
. O
, O
2020a O
; O
Bisk O
et O
al O
. O
, O
2020 O
) O
. O
Verbs O
are O
especially O
challenging O
to O
model O
, O
given O
that O
they O
take O
place O
over O
time O
. O
Image O
and O
video O
data O
alone O
is O
insufficient O
to O
fully O
capture O
verb O
semantics O
, O
as O
demonstrated O
by O
prior O
work O
( O
Yatskar O
et O
al O
. O
, O
2016 O
) O
, O
in O
many O
cases O
failing O
to O
isolate O
the O
meaning O
of O
the O
verb O
from O
context O
in O
which O
it O
typically O
occurs O
. O
For O
example O
, O
Chao O
et O
al O
. O
2018 O
show O
that O
an O
image O
of O
a O
person O
laying O
in O
the O
snow O
next O
to O
a O
snowboard O
is O
labeled O
" O
standing O
on O
a O
snowboard O
" O
. O
Moreover O
, O
recent O
work O
has O
introduced O
datasets O
and O
benchmarks O
based O
on O
situated O
3D O
environments O
( O
Gan O
et O
al O
. O
, O
2020 O
; O
Deitke O
et O
al O
. O
, O
2020 O
; O
Ebert O
and O
Pavlick O
, O
2020 O
; O
Shridhar O
et O
al O
. O
, O
2020a O
) O
that O
demonstrate O
the O
challenges O
of O
learning O
task O
- O
oriented O
behavior O
, O
which O
demands O
a O
combination O
of O
object B-TaskName
and I-TaskName
verb I-TaskName
grounding I-TaskName
. O

In O
this O
paper O
, O
we O
test O
the O
hypothesis O
that O
the O
semantics O
of O
( O
concrete O
) O
verbs O
are O
grounded O
in O
the O
3D O
trajectories O
of O
objects O
: O
i.e. O
, O
the O
absolute O
and O
relative O
paths O
objects O
take O
through O
3D O
space O
. O
We O
investigate O
if O
and O
when O
verb O
meanings O
appear O
to O
be O
a O
product O
of O
raw O
perception O
of O
objects O
in O
3D O
space O
, O
and O
when O
differentiating O
verb O
meanings O
requires O
additional O
abstraction O
and O
representation O
beyond O
what O
is O
available O
via O
direct O
perception O
. O
To O
study O
this O
, O
we O
collect O
a O
clean O
dataset O
of O
3D O
object O
trajectories O
in O
simulation O
. O
We O
collect O
human O
descriptions O
of O
these O
perceived O
world O
dynamics O
, O
i.e. O
, O
to O
determine O
whether O
or O
not O
a O
given O
event O
constitutes O
a O
fall O
or O
a O
tumble O
. O
We O
then O
propose O
a O
self O
- O
supervised O
pretraining O
approach O
, O
whereby O
we O
train O
a O
time O
- O
series O
prediction O
model O
to O
obtain O
representations O
of O
trajectories O
in O
a O
3D O
environment O
without O
any O
linguistic O
input O
. O
We O
evaluate O
the O
learned O
representations O
on O
how O
well O
they O
encode O
verb O
semantics O
for O
specific O
verbs O
. O
We O
show O
that O
the O
pretrained O
model O
learns O
to O
represent O
events O
in O
a O
way O
that O
aligns O
well O
with O
the O
meaning O
of O
English O
verbs O
, O
e.g. O
differentiating O
slide O
from O
roll O
. O
In O
summary O
, O
our O
primary O
contributions O
are O
: O

1 O
. O
We O
introduce O
a O
new O
, O
clean O
dataset O
of O
3D O
object O
trajectories O
paired O
with O
human O
judgments O
about O
whether O
or O
not O
each O
trajectory O
falls O
within O
the O
extension O
of O
each O
of O
24 O
different O
verbs O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
dataset O
of O
its O
kind O
, O
and O
provides O
a O
valuable O
resource O
for O
empirical O
studies O
of O
lexical O
semantics O
. O
Our O
data O
is O
available O
at O
https O
: O

Related O
Work O

Grounded O
Language O
with O
Deep O
Learning O
. O

Our O
contributions O
add O
to O
a O
large O
body O
of O
work O
on O
grounded O
representation O
learning O
. O
Much O
of O
this O
work O
augments O
language O
modeling O
objectives O
with O
images O
( O
Silberer O
and O
Lapata O
, O
2012 O
; O
Lazaridou O
et O
al O
. O
, O
2015 O
; O
Kiela O
et O
al O
. O
, O
2017 O
) O
and O
videos O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
. O
In O
this O
work O
, O
we O
focus O
on O
representations O
that O
encode O
verb O
semantics O
. O
Prior O
work O
on O
verb O
learning O
has O
been O
conducted O
in O
the O
computer O
vision O
community O
, O
typically O
described O
as O
" O
humanobject O
interactions O
" O
( O
Regneri O
et O
al O
. O
, O
2013 O
; O
Chao O
et O
al O
. O
, O
2018 O
; O
Ji O
et O
al O
. O
, O
2019 O
) O
. O
Most O
closely O
related O
to O
our O
approach O
, O
which O
focuses O
on O
trajectory O
data O
, O
is O
work O
on O
learning O
affordances O
for O
human O
- O
robot O
communication O
. O
For O
example O
, O
Kalkan O
et O
al O
. O
( O
2014 O
) O
; O
Ugur O
et O
al O
. O
( O
2009 O
) O
learn O
affordance O
representations O
based O
on O
the O
state O
changes O
of O
objects O
, O
but O
do O
not O
encode O
the O
full O
trajectory O
between O
states O
. O
Also O
related O
is O
work O
in O
grounded O
language O
in O
text O
- O
only O
models O
which O
investigates O
models O
ability O
to O
reason O
about O
objects O
through O
space O
and O
time O
( O
Aroca O
- O
Ouellette O
et O
al O
. O
, O
2021 O
) O
. O
Outside O
of O
NLP O
, O
models O
have O
been O
trained O
on O
trajectory O
data O
for O
applications O
like O
human O
motion O
path O
forecasting O
( O
Giuliari O
et O
al O
. O
, O
2021 O
) O
and O
human O
activity O
recognition O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
Our O
work O
lies O
at O
the O
intersection O
of O
grounded O
language O
learning O
and O
spatiotemporal O
machine O
learning O
, O
using O
representations O
of O
trajectory O
data O
to O
study O
verb O
semantics O
. O

Grounding O
and O
Lexical O
Semantics O
. O
Prior O
work O
in O
formal O
semantics O
attempts O
to O
build O
feature O
- O
based O
representations O
of O
verb O
meaning O
in O
terms O
of O
the O
3D O
trajectories O
and O
state O
transitions O
entailed O
by O
those O
verbs O
( O
Pustejovsky O
and O
Krishnaswamy O
, O
2014 O
; O
Siskind O
, O
2001 O
; O
Steedman O
, O
2002 O
) O
. O
Such O
work O
is O
related O
more O
generally O
to O
the O
idea O
of O
mental O
simulation O
as O
a O
means O
for O
representing O
and O
reasoning O
about O
linguistic O
concepts O
( O
Feldman O
, O
2008 O
; O
Bergen O
et O
al O
. O
, O
2007 O
; O
Bergen O
, O
2012 O
) O
. O
We O
view O
our O
contribution O
as O
consistent O
with O
and O
complementary O
to O
this O
formal O
semantics O
program O
. O
While O
the O
prior O
work O
has O
sought O
to O
codify O
the O
precise O
truth O
conditions O
of O
motion O
verbs O
, O
we O
investigate O
whether O
such O
representations O
could O
emerge O
organically O
from O
datadriven O
processes O
. O

While O
we O
focus O
on O
concrete O
verbs O
in O
this O
paper O
, O
other O
work O
has O
argued O
that O
motor O
processing O
and O
mental O
simulation O
plays O
a O
more O
general O
role O
in O
language O
processing O
. O
For O
example O
, O
Gärdenfors O
( O
2019 O
) O
makes O
a O
case O
for O
grounded O
distributional O
" O
conceptual O
spaces O
" O
as O
the O
foundation O
for O
modeling O
linguistic O
concepts O
. O
Dorr O
and O
Olsen O
( O
2018 O
) O
discusses O
the O
role O
of O
metaphor O
in O
modeling O
abstract O
uses O
of O
words O
like O
push O
. O
Borghi O
and O
Riggio O
( O
2009 O
) O
argues O
for O
the O
notion O
of O
a O
" O
motor O
prototype O
" O
as O
a O
key O
component O
of O
recognizing O
and O
processing O
objects O
, O
and O
Mazzuca O
et O
al O
. O
( O
2021 O
) O
presents O
evidence O
that O
the O
sensorimotor O
system O
( O
in O
particular O
the O
interactive O
aspects O
) O
drive O
acquisition O
of O
abstract O
concepts O
. O

Dataset O
3.1 O
Overview O

To O
carry O
out O
the O
proposed O
study O
, O
we O
require O
a O
dataset O
that O
contains O
continuous O
3D O
recordings O
of O
an O
agent O
interacting O
with O
an O
object O
. O
While O
our O
representation O
learning O
methods O
will O
not O
use O
linguistic O
supervision O
, O
we O
require O
verb O
labels O
in O
order O
to O
evaluate O
our O
models O
. O
Thus O
, O
in O
our O
data O
, O
we O
require O
that O
each O
recording O
is O
annotated O
with O
verbs O
describing O
the O
motion O
of O
the O
object O
. O
For O
example O
, O
if O
the O
agent O
throws O
a O
bouncy O
ball O
across O
a O
room O
, O
we O
'd O
expect O
the O
recording O
to O
be O
annotated O
with O
a O
verb O
sequence O
such O
as O
be O
thrown O
, O
fall O
, O
bounce O
, O
bounce O
, O
bounce O
, O
roll O
, O
stop O
. O

To O
produce O
such O
data O
, O
we O
build O
a O
simple O
Markovian O
agent O
which O
interacts O
with O
a O
variety O
of O
objects O
in O
a O
3D O
virtual O
environment O
. O
We O
record O
the O
result O
- O
Figure O
1 O
: O
The O
Simulated B-DatasetName
Spatial I-DatasetName
Dataset O
consists O
of O
procedurally O
generated O
motion O
data O
of O
a O
virtual O
agent O
interacting O
with O
an O
object O
. O
In O
this O
sequence O
the O
agent O
( O
red O
sphere O
) O
pushes O
the O
object O
( O
blue O
sphere O
) O
. O
At O
t=0 O
and O
t=1 O
, O
the O
agent O
approaches O
the O
ball O
. O
Then O
, O
in O
t=2 O
and O
t=3 O
, O
the O
agent O
pushes O
to O
ball O
. O
Finally O
, O
at O
t=4 O
, O
the O
ball O
is O
rolling O
away O
from O
the O
agent O
. O

ing O
trajectory O
of O
the O
object O
and O
then O
, O
using O
crowdsourcing O
, O
ask O
humans O
to O
determine O
which O
verbs O
could O
accurately O
describe O
which O
portions O
of O
the O
object O
's O
movement O
. O
An O
example O
sequence O
from O
the O
dataset O
is O
shown O
in O
Figure O
1 O
. O

Data O
Generation O
and O
Terminology O

In O
this O
section O
we O
provide O
details O
on O
how O
we O
generate O
the O
data O
, O
and O
introduce O
terminology O
that O
will O
be O
used O
throughout O
the O
rest O
of O
the O
paper O
. O

Environment O
. O
The O
dataset O
is O
generated O
in O
Unity O
, O
a O
game O
engine O
seeing O
increased O
use O
by O
researchers O
( O
Deitke O
et O
al O
. O
, O
2020 O
; O
Gan O
et O
al O
. O
, O
2020 O
) O
for O
its O
accessible O
rendering O
and O
physics O
simulation O
via O
the O
underlying O
Nvidia O
PhysX O
physics O
engine O
. O
The O
dataset O
and O
simulation O
source O
code O
are O
publicly O
available O
. O
1 O
Trajectory O
data O
. O
We O
define O
trajectory O
data O
as O
the O
position O
and O
rotation O
of O
entities O
in O
space O
, O
represented O
with O
three O
- O
dimensional O
XY O
Z O
coordinates O
and O
four O
- O
dimensional O
XY O
ZW O
quaternions O
respectively O
. O
We O
choose O
to O
focus O
on O
only O
these O
features O
, O
ignoring O
other O
possibilities O
like O
object O
shape O
or O
identity O
, O
in O
order O
to O
focus O
on O
learning O
generalizable O
aspects O
of O
verb O
semantics O
that O
are O
independent O
of O
the O
object O
. O

Sessions O
. O
The O
dataset O
is O
generated O
in O
3 O
- O
minute O
continuous O
segments O
we O
refer O
to O
as O
sessions O
. O
Within O
each O
session O
, O
several O
parameters O
are O
randomized O
, O
including O
object O
shape O
, O
mass O
, O
drag O
, O
friction O
, O
and O
bounciness O
. O

Action O
Primitives O
. O
The O
data O
generation O
is O
driven O
by O
a O
Markov O
Chain O
with O
a O
set O
of O
randomly O
parameterized O
action O
primitives O
. O
In O
this O
Markov O
Chain O
, O
the O
States O
are O
whether O
the O
object O
is O
Held O
, O
On O
- O
Counter O
and O
OnGround O
. O
The O
transitions O
between O
these O
states O
are O
action O
primitives O
like O
PickUp O
, O
Put O
- O
Down O
, O
or O
Throw O
. O
For O
example O
, O
when O
the O
object O
is O
in O
the O
state O
OnCounter O
, O
the O
agent O
may O
execute O
a O
PickUp O
, O
after O
which O
the O
object O
is O
Held O
. O
These O
action O
primitives O
, O
combined O
with O
the O
physics O
of O
the O
objects O
( O
e.g. O
, O
their O
shape O
, O
mass O
, O
friction O
, O
bounciness O
, O
etc O
) O
are O
intended O
to O
produce O
a O
wide O
range O
of O
object O
motions O
corresponding O
to O
a O
range O
of O
verbs O
, O
and O
we O
do O
not O
expect O
that O
the O
primitives O
will O
map O
directly O
to O
the O
verbs O
that O
one O
would O
use O
to O
describe O
the O
resulting O
object O
behavior O
. O
For O
example O
, O
when O
we O
simulate O
a O
Throw O
primitive O
, O
the O
result O
might O
be O
that O
the O
object O
flies O
across O
the O
room O
, O
hits O
the O
wall O
, O
falls O
to O
the O
floor O
, O
and O
bounces O
until O
it O
comes O
to O
a O
rest O
. O
We O
parameterize O
the O
execution O
of O
each O
action O
with O
action O
- O
specific O
parameters O
, O
e.g. O
the O
force O
of O
a O
throw O
. O
The O
combination O
of O
session O
- O
and O
actionlevel O
parameters O
can O
result O
in O
a O
wide O
variety O
of O
object O
motion O
from O
each O
primitive O
action O
. O
A O
full O
description O
of O
the O
parameters O
for O
each O
action O
can O
be O
found O
in O
Appendix O
A O
. O

Verbs O
. O
We O
highlight O
a O
distinction O
between O
action O
primitives O
and O
the O
high O
- O
level O
actions O
or O
verbs O
that O
emerge O
from O
them O
. O
For O
example O
, O
if O
the O
object O
is O
pushed O
, O
it O
may O
then O
slide O
, O
bounce O
, O
roll O
, O
tumble O
, O
or O
any O
combination O
thereof O
. O
We O
refer O
to O
all O
of O
these O
as O
verbs O
, O
though O
only O
push O
is O
an O
action O
primitive O
. O
We O
highlight O
this O
distinction O
because O
we O
are O
most O
interested O
in O
studying O
the O
nuanced O
verbs O
that O
emerge O
from O
the O
simulation O
, O
rather O
than O
the O
action O
primitives O
that O
drive O
it O
explicitly O
. O

Frames O
. O
Our O
atomic O
unit O
is O
frames O
, O
also O
referred O
to O
as O
timesteps O
, O
which O
represent O
a O
single O
point O
in O
time O
. O
Our O
dataset O
is O
collected O
at O
60 B-HyperparameterValue
fps B-HyperparameterName
, O
or O
10,800 B-HyperparameterValue
frames B-HyperparameterName
per I-HyperparameterName
session I-HyperparameterName
. O
For O
each O
frame O
, O
we O
record O
the O
position O
and O
rotation O
of O
the O
object O
, O
as O
well O
as O
the O
position O
of O
the O
agent O
. O
This O
is O
sufficient O
to O
reconstruct O
and O
render O
the O
scene O
from O
an O
arbitrary O
perspective O
as O
needed O
. O
We O
choose O
this O
high O
framerate O
because O
it O
's O
relatively O
fast O
and O
inexpensive O
to O
rapidly O
produce O
trajectory O
data O
, O
which O
can O
be O
subsampled O
as O
needed O
for O
rendering O
or O
modeling O
. O

Crowdsourced O
Annotation O

We O
collect O
labels O
for O
which O
verbs O
occur O
in O
the O
data O
, O
and O
when O
they O
occur O
. O
To O
do O
this O
, O
we O
extract O
short O
clips O
from O
the O
dataset O
, O
and O
ask O
crowdworkers O
to O
provide O
binary O
judgments O
on O
whether O
the O
clip O
falls O
in O
the O
extension O
of O
the O
verb O
. O

Clips O
. O
We O
extract O
short O
clips O
from O
the O
dataset O
using O
Hierarchical O
Dynamic O
Clustering O
with O
Motion O
energy O
- O
based O
pooling O
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
, O
a O
selfsupervised O
action O
segmentation O
framework O
that O
can O
be O
summarized O
as O
follows O
: O

1 O
. O
The O
3D O
space O
is O
divided O
into O
clusters O
using O
the O
provided O
trajectory O
data O
. O
The O
framework O
uses O
Hierarchical O
Dynamic O
Clustering O
, O
which O
is O
similar O
to O
k O
- O
means O
but O
shown O
to O
outperform O
it O
on O
human O
motion O
parsing O
tasks O
. O
2 O
. O
A O
sliding O
window O
is O
applied O
to O
the O
cluster O
labels O
for O
a O
given O
positional O
sequence O
. O
The O
number O
of O
transitions O
between O
clusters O
in O
a O
window O
are O
defined O
as O
its O
motion O
energy O
. O
3 O
. O
The O
subsequent O
motion O
energy O
curve O
is O
smoothed O
using O
a O
Gaussian O
kernel O
with O
a O
tuned O
smoothing O
factor O
. O
4 O
. O
The O
peaks O
of O
the O
motion O
energy O
curve O
are O
considered O
motion O
segments O
, O
with O
lengths O
varying O
with O
respect O
to O
the O
width O
of O
the O
peak O
. O

This O
algorithm O
is O
shown O
to O
perform O
well O
on O
human O
motion O
parsing O
, O
which O
we O
find O
transfers O
well O
to O
our O
dataset O
when O
applied O
to O
object O
position O
. O
This O
yields O
easily O
identifiable O
patterns O
of O
motion O
, O
e.g. O
from O
the O
time O
the O
object O
is O
thrown O
to O
when O
it O
slows O
to O
a O
stop O
. O
We O
find O
that O
, O
in O
contrast O
to O
a O
random O
sliding O
window O
, O
this O
approach O
avoids O
cutting O
clips O
in O
the O
middle O
of O
salient O
patterns O
of O
motion O
. O

In O
our O
case O
, O
a O
disadvantage O
of O
this O
approach O
is O
that O
the O
extracted O
segments O
are O
variable O
- O
length O
. O
To O
simplify O
our O
pipeline O
, O
we O
filter O
to O
only O
segments O
of O
length O
72 O
to O
96 O
, O
then O
crop O
the O
segment O
to O
length O
90 O
, O
or O
1.5 O
seconds O
. O
We O
call O
each O
1.5s O
segment O
a O
clip O
. O
We O
choose O
this O
length O
to O
make O
the O
clip O
as O
short O
as O
possible O
to O
avoid O
crowdworker O
fatigue O
, O
but O
give O
sufficient O
time O
for O
a O
human O
observer O
to O
recognize O
what O
's O
happening O
. O

Verbs O
. O
We O
produce O
24 O
queries O
, O
each O
corresponding O
to O
a O
verb O
, O
e.g. O
Does O
the O
object O
bounce O
? O
To O
do O
this O
, O
the O
authors O
curate O
a O
list O
of O
24 O
verbs O
2 O
of O
interest O
which O
are O
likely O
to O
occur O
in O
the O
simulated O
data O
and O
range O
from O
general O
descriptions O
( O
e.g. O
, O
fall O
) O
to O
more O
subtle O
descriptions O
of O
object O
motion O
( O
e.g. O
, O
tumble O
) O
. O
When O
asking O
annotators O
whether O
a O
verb O
applies O
to O
a O
clip O
, O
we O
always O
frame O
the O
question O
with O
the O
object O
as O
the O
subject O
. O
That O
is O
, O
when O
a O
carry O
event O
occurs O
, O
annotators O
are O
asked O
" O
is O
the O
object O
carried O
" O
. O

We O
then O
consider O
every O
possible O
( O
clip O
, O
query O
) O
pair O
a O
potential O
crowdsourcing O
task O
. O
We O
apply O
conservative O
heuristics O
to O
filter O
out O
( O
clip O
, O
query O
) O
pairs O
that O
are O
guaranteed O
to O
have O
a O
negative O
label O
. O
For O
example O
, O
if O
the O
Held O
state O
was O
never O
present O
in O
a O
clip O
, O
we O
do O
n't O
ask O
if O
the O
object O
is O
carried O
. O
This O
results O
in O
approximately O
110k O
tasks O
, O
from O
which O
we O
sample O
100 O
tasks O
per O
query O
, O
for O
a O
total O
2400 O
crowdsourcing O
tasks O
, O
such O
as O
the O
one O
shown O
in O
Figure O
2 O
. O

Labels O
. O
For O
each O
crowdsourcing O
task O
, O
we O
obtain O
responses O
from O
five O
workers O
, O
then O
take O
the O
majority O
response O
as O
the O
label O
for O
that O
clip O
. O
The O
same O
clip O
is O
shown O
for O
all O
applicable O
queries O
, O
resulting O
in O
a O
supervised O
dataset O
of O
24 O
- O
dimensional O
vectors O
, O
representing O
binary O
verb O
labels O
for O
each O
clip O
. O
3 O
The O
dataset O
and O
all O
unaggregated O
annotations O
are O
available O
for O
download O
. O
4 O
Figure O
3 O
: O
Crowd O
annotation O
agreement O
by O
verb O
. O
Workers O
agree O
most O
on O
when O
verbs O
of O
gravity O
occur O
, O
such O
as O
fall O
, O
drop O
, O
bounce O
, O
and O
least O
on O
when O
verbs O
of O
rotation O
occur O
, O
i.e. O
turn O
, O
spin O
, O
tip O
. O

Dataset O
Analysis O

In O
this O
section O
, O
we O
analyze O
trends O
in O
the O
dataset O
annotations O
, O
including O
worker O
agreement O
, O
and O
comparisons O
between O
semantically O
related O
verbs O
. O

Agreement O

Annotation O
agreement O
on O
a O
clip O
is O
the O
proportion O
of O
responses O
that O
match O
the O
majority O
label O
for O
that O
clip O
. O
Figure O
3 O
shows O
annotation O
agreement O
by O
verb O
. O
A O
noticeable O
trend O
is O
that O
agreement O
is O
higher O
for O
particular O
semantic O
categories O
. O
Specifically O
, O
verbs O
that O
involve O
gravity O
, O
i.e. O
fall O
, O
fall O
off O
, O
drop O
, O
and O
bounce O
have O
higher O
agreement O
. O
On O
the O
other O
hand O
, O
verbs O
of O
rotation O
, O
i.e. O
turn O
, O
spin O
, O
tip O
, O
flip O
have O
lower O
agreement O
, O
alongside O
abstract O
verbs O
start O
and O
stop O
. O
For O
start O
in O
particular O
, O
we O
even O
received O
feedback O
from O
crowdworkers O
that O
they O
were O
n't O
sure O
whether O
the O
object O
started O
moving O
during O
the O
clip O
or O
not O
. O

Co O
- O
occurrence O

Figure O
4 O
shows O
co O
- O
occurrence O
: O
specifically O
, O
given O
that O
a O
clip O
is O
labeled O
by O
at O
least O
one O
worker O
as O
verb O
v O
1 O
, O
how O
often O
is O
it O
labeled O
by O
other O
workers O
as O
verb O
v O
2 O
? O
Co O
- O
occurrence O
allows O
us O
to O
answer O
questions O
like O
how O
often O
is O
a O
toss O
considered O
a O
throw O
? O
and O
vice O
- O
versa O
. O
We O
highlight O
some O
interesting O
verb O
relationships O
. O

General O
co O
- O
occurrence O
. O
Verb O
co O
- O
occurrence O
is O
high O
in O
general O
. O
The O
average O
number O
of O
verbs O
used O
to O
describe O
a O
given O
clip O
is O
4 O
( O
where O
a O
verb O
is O
considered O
" O
used O
" O
if O
at O
least O
three O
workers O
use O
it O
) O
. O
This O
highlights O
the O
challenge O
of O
verb O
learning O
, O
as O
opposed O
to O
more O
concrete O
nouns O
and O
adjectives O
. O
Verbs O
are O
applicable O
to O
a O
wide O
variety O
of O
behavior O
, O
even O
if O
it O
is O
n't O
a O
prototypical O
instance O
of O
that O
verb O
. O

Lexical O
entailments O
. O
All O
dogs O
are O
animals O
but O
not O
all O
animals O
are O
dogs O
. O
These O
types O
of O
semantic O
containment O
relationships O
are O
also O
ascribed O
to O
verbs O
. O
Analyzing O
our O
collected O
data O
, O
in O
some O
cases O
, O
we O
observe O
the O
opposite O
of O
what O
's O
expected O
. O
For O
example O
, O
according O
to O
WordNet O
( O
Fellbaum O
, O
2010 O
) O
, O
toss O
is O
a O
type O
of O
throw O
. O
However O
, O
using O
the O
majority O
labels O
, O
we O
find O
throws O
to O
be O
annotated O
as O
tosses O
more O
often O
tosses O
than O
are O
annotated O
as O
throws O
. O
That O
is O
, O
p O
( O
toss|throw O
) O
= O
.67 O
< O
p O
( O
throw|toss O
) O
= O
.75 O
. O

Frequent O
co O
- O
occurrences O
. O
Hit O
, O
push O
, O
and O
bump O
stand O
out O
as O
the O
most O
frequently O
co O
- O
occurring O
verbs O
, O
having O
over O
90 O
% O
co O
- O
occurrence O
with O
each O
other O
. O
These O
likewise O
occur O
when O
many O
other O
verbs O
do O
, O
but O
not O
reciprocally O
. O
For O
example O
, O
most O
slaps O
are O
hits O
, O
but O
only O
41 O
% O
of O
hits O
are O
slaps O
. O
In O
many O
cases O
, O
this O
can O
be O
explained O
by O
other O
verbs O
being O
immediately O
preceded O
by O
the O
agent O
making O
contact O
with O
the O
object O
, O
which O
gets O
labeled O
hit O
, O
push O
, O
and O
bump O
. O

Fine O
- O
grained O
distinctions O
. O
Workers O
distinguish O
roll O
from O
slide O
-only O
50 O
% O
of O
rolls O
are O
also O
considered O
slides O
, O
and O
vice O
- O
versa O
. O
This O
validates O
that O
verbs O
with O
similar O
trajectories O
, O
which O
may O
be O
challenging O
for O
models O
, O
are O
indeed O
differentiated O
by O
humans O
. O
Additionally O
, O
verbs O
with O
similar O
but O
nuanced O
meanings O
are O
differentiated O
. O
For O
example O
, O
tip O
, O
tumble O
, O
fall O
over O
, O
and O
topple O
tend O
to O
co O
- O
occur O
around O
70 O
- O
80 O
% O
of O
the O
time O
. O
These O
also O
fall O
into O
" O
verbs O
of O
rotation O
" O
category O
, O
which O
have O
the O
lowest O
annotator O
agreement O
. O
noise O
. O

Experiments O

Our O
hypothesis O
is O
that O
representation O
learning O
in O
the O
3D O
visuospatial O
world O
( O
without O
language O
supervision O
) O
can O
yield O
concept O
representations O
that O
align O
to O
English O
verb O
semantics O
- O
i.e. O
can O
the O
representations O
capture O
nuanced O
distinctions O
like O
throw O
vs. O
toss O
or O
slide O
vs. O
roll O
? O
To O
test O
this O
, O
we O
pretrain O
a O
self O
- O
supervised O
model O
on O
a O
time O
- O
series O
prediction O
task O
, O
and O
then O
use O
a O
perceptron O
classifier O
to O
evaluate O
its O
learned O
representations O
. O

We O
evaluate O
four O
approaches O
, O
described O
in O
detail O
below O
. O
First O
, O
we O
train O
a O
simple O
perceptron B-MethodName
to O
evaluate O
the O
representational O
capacity O
of O
the O
trajectory O
data O
as O
- O
is O
, O
as O
a O
comparative O
baseline O
. O
Second O
, O
we O
train O
a O
fully B-MethodName
supervised I-MethodName
model O
to O
determine O
a O
soft O
upper O
bound O
on O
the O
task O
without O
pretraining O
. O
Third O
, O
we O
evaluate O
our O
self O
- O
supervised O
model O
. O
And O
finally O
, O
we O
fine O
- O
tune O
the O
self O
- O
supervised O
model O
to O
determine O
an O
upper O
bound O
with O
pretraining O
. O

Experimental O
Setup O

For O
all O
approaches O
, O
we O
evaluate O
representation O
quality O
with O
a O
multi O
- O
way O
verb O
classification O
task O
. O
Specif O
- O
ically O
, O
we O
predict O
the O
verb O
labels O
for O
the O
1.5s O
clips O
gathered O
through O
the O
crowdsourcing O
task O
described O
in O
Section O
3.3 O
. O

Each O
input O
sample O
X O
t O
1 O
.. O
90 O
is O
a O
90x10 O
matrix O
of O
position O
and O
rotation O
data O
, O
corresponding O
to O
90 O
frames O
per O
clip O
and O
10 O
spatial O
features O
5 O
per O
frame O
. O
The O
output O
Y O
is O
a O
24 O
- O
dimensional O
multi O
- O
hot O
vector O
indicating O
the O
whether O
each O
of O
our O
24 O
verb O
classes O
apply O
to O
the O
clip O
. O

Approaches O

Perceptron B-MethodName
. O
We O
wish O
to O
evaluate O
the O
representational O
capacity O
of O
the O
raw O
trajectory O
data O
itself O
. O
To O
do O
so O
, O
we O
train O
a O
single O
24 O
- O
dimensional O
dense O
layer O
with O
sigmoid O
activation O
, O
equivalent O
to O
a O
perceptron O
for O
each O
class O
. O
While O
very O
simple O
, O
this O
approach O
gives O
an O
idea O
of O
how O
well O
trajectory O
data O
represents O
verbs O
as O
- O
is O
, O
and O
provides O
a O
naive O
comparative O
baseline O
against O
which O
to O
evaluate O
our O
more O
complex O
pretraining O
techniques O
. O

Fully B-MethodName
Supervised I-MethodName
. O
The O
fully B-MethodName
supervised I-MethodName
approach O
is O
similar O
to O
the O
perceptron B-MethodName
, O
but O
adds O
a O
dense O
layer O
and O
LSTM O
layer O
in O
- O
between O
. O
This O
is O
equiv O
- O
Figure O
5 O
: O
Our O
pretraining O
setup O
. O
During O
pretraining O
, O
the O
model O
learns O
to O
encode O
and O
represent O
input O
timesteps O
for O
time O
- O
series O
prediction O
. O
To O
evaluate O
these O
learned O
representations O
, O
a O
perceptron O
probe O
is O
trained O
on O
the O
LSTM O
outputs O
, O
without O
propagating O
gradients O
to O
the O
pretrained O
model O
. O
alent O
to O
the O
model O
shown O
in O
Figure O
5 O
, O
but O
trained O
end O
- O
to O
- O
end O
without O
pretraining O
. O
The O
purpose O
of O
this O
approach O
is O
to O
provide O
an O
upper O
bound O
to O
the O
experimental O
setup O
without O
pretraining O
. O

Self B-MethodName
- I-MethodName
supervised I-MethodName
Pretraining I-MethodName
. O
To O
evaluate O
the O
capacity O
of O
self O
- O
supervised O
models O
to O
represent O
trajectory O
data O
, O
we O
pretrain O
a O
time O
- O
series O
prediction O
model O
on O
a O
large O
unlabeled O
dataset O
of O
400k O
sessions O
. O
That O
is O
, O
given O
n O
input O
frames O
X O
t O
1 O
.. O
n O
, O
the O
model O
is O
trained O
to O
predict O
k O
output O
frames O
Y O
t O
n+1 O
.. O
n+k O
. O
The O
model O
consists O
of O
a O
dense O
layer O
followed O
by O
an O
LSTM O
layer O
unrolled O
k O
timesteps O
, O
as O
shown O
in O
Figure O
5 O
. O
We O
use O
a O
discounted O
mean O
squared O
error O
loss O
as O
shown O
in O
Equation O
1 O
, O
which O
discounts O
loss O
by O
how O
far O
it O
is O
into O
the O
future O
by O
factor O
γ O
. O

γMSE O
= O
n+k O
t O
= O
n O
γ O
t O
( O
y O
t O
−ŷ O
t O
) O
2 O

( O
1 O
) O

We O
tune O
discount B-HyperparameterName
factor I-HyperparameterName
γ B-HyperparameterName
, O
output B-HyperparameterName
length I-HyperparameterName
k B-HyperparameterName
, O
model B-HyperparameterName
width I-HyperparameterName
, O
and O
batch B-HyperparameterName
size I-HyperparameterName
using O
a O
grid O
search O
on O
validation O
performance O
, O
resulting O
in O
values O
of O
0.85 B-HyperparameterValue
, O
60 B-HyperparameterValue
, O
128 B-HyperparameterValue
, O
and O
1024 B-HyperparameterValue
, O
respectively O
. O
Input B-HyperparameterName
length I-HyperparameterName
n B-HyperparameterName
is O
fixed O
at O
90 B-HyperparameterValue
to O
match O
the O
length O
of O
clips O
. O
We O
consider O
the O
concatenated O
LSTM O
outputs O
as O
the O
representation O
of O
a O
clip O
. O
To O
evaluate O
this O
representation O
compared O
to O
raw O
trajectory O
data O
, O
we O
freeze O
the O
weights O
of O
the O
pretrained O
model O
and O
, O
as O
when O
evaluating O
the O
raw O
trajectory O
data O
, O
train O
a O
perceptron O
for O
each O
class O
. O
Fine O
- O
tuning O
. O
To O
provide O
an O
upper O
bound O
for O
our O
experimental O
setup O
with O
pretraining O
, O
we O
fine O
- O
tune O
the O
self O
- O
supervised O
model O
. O
This O
is O
the O
same O
as O
the O
previous O
approach O
, O
but O
allows O
the O
gradients O
in O
the O
perceptron O
step O
to O
pass O
through O
the O
entire O
model O
. O

Results O

We O
report O
Mean B-MetricName
Average I-MetricName
Precision I-MetricName
on O
unseen O
test O
data O
for O
each O
approach O
in O
Table O
1 O
. O
We O
compare O
these O
to O
random O
stratified O
predictions O
that O
are O
based O
on O
the O
class O
distribution O
of O
the O
training O
data O
. O

Perceptron B-MethodName
. O
The O
perceptron B-MethodName
approach O
evaluates O
the O
representational O
capacity O
of O
raw O
trajectory O
data O
as O
- O
is O
, O
with O
a O
lower O
bound O
of O
random O
stratified O
and O
soft O
upper O
bound O
of O
fully B-MethodName
supervised I-MethodName
. O
The O
perceptron O
performs O
relatively O
well O
for O
its O
simplicity O
, O
being O
only O
7 O
points O
below O
the O
fully O
supervised O
upper O
bound O
. O
This O
suggests O
that O
the O
trajectory O
data O
itself O
encodes O
a O
significant O
amount O
of O
verb O
meaning O
, O
but O
leaves O
plenty O
of O
room O
for O
improvement O
. O
, O
the O
fully B-MethodName
supervised I-MethodName
model O
( O
which O
represents O
a O
soft O
upper O
bound O
on O
how O
well O
one O
can O
do O
given O
raw O
state O
information O
without O
any O
pretraining O
) O
is O
visualized O
as O
0 O
, O
and O
all O
other O
models O
are O
visualized O
relative O
to O
that O
. O
Human B-MethodName
performance O
is O
inner O
- O
annotator O
agreement O
( O
% O
of O
workers O
who O
agree O
on O
the O
majority O
label O
) O
. O

Self B-MethodName
- I-MethodName
supervised I-MethodName
pretraining I-MethodName
. O
The O
pretraining B-MethodName
+ I-MethodName
probe I-MethodName
approach O
evaluates O
the O
ability O
of O
selfsupervised O
models O
to O
encode O
verb O
meaning O
from O
trajectory O
data O
. O
This O
is O
equivalent O
to O
the O
perceptron B-MethodName
approach O
, O
but O
with O
learned O
hidden O
representations O
as O
input O
rather O
than O
raw O
trajectory O
data O
. O
The O
pretrained O
model O
does O
outperform O
the O
perceptron O
, O
as O
well O
as O
the O
fully O
supervised O
approach O
. O
Fine B-MethodName
- I-MethodName
tuning I-MethodName
only O
improves O
on O
this O
slightly O
, O
highlighting O
that O
self B-MethodName
- I-MethodName
supervised I-MethodName
pretraining I-MethodName
can O
yield O
representations O
that O
successfully O
encode O
verb O
meaning O
. O

Breakdown O
by O
verb O
. O
Figure O
6 O
shows O
a O
comparison O
of O
average O
precision O
for O
each O
verb O
. O
There O
are O
some O
patterns O
worth O
highlighting O
. O
In O
particular O
, O
we O
can O
categorize O
verbs O
into O
three O
main O
groups O
: O
trivial O
, O
tractable O
, O
and O
hard O
. O

Trivial O
verbs O
are O
verbs O
that O
can O
are O
wellrepresented O
by O
trajectory O
data O
as O
- O
is O
, O
i.e. O
those O
with O
high O
performance O
with O
the O
perceptron O
approach O
. O
These O
include O
fall O
, O
fall O
off O
, O
fall O
over O
and O
pick O
up O
. O
6 O
. O
Many O
of O
these O
have O
high O
agreement O
, O
and O
may O
be O
explained O
by O
the O
object O
's O
change O
in O
height O
. O

Tractable O
verbs O
are O
those O
that O
see O
significant O
benefit O
from O
pretraining O
, O
including O
slide O
, O
roll O
, O
throw O
, O
toss O
, O
put O
down O
, O
turn O
, O
flip O
, O
and O
stop O
. O
An O
intuition O
behind O
this O
is O
that O
these O
verbs O
involve O
manner O
distinctions O
, O
and O
in O
particular O
, O
rotations O
of O
the O
object O
relative O
to O
itself O
. O
Such O
information O
does O
n't O
fall O
directly O
out O
of O
raw O
state O
descriptions O
, O
but O
is O
likely O
to O
be O
well O
modeled O
by O
a O
pretraining O
objective O
that O
tries O
to O
predict O
the O
object O
's O
future O
position O
. O

Hard O
verbs O
are O
those O
with O
low O
performance O
that O
do O
n't O
benefit O
much O
from O
pretraining O
. O
These O
include O
bounce O
, O
drop O
, O
tip O
, O
topple O
, O
and O
spin O
. O
Many O
of O
these O
are O
verbs O
which O
have O
lower O
agreement O
. O
Bounce O
, O
slap O
and O
spin O
appear O
to O
benefit O
a O
bit O
from O
both O
pretraining O
and O
fine O
- O
tuning O
, O
suggesting O
that O
they O
may O
be O
tractable O
with O
similar O
but O
more O
robust O
pretraining O
. O
Tip O
and O
topple O
have O
fairly O
high O
performance O
, O
and O
may O
almost O
be O
categorized O
as O
trivial O
, O
perhaps O
being O
explained O
by O
the O
object O
's O
change O
in O
rotation O
. O
However O
, O
they O
are O
noticeably O
lower O
than O
other O
trivial O
verbs O
, O
despite O
seeing O
no O
benefit O
from O
pretraining O
, O
suggesting O
that O
there O
is O
nuance O
to O
their O
meaning O
in O
the O
dataset O
, O
which O
is O
n't O
captured O
by O
any O
approach O
. O
Finally O
, O
drop O
is O
a O
great O
example O
of O
a O
hard O
verb O
, O
as O
it O
is O
similar O
to O
trivial O
verbs O
like O
fall O
. O
However O
, O
drop O
involves O
interaction O
between O
the O
agent O
and O
object O
that O
is O
highly O
agreed O
upon O
by O
annotators O
, O
but O
does O
n't O
appear O
to O
be O
captured O
by O
our O
approaches O
, O
despite O
the O
model O
receiving O
both O
object O
and O
agent O
data O
. O
More O
challenging O
examples O
may O
be O
able O
to O
unveil O
a O
similar O
story O
for O
other O
verbs O
of O
interaction O
like O
pick O
up O
and O
put O
down O
. O

Discussion O
and O
Conclusion O

We O
test O
the O
hypothesis O
that O
verb O
meanings O
can O
be O
grounded O
in O
3D O
trajectories O
, O
i.e. O
, O
the O
position O
of O
objects O
over O
time O
. O
Specifically O
, O
we O
investigate O
the O
extent O
to O
which O
representations O
of O
object O
trajectories O
, O
learned O
without O
any O
linguistic O
supervision O
, O
naturally O
encode O
concepts O
that O
align O
to O
English O
verb O
semantics O
. O
Our O
primary O
contributions O
are O
twofold O
. O
First O
, O
we O
build O
a O
procedurally O
generated O
agent O
- O
object O
- O
interaction O
dataset O
for O
which O
we O
collect O
crowdsourced O
annotations O
. O
This O
is O
the O
first O
dataset O
of O
its O
kind O
, O
and O
provides O
a O
rich O
inventory O
of O
human O
judgments O
about O
the O
extensions O
of O
24 O
verbs O
of O
motion O
. O
Second O
, O
we O
compare O
a O
variety O
of O
representation O
learning O
approaches O
, O
specifically O
contrasting O
approaches O
which O
operate O
directly O
on O
perceptual O
inputs O
to O
approaches O
which O
learn O
abstractions O
over O
the O
raw O
perception O
( O
via O
pretraining O
) O
. O
We O
find O
that O
some O
verbs O
meanings O
( O
e.g. O
, O
fall O
and O
push O
) O
are O
captured O
easily O
by O
the O
raw O
state O
information O
, O
while O
others O
( O
e.g. O
, O
roll O
and O
turn O
) O
require O
additional O
processing O
to O
be O
represented O
well O
. O
This O
work O
is O
a O
first O
step O
toward O
exploring O
ways O
to O
capture O
fine O
- O
grained O
distinctions O
in O
grounded O
verb O
semantics O
that O
are O
trivial O
for O
humans O
, O
but O
challenging O
for O
models O
. O
Recent O
benchmarks O
at O
the O
intersection O
of O
NLP O
, O
vision O
and O
robotics O
( O
Deitke O
et O
al O
. O
, O
2020 O
; O
Shridhar O
et O
al O
. O
, O
2020b O
) O
illuminate O
unsolved O
challenges O
in O
AI O
that O
demand O
a O
more O
robust O
understanding O
of O
verb O
semantics O
and O
spatial O
reasoning O
. O
As O
these O
benchmarks O
continue O
to O
be O
developed O
, O
and O
rich O
multimodal O
datasets O
from O
technologies O
like O
virtual O
reality O
become O
increasingly O
abundant O
, O
we O
envision O
that O
future O
work O
in O
this O
vein O
will O
be O
especially O
relevant O
. O

In O
the O
future O
, O
we O
plan O
to O
explore O
more O
sophisticated O
models O
for O
self O
- O
supervised O
pretraining O
, O
and O
evaluate O
how O
well O
these O
models O
transfer O
to O
more O
naturalistic O
language O
learning O
settings O
( O
Ebert O
and O
Pavlick O
, O
2020 O
) O
. O
Beyond O
this O
, O
there O
is O
a O
large O
body O
of O
related O
research O
questions O
to O
be O
explored O
. O
For O
example O
, O
can O
representations O
of O
trajectory O
data O
be O
fused O
with O
visually O
- O
grounded O
representations O
to O
yield O
better O
encodings O
of O
verb O
semantics O
? O
Collaborative O
efforts O
will O
be O
key O
to O
addressing O
these O
next O
milestones O
in O
natural O
language O
understanding O
. O

Acknowledgments O

This O
work O
was O
supported O
by O
the O
DARPA O
GAILA O
program O
. O
Thank O
you O
to O
George O
Konidaris O
, O
Carsten O
Eickhoff O
, O
Roman O
Feiman O
, O
Jack O
Merullo O
, O
Charles O
Lovering O
, O
Gabor O
Brody O
, O
and O
the O
members O
of O
the O
Brown O
LUNAR O
lab O
for O
helpful O
discussion O
and O
feedback O
. O

A O
Dataset O
parameters O

The O
following O
tables O
describe O
the O
session O
- O
level O
and O
action O
- O
level O
parameters O
for O
our O
procedural O
data O
generation O
protocol O
described O
in O
Section O
3.2 O
. O
Friction O
when O
object O
is O
not O
moving O
( O
0 O
, O
1 O
) O
Bounciness O
Energy O
retained O
on O
bounce O
( O
0 O
, O
1 O
) O

Peek O
Across O
: O
Improving O
Multi O
- O
Document O
Modeling O
via O
Cross B-TaskName
- I-TaskName
Document I-TaskName
Question I-TaskName
- I-TaskName
Answering I-TaskName

The O
integration O
of O
multi O
- O
document O
pre O
- O
training O
objectives O
into O
language O
models O
has O
resulted O
in O
remarkable O
improvements O
in O
multi O
- O
document O
downstream O
tasks O
. O
In O
this O
work O
, O
we O
propose O
extending O
this O
idea O
by O
pre O
- O
training O
a O
generic O
multi O
- O
document O
model O
from O
a O
novel O
crossdocument O
question O
answering O
pre O
- O
training O
objective O
. O
To O
that O
end O
, O
given O
a O
set O
( O
or O
cluster O
) O
of O
topically O
- O
related O
documents O
, O
we O
systematically O
generate O
semantically O
- O
oriented O
questions O
from O
a O
salient O
sentence O
in O
one O
document O
and O
challenge O
the O
model O
, O
during O
pre O
- O
training O
, O
to O
answer O
these O
questions O
while O
" O
peeking O
" O
into O
other O
topically O
- O
related O
documents O
. O
In O
a O
similar O
manner O
, O
the O
model O
is O
also O
challenged O
to O
recover O
the O
sentence O
from O
which O
the O
question O
was O
generated O
, O
again O
while O
leveraging O
cross O
- O
document O
information O
. O
This O
novel O
multidocument O
QA O
formulation O
directs O
the O
model O
to O
better O
recover O
cross O
- O
text O
informational O
relations O
, O
and O
introduces O
a O
natural O
augmentation O
that O
artificially O
increases O
the O
pre O
- O
training O
data O
. O
Further O
, O
unlike O
prior O
multi O
- O
document O
models O
that O
focus O
on O
either O
classification O
or O
summarization O
tasks O
, O
our O
pre O
- O
training O
objective O
formulation O
enables O
the O
model O
to O
perform O
tasks O
that O
involve O
both O
short O
text O
generation O
( O
e.g. O
, O
QA B-TaskName
) O
and O
long O
text O
generation O
( O
e.g. O
, O
summarization B-TaskName
) O
. O
Following O
this O
scheme O
, O
we O
pre O
- O
train O
our O
model O
-termed O
QAMDEN B-MethodName
-and O
evaluate O
its O
performance O
across O
several O
multi O
- O
document O
tasks O
, O
including O
multi B-TaskName
- I-TaskName
document I-TaskName
QA I-TaskName
, O
summarization B-TaskName
, O
and O
query B-TaskName
- I-TaskName
focused I-TaskName
summarization I-TaskName
, O
yielding O
improvements O
of O
up O
to O
7 B-MetricValue
% I-MetricValue
, O
and O
significantly O
outperforms O
zero B-MethodName
- I-MethodName
shot I-MethodName
GPT-3.5 I-MethodName
and O
GPT-4 B-MethodName
. O
1 O
* O
Work O
partly O
done O
as O
an O
intern O
at O
AI2 O
. O

Introduction O

Among O
recent O
NLP O
research O
, O
multi O
- O
document O
processing O
is O
gaining O
increasing O
attention O
, O
due O
to O
the O
need O
to O
handle O
and O
process O
an O
increasing O
amount O
of O
textual O
data O
and O
available O
documents O
online O
. O
A O
Figure O
1 O
: O
Illustration O
of O
our O
pre O
- O
training O
and O
data O
generation O
. O
Per O
a O
considered O
set O
of O
related O
documents O
( O
1 O
) O
which O
we O
split O
into O
context O
documents O
( O
2 O
) O
and O
a O
held O
- O
out O
document O
( O
3 O
) O
, O
we O
select O
the O
most O
salient O
sentence O
( O
4 O
) O
that O
is O
used O
for O
generating O
a O
question O
- O
answer O
pair O
( O
5 O
) O
. O
Then O
, O
we O
pre O
- O
train O
a O
model O
by O
generating O
the O
proper O
answer O
and O
the O
salient O
sentence O
, O
given O
the O
question O
and O
the O
context O
documents O
( O
6 O
) O
. O
number O
of O
prominent O
applications O
that O
are O
concerned O
with O
aggregating O
information O
from O
multiple O
texts O
are O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
( O
Fabbri O
et O
al O
. O
, O
2019 O
; O
, O
query B-TaskName
- I-TaskName
focused I-TaskName
multidocument I-TaskName
summarization I-TaskName
( O
Xu O
and O
Lapata O
, O
2020 O
; O
Pasunuru O
et O
al O
. O
, O
2021a O
) O
, O
and O
multi B-TaskName
- I-TaskName
hop I-TaskName
question I-TaskName
answering I-TaskName
( O
Yang O
et O
al O
. O
, O
2018 O
; O
Welbl O
et O
al O
. O
, O
2018 O
) O
. O
These O
tasks O
remain O
challenging O
mostly O
since O
existing O
NLP O
models O
are O
designed O
to O
handle O
single O
texts O
, O
rather O
than O
processing O
multiple O
documents O
at O
once O
. O

Early O
solutions O
for O
multi O
- O
text O
processing O
were O
task O
- O
specific O
and O
used O
complex O
architectures O
that O
were O
difficult O
to O
generalize O
across O
different O
multidocument O
tasks O
( O
Liu O
and O
Lapata O
, O
2019 O
; O
Ginzburg O
et O
al O
. O
, O
2021 O
) O
. O
Efficient O
LMs O
( O
Tay O
et O
al O
. O
, O
2021 O
; O
Beltagy O
et O
al O
. O
, O
2020 O
) O
recently O
demonstrated O
that O
by O
simply O
concatenating O
multiple O
documents O
into O
a O
single O
sequence O
, O
the O
transformer O
can O
offload O
the O
goal O
of O
identifying O
and O
connecting O
relevant O
information O
between O
the O
documents O
. O
Recently O
, O
it O
was O
suggested O
that O
these O
long O
- O
context O
LMs O
can O
be O
equipped O
with O
new O
pre O
- O
training O
objectives O
to O
enable O
them O
to O
process O
multiple O
documents O
more O
effectively O
Xiao O
et O
al O
. O
, O
2022 O
; O
Yasunaga O
et O
al O
. O
, O
2022 O
) O
. O

These O
pre O
- O
trained O
models O
demonstrated O
state O
- O
ofthe O
- O
art O
performance O
on O
a O
variety O
of O
multi O
- O
document O
downstream O
tasks O
, O
and O
outperformed O
underlying O
LMs O
and O
task O
- O
specific O
architectures O
. O
Such O
models O
are O
often O
pre O
- O
trained O
using O
a O
dataset O
where O
each O
instance O
is O
a O
set O
of O
related O
documents O
( O
e.g. O
, O
news O
articles O
all O
discussing O
a O
specific O
event O
) O
, O
which O
facilitates O
modeling O
of O
cross O
- O
text O
relationships O
. O
Existing O
multi O
- O
document O
pre O
- O
training O
objectives O
involve O
unmasking O
tokens O
in O
a O
document O
, O
or O
generating O
a O
salient O
masked O
sentence O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Xiao O
et O
al O
. O
, O
2022 O
) O
, O
encouraging O
the O
model O
to O
recover O
missing O
information O
using O
other O
documents O
. O
While O
successful O
, O
these O
models O
are O
either O
limited O
to O
classification O
tasks O
or O
primarily O
designed O
for O
summarization B-TaskName
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Xiao O
et O
al O
. O
, O
2022 O
) O
. O

In O
this O
work O
, O
we O
propose O
a O
novel O
pre O
- O
training O
objective O
that O
supports O
both O
short B-TaskName
and I-TaskName
long I-TaskName
text I-TaskName
generation I-TaskName
, O
resulting O
in O
a O
versatile O
and O
general O
multidocument O
language O
model O
. O
In O
particular O
, O
we O
hypothesize O
that O
using O
questions O
and O
answers O
involving O
multiple O
documents O
can O
encourage O
the O
model O
to O
better O
learn O
and O
incorporate O
both O
fine O
- O
grained O
information O
( O
by O
asking O
questions O
about O
core O
information O
units O
in O
a O
specific O
sentence O
) O
as O
well O
as O
coarsegrained O
cross O
- O
document O
relationships O
required O
to O
generate O
a O
long O
text O
such O
as O
a O
summary O
. O
We O
show O
that O
this O
approach O
holds O
not O
only O
for O
summarization B-TaskName
, O
but O
for O
other O
multi O
- O
document O
downstream O
tasks O
as O
well O
. O

During O
the O
pre O
- O
training O
of O
existing O
multidocument O
language O
models O
, O
the O
goal O
is O
to O
unmask O
spans O
( O
for O
encoder O
- O
only O
models O
) O
or O
generate O
masked O
textual O
spans O
( O
for O
encoder O
- O
decoder O
models O
) O
under O
a O
multi O
- O
document O
context O
. O
To O
that O
end O
, O
multiple O
concatenated O
sequences O
of O
related O
documents O
are O
fed O
during O
pre O
- O
training O
, O
thus O
requiring O
a O
large O
number O
of O
sets O
of O
related O
documents O
for O
an O
effective O
pre O
- O
training O
phase O
( O
Hoffmann O
et O
al O
. O
, O
2022 O
) O
. O
In O
a O
variety O
of O
existing O
multi O
- O
document O
benchmarks O
, O
such O
as O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
, O
only O
small O
to O
medium O
- O
scale O
document O
clusters O
are O
readily O
available O
. O
These O
are O
acquired O
either O
automatically O
with O
lexical O
similarity O
and O
retrieval O
( O
Fabbri O
et O
al O
. O
, O
2019 O
) O
or O
semi O
- O
automatically O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
, O
but O
generally O
, O
this O
process O
requires O
a O
substantial O
amount O
of O
human O
effort O
for O
filtering O
instances O
and O
generating O
high O
quality O
corpora O
. O

By O
employing O
a O
novel O
multi O
- O
document O
question O
- O
answer O
generation O
procedure O
, O
we O
propose O
an O
effective O
method O
for O
expanding O
the O
multi O
- O
document O
pre O
- O
training O
corpora O
. O
Our O
approach O
allows O
us O
to O
provide O
multiple O
views O
for O
every O
single O
cluster O
of O
documents O
, O
thereby O
artificially O
increasing O
the O
pretraining O
data O
size O
( O
in O
terms O
of O
number O
of O
instances O
) O
via O
augmentation O
. O
To O
expose O
the O
model O
to O
a O
variety O
of O
contexts O
and O
diversify O
the O
pre O
- O
training O
data O
, O
we O
propose O
to O
generate O
multiple O
pairs O
of O
questions O
and O
answers O
and O
condition O
them O
on O
a O
subset O
of O
the O
documents O
' O
cluster O
. O
We O
select O
a O
salient O
sentence O
in O
one O
held O
- O
out O
document O
and O
then O
employ O
a O
recent O
parser O
to O
generate O
a O
high O
- O
quality O
question O
- O
answer O
pair O
about O
one O
predicate O
in O
the O
selected O
sentence O
, O
using O
a O
systematic O
semantically O
- O
oriented O
approach O
( O
Klein O
et O
al O
. O
, O
2022 O
) O
. O
This O
new O
multi O
- O
document O
pre O
- O
training O
objective O
challenges O
the O
model O
to O
generate O
both O
the O
answer O
to O
the O
question O
as O
well O
as O
the O
salient O
sentence O
, O
while O
discarding O
the O
held O
- O
out O
document O
or O
parts O
of O
it O
( O
see O
Figures O
1 O
, O
2 O
for O
illustration O
) O
. O
This O
procedure O
exposes O
the O
model O
to O
a O
variety O
of O
contexts O
-a O
question O
and O
a O
different O
subset O
of O
the O
documents O
in O
the O
cluster O
per O
instance O
, O
in O
contrast O
to O
prior O
methods O
that O
provide O
only O
a O
single O
view O
of O
the O
cluster O
. O
Our O
contributions O
are O
summarized O
below O
: O

• O
A O
new O
pre O
- O
training O
approach O
for O
multidocument O
modeling O
, O
formulated O
as O
a O
crossdocument B-TaskName
question I-TaskName
answering I-TaskName
task O
, O
further O
directing O
the O
LM O
to O
model O
cross O
- O
text O
relationships O
, O
focusing O
on O
both O
fine O
- O
and O
coarsegrained O
information O
. O

Related O
Work O

Long O
- O
context O
efficient O
text O
generation O
transformers O
( O
Tay O
et O
al O
. O
, O
2021 O
( O
Tay O
et O
al O
. O
, O
, O
2022 O
extend O
earlier O
transformer O
models O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
for O
processing O
long O
sequences O
, O
often O
using O
a O
sparse O
self O
- O
attention O
architecture O
. O
Examples O
include O
the O
Longformer B-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
( O
LED B-MethodName
) O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
, O
and O
LongT5 B-MethodName
. O
These O
models O
demonstrated O
that O
single O
- O
text O
approaches O
be O
can O
adapted O
to O
multi O
- O
document O
tasks O
by O
concatenat O
- O
ing O
multiple O
documents O
into O
a O
single O
sequence O
and O
processing O
them O
using O
their O
sparse O
attention O
patterns O
. O
They O
sparsify O
the O
full O
self O
- O
attention O
matrix O
of O
transformers O
by O
using O
a O
combination O
of O
a O
localized O
sliding O
window O
( O
called O
local O
attention O
) O
, O
as O
well O
as O
a O
global O
attention O
pattern O
on O
a O
few O
specific O
input O
locations O
. O
LED B-MethodName
is O
build O
upon O
the O
BART O
model O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
by O
using O
additional O
positional O
embeddings O
and O
global O
attention O
weights O
, O
and O
introduces O
the O
global O
attention O
mode O
that O
operates O
over O
pre O
- O
selected O
tokens O
. O
LongT5 B-MethodName
extends O
the O
T5 O
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
by O
using O
a O
similar O
technique O
introduced O
in O
the O
ETC O
and O
BIGBIRD O
models O
Zaheer O
et O
al O
. O
, O
2020 O
) O
, O
relieving O
the O
requirement O
to O
manually O
select O
global O
tokens O
by O
automatically O
globalizing O
the O
aggregated O
representations O
of O
groups O
of O
tokens O
. O

Further O
strategies O
have O
been O
proposed O
for O
increasing O
these O
models O
' O
abilities O
in O
multi O
- O
document O
tasks O
. O
The O
Cross O
- O
Document O
Language O
Model O
( O
CDLM O
) O
suggested O
pretraining O
a O
Longformer O
- O
encoder O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
over O
sets O
of O
related O
documents O
, O
and O
showed O
superior O
performance O
results O
over O
several O
multidocument O
tasks O
. O
Following O
this O
methodology O
, O
the O
authors O
of O
LinkBERT O
( O
Yasunaga O
et O
al O
. O
, O
2022 O
) O
used O
a O
similar O
approach O
, O
but O
utilized O
Wikipedia O
's O
hyperlinks O
in O
order O
to O
curate O
informative O
pairs O
of O
linked O
documents O
for O
LM O
pre O
- O
training O
. O

In O
order O
to O
adopt O
the O
multi O
- O
document O
pretraining O
approach O
for O
sequence O
- O
to O
- O
sequence O
tasks O
, O
PRIMERA B-MethodName
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
, O
which O
is O
built O
on O
top O
of O
the O
Longformer B-MethodName
encoder I-MethodName
- I-MethodName
decoder I-MethodName
model O
( O
LED B-MethodName
) O
, O
selected O
salient O
sentences O
within O
clusters O
of O
related O
documents O
using O
a O
pyramid O
estimation O
approach O
, O
resembling O
the O
method O
presented O
for O
pre O
- O
training O
the O
single O
- O
document O
PEGASUS B-MethodName
model O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O
While O
this O
work O
is O
the O
closest O
to O
ours O
, O
it O
was O
pre O
- O
trained O
to O
generate O
masked O
salient O
sentences O
without O
any O
control O
, O
which O
makes O
the O
model O
potentially O
hallucinate O
while O
generating O
text O
, O
while O
our O
model O
uses O
a O
controlled O
QA O
- O
based O
objective O
. O
Furthermore O
, O
unlike O
these O
works O
, O
our O
method O
generates O
significantly O
more O
data O
then O
used O
to O
pre O
- O
train O
PRIMERA B-MethodName
, O
which O
is O
possible O
to O
obtain O
by O
the O
singledocument O
QA B-TaskName
generation I-TaskName
approach O
. O
Our O
QA O
pretraining O
formulation O
allows O
us O
to O
generate O
multiple O
contexts O
per O
document O
cluster O
. O

Another O
related O
line O
of O
work O
includes O
methods O
that O
incorporate O
large O
- O
scale O
QA O
- O
generated O
data O
for O
pre O
- O
training O
LMs O
Jia O
et O
al O
. O
, O
2022 O
; O

( O
a O
) O
The O
held O
- O
out O
document O
is O
discarded O
from O
the O
context O
( O
c O
) O
The O
held O
- O
out O
document O
is O
included O
in O
the O
context O
, O
but O
the O
answer O
in O
the O
anchor O
sentence O
is O
masked O
( O
b O
) O
The O
held O
- O
out O
document O
is O
included O
in O
the O
context O
, O
but O
the O
anchor O
sentence O
is O
masked O

Figure O
2 O
: O
A O
schematic O
of O
our O
pretraining O
data O
modes O
. O
The O
salient O
sentence O
which O
is O
used O
for O
QA B-TaskName
generation I-TaskName
is O
colored O
in O
yellow O
. O
( O
a O
) O
The O
context O
does O
not O
include O
the O
held O
- O
out O
document O
, O
therefore O
this O
mode O
is O
the O
most O
challenging O
. O
( O
b O
) O
The O
held O
- O
out O
document O
is O
present O
in O
the O
context O
, O
but O
the O
salient O
sentence O
used O
for O
the O
QA B-TaskName
generation I-TaskName
is O
masked O
( O
red O
) O
. O
( O
c O
) O
The O
held O
- O
out O
document O
is O
present O
in O
the O
context O
, O
but O
the O
answer O
span O
within O
the O
salient O
sentence O
is O
masked O
( O
red O
) O
. O
Huber O
et O
al O
. O
, O
2022 O
) O
. O
These O
works O
hypothesize O
and O
show O
that O
pre O
- O
training O
by O
utilizing O
generated O
QA O
data O
can O
encourage O
contextual O
representations O
to O
encode O
useful O
semantic O
information O
for O
other O
non O
- O
QA O
downstream O
tasks O
. O
Inspired O
by O
that O
, O
we O
conjecture O
that O
LMs O
can O
strongly O
benefit O
from O
infusing O
QA O
during O
pre O
- O
training O
in O
the O
multi O
- O
document O
setup O
, O
for O
adding O
an O
additional O
signal O
for O
modelling O
cross O
- O
text O
relationships O
. O

Augmenting O
the O
Multi O
- O
Document O

Pre O
- O
training O
objective O

In O
this O
section O
, O
we O
provide O
the O
required O
steps O
for O
compiling O
the O
pre O
- O
training O
dataset O
for O
QAMDEN B-MethodName
. O
We O
next O
elaborate O
on O
the O
details O
of O
the O
data O
creation O
and O
provide O
analysis O
of O
the O
resulted O
corpus O
. O

Recent O
works O
have O
shown O
that O
for O
text B-TaskName
summarization I-TaskName
, O
pre O
- O
training O
LMs O
to O
generate O
a O
" O
summarylike O
" O
sequence O
, O
termed O
pseudo O
summary O
, O
inherently O
provides O
gains O
over O
general O
- O
purpose O
pre O
- O
trained O
LMs O
( O
PEGASUS B-MethodName
, O
PRIMERA B-MethodName
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Xiao O
et O
al O
. O
, O
2022 O
) O
. O
The O
data O
in O
which O
the O
PEGASUS B-MethodName
and O
PRIMERA B-MethodName
models O
were O
pre O
- O
trained O
on O
was O
constructed O
using O
the O
Gap O
Sentence O
Generation O
( O
GSG O
) O
method O
, O
which O
suggests O
masking O
highly O
- O
ranked O
salient O
sentences O
, O
where O
salience O
is O
pre O
- O
determined O
by O
a O
sentence O
- O
scoring O
method O
of O
interest O
. O
Particularly O
, O
in O
PEGASUS B-MethodName
, O
GSG O
has O
been O
adopted O
as O
its O
pre O
- O
training O
objective O
, O
where O
some O
sentences O
in O
a O
single O
document O
are O
masked O
in O
the O
input O
and O
the O
model O
is O
tasked O
to O
generate O
them O
. O

Formally O
, O
for O
each O
sentence O
s O
i O
in O
a O
given O
input O
document O
D O
, O
PEGASUS B-MethodName
computes O
its O
salience O
score O
based O
on O
its O
ROUGE B-MetricName
score O
( O
Lin O
, O
2004 O
) O
w.r.t O
the O
rest O
of O
the O
sentences O
within O
the O
document O
( O
D O
/ O
{ O
s O
i O
} O
) O
, O
i.e. O
Score O
( O
s O
i O
) O
= O
ROUGE B-MetricName
( O
s O
i O
, O
D O
/ O
{ O
s O
i O
} O
) O
. O
Intuitively O
, O
… O
Pokemon O
Sword O
and O
Shield O
might O
have O
already O
been O
announced O
, O
but O
we O
now O
know O
there O
's O
another O
new O
Pokemon O
game O
on O
the O
way O
from O
DeNA O
… O
QASem O
QA O
generation O
( O
Klein O
et O
al O
. O
, O
2022 O
) O

Selected O

Figure O
3 O
: O
A O
schematic O
of O
the O
process O
of O
QA B-TaskName
generation I-TaskName
using O
QASEM B-MethodName
( O
Klein O
et O
al O
. O
, O
2022 O
) O
and O
the O
contextualization O
model O
from O
Pyatkin O
et O
al O
. O
( O
2021 O
) O
. O
This O
is O
an O
actual O
sample O
that O
was O
created O
and O
used O
for O
pre O
- O
training O
QAMDEN B-MethodName
, O
where O
the O
document O
is O
taken O
from O
New O
- O
SHead O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
. O

this O
metric O
assigns O
a O
high O
score O
to O
the O
sentences O
that O
have O
a O
high O
overlap O
and O
share O
more O
lexical O
information O
with O
the O
rest O
of O
the O
sentences O
in O
the O
document O
, O
thus O
assigning O
high O
scores O
to O
prominent O
sentences O
. O
PRIMERA B-MethodName
has O
generalized O
this O
notion O
to O
support O
the O
multi O
- O
document O
setup O
, O
by O
applying O
a O
GSG O
variant O
over O
a O
cluster O
of O
related O
documents O
. O

Cross O
- O
Document O
GSG O
. O
We O
propose O
augmenting O
the O
GSG O
technique O
to O
formulate O
a O
cross O
- O
document O
question O
answering O
pre O
- O
training O
objective O
for O
multidocument O
tasks O
, O
instead O
of O
the O
existing O
pseudo O
summary O
generation O
methods O
. O
Our O
approach O
supports O
identification O
of O
both O
fine O
- O
and O
coarse O
- O
grained O
information O
as O
we O
describe O
below O
, O
and O
results O
in O
a O
substantially O
larger O
amount O
of O
pre O
- O
training O
examples O
compared O
to O
the O
preceding O
methods O
. O

Formally O
, O
we O
are O
given O
a O
cluster O
of O
related O
documents O
S O
= O
D O
1 O
, O
D O
2 O
, O
. O
. O
. O
, O
D O
|S| O
in O
a O
corpus O
C. O
Our O
cross B-MetricName
- I-MetricName
document I-MetricName
( I-MetricName
CD I-MetricName
) I-MetricName
GSG I-MetricName
salience I-MetricName
score I-MetricName
for O
the O
i O
th O
sentence O
within O
the O
k O
th O
document O
in O
the O
set O
( O
s O
i O
k O
) O
, O
is O
defined O
by O
its O
ROUGE B-MetricName
score O
w.r.t O
the O
rest O
of O
the O
sentences O
within O
the O
document O
( O
D O
k O
/ O
{ O
s O
i O
k O
} O
) O
as O
well O
as O
the O
other O
documents O
( O
S O
/ O
D O
k O
) O
, O
i.e. O
CD B-MetricName
- I-MetricName
GSG I-MetricName
- I-MetricName
Score I-MetricName
( O
s O
i O
k O
) O
= O
ROUGE B-MetricName
( O
s O
i O
k O
, O
S O
/ O
{ O
s O
i O
k O
} O
) O
. O
Then O
, O
for O
every O
document O
k O
, O
following O
Zhang O
et O
al O
. O
( O
2020 O
) O
; O
Xiao O
et O
al O
. O
( O
2022 O
) O
we O
select O
the O
top O
- O
scored O
sentence O
s O
* O
k O
, O
and O
then O
we O
use O
this O
sentence O
to O
generate O
a O
pair O
of O
a O
question O
and O
an O
answer O
. O

Generating O
Cross O
- O
Document O
QAs O
. O
For O
generating O
the O
cross O
- O
document O
questions O
and O
their O
answers O
, O
we O
employ O
QASEM B-MethodName
, O
a O
recent O
semantic O
parsing O
framework O
for O
question O
generation O
( O
Klein O
et O
al O
. O
, O

D O
← O
∅ O
; O
2 O
for O
n O
← O
1 O
to O
|C| O
do O
3 O
for O
k O
← O
1 O
to O
|Sn| O
do O
4 O
s O
* O
k O
← O
arg O
max O
i O
CD B-MetricName
- I-MetricName
GSG I-MetricName
- I-MetricName
Score I-MetricName
( O
s O
i O
k O
) O
; O
5 O
( O
q O
* O
k O
, O
a O
* O
k O
) O
← O
QASEM O
( O
s O
* O
k O
) O
; O
6 O
t O
* O
k O
= O
[ O
a O
* O
k O
, O
s O
* O
k O
] O
# O
target O
text O
; O
7 O
D O
← O
D O
∪ O
{ O
( O
[ O
Sn O
/ O
D O
k O
, O
q O
* O
k O
] O
, O
t O
* O
k O
) O
} O
# O
( O
a O
) O
; O
8 O
D O
← O
D O
∪ O
{ O
( O
[ O
Sn O
/ O
{ O
s O
* O
k O
} O
, O
q O
* O
k O
] O
, O
t O
* O
k O
) O
} O
# O
( O
b O
) O
; O
9 O
D O
← O
D O
∪ O
{ O
( O
[ O
Sn O
/ O
{ O
a O
* O
k O
} O
, O
q O
* O
k O
] O
, O
t O
* O
k O
) O
} O
# O
( O
c O
) O
; O
10 O
Return O
D O
; O

2022 O
) O
. O
2 O
QASEM B-MethodName
intended O
soliciting O
a O
manageable O
, O
discrete O
account O
of O
information O
in O
a O
text O
for O
the O
sake O
of O
building O
natural O
language O
semantic O
representations O
. O
It O
automatically O
labels O
each O
verbal O
predicate O
- O
argument O
relation O
with O
a O
questionanswer O
pair O
, O
where O
a O
natural O
language O
question O
represents O
a O
semantic O
role O
, O
while O
the O
answers O
correspond O
to O
the O
arguments O
that O
appear O
in O
the O
input O
text O
. O
QASEM B-MethodName
is O
thus O
an O
appealing O
approach O
since O
it O
is O
capable O
of O
generating O
multiple O
high O
- O
quality O
questions O
given O
a O
sentence O
. O
We O
apply O
QASEM B-MethodName
over O
the O
sentences O
withing O
the O
pre O
- O
training O
data O
in O
order O
to O
generate O
question O
- O
answer O
pairs O
, O
and O
then O
apply O
the O
model O
from O
Pyatkin O
et O
al O
. O
( O
2021 O
) O
which O
transforms O
the O
question O
into O
a O
more O
natural O
and O
clear O
form O
, O
with O
contextualized O
arguments O
( O
see O
example O
in O
Figure O
3 O
) O
. O
In O
order O
to O
resemble O
a O
summarization B-TaskName
task O
where O
the O
generated O
text O
is O
typically O
long O
, O
we O
select O
the O
question O
- O
answer O
pair O
with O
the O
longest O
argument O
produced O
by O
QASEM O
. O
Formally O
, O
QASEM O
( O
• O
) O
receives O
a O
sentence O
s O
* O
k O
as O
an O
input O
, O
and O
produces O
question O
- O
answer O
pair O
( O
q O
* O
k O
, O
a O
* O
k O
) O
, O
where O
a O
* O
k O
is O
the O
longest O
among O
the O
generated O
answers O
. O
See O
a O
detailed O
example O
and O
full O
description O
in O
App O
. O
A.1 O
. O

Considering O
the O
question O
- O
answer O
pair O
, O
our O
goal O
is O
to O
encourage O
the O
LM O
to O
generate O
the O
correct O
answer O
as O
well O
as O
the O
salient O
sentence O
in O
a O
multi O
- O
document O
context O
in O
order O
to O
learn O
cross O
- O
text O
relationships O
. O

Data O
Generation O
Process O
. O
In O
order O
to O
facilitate O
the O
construction O
of O
a O
multi O
- O
document O
context O
, O
we O
propose O
three O
different O
modes O
, O
each O
one O
is O
responsible O
for O
uncovering O
information O
by O
using O
different O
contexts O
. O
For O
all O
the O
modes O
, O
we O
first O
generate O
a O
QA O
pair O
out O
of O
the O
most O
salient O
sentence O
in O
the O
held O
- O
out O
document O
. O

( O
a O
) O
Excluding O
the O
source O
document O
. O
In O
this O
mode O
we O
disregard O
the O
held O
- O
out O
document O
D O
k O
from O
the O
context O
S O
n O
given O
to O
the O
model O
, O
i.e O
, O
S O
n O
/ O
D O
k O
. O
Hence O
, O
the O
model O
is O
tasked O
to O
predict O
the O
answer O
without O
having O
access O
to O
the O
source O
document O
at O
all O
, O
and O
is O
restricted O
to O
observe O
only O
the O
other O
documents O
in O
the O
set O
. O
Thus O
, O
this O
mode O
is O
considered O
as O
the O
most O
challenging O
one O
. O

( O
b O
) O
Masking O
the O
salient O
sentence O
. O
In O
this O
mode O
, O
the O
source O
salient O
sentence O
is O
masked O
, O
i.e O
, O
S O
n O
/ O
{ O
s O
* O
k O
} O
. O
The O
model O
has O
access O
to O
the O
surrounding O
context O
of O
the O
masked O
sentence O
in O
the O
held O
- O
out O
document O
, O
as O
well O
as O
the O
other O
documents O
in O
the O
set O
. O

( O
c O
) O
Masking O
the O
answer O
. O
In O
this O
mode O
, O
only O
the O
answer O
span O
within O
the O
salient O
sentence O
is O
masked O
, O
i.e O
, O
S O
n O
/ O
{ O
a O
* O
k O
} O
. O
The O
model O
has O
access O
to O
the O
surrounding O
salient O
sentence O
, O
as O
well O
as O
all O
the O
documents O
in O
the O
set O
. O

As O
part O
of O
the O
new O
pre O
- O
training O
process O
of O
our O
novel O
multi O
- O
document O
model O
, O
we O
append O
the O
question O
after O
the O
context O
and O
instruct O
the O
model O
to O
generate O
an O
answer O
followed O
by O
its O
salient O
sentence O
, O
i.e. O
, O
output O
= O
⟨answer⟩ O
, O
⟨sentence⟩ O
, O
inspired O
by O
Bohnet O
et O
al O
. O
( O
2022 O
) O
. O
Generating B-TaskName
the I-TaskName
salient I-TaskName
sentence I-TaskName
introduces O
a O
copying O
mechanism O
( O
allows O
the O
model O
to O
also O
learn O
to O
copy O
information O
from O
the O
source O
directly O
) O
as O
well O
as O
allowing O
longtext O
generation O
, O
which O
is O
crucial O
for O
summarization O
downstream O
tasks O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
as O
well O
as O
outperforming O
a O
model O
which O
was O
pre O
- O
trained O
for O
generating O
the O
answer O
solely O
-according O
to O
the O
ablations O
study O
, O
this O
setup O
yields O
the O
best O
performance O
results O
( O
§ O
4.4 O
) O
. O
In O
the O
pre O
- O
training O
evaluation O
phase O
, O
the O
held O
- O
out O
set O
was O
split O
and O
the O
loss O
was O
measured O
separately O
for O
each O
mode O
of O
the O
data O
. O
As O
expected O
, O
we O
observed O
that O
the O
loss O
for O
( O
a O
) O
was O
significantly O
higher O
than O
those O
for O
the O
other O
modes O
, O
with O
( O
a O
) O
≻ O
( O
b O
) O
≻ O
( O
c O
) O
ranking O
highest O
. O
The O
procedure O
for O
generating O
the O
pre O
- O
training O
data O
is O
summarized O
in O
Algorithm O
1 O
and O
Figure O
2 O
. O

The O
resulted O
pre O
- O
training O
corpus O
. O
We O
applied O
our O
procedure O
over O
the O
NewSHead B-DatasetName
corpus O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
, O
which O
consists O
of O
a O
set O
of O
related O
documents O
per O
instance O
. O
This O
is O
the O
exact O
same O
pre O
- O
training O
corpus O
used O
also O
by O
our O
main O
baseline O
PRIMERA B-MethodName
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
, O
when O
concatenating O
the O
documents O
and O
the O
question O
, O
we O
add O
a O
special O
document O
separator O
token O
( O
< O
doc O
- O
sep O
> O
) O
between O
the O
documents O
to O
signal O
to O
the O
model O
to O
be O
aware O
of O
the O
document O
boundaries O
. O
We O
also O
assign O
the O
global O
attention O
mode O
to O
these O
tokens O
which O
enables O
the O
model O
to O
share O
information O
across O
documents O
. O
For O
further O
hyperparameter O
and O
pre O
- O
training O
execution O
details O
, O
see O
App O
. O
B O
. O

Multi B-TaskName
- I-TaskName
Document I-TaskName
Question I-TaskName
Answering I-TaskName

Multi B-TaskName
- I-TaskName
document I-TaskName
QA I-TaskName
is O
the O
task O
of O
generating O
the O
correct O
answer O
, O
given O
a O
set O
of O
related O
multiple O
documents O
. O
For O
several O
multi O
- O
document O
QA O
benchmarks O
, O
models O
are O
often O
tasked O
to O
implicitly O
solve O
multiple O
sub O
- O
tasks O
or O
follow O
intermediate O
steps O
, O
such O
as O
comprehending O
the O
question O
, O
filtering O
out O
distracting O
documents O
in O
the O
context O
, O
and O
stitching O
pieces O
of O
information O
across O
the O
relevant O
documents O
( O
Geva O
et O
al O
. O
, O
2021 O
; O
. O
Recall O
that O
QAMDEN B-MethodName
was O
pre O
- O
trained O
over O
a O
automatically O
generated O
multi O
- O
document O
QA O
dataset O
. O
Hence O
, O
as O
a O
preliminary O
assessment O
, O
we O
first O
investigate O
QAMDEN B-MethodName
's O
performance O
over O
two O
multi O
- O
document O
QA O
benchmarks O
, O
HopotQAdistractor B-DatasetName
( O
Yang O
et O
al O
. O
, O
2018 O
) O
and O
WikiHop B-DatasetName
( O
Welbl O
et O
al O
. O
, O
2018 O
) O
( O
see O
more O
details O
of O
the O
datasets O
in O
App O
. O
C.1 O
) O
, O
and O
compare O
to O
other O
models O
that O
were O
pre O
- O
trained O
using O
underling O
un O
- O
masking O
objectives O
. O

Fine O
- O
Tuning O
Format O
. O
To O
follow O
our O
pre O
- O
training O
scheme O
, O
we O
append O
the O
question O
to O
the O
context O
and O
fine O
- O
tune O
the O
model O
to O
generate O
the O
correct O
answer O
. O
We O
use O
the O
Longformer B-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
( O
LED B-MethodName
) O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
and O
PRIMERA B-MethodName
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
as O
the O
baselines O
, O
for O
assesing O
the O
contribution O
of O
our O
pre O
- O
trainig O
format O
. O
Confirmed O
by O
Beltagy O
et O
al O
. O
( O
2020 O
) O
, O
we O
found O
out O
that O
appending O
the O
question O
: O
and O
context O
: O
prefixes O
before O
the O
question O
and O
the O
context O
tokens O
, O
respectively O
, O
resulted O
in O
better O
performance O
. O

Baselines O
. O
We O
compare O
QAMDEN B-MethodName
( O
447 O
M O
parameters O
) O
against O
a O
set O
of O
strong O
long O
- O
context O
transformer O
baselines O
, O
including O
LED B-MethodName
( O
447 O
M O
parameters O
) O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
, O
PRIMERA B-MethodName
( O
447 O
M O
parameters O
) O
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
, O
4 O
and O
LongT5 B-MethodName
- I-MethodName
xl I-MethodName
( O
3B O
parameters O
) O
5 O
) O
( O
see O
§ O
2 O
) O
. O
6 O
Results O
. O
The O
results O
on O
multi O
- O
document O
QA O
are O
shown O
in O
Table O
2 O
. O
We O
adopted O
the O
F1 B-MetricName
and O
Exact B-MetricName
Match I-MetricName
( O
EM B-MetricName
) O
evaluation O
metrics O
corresponding O
to O
the O
original O
works O
. O
Our O
QAMDEN B-MethodName
outperforms O
both O
PRIMERA B-MethodName
, O
LED B-MethodName
, O
and O
LongT5 B-MethodName
, O
confirming O
that O
our O
pre O
- O
training O
data O
and O
input O
format O
are O
beneficial O
for O
both O
capturing O
cross O
- O
document O
relationships O
( O
QAMDEN≻LED B-MethodName
) O
as O
well O
as O
exploiting O
both O
context O
and O
question O
( O
QAMDEN≻PRIMERA B-MethodName
) O
. O

Multi B-TaskName
- I-TaskName
Document I-TaskName
Summarization I-TaskName
( O
MDS B-TaskName
) O

This O
task O
aims O
at O
generating O
a O
summary O
for O
a O
given O
set O
of O
topically O
- O
related O
documents O
. O
Inherently O
, O
end- O
Results O
. O
Tables O
3 O
and O
4 O
present O
the O
evaluation O
results O
over O
the O
Multi B-DatasetName
- I-DatasetName
News I-DatasetName
and O
Multi B-DatasetName
- I-DatasetName
XScience I-DatasetName
datasets O
, O
respectively O
. O
Following O
previous O
MDS B-TaskName
works O
, O
we O
report O
the O
ROUGE B-MetricName
R-1 B-MetricName
, O
-2 B-MetricName
, O
and O
-L O
scores O
, O
which O
are O
the O
standard O
MDS O
evaluation O
metrics O
( O
see O
App O
. O
C.2 O
for O
details O
) O
. O
For O
a O
fair O
comparison O
, O
we O
include O
the O
results O
of O
PRIMERA O
as O
well O
as O
the O
results O
of O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
methods O
( O
Pasunuru O
et O
al O
. O
( O
2021b O
) O
and O
Lu O
et O
al O
. O
( O
2020 O
) O
, O
for O
Multi B-DatasetName
- I-DatasetName
News I-DatasetName
and O
for O
Multi B-DatasetName
- I-DatasetName
XScience I-DatasetName
, O
respectively O
) O
, O
and O
LED B-MethodName
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
. O
As O
shown O
in O
the O
results O
tables O
, O
QAMDEN B-MethodName
exhibits O
the O
best O
performance O
across O
most O
of O
the O
examined O
models O
and O
benchmarks O
, O
especially O
on O
the O
Multi B-DatasetName
- I-DatasetName
News I-DatasetName
dataset O
, O
clearly O
demonstrating O
its O
consistent O
advan- O
Pasunuru O
et O
al O
. O
( O
2021b O
) O
49.2 O
19.6 O
24.5 O
LED B-MethodName
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
47.4 B-MetricValue
20.7 B-MetricValue
23.7 B-MetricValue
LongT5 B-MethodName
- I-MethodName
xl I-MethodName
47.4 B-MetricValue
20.7 B-MetricValue
23.7 B-MetricValue
PRIMERA B-MethodName
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
49 O
. O
tage O
. O
This O
excludes O
the O
results O
for O
Multi B-DatasetName
- I-DatasetName
XScience I-DatasetName
where O
QAMDEN B-MethodName
slightly O
underperforms O
the O
prior O
work O
and O
LongT5 B-MethodName
. O
An O
explanation O
which O
Xiao O
et O
al O
. O
( O
2022 O
) O
points O
refers O
to O
the O
fact O
that O
the O
clusters O
in O
Multi B-DatasetName
- I-DatasetName
XScience I-DatasetName
have O
less O
overlapping O
information O
compared O
to O
the O
corpus O
we O
used O
, O
attributed O
to O
the O
use O
of O
abstracts O
as O
the O
input O
documents O
in O
Multi B-DatasetName
- I-DatasetName
XScience I-DatasetName
. O
In O
addition O
, O
LongT5 B-MethodName
advantage O
over O
QAMDEN B-MethodName
is O
attributed O
to O
significantly O
larger O
number O
of O
parameters O
of O
LongT5 B-MethodName
- I-MethodName
xl I-MethodName
. O

Model O
R-1 B-MetricName
R-2 B-MetricName
R B-MetricName
- I-MetricName
L I-MetricName

Query B-TaskName
- I-TaskName
Focused I-TaskName
Multi I-TaskName
- I-TaskName
Document I-TaskName
Abstractive I-TaskName
Summarization I-TaskName

The O
task O
of O
Query B-TaskName
- I-TaskName
focused I-TaskName
Multi I-TaskName
- I-TaskName
Document I-TaskName
Summarization I-TaskName
( O
QMDS B-TaskName
) O
aims O
at O
generating O
a O
summary O
from O
a O
set O
of O
documents O
, O
that O
answers O
a O
specific O
given O
query O
. O
Unlike O
MDS B-TaskName
, O
QMDS B-TaskName
tries O
to O
solve O
more O
realistic O
query O
- O
based O
scenarios O
, O
since O
it O
suggests O
summarizing O
only O
predefined O
salient O
information O
of O
interest O
that O
best O
answers O
the O
query O
. O
Since O
we O
proposed O
pre O
- O
trainng O
under O
the O
multi O
- O
document O
question O
answering O
setup O
, O
we O
posit O
that O
QAMDEN B-MethodName
might O
be O
effective O
for O
QMDS B-TaskName
. O

We O
consider O
the O
datasets O
constructed O
by O
Pasunuru O
et O
al O
. O
( O
2021a O
) O
, O
QMDSCNN B-DatasetName
and O
QMDSIR B-DatasetName
( O
see O
more O
details O
of O
the O
datasets O
in O
App O
. O
C.3 O
) O
as O
well O
as O
their O
strong O
baseline O
, O
and O
include O
also O
the O
results O
of O
PRIMERA B-MethodName
and O
LED B-MethodName
. O

Baselines O
. O
Similar O
to O
the O
previous O
experiments O
, O
we O
compare O
QAMDEN B-MethodName
against O
LED B-MethodName
, O
PRIMERA B-MethodName
, O
LongT5 B-MethodName
- I-MethodName
xl I-MethodName
. O
In O
addition O
, O
we O
consider O
also O
the O
baseline O
from O
Pasunuru O
et O
al O
. O
( O
2021a O
) O
. O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
32.3 B-MetricValue
14.3 B-MetricValue
30.9 B-MetricValue
LongT5 B-MethodName
- I-MethodName
xl I-MethodName
35.5 B-MetricValue
15.9 B-MetricValue
34.3 B-MetricValue
PRIMERA B-MethodName
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
36 O
Results O
. O
Tables O
5 O
and O
6 O

Ablation O
Study O

Data O
Generation O
. O
We O
next O
turn O
to O
a O
broad O
ablation O
study O
, O
for O
assessing O
our O
configuration O
and O
design O
choices O
across O
our O
suggested O
pipeline O
. O
First O
, O
we O
show O
the O
advantage O
of O
combining O
the O
three O
proposed O
data O
modes O
, O
rather O
than O
using O
a O
subset O
of O
them O
. O
We O
evaluate O
all O
the O
resulted O
models O
by O
fine O
- O
tuning O
them O
over O
HopotQA B-DatasetName
- I-DatasetName
distractor I-DatasetName
( O
§ O
4.1 O
) O
, O
Multi B-DatasetName
- I-DatasetName
XScience I-DatasetName
( O
§ O
4.2 O
) O
, O
and O
QMDSIR B-DatasetName
( O
§ O
4.3 O
) O
. O
For O
HopotQA B-DatasetName
- I-DatasetName
distractor I-DatasetName
we O
report O
the O
Exact B-MetricName
Match I-MetricName
( O
EM B-MetricName
) O
score O
, O
and O
for O
the O
summarization O
tasks O
we O
report O
the O
ROUGE-1 B-MetricName
( O
R-1 B-MetricName
) O
score O
. O

Baselines O
. O
We O
pre O
- O
train O
QAMDEN B-MethodName
for O
100k O
steps O
, O
for O
using O
every O
subset O
of O
the O
set O
of O
the O
set O
( O
superset O
) O
of O
modes O
{ O
( O
a O
) O
, O
( O
b O
) O
, O
( O
c O
) O
} O
( O
all O
its O
possible O
combinations O
) O
of O
the O
generated O
pre O
- O
training O
data O
modes O
presented O
in O
§ O
3 O
. O
Note O
that O
our O
QAMDEN B-MethodName
model O
is O
referred O
to O
as O
using O
all O
the O
modes O
, O
i.e. O
, O
Results O
. O
Figure O
4 O
shows O
the O
ablation O
results O
. O
In O
all O
tasks O
, O
pre O
- O
training O
using O
all O
modes O
yields O
the O
best O
results O
. O
Among O
all O
modes O
, O
mode O
( O
c O
) O
appears O
to O
be O
the O
most O
effective O
for O
QA B-TaskName
, O
since O
this O
is O
an O
extractive B-TaskName
QA I-TaskName
task O
, O
and O
mode O
( O
c O
) O
provides O
data O
in O
this O
format O
. O
Mode O
( O
a O
) O
excels O
at O
the O
summarization B-TaskName
tasks O
, O
attributed O
to O
their O
abstractive O
nature O
as O
well O
as O
the O
requirement O
of O
all O
the O
documents O
for O
generating O
appropriate O
summaries O
. O

Input O
Format O
We O
repeat O
the O
previous O
experiment O
and O
ablate O
the O
pre O
- O
training O
input O
format O
according O
to O
the O
multiple O
different O
formats O
, O
and O
compare O
to O
the O
model O
pre O
- O
training O
format O
described O
in O
§ O
3 O
( O
with O
the O
same O
pre O
- O
training O
data O
) O
: O
without O
questions O
, O
with O
random O
question O
, O
with O
random O
context O
document O
, O
with O
prefixes O
, O
placing O
the O
question O
before O
the O
context O
, O
with O
question O
filtering O
, O
and O
without O
generating O
the O
salient O
sentence O
. O
Additionally O
, O
we O
assess O
the O
choice O
of O
QASEM B-MethodName
as O
our O
questionanswer O
generation O
module O
by O
using O
the O
generators O
from O
Jia O
et O
al O
. O
( O
2022 O
) O
and O
Khashabi O
et O
al O
. O
( O
2022 O
) O
. O
Finally O
, O
we O
also O
include O
the O
results O
of O
PRIMERA B-MethodName
, O
which O
was O
further O
pre O
- O
trained O
for O
additional O
300k B-HyperparameterValue
steps B-HyperparameterName
( O
fine O
- O
tuning O
LED B-MethodName
for O
400k B-HyperparameterValue
steps B-HyperparameterName
in O
total O
) O
, O
for O
a O
fair O
comparison O
to O
QAMDEN B-MethodName
ablated O
models O
. O
See O
full O
details O
regarding O
all O
the O
ablations O
in O
App O
. O
D O
. O

Results O
. O
Overall O
, O
our O
QAMDEN B-MethodName
model O
outperforms O
the O
ablation O
models O
on O
most O
of O
the O
tasks O
, O
which O
a O
significant O
margin O
. O

Pre O
- O
training O
the O
model O
without O
any O
questions O
during O
or O
using O
random O
questions O
, O
negatively O
impacts O
the O
results O
of O
downstream O
tasks O
. O
An O
impor- O
tant O
function O
of O
the O
question O
is O
to O
facilitate O
the O
model O
's O
ability O
to O
generate O
the O
appropriate O
answer O
and O
the O
source O
sentence O
. O
This O
aligns O
with O
the O
findings O
from O
, O
who O
showed O
that O
pre O
- O
training O
with O
random O
documents O
rather O
than O
related O
ones O
is O
sub O
- O
optimal O
. O
The O
use O
of O
question O
and O
context O
prefixes O
for O
positioning O
input O
appears O
to O
be O
helpful O
for O
QA B-TaskName
, O
but O
is O
inferior O
when O
applied O
to O
summarization B-TaskName
tasks O
due O
to O
its O
unique O
format O
, O
which O
is O
well O
suited O
for O
QA B-TaskName
but O
seems O
to O
generalize O
harder O
for O
other O
setups O
. O
When O
the O
question O
is O
placed O
before O
the O
context O
, O
performance O
slightly O
decreases O
over O
query O
- O
based O
tasks O
, O
while O
maintaining O
the O
same O
results O
for O
summarization B-TaskName
( O
where O
the O
question O
location O
is O
irrelevant O
) O
. O

Using O
question O
filtering O
is O
found O
to O
harm O
the O
downstream O
results O
of O
QAMDEN B-MethodName
, O
in O
accordance O
to O
other O
QA B-TaskName
- O
based O
pre O
- O
training O
prior O
works O
( O
Jia O
et O
al O
. O
, O
2022 O
) O
. O

Pre O
- O
training O
without O
generating O
the O
attributed O
source O
sentence O
introduces O
a O
significant O
flow O
to O
the O
model O
, O
particularly O
for O
the O
summarization B-TaskName
downstream O
tasks O
. O
As O
mentioned O
before O
, O
generating O
longer O
sequences O
, O
as O
well O
as O
teaching O
the O
model O
to O
copy O
text O
, O
is O
beneficial O
for O
summarization B-TaskName
tasks O
. O

Applying O
a O
different O
question O
generator O
rather O
then O
QASEM B-MethodName
yields O
inferior O
results O
overall O
, O
since O
the O
other O
generators O
produce O
open O
- O
ended O
questions O
and O
answers O
which O
are O
more O
prone O
to O
errors O
, O
while O
QASEM B-MethodName
utilizes O
an O
existing O
span O
in O
the O
context O
as O
the O
answer O
. O
In O
addition O
, O
QASEM B-MethodName
generated O
local O
questions O
, O
which O
allows O
QAMDEN B-MethodName
to O
focus O
on O
the O
fine O
- O
grained O
details O
, O
and O
not O
only O
the O
coarsegrained O
information O
in O
the O
multi O
- O
document O
context O
. O

When O
PRIMERA B-MethodName
is O
pre O
- O
trained O
with O
400k B-HyperparameterValue
steps B-HyperparameterName
( O
to O
match O
QAMDEN B-MethodName
's O
number O
of O
further O
pretraining O
steps O
) O
, O
it O
underperforms O
QAMDEN B-MethodName
and O
even O
fails O
to O
add O
any O
significant O
improvements O
over O
its O
100 O
K O
checkpoint O
, O
possibly O
due O
to O
the O
small O
amount O
of O
pre O
- O
training O
data O
it O
contains O
. O

Comparison O
with O
Large O
Language O
Models O

In O
order O
to O
get O
insights O
into O
how O
QAMDEN B-MethodName
compares O
with O
state O
- O
of O
- O
the O
- O
art O
Generalist O
Large O
Language O
Models O
( O
LLMs O
) O
, O
we O
provide O
a O
small O
comparison O
with O
two O
capable O
models O
, O
GPT-3.5 B-MethodName
turbo I-MethodName
( O
Ouyang O
et O
al O
. O
, O
2022 O
) O
and O
GPT-4 B-MethodName
8 O
( O
OpenAI O
, O
2023 O
) O
( O
including O
the O
8k B-HyperparameterValue
input B-HyperparameterName
length I-HyperparameterName
version O
) O
evaluated O
on O
the O
zero O
- O
shot O
setting O
. O

For O
a O
fair O
comparison O
, O
we O
used O
the O
same O
context B-HyperparameterName
window I-HyperparameterName
size I-HyperparameterName
of O
4 B-HyperparameterValue
K I-HyperparameterValue
tokens O
for O
all O
models O
( O
and O
up O
to O
8k B-HyperparameterValue
for O
GPT-4 B-MethodName
8k I-MethodName
) O
. O
Due O
to O
the O
fact O
that O
multidocument O
tasks O
involve O
processing O
long O
sequences O
, O
the O
cost O
of O
API O
calls O
is O
significant O
for O
a O
comprehensive O
evaluation O
across O
all O
datasets O
. O
Therefore O
, O
we O
only O
evaluate O
on O
a O
sample O
of O
200 O
instances O
from O
the O
multi O
- O
news O
dataset O
( O
see O
prompting O
details O
in O
App O
. O
E O
) O
. O
Table O
8 O
depicts O
the O
results O
. O
We O
observe O
that O
QAMDEN B-MethodName
significantly O
outperforms O
both O
GPT-3.5 B-MethodName
and O
GPT-4 B-MethodName
models O
, O
though O
the O
performance O
of O
GPT-4 B-MethodName
and O
GPT-3.5 B-MethodName
is O
comparable O
. O
We O
leave O
more O
comprehensive O
comparisons O
with O
LLMs O
to O
future O
work O
. O

We O
further O
assessed O
QAMDEN B-MethodName
through O
manual B-MetricName
comparison I-MetricName
against O
PRIMERA B-MethodName
, O
GPT-3.5 B-MethodName
, O
and O
GPT-4 B-MethodName
8k I-MethodName
. O
NLP B-MetricName
graduate I-MetricName
students I-MetricName
were O
shown O
summaries O
for O
a O
given O
topic O
from O
the O
three O
systems O
and O
QAMDEN B-MethodName
in O
arbitrary O
order O
, O
along O
with O
a O
corresponding O
reference O
summary O
. O
Following O
( O
Ernst O
et O
al O
. O
, O
2022 O
) O
, O
participants O
were O
asked O
to O
rank O
the O
systems O
based O
on O
Content B-MetricName
( O
overlap O
with O
the O
reference O
) O
, O
Readability B-MetricName
( O
the O
readability O
of O
a O
summary O
) O
, O
Grammaticality B-MetricName
( O
avoiding O
grammar O
errors O
) O
, O
and O
Non B-MetricName
- I-MetricName
Redundancy I-MetricName
( O
avoiding O
repetitions O
) O
, O
and O
we O
extract O
the O
pairwise O
results O
out O
of O
the O
rankings O
( O
see O
( O
Ernst O
et O
al O
. O
, O
2022 O
) O
for O
further O
details O
) O
. O
In O
App O
. O
F O
, O
we O
provide O
several O
examples O
to O
system O
summaries O
and O
their O
corresponding O
reference O
summaries O
. O

The O
results O
of O
this O
study O
are O
presented O
in O
Table O
9 O
. O
Under O
each O
evaluation O
criterion O
, O
it O
indicates O
the O
percentage O
of O
cases O
where O
QAMDEN B-MethodName
was O
preferred O
over O
both O
baselines O
. O
QAMDEN B-MethodName
was O
favored O
in O
all O
cases O
except O
for O
grammatical O
errors O
and O
readability O
( O
which O
corresponds O
to O
the O
Reinforcement O
Learning O
from O
Human O
Feedback O
phase O
of O
the O
GPT O
models O
) O
. O

Conclusions O

In O
this O
work O
, O
we O
present O
a O
novel O
pre O
- O
training O
scheme O
for O
multi O
- O
document O
tasks O
. O
First O
, O
our O
approach O
suggests O
to O
augment O
the O
existing O
multidocument O
pre O
- O
training O
objectives O
into O
a O
crossdocument B-TaskName
question I-TaskName
answering I-TaskName
task O
. O
Second O
, O
we O
generate O
high O
- O
quality O
large O
- O
scale O
QA B-TaskName
pre O
- O
training O
data O
using O
a O
controlled O
generation O
approach O
, O
in O
which O
each O
QA B-TaskName
pair O
originates O
from O
a O
salient O
sentence O
in O
one O
of O
the O
documents O
in O
the O
set O
. O

During O
pre O
- O
training O
, O
we O
task O
the O
the O
Longformer B-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
( O
LED B-MethodName
) O
model O
to O
generate O
the O
answer O
and O
the O
salient O
sentence O
on O
the O
basis O
of O
the O
remaining O
context O
. O
This O
objective O
encourages O
the O
LED B-MethodName
model O
to O
elicit O
cross O
- O
document O
relationships O
, O
and O
stitch O
pieces O
of O
information O
across O
the O
input O
documents O
, O
which O
are O
relevant O
for O
performing O
multi O
- O
document O
tasks O
. O
The O
resulted O
model O
QAMDEN B-MethodName
shows O
significant O
performance O
improvements O
compared O
to O
prior O
models O
under O
extensive O
experimentation O
over O
multiple O
challenging O
multidocument O
summarization O
and O
QA B-TaskName
datasets O
. O

Future O
work O
can O
extend O
the O
ideas O
in O
this O
work O
for O
equipping O
decoder O
- O
only O
large O
LMs O
with O
crossdocument O
modeling O
using O
our O
proposed O
method O
, O
also O
in O
the O
setup O
of O
in O
- O
context O
learning O
and O
prompt O
tuning O
. O
We O
foresee O
that O
our O
method O
should O
be O
significant O
specifically O
for O
retrieval O
- O
augmented O
language O
modeling O
setups O
( O
Izacard O
et O
al O
. O
, O
2022 O
) O
, O
where O
there O
is O
a O
use O
of O
related O
documents O
as O
an O
outsourced O
external O
non O
- O
parametric O
knowledge O
source O
. O
Finally O
, O
the O
use O
of O
a O
single O
document O
in O
order O
to O
trigger O
cross O
- O
document O
relationships O
, O
as O
firstly O
introduced O
in O
this O
work O
, O
might O
be O
further O
investigated O
. O

Limitations O

While O
our O
work O
tries O
to O
focus O
around O
reasoning O
over O
both O
fine O
- O
and O
coarse O
- O
grained O
cross O
- O
document O
relationships O
, O
QAMDEN B-MethodName
, O
the O
resulted O
pre O
- O
trained O
model O
, O
might O
still O
suffer O
from O
factual O
consistency O
errors O
while O
generating O
information O
given O
a O
query O
, O
and O
there O
is O
no O
guarantee O
that O
it O
will O
always O
generate O
factual O
and O
reasonable O
content O
without O
any O
further O
fine O
- O
tuning O
. O

The O
QASEM B-MethodName
question O
generation O
model O
that O
we O
used O
may O
also O
have O
been O
a O
source O
of O
these O
problems O
. O
There O
is O
a O
possibility O
that O
QASEM B-MethodName
produces O
inadequate O
questions O
that O
could O
harm O
the O
pre O
- O
training O
process O
of O
the O
model O
. O
An O
attempt O
was O
made O
to O
filter O
out O
noise O
using O
a O
question O
model O
, O
but O
the O
results O
were O
inferior O
to O
non O
- O
filtering O
. O
Consequently O
, O
if O
the O
model O
is O
not O
fine O
- O
tuned O
, O
inconsistency O
( O
hallucinations O
) O
may O
occur O
more O
frequently O
. O

In O
addition O
, O
by O
using O
the O
Newshead O
corpus O
as O
the O
pre O
- O
training O
data O
source O
, O
we O
assume O
that O
it O
is O
comprised O
of O
high O
quality O
documents O
. O
We O
also O
take O
into O
account O
the O
fact O
that O
Newshead O
is O
limited O
to O
documents O
in O
the O
news O
domain O
, O
while O
some O
of O
the O
benchmarks O
used O
for O
evaluating O
QAMDEN B-MethodName
include O
another O
topics O
of O
interest O
. O
Future O
work O
may O
further O
assess O
the O
quality O
of O
the O
documents O
, O
such O
as O
checking O
for O
duplications O
or O
wrong O
statements O
, O
and O
diversify O
the O
corpus O
domains O
. O
This O
is O
crucial O
for O
productizing O
models O
like O
QAMDEN B-MethodName
in O
interactive O
multi O
- O
text O
applications O
( O
chatbots O
) O
and O
semantic O
search O
applications O
which O
are O
gaining O
attraction O
nowadays O
( O
Hirsch O
et O
al O
. O
, O
2021 O
; O
Eirew O
et O
al O
. O
, O
2022 O
) O
. O

Finally O
, O
the O
resulted O
model O
QAMDEN B-MethodName
was O
pretrained O
on O
sets O
of O
related O
documents O
, O
by O
answering O
questions O
that O
matched O
their O
content O
. O
As O
in O
an O
out O
- O
of O
- O
domain O
scenario O
, O
QAMDEN B-MethodName
's O
use O
over O
sets O
of O
documents O
that O
are O
not O
related O
, O
or O
over O
single O
documents O
, O
might O
be O
unexpected O
. O
Such O
settings O
may O
be O
the O
subject O
of O
another O
research O
direction O
in O
the O
future O
. O

Ethics O
Statement O

Despite O
the O
limited O
risk O
associated O
with O
our O
work O
, O
similar O
to O
existing O
state O
- O
of O
- O
the O
- O
art O
generation O
language O
models O
, O
there O
is O
no O
guarantee O
that O
QAM B-MethodName
- I-MethodName
DEN I-MethodName
, O
our O
model O
, O
will O
always O
generate O
factual O
information O
. O
The O
model O
should O
therefore O
be O
used O
with O
caution O
in O
a O
practical O
environment O
and O
be O
carefully O
tested O
before O
deployment O
. O
It O
is O
possible O
, O
for O
example O
, O
that O
frequent O
anecdotal O
events O
in O
the O
pre O
- O
training O
dataset O
are O
generated O
in O
an O
unexpected O
manner O
. O

A O
Data O
Creation O

As O
noted O
in O
§ O
3 O
, O
we O
used O
the O
NewSHead B-DatasetName
corpus O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
. O
We O
followed O
the O
data O
pre O
- O
processing O
procedure O
suggested O
by O
Xiao O
et O
al O
. O
( O
2022 O
) O
which O
supplied O
each O
sentence O
in O
the O
NewSHead B-DatasetName
corpus O
with O
their O
PEGASUS B-MethodName
scores O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O
9 O
A.1 O
QASEM B-MethodName
Details O
QASEM B-MethodName
( O
Klein O
et O
al O
. O
, O
2022 O
) O
is O
a O
unified O
tool O
for O
parsing O
sentences O
into O
a O
systematic O
set O
of O
QAs B-TaskName
that O
represent O
each O
sentence O
. O
The O
following O
three O
types O
of O
predication O
are O
included O
in O
this O
set O
: O
verbs O
, O
deverbal O
nominalizations O
, O
and O
informational O
discourse O
relations O
, O
and O
they O
represent O
the O
core O
units O
of O
information O
in O
a O
sentence O
. O

For O
producing O
the O
pre O
- O
training O
data O
for O
our O
QAMDEN B-MethodName
model O
, O
we O
specifically O
targeted O
the O
verbal O
predicates O
for O
question O
- O
answer O
generation O
, O
since O
their O
corresponding O
training O
examples O
origin O
from O
the O
Question B-DatasetName
Answer I-DatasetName
driven I-DatasetName
Semantic I-DatasetName
Role I-DatasetName
Labeling I-DatasetName
( O
QA B-DatasetName
- I-DatasetName
SRL I-DatasetName
) O
dataset O
( O
He O
et O
al O
. O
, O
2015 O
) O
which O
covers O
the O
largest O
part O
of O
the O
joint O
QASEM B-MethodName
training O
data O
, O
and O
obtained O
the O
best O
empirical O
results O
during O
evaluation O
, O
compared O
to O
the O
other O
types O
( O
nominalizations O
and O
discourse O
relations O
) O
. O
Using O
the O
QA B-DatasetName
- I-DatasetName
SRL I-DatasetName
formalism O
, O
every O
predicate O
- O
argument O
relation O
is O
labeled O
with O
a O
question O
- O
answer O
pair O
, O
and O
so O
natural O
language O
questions O
represent O
semantic O
roles O
, O
while O
answers O
correspond O
to O
arguments O
. O

QASEM B-MethodName
first O
executes O
sentence O
- O
level O
preprocessing O
for O
QA B-DatasetName
- I-DatasetName
SRL I-DatasetName
by O
running O
a O
part O
- O
ofspeech O
tagger O
to O
identify O
verbs O
. O
10 O
. O
Then O
, O
the O
parser O
itself O
is O
based O
on O
a O
fine O
- O
tuned O
T5 O
- O
small O
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
which O
is O
given O
a O
single O
marked O
predicate O
in O
context O
at O
a O
time O
, O
and O
is O
trained O
on O
the O
task O
of O
producing O
the O
full O
set O
of O
question O
- O
answer O
pairs O
targeting O
this O
predicate O
. O
11 O
The O
input O
sequence O
consists O
of O
the O
unique O
task O
prefix O
, O
the O
sentence O
, O
special O
markers O
for O
the O
target O
predicate O
, O
and O
the O
basic O
verbal O
- O
form O
of O
the O
predicate O
. O
The O
output O
is O
a O
set O
of O
QAs B-TaskName
, O
and O
we O
select O
one O
pair O
according O
to O
the O
length O
of O
the O
answer O
( O
§ O
3 O
) O
. O
Since O
QASEM B-MethodName
generates O
" O
abstractive O
" O
questions O
that O
replace O
arguments O
with O
placeholders O
, O
we O
follow O
Pyatkin O
et O
al O
. O
( O
2021 O
) O
and O
use O
their O
model O
to O
convert O
the O
generated O
question O
into O
a O
more O
natural O
form O
, O
with O
contextualized O
arguments O
. O
Overall O
, O
we O
observed O
that O
this O
approach O
generally O
improves O
the O
quality O
of O
the O
questions O
, O
in O
addition O
to O
the O
contextualization O
utility O
. O
Figure O
3 O
shows O
an O
example O
from O
our O
dataset O
( O
based O
on O
a O
salient O
sentence O
from O
NewSHead O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
) O
that O
follows O
the O
description O
provided O
above O
. O

B O
Pre O
- O
training O
Technical O
Details O

We O
pretrain O
QAMDEN B-MethodName
for O
a O
total O
number O
of O
400 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
( O
the O
validation O
loss O
kept O
decreasing O
along O
the O
entire O
pre O
- O
training O
process O
) O
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
, O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
3e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
and O
with O
10k B-HyperparameterValue
warmup B-HyperparameterName
steps I-HyperparameterName
and O
linear O
decay B-HyperparameterName
, O
all O
follows O
prior O
works O
( O
Beltagy O
et O
al O
. O
, O
2020 O
; O
Xiao O
et O
al O
. O
, O
2022 O
) O
. O
The O
pre O
- O
training O
process O
takes O
likely O
eight O
days O
on O
eight O
48 O
GB O
RTX8000 O
GPUs O
. O
Since O
the O
backbone O
of O
both O
QAMDEN B-MethodName
and O
PRIMERA B-MethodName
is O
the O
Longformer B-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
model O
( O
LED B-MethodName
) O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
large O
version O
, O
they O
all O
have O
the O
same O
number O
of O
parameters O
( O
447 O
M O
) O
. O
LED B-MethodName
uses O
a O
sparse O
local+global O
attention O
pattern O
in O
the O
encoder O
self O
- O
attention O
side O
, O
while O
using O
the O
full O
attention O
on O
decoder O
and O
crossattention O
. O

C O
Benchmarks O
Description O

In O
this O
section O
, O
we O
provide O
further O
details O
regarding O
the O
datasets O
we O
used O
for O
the O
model O
and O
baselines O
evaluation O
. O

C.1 O
Question B-TaskName
Answering I-TaskName
Benchmarks O

We O
first O
describe O
in O
detail O
multi B-TaskName
- I-TaskName
document I-TaskName
question I-TaskName
answering I-TaskName
tasks O
, O
and O
particularly O
the O
task O
of O
multi B-TaskName
- I-TaskName
hop I-TaskName
question I-TaskName
answering I-TaskName
. O
Multi B-TaskName
- I-TaskName
hop I-TaskName
question I-TaskName
answering I-TaskName
involves O
using O
a O
model O
to O
gather O
relevant O
information O
from O
multiple O
documents O
and O
combining O
it O
to O
provide O
the O
correct O
answer O
. O

HotPotQA B-DatasetName
( O
Yang O
et O
al O
. O
, O
2018 O
) O
. O
This O
question B-TaskName
answering I-TaskName
dataset O
consists O
of O
questions O
and O
10 O
paragraphs O
from O
various O
Wikipedia O
documents O
, O
with O
two O
of O
the O
paragraphs O
containing O
the O
necessary O
information O
to O
correctly O
answer O
the O
question O
and O
eight O
additional O
paragraphs O
serving O
as O
distractors O
. O
The O
task O
involves O
identifying O
the O
correct O
answer O
span O
and O
identifying O
supporting O
evidence O
sentences O
. O
( O
For O
more O
details O
on O
the O
dataset O
, O
see O
Yang O
et O
al O
. O
( O
2018 O
) O
. O
) O

WikiHop B-DatasetName
( O
Welbl O
et O
al O
. O
, O
2018 O
) O
. O
WikiHop B-DatasetName
is O
a O
dataset O
that O
includes O
a O
question O
, O
several O
potential O
answers O
( O
ranging O
from O
2 O
to O
79 O
options O
) O
, O
and O
supporting O
contexts O
( O
ranging O
from O
3 O
to O
63 O
paragraphs O
) O
, O
and O
the O
correct O
answer O
. O
This O
dataset O
does O
not O
provide O
any O
information O
about O
the O
intermediate O
steps O
required O
to O
arrive O
to O
the O
correct O
answer O
, O
so O
models O
are O
therefore O
tasked O
to O
deduce O
these O
steps O
based O
on O
the O
provided O
question O
and O
context O
. O

C.2 O
Multi B-TaskName
- I-TaskName
Document I-TaskName
Summarization I-TaskName
Benchmarks O

We O
used O
https O
: O
/ O
/ O
github.com O
/ O
google O
- O
research O
/ O
googleresearch O
/ O
tree O
/ O
master O
/ O
rouge O
for O
computing O
the O
ROUGE B-MetricName
score O
( O
Lin O
and O
Rey O
, O
2004 O
) O
with O
the O
default O
stemmer O
settings O
during O
the O
evaluation O
. O

Multi B-DatasetName
- I-DatasetName
News I-DatasetName
( O
Fabbri O
et O
al O
. O
, O
2019 O
) O
. O
This O
dataset O
is O
a O
collection O
of O
56,216 O
pairs O
of O
news O
articles O
and O
professional O
editors O
- O
written O
summaries O
, O
all O
sourced O
from O
the O
web O
( O
newser.com O
) O
. O
These O
pairs O
include O
trace O
- O
back O
links O
to O
the O
original O
documents O
. O
The O
authors O
of O
the O
dataset O
have O
also O
compared O
it O
to O
other O
datasets O
in O
terms O
of O
coverage O
, O
density O
, O
and O
compression O
, O
and O
found O
that O
the O
it O
is O
plausibly O
diverse O
compared O
to O
other O
similar O
benchmarks O
. O

Multi B-DatasetName
- I-DatasetName
X I-DatasetName
- I-DatasetName
Science I-DatasetName
( O
Lu O
et O
al O
. O
, O
2020 O
) O
. O
This O
dataset O
is O
sourced O
from O
Arxiv O
and O
Microsoft O
academic O
graphs O
, O
where O
the O
summaries O
are O
paragraphs O
of O
related O
work O
sections O
, O
while O
source O
documents O
include O
the O
abstracts O
of O
the O
query O
and O
referred O
papers O
. O
It O
is O
considered O
to O
have O
fewer O
positional O
and O
extractive O
biases O
than O
the O
Multi B-DatasetName
- I-DatasetName
News I-DatasetName
dataset O
, O
transforming O
it O
into O
a O
more O
challenging O
benchmark O
since O
the O
drawback O
of O
getting O
higher O
scores O
for O
a O
copied O
sentence O
at O
a O
specific O
position O
can O
be O
reduced O
. O

C.3 O
Query B-TaskName
- I-TaskName
Focused I-TaskName
Multi I-TaskName
- I-TaskName
Document I-TaskName
Summarization I-TaskName
Benchmarks O

In O
this O
section O
, O
we O
describe O
the O
pair O
of O
datasets O
from O
Pasunuru O
et O
al O
. O
( O
2021a O
) O
that O
were O
used O
in O
our O
experiments O
. O
Similarly O
to O
the O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
experiments O
( O
Appendix O
C.2 O
) O
, O
we O
used O
https O
: O
/ O
/ O
github.com O
/ O
google O
- O
research O
/ O
googleresearch O
/ O
tree O
/ O
master O
/ O
rouge O
for O
computing O
the O
ROUGE B-MetricName
score O
( O
Lin O
and O
Rey O
, O
2004 O
) O
with O
the O
default O
stemmer O
settings O
during O
the O
evaluation O
. O

QmdsCnn B-DatasetName
. O
This O
dataset O
is O
based O
on O
the O
singledocument O
CNN O
/ O
Daily O
Mail O
( O
CNN O
/ O
DM O
) O
summarizastion O
dataset O
( O
Hermann O
et O
al O
. O
, O
2015 O
) O
, O
where O
its O
documents O
are O
news O
articles O
available O
online O
and O
the O
summaries O
are O
their O
human O
written O
highlights O
. O
This O
dataset O
is O
transformed O
to O
multi O
- O
document O
one O
by O
firstly O
chunking O
the O
documents O
into O
small O
documents O
of O
paragraphs O
. O
Then O
, O
the O
titles O
of O
the O
articles O
serve O
as O
the O
queries O
which O
are O
fed O
to O
a O
BM25 O
search O
engine O
( O
Robertson O
and O
Walker O
, O
1994 O
) O
, O
that O
returns O
chunks O
from O
the O
entire O
dataset O
that O
are O
related O
to O
the O
title O
, O
and O
serve O
as O
the O
context O
documents O
. O

QmdsIr B-DatasetName
. O
In O
this O
datasets O
, O
the O
authors O
suggested O
using O
an O
alternative O
to O
the O
queries O
that O
are O
based O
on O
titles O
of O
articles O
-they O
use O
instead O
queries O
that O
are O
issued O
by O
actual O
search O
engine O
users O
, O
which O
is O
more O
realistic O
scenario O
for O
search O
use O
- O
cases O
. O
They O
collect O
queries O
and O
their O
top-10 O
results O
obtained O
by O
the O
Bing O
( O
www.bing.com O
) O
search O
engine O
. O
The O
target O
summary O
is O
derived O
from O
the O
answer O
passage O
, O
which O
is O
extracted O
from O
one O
of O
the O
top O
- O
ranked O
documents O
by O
Bing O
's O
production O
QA B-TaskName
system O
. O
Next O
, O
they O
omit O
the O
document O
that O
contains O
the O
answer O
passage O
from O
the O
context O
documents O
. O

D O
Ablation O
Study O
Details O

In O
this O
section O
, O
we O
provide O
details O
regarding O
the O
baselines O
used O
during O
the O
input O
format O
ablation O
study O
that O
we O
conducted O
, O
and O
was O
presented O
in O
§ O
4.4 O
. O

The O
following O
list O
includes O
the O
detailed O
descriptions O
for O
all O
the O
ablations O
we O
used O
: O

• O
Pre O
- O
training O
without O
questions O
. O
Following O
Jia O
et O
al O
. O
( O
2022 O
) O
, O
we O
omit O
the O
generated O
question O
, O
and O
pre O
- O
train O
the O
model O
to O
predict O
the O
answer O
with O
no O
visible O
question O
within O
the O
context O
. O

• O
Pre O
- O
training O
using O
random O
questions O
per O
context O
documents O
. O
Given O
context O
documents O
, O
we O
sample O
a O
random O
held O
- O
out O
document O
from O
other O
clusters O
, O
and O
generate O
an O
unrelated O
question O
which O
is O
use O
for O
the O
irrelevant O
context O
. O
It O
is O
an O
alternative O
to O
using O
a O
question O
generated O
by O
one O
of O
the O
documents O
in O
the O
context O
. O
• O
Pre O
- O
training O
with O
prefixes O
. O
We O
add O
the O
question O
: O
and O
context O
: O
prefixes O
during O
training O
and O
inference O
. O
These O
should O
further O
direct O
the O
model O
with O
the O
locations O
of O
the O
question O
and O
context O
. O
While O
this O
setup O
slightly O
helps O
for O
QA B-TaskName
, O
we O
show O
that O
for O
MDS B-TaskName
, O
the O
noprefix O
setup O
is O
preferable O
. O

• O
Pre O
- O
training O
while O
placing O
the O
question O
before O
the O
context O
. O
Recall O
that O
QAMDEN B-MethodName
appends O
the O
question O
tokens O
to O
the O
end O
of O
the O
input O
sequence O
, O
after O
the O
context O
documents O
. O
Therefore O
, O
we O
establish O
a O
baseline O
for O
ablating O
this O
setup O
, O
and O
placing O
the O
question O
at O
the O
beginning O
of O
the O
input O
. O

• O
Pre O
- O
training O
with O
question O
filtering O
. O
The O
QASEM B-MethodName
parser O
question O
generation O
model O
can O
be O
noisy O
, O
resulting O
in O
a O
question O
that O
can O
not O
be O
answered O
or O
with O
an O
incorrect O
answer O
to O
a O
generated O
question O
. O
We O
therefore O
follow O
a O
recent O
automatic O
QA O
filtering O
strategy O
that O
suggests O
using O
a O
strong O
QA O
model O
to O
ensure O
that O
valid O
question O
- O
answer O
pairs O
are O
present O
in O
the O
dataset O
( O
Alberti O
et O
al O
. O
, O
2019 O
; O
Fang O
et O
al O
. O
, O
2020 O
) O
. O
pre O
- O
training O
after O
questionanswer O
filtering O
, O
using O
the O
strong O
UnifiedQA B-MethodName
- I-MethodName
v2 I-MethodName
model O
( O
Khashabi O
et O
al O
. O
, O
2022 O
) O
that O
follows O
previous O
UnifiedQA O
( O
Khashabi O
et O
al O
. O
, O
2020 O
) O
and O
trains O
on O
more O
supervised O
datasets O
. O
We O
took O
the O
fine B-MethodName
- I-MethodName
tuned I-MethodName
BART I-MethodName
- I-MethodName
large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
as O
the O
question O
filter O
for O
a O
fair O
comparison O
with O
QASEM B-MethodName
. O
We O
applied O
UnifiedQA B-MethodName
- I-MethodName
v2 I-MethodName
over O
the O
question O
- O
context O
- O
answer O
triplets O
and O
took O
only O
the O
answerable O
questions O
according O
to O
the O
model O
, O
which O
left O
us O
with O
roughly O
25 O
% O
of O
the O
entire O
pre O
- O
training O
data O
. O

• O
Pre O
- O
training O
without O
generating O
the O
salient O
sentence O
. O
Recall O
that O
we O
task O
QAMDEN B-MethodName
to O
generate O
the O
salient O
sentence O
which O
was O
used O
to O
produce O
the O
question O
and O
answer O
. O
This O
should O
enable O
the O
model O
to O
generate O
longer O
sequences O
and O
improve O
the O
coping O
mechanism O
, O
which O
is O
useful O
for O
tasks O
such O
as O
summarization O
. O
This O
hypothesis O
is O
assessed O
by O
executing O
the O
same O
pre O
- O
training O
procedure O
but O
without O
generating O
the O
salient O
sentence O
-only O
the O
answer O
of O
the O
generated O
question O
. O

• O
Using O
alternative O
QA O
generators O
from O
recent O
related O
works O
. O
We O
pre O
- O
train O
a O
model O
based O
on O
the O
QAs O
generated O
by O
two O
QA O
generators O
, O
based O
on O
the O
BART B-MethodName
- I-MethodName
large I-MethodName
model I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
: O
The O
first O
is O
taken O
from O
Jia O
et O
al O
. O
( O
2022 O
) O
12 O
, O
which O
trained O
a O
model O
over O
the O
data O
from O
the O
MRQA O
2019 O
Shared O
Task O
( O
Fisch O
et O
al O
. O
, O
2019 O
) O
and O
the O
second O
is O
the O
QA O
generator O
from O
( O
Khashabi O
et O
al O
. O
, O
2022 O
) O
which O
was O
trained O
on O
eight O
different O
QA O
benchmarks O
( O
see O
full O
list O
and O
references O
in O
Khashabi O
et O
al O
. O
( O
2022 O
, O
Appendix O
A O
) O
) O
. O

• O
Additional O
pre O
- O
training O
for O
PRIMERA B-MethodName
( O
Xiao O
et O
al O
. O
, O
2022 O
) O
-We O
resume O
the O
pre O
- O
training O
of O
the O
100k O
publicly O
released O
checkpoint O
of O
PRIMERA B-MethodName
, O
and O
pre O
- O
train O
for O
an O
additional O
number O
of O
300k B-HyperparameterValue
steps B-HyperparameterName
( O
using O
the O
same O
pre O
- O
training O
format O
and O
procedure O
described O
in O
Xiao O
et O
al O
. O
( O
2022 O
) O
) O
, O
to O
reach O
the O
number O
of O
steps O
used O
for O
pre O
- O
training O
QAMDEN B-MethodName
and O
its O
ablations O
described O
above O
. O

E O
API O
- O
Based O
Models O
Prompting O
Details O

We O
manually O
explored O
several O
prompts O
for O
the O
GPT-3.5 B-MethodName
and O
GPT-4 B-MethodName
chat O
API O
- O
based O
models O
, O
and O
proceeded O
with O
the O
one O
that O
appeared O
to O
be O
the O
most O
effective O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
multi I-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
, O
as O
follows O
. O

Per O
a O
Multi O
- O
News O
example O
where O
we O
are O
given O
k O
context O
documents O
D O
1 O
, O
D O
2 O
, O
. O
. O
. O
, O
D O
k O
, O
we O
prompt O
each O
model O
to O
provide O
an O
summary O
using O
the O
system O
format O
: O
" O
You O
are O
a O
helpful O
assistant O
that O
summarizes O
important O
information O
from O
multiple O
documents O
. O
" O
, O
and O
the O
user O
format O
: O
" O
Summarize O
the O
following O
documents O
into O
a O
single O
summary O
: O
Document O
1 O
: O
D O
1 O
Document O
2 O
: O
D O
2 O
. O
. O
. O
Prince O
William O
and O
wife O
Kate O
Middleton O
have O
adopted O
a O
new O
addition O
to O
the O
family O
: O
a O
cocker O
spaniel O
puppy O
. O
The O
Telegraph O
reports O
that O
the O
couple O
has O
adopted O
a O
baby O
boy O
, O
but O
it O
's O
not O
a O
baby O
. O
The O
puppy O
is O
just O
a O
few O
months O
old O
and O
is O
the O
son O
of O
Kate O
's O
mother O
's O
dog O
, O
Ella O
. O
" O
William O
and O
Catherine O
fell O
in O
love O
with O
the O
pup O
instantly O
and O
it O
was O
n't O
long O
before O
they O
decided O
to O
keep O
him O
, O
" O
a O
palace O
aide O
tells O
US O
Weekly O
. O
" O
He O
's O
now O
part O
of O
the O
royal O
fold O
. O
" O

A O
rush O
- O
hour O
collision O
between O
a O
Chicago O
Transit O
Authority O
bus O
and O
several O
other O
vehicles O
yesterday O
left O
one O
person O
dead O
and O
at O
least O
eight O
others O
injured O
, O
one O
of O
them O
critically O
, O
authorities O
say O
. O
The O
accident O
occurred O
around O
6 O
pm O
in O
the O
north O
Loop O
. O
Authorities O
say O
the O
articulated O
Route O
148 O
Clarendon O
/ O
Michigan O
Express O
bus O
collided O
with O
at O
least O
three O
other O
vehicles O
at O
Michigan O
Avenue O
and O
Lake O
Street O
. O
The O
bus O
went O
onto O
the O
sidewalk O
, O
and O
at O
one O
point O
a O
pedestrian O
was O
pinned O
underneath O
. O
She O
was O
taken O
away O
covered O
in O
a O
sheet O
, O
a O
witness O
who O
ran O
to O
help O
tells O
the O
Chicago O
Tribune O
. O
NBC O
Chicago O
describes O
the O
fatality O
as O
a O
51 O
- O
year O
- O
old O
woman O
. O
The O
driver O
, O
who O
was O
treated O
for O
non O
- O
life O
- O
threatening O
injuries O
, O
was O
the O
only O
person O
on O
the O
bus O
, O
and O
investigators O
are O
looking O
at O
video O
from O
a O
camera O
that O
records O
the O
interior O
of O
the O
bus O
. O

A O
Chicago O
Transit O
Authority O
bus O
was O
involved O
in O
a O
serious O
crash O
during O
rush O
hour O
, O
resulting O
in O
one O
fatality O
and O
eight O
injuries O
. O
The O
bus O
collided O
with O
several O
other O
vehicles O
at O
North O
Michigan O
Avenue O
and O
East O
Lake O
Street O
. O
The O
bus O
driver O
has O
been O
cited O
for O
failing O
to O
stop O
at O
a O
red O
light O
and O
for O
" O
failure O
to O
exercise O
due O
care O
. O
" O
The O
accident O
is O
still O
under O
investigation O
. O
The O
deceased O
has O
been O
identified O
as O
51 O
- O
year O
- O
old O
Aimee O
Coath O
of O
Flossmoor O
. O
The O
eight O
other O
individuals O
, O
including O
the O
bus O
driver O
, O
were O
hospitalized O
with O
non O
- O
life O
- O
threatening O
injuries O
. O

A O
bus O
crash O
in O
downtown O
Chicago O
last O
night O
left O
one O
person O
dead O
and O
eight O
others O
injured O
, O
including O
the O
bus O
driver O
, O
at O
least O
10 O
ambulances O
were O
called O
to O
the O
scene O
, O
reports O
NBC O
Chicago O
. O
The O
fatality O
has O
been O
identified O
as O
51 O
- O
year O
- O
old O
Aimee O
Coath O
of O
Flossmoor O
, O
reports O
the O
Chicago O
Tribune O
. O
Coath O
was O
the O
only O
person O
on O
the O
Chicago O
Transit O
Authority O
bus O
at O
the O
time O
of O
the O
crash O
. O

Geez O
, O
the O
French O
are O
even O
sophisticated O
while O
performing O
wanton O
acts O
of O
destruction O
. O
The O
Verge O
reports O
a O
young O
man O
was O
caught O
on O
video O
calmly O
and O
methodically O
wrecking O
up O
an O
Apple O
Store O
in O
France O
over O
a O
refund O
disagreement O
. O
The O
man O
used O
a O
steel O
ball O
-- O
apparently O
the O
kind O
used O
in O
a O
French O
lawn O
game O
-- O
to O
break O
at O
least O
10 O
iPhones O
and O
a O
MacBook O
Air O
, O
one O
at O
a O
time O
, O
before O
being O
arrested O
outside O
the O
store O
. O
" O
Apple O
is O
a O
company O
that O
violated O
European O
consumers O
' O
rights O
, O
" O
the O
Daily O
Dot O
quotes O
the O
man O
as O
saying O
in O
French O
during O
his O
iPhone O
smashing O
. O
" O
They O
refused O
to O
reimburse O
me O
. O
I O
told O
them O
: O
' O
Give O
me O
my O
money O
back O
. O
' O
They O
said O
no O
. O
So O
you O
know O
what O
's O
happening O
? O
This O
is O
happening O
! O
" O
An O
Apple O
Store O
in O
Dijon O
, O
France O
was O
vandalized O
by O
an O
irate O
customer O
who O
used O
a O
steel O
ball O
to O
smash O
iPhones O
, O
MacBooks O
, O
and O
iPads O
. O
According O
to O
reports O
, O
the O
customer O
was O
in O
a O
dispute O
with O
Apple O
over O
a O
refund O
and O
claimed O
that O
the O
company O
violated O
European O
consumers O
' O
rights O
. O
He O
was O
eventually O
apprehended O
by O
security O
and O
arrested O
after O
causing O
significant O
damage O
to O
the O
store O
. O

A O
video O
of O
an O
angry O
man O
destroying O
everything O
in O
a O
French O
Apple O
Store O
is O
making O
the O
rounds O
on O
the O
Internet O
is O
making O
headlines O
, O
and O
it O
's O
not O
for O
the O
first O
time O
. O
The O
video O
shows O
a O
man O
hurling O
a O
steel O
ball O
through O
a O
store O
's O
windows O
, O
smashing O
everything O
in O
sight O
, O
and O
then O
calmly O
waiting O
for O
security O
to O
come O
and O
stop O
him O
, O
reports O
the O
BBC O
. O
The O
man O
, O
who O
is O
in O
his O
20s O
, O
is O
identified O
as O
a O
French O
citizen O
who O
lives O
in O
the O
Paris O
suburb O
of O
Montpellier O
. O
He O
was O
caught O
on O
surveillance O
video O
at O
the O
store O
on O
Wednesday O
. O

Acknowledgements O

The O
work O
described O
herein O
was O
supported O
by O
the O
PBC O
fellowship O
for O
outstanding O
PhD O
candidates O
in O
data O
science O
, O
in O
part O
by O
grants O
from O
the O
Israel O
Science O
Foundation O
grant O
2827 O
/ O
21 O
, O
and O
by O
a O
grant O
from O
the O
Israel O
Ministry O
of O
Science O
and O
Technology O
. O

F O
System O
Summary O
Examples O
of O
In O
Table O
10 O
, O
we O
include O
three O
examples O
of O
system O
summaries O
produced O
by O
GPT-3.5 B-MethodName
and O
QAMDEN B-MethodName
, O
as O
well O
as O
the O
corresponding O
reference O
( O
groundtruth O
) O
summary O
. O
In O
general O
, O
QAMDEN B-MethodName
's O
summaries O
are O
more O
concise O
, O
include O
less O
redundant O
information O
, O
do O
not O
include O
anecdotal O
information O
, O
and O
overall O
were O
preferred O
by O
the O
human B-MetricName
evaluators I-MetricName
. O

G O
List O
of O
Software O
and O
Data O
Licences O
Used O
in O
this O
Work O

Our O
code O
will O
be O
released O
and O
licensed O
under O
the O
Apache O
License O
2.0 O
license O
. O
Our O
framework O
dependencies O
are O
: O

• O
PRIMERA B-MethodName
: O
https O
: O
/ O
/ O
github.com O
/ O
allenai O
/ O
PRIMER O
/ O
blob O
/ O
main O
/ O
LICENSE O
, O
under O
an O
Apache O
License O
2.0 O
. O
• O
NLTK O
: O
https O
: O
/ O
/ O
github.com O
/ O
nltk O
/ O
nltk O
, O
under O
an O
Apache O
License O
2.0 O
. O

• O
NumPy O
: O
https O
: O
/ O
/ O
github.com O
/ O
numpy O
/ O
numpy O
/ O
blob O
/ O
main O
/ O
LICENSE O
. O
txt O
, O
under O
a O
BSD O
3 O
- O
Clause O
" O
New O
" O
or O
" O
Revised O
" O
License O
. O

• O
seaborn O
: O

https O
: O
/ O
/ O
github.com O
/ O
mwaskom O
/ O
seaborn O
/ O
blob O
/ O
master O
/ O
LICENSE.md O
, O
under O
a O
BSD O
3 O
- O
Clause O
" O
New O
" O
or O
" O
Revised O
" O
License O
. O

• O
openai O
: O

https O
: O
/ O
/ O
github.com O
/ O
openai O
/ O
openai O
- O
python O
/ O
blob O
/ O
main O
/ O
LICENSE O
, O
under O
a O
MIT O
License O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Not O
applicable O
. O
Left O
blank O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Not O
applicable O
. O
Left O
blank O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Not O
applicable O
. O
Left O
blank O
. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Section O
3 O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Section O
4 O
. O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Section O
4 O
, O
Appendix O
B O
. O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Just O
Like O
a O
Human O
Would O
, O
Direct B-MethodName
Access I-MethodName
to I-MethodName
Sarcasm I-MethodName
Augmented I-MethodName
with I-MethodName
Potential I-MethodName
Result I-MethodName
and I-MethodName
Reaction I-MethodName

Sarcasm O
, O
as O
a O
form O
of O
irony O
conveying O
mockery O
and O
contempt O
, O
has O
been O
widespread O
in O
social O
media O
such O
as O
Twitter O
and O
Weibo O
, O
where O
the O
sarcastic O
text O
is O
commonly O
characterized O
as O
an O
incongruity O
between O
the O
surface O
positive O
and O
negative O
situation O
. O
Naturally O
, O
it O
has O
an O
urgent O
demand O
to O
automatically O
identify B-TaskName
sarcasm I-TaskName
from O
social O
media O
, O
so O
as O
to O
illustrate O
people O
's O
real O
views O
toward O
specific O
targets O
. O
In O
this O
paper O
, O
we O
develop O
a O
novel O
sarcasm B-TaskName
detection I-TaskName
method O
, O
namely O
Sarcasm B-MethodName
Detector I-MethodName
with I-MethodName
Augmentation I-MethodName
of I-MethodName
Potential I-MethodName
Result I-MethodName
and I-MethodName
Reaction I-MethodName
( O
SD B-MethodName
- I-MethodName
APRR I-MethodName
) O
. O
Inspired O
by O
the O
direct O
access O
view O
, O
we O
treat O
each O
sarcastic O
text O
as O
an O
incomplete O
version O
without O
latent O
content O
associated O
with O
implied O
negative O
situations O
, O
including O
the O
result O
and O
human O
reaction O
caused O
by O
its O
observable O
content O
. O
To O
fill O
the O
latent O
content O
, O
we O
estimate O
the O
potential O
result O
and O
human O
reaction O
for O
each O
given O
training O
sample O
by O
[ O
xEffect O
] O
and O
[ O
xReact O
] O
relations O
inferred O
by O
the O
pre O
- O
trained O
commonsense O
reasoning O
tool O
COMET O
, O
and O
integrate O
the O
sample O
with O
them O
as O
an O
augmented O
one O
. O
We O
can O
then O
employ O
those O
augmented O
samples O
to O
train O
the O
sarcasm O
detector O
, O
whose O
encoder O
is O
a O
graph O
neural O
network O
with O
a O
denoising O
module O
. O
We O
conduct O
extensive O
empirical O
experiments O
to O
evaluate O
the O
effectiveness O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
. O
The O
results O
demonstrate O
that O
SD B-MethodName
- I-MethodName
APRR I-MethodName
can O
outperform O
strong O
baselines O
on O
benchmark O
datasets O
. O

Introduction O

Sarcasm O
, O
as O
subtle O
figures O
of O
speech O
, O
serves O
many O
communicative O
purposes O
in O
human O
daily O
life O
( O
Ivanko O
and O
Pexman O
, O
2003 O
) O
, O
commonly O
used O
to O
criticize O
an O
individual O
. O
Refer O
to O
the O
formal O
description O
of O
sarcasm O
from O
the O
Oxford O
English O
Dictionary O
: O
1 O
" O
A O
way O
of O
using O
words O
that O
are O
the O
opposite O
of O
what O
you O
mean O
in O
order O
to O
be O
unpleasant O
to O
somebody O
or O
to O
make O
fun O
of O
them O
. O
" O

The O
sarcastic O
text O
is O
typically O
characterized O
as O
an O
incongruity O
between O
the O
positive O
surface O
and O
negative O
situation O
( O
Riloff O
et O
al O
. O
, O
2013 O
; O
. O
For O
example O
, O
as O
an O
obvious O
sarcasm O
" O
I O
love O
working O
for O
six O
hours O
every O
day O
for O
free O
" O
, O
its O
surface O
meaning O
tends O
to O
be O
positive O
, O
conveyed O
by O
the O
sentiment O
word O
" O
love O
" O
, O
but O
it O
corresponds O
to O
a O
negative O
situation O
" O
work O
for O
free O
" O
, O
conveying O
people O
's O
complaint O
. O

Detecting B-TaskName
sarcasm I-TaskName
from O
social O
media O
is O
a O
significant O
task O
due O
to O
the O
universal O
existence O
of O
sarcasm O
, O
but O
its O
complicated O
nature O
makes O
the O
task O
challenging O
. O
To O
resolve O
this O
task O
, O
the O
community O
has O
recently O
proposed O
a O
number O
of O
Sarcasm B-TaskName
Detection I-TaskName
( O
SD B-TaskName
) O
methods O
, O
whose O
major O
idea O
is O
to O
capture O
the O
incongruity O
characteristic O
of O
sarcasm O
( O
Joshi O
et O
al O
. O
, O
2017 O
; O
Xiong O
et O
al O
. O
, O
2019 O
; O
Pan O
et O
al O
. O
, O
2020 O
; O
Agrawal O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2021b O
; O
Lou O
et O
al O
. O
, O
2021 O
) O
. O
For O
example O
, O
several O
early O
SD B-TaskName
studies O
express O
the O
incongruity O
by O
extracting O
positive O
- O
negative O
pairs O
from O
observable O
text O
content O
, O
such O
as O
rule O
- O
based O
method O
( O
Joshi O
et O
al O
. O
, O
2017 O
) O
and O
neural O
networks O
with O
co O
- O
attention O
tricks O
( O
Xiong O
et O
al O
. O
, O
2019 O
; O
Pan O
et O
al O
. O
, O
2020 O
) O
. O
Unfortunately O
, O
those O
methods O
can O
not O
accurately O
capture O
the O
negative O
situations O
, O
which O
are O
mostly O
implied O
and O
associated O
with O
contexts O
and O
background O
information O
. O
To O
alleviate O
this O
issue O
, O
the O
recent O
arts O
of O
SD B-TaskName
express O
the O
negative O
situations O
with O
external O
knowledge O
bases O
. O
From O
the O
perspective O
of O
sentiments O
, O
some O
SD B-TaskName
methods O
employ O
auxiliary O
affective O
lexicons O
, O
e.g. O
, O
SenticNet O
( O
Cambria O
et O
al O
. O
, O
2020 O
) O
, O
to O
estimate O
the O
implied O
affective O
correlations O
among O
words O
and O
phrases O
of O
samples O
( O
Agrawal O
et O
al O
. O
, O
2020 O
; O
Lou O
et O
al O
. O
, O
2021 O
) O
. O
Additionally O
, O
the O
SarDeCK O
method O
( O
Li O
et O
al O
. O
, O
2021b O
) O
employs O
the O
pre O
- O
trained O
commonsense O
reasoning O
tool O
COMET O
( O
Hwang O
et O
al O
. O
, O
2021 O
) O
to O
infer O
the O
relations O
behind O
samples O
as O
their O
implied O
situations O
. O
Despite O
the O
promising O
performance O
, O
their O
expressions O
of O
implied O
negative O
situations O
are O
still O
a O
bit O
abstract O
and O
impalpable O
. O
As O
complicated O
figures O
of O
speech O
, O
we O
are O
particularly O
interested O
in O
how O
do O
human O
beings O
accurately O
identify B-TaskName
sarcasm I-TaskName
? O
Through O
referring O
to O
the O
prior O
psychological O
, O
cognitive O
, O
and O
linguistic O
literature O
( O
Gibbs O
, O
1986 O
; O
W.Gibbs O
, O
2002 O
; O
Ivanko O
and O
Pexman O
, O
2003 O
) O
, O
we O
are O
agreeable O
with O
two O
significant O
viewpoints O
. O
First O
, O
the O
negative O
situations O
of O
sarcasm O
are O
mostly O
associated O
with O
certain O
social O
events O
( O
Pickering O
et O
al O
. O
, O
2018 O
) O
, O
and O
human O
beings O
can O
often O
easily O
identify O
the O
events O
with O
the O
background O
information O
in O
the O
brain O
. O
Second O
, O
from O
the O
direct O
access O
view O
( O
Giora O
and O
Fein O
, O
1999 O
; O
W.Gibbs O
, O
2002 O
; O
Ivanko O
and O
Pexman O
, O
2003 O
) O
, O
human O
beings O
are O
likely O
to O
directly O
understand O
the O
whole O
sarcastic O
text O
with O
both O
literal O
meanings O
and O
implied O
negative O
situations O
, O
which O
can O
be O
easily O
captured O
by O
them O
. O

Based O
on O
the O
analysis O
, O
what O
we O
expect O
is O
to O
develop O
a O
novel O
SD B-TaskName
method O
by O
simulating O
the O
way O
of O
human O
thinking O
. O
Inspired O
by O
the O
direct O
access O
view O
, O
we O
treat O
each O
sarcastic O
text O
as O
an O
incomplete O
version O
without O
latent O
content O
associated O
with O
implied O
negative O
situations O
. O
We O
can O
use O
the O
associated O
social O
events O
to O
express O
the O
negative O
situations O
due O
to O
their O
strong O
connection O
. O
Further O
, O
we O
assume O
the O
social O
events O
can O
be O
mainly O
expressed O
by O
the O
potential O
results O
and O
human O
reactions O
that O
the O
events O
produced O
( O
see O
examples O
in O
Table O
1 O
) O
. O
Accordingly O
, O
for O
each O
given O
sample O
we O
can O
estimate O
its O
potential O
result O
and O
human O
reaction O
by O
pre O
- O
trained O
commonsense O
reasoning O
tools O
( O
acted O
as O
background O
information O
) O
, O
and O
then O
integrate O
the O
observable O
text O
content O
with O
them O
as O
an O
augmented O
sample O
( O
acted O
as O
the O
whole O
text O
) O
. O
Finally O
, O
we O
can O
use O
those O
augmented O
samples O
to O
train O
the O
sarcasm O
detector O
( O
just O
like O
a O
human O
would O
) O
. O

Upon O
these O
ideas O
, O
we O
propose O
a O
novel O
SD B-TaskName
method O
, O
namely O
Sarcasm B-MethodName
Detector I-MethodName
with I-MethodName
Augmentation I-MethodName
of I-MethodName
Potential I-MethodName
Result I-MethodName
and I-MethodName
Reaction I-MethodName
( O
SD B-MethodName
- I-MethodName
APRR I-MethodName
) O
. O
Specifically O
, O
we O
estimate O
the O
potential O
result O
and O
human O
reaction O
for O
each O
training O
sample O
by O
[ O
xEffect O
] O
and O
[ O
xReact O
] O
relations O
inferred O
by O
the O
auxiliary O
commonsense O
reasoning O
tool O
COMET O
( O
Hwang O
et O
al O
. O
, O
2021 O
) O
, O
and O
then O
integrate O
the O
sample O
with O
them O
to O
generate O
an O
augmented O
one O
, O
dubbed O
as O
event O
- O
augmented O
sample O
. O
By O
analogy O
to O
( O
Lou O
et O
al O
. O
, O
2021 O
; O
Liang O
et O
al O
. O
, O
2022 O
) O
, O
we O
assume O
that O
the O
syntactic O
information O
of O
eventaugmented O
samples O
can O
intuitively O
imply O
the O
incongruity O
of O
sarcasm O
. O
Accordingly O
, O
we O
transform O
each O
event O
- O
augmented O
sample O
into O
a O
dependency O
graph O
( O
Nivre O
, O
2003 O
) O
, O
and O
suggest O
a O
graph O
- O
based O
encoder O
to O
generate O
sample O
embeddings O
. O
Additionally O
, O
to O
resolve O
the O
noisy O
results O
and O
reactions O
inferred O
by O
COMET O
, O
we O
suggest O
a O
denoising O
module O
with O
the O
dynamic O
masking O
trick O
( O
Yang O
et O
al O
. O
, O
2021 O
) O
, O
enabling O
to O
improve O
the O
quality O
of O
sample O
embeddings O
. O
With O
those O
embeddings O
, O
a O
single O
- O
layer O
MLP O
is O
used O
as O
the O
sarcastic O
classifier O
finally O
. O
To O
examine O
the O
effectiveness O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
, O
we O
conduct O
extensive O
experiments O
on O
benchmark O
datasets O
. O
The O
empirical O
results O
demonstrate O
that O
SD B-MethodName
- I-MethodName
APRR I-MethodName
can O
outperform O
the O
existing O
baseline O
methods O
. O

The O
contributions O
of O
this O
work O
can O
be O
summarized O
as O
follows O
: O

• O
We O
propose O
a O
novel O
SD B-TaskName
method O
, O
named O
SD B-MethodName
- I-MethodName
APRR I-MethodName
, O
with O
event O
- O
augmented O
samples O
formed O
by O
the O
auxiliary O
commonsense O
reasoning O
tool O
COMET O
. O

• O
We O
suggest O
a O
graph O
- O
based O
encoder O
with O
a O
denoising O
module O
, O
enabling O
to O
generate O
strong O
sample O
embeddings O
. O

• O
The O
experimental O
results O
indicate O
that O
SD B-MethodName
- I-MethodName
APRR I-MethodName
can O
achieve O
competitive O
performance O
compared O
with O
existing O
baselines O
. O

2 O
Related O
Works O

Sarcasm B-TaskName
Detection I-TaskName

Early O
SD B-TaskName
methods O
are O
mostly O
based O
on O
special O
rules O
and O
evidence O
( O
Maynard O
and O
Greenwood O
, O
2014 O
; O
Bharti O
et O
al O
. O
, O
2015 O
; O
Riloff O
et O
al O
. O
, O
2013 O
) O
. O
For O
instance O
, O
the O
study O
( O
Maynard O
and O
Greenwood O
, O
2014 O
) O
treats O
the O
hashtag O
sentiment O
as O
the O
key O
indicator O
of O
sarcasm O
since O
the O
hashtags O
are O
usually O
taken O
to O
highlight O
sarcasm O
in O
Tweets O
; O
and O
other O
methods O
employ O
various O
evidence O
, O
such O
as O
parser O
- O
based O
negative O
phrase O
matching O
, O
interjections O
( O
Bharti O
et O
al O
. O
, O
2015 O
) O
, O
and O
positive O
- O
negative O
word O
pairs O
( O
Riloff O
et O
al O
. O
, O
2013 O
) O
. O
Some O
other O
methods O
form O
incongruity O
- O
specific O
embeddings O
for O
sarcastic O
texts O
, O
such O
as O
shape O
and O
pointedness O
of O
words O
( O
Ptáček O
et O
al O
. O
, O
2014 O
) O
, O
extensions O
of O
words O
( O
Rajadesingan O
et O
al O
. O
, O
2015 O
) O
, O
and O
unexpectedness O
( O
Reyes O
et O
al O
. O
, O
2012 O
) O
. O
Due O
to O
the O
success O
of O
neural O
networks O
, O
the O
mainstream O
SD B-TaskName
methods O
nowadays O
apply O
them O
to O
capture O
the O
incongruity O
between O
positive O
surface O
and O
negative O
situations O
within O
the O
sarcastic O
text O
. O
Early O
methods O
mainly O
capture O
the O
incongruity O
from O
the O
observable O
text O
content O
( O
Tay O
et O
al O
. O
, O
2018 O
; O
Xiong O
et O
al O
. O
, O
2019 O
; O
Pan O
et O
al O
. O
, O
2020 O
) O
. O
For O
instance O
, O
the O
methods O
of O
( O
Xiong O
et O
al O
. O
, O
2019 O
; O
Pan O
et O
al O
. O
, O
2020 O
) O
extract O
positive O
- O
negative O
word O
pairs O
and O
phrase O
pairs O
with O
co O
- O
attention O
tricks O
. O
However O
, O
those O
methods O
can O
not O
fully O
understand O
the O
negative O
situation O
due O
to O
its O
implicit O
nature O
. O
To O
resolve O
this O
issue O
, O
the O
recent O
methods O
employ O
external O
resources O
to O
capture O
negative O
situations O
and O
further O
incongruities O
of O
sarcastic O
texts O
( O
Agrawal O
et O
al O
. O
, O
2020 O
; O
Lou O
et O
al O
. O
, O
2021 O
; O
Li O
et O
al O
. O
, O
2021b O
; O
. O
For O
example O
, O
the O
ADGCN O
method O
( O
Lou O
et O
al O
. O
, O
2021 O
) O
employs O
the O
affective O
lexicon O
SenticNet O
( O
Cambria O
et O
al O
. O
, O
2020 O
) O
to O
represent O
intra O
- O
sentence O
affective O
relations O
; O
and O
the O
DC O
- O
Net O
method O
exploits O
sentiment O
lexicon O
to O
separate O
literal O
meanings O
from O
texts O
and O
further O
estimates O
sentiment O
conflicts O
. O
Orthogonal O
to O
the O
aforementioned O
methods O
, O
our O
SD B-MethodName
- I-MethodName
APRR I-MethodName
forms O
augmented O
samples O
by O
commonsense O
reasoning O
and O
treats O
the O
augmented O
ones O
as O
the O
whole O
versions O
of O
sarcastic O
texts O
from O
the O
direct O
access O
view O
( O
Giora O
and O
Fein O
, O
1999 O
; O
W.Gibbs O
, O
2002 O
; O
Ivanko O
and O
Pexman O
, O
2003 O
) O
. O

Commonsense O
Knowledge O
Graph O

Large O
- O
scale O
commonsense O
knowledge O
graphs O
( O
Lin O
et O
al O
. O
, O
2019 O
; O
Yin O
et O
al O
. O
, O
2022 O
) O
can O
conduct O
reasoning O
for O
texts O
to O
infer O
the O
commonsense O
knowledge O
behind O
them O
, O
and O
they O
have O
been O
widely O
applied O
to O
a O
wide O
range O
of O
natural O
language O
processing O
tasks O
, O
such O
as O
dialogue O
generation O
( O
Sabour O
et O
al O
. O
, O
2022 O
) O
, O
relation O
classification O
( O
Hosseini O
et O
al O
. O
, O
2022 O
) O
, O
and O
emotion O
recognition O
( O
Li O
et O
al O
. O
, O
2021a O
) O
. O
To O
our O
knowledge O
, O
some O
representatives O
include O
Concept O
- O
Net O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
, O
ATOMIC O
( O
Sap O
et O
al O
. O
, O
2019 O
) O
, O
and O
TransOMCS O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O
The O
Con- O

The O
Proposed O
SD B-MethodName
- I-MethodName
APRR I-MethodName
Method O

In O
this O
section O
, O
we O
briefly O
describe O
the O
task O
definition O
of O
SD B-TaskName
and O
the O
commonsense O
reasoning O
tool O
COMET O
. O
We O
then O
introduce O
the O
proposed O
SD B-MethodName
- I-MethodName
APRR I-MethodName
method O
in O
more O
detail O
. O
For O
clarity O
, O
we O
summarize O
the O
important O
notations O
in O
Table O
2 O
. O
The O
outputs O
of O
the O
two O
relations O
can O
be O
directly O
used O
as O
the O
auxiliary O
augmentation O
in O
SD B-MethodName
- I-MethodName
APRR I-MethodName
. O
We O
declare O
that O
the O
COMET O
takes O
the O
large O
version O
of O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
as O
the O
backbone O
, O
which O
contains O
24 O
layers O
, O
1024 O
- O
dimensional O
hidden O
embeddings O
, O
and O
16 O
heads O
for O
self O
- O
attention O
. O
2 O
Then O
it O
was O
fine O
- O
tuned O
over O
ATOMIC O
20 O
20 O
. O

Overview O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName

As O
depicted O
in O
Fig O
. O
1 O
, O
our O
SD B-MethodName
- I-MethodName
APRR I-MethodName
mainly O
consists O
of O
three O
components O
. O
( O
1 O
) O
Event O
- O
augmented O
samples O
generation O
: O
For O
each O
raw O
text O
s O
i O
, O
we O
employ O
COMET O
to O
infer O
its O
result O
e O
r O
i O
and O
human O
reaction O
e O
h O
i O
, O
and O
then O
concatenate O
them O
to O
form O
the O
corresponding O
event O
- O
augmented O
sample O
s O
e O
i O
. O

( O
2 O
) O
Maksed O
graph O
- O
based O
encoder O
: O
For O
each O
event O
- O
augmented O
sample O
s O
e O
i O
, O
we O
transform O
it O
into O
a O
dependency O
graph O
G O
i O
, O
and O
encode O
G O
i O
as O
the O
sample O
embedding O
z O
i O
by O
leveraging O
a O
graph O
neural O
network O
encoder O
with O
dynamic O
masking O
. O
( O
3 O
) O
Sarcastic O
classifier O
: O
With O
z O
i O
, O
we O
predict O
the O
category O
label O
by O
employing O
a O
singlelayer O
MLP O
finally O
. O
In O
the O
following O
, O
we O
introduce O
each O
component O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
in O
more O
detail O
. O

e O
r O
i O
= O
{ O
w O
i1 O
, O
• O
• O
• O
, O
w O
iM O
} O
and O
e O
h O
i O
= O
{ O
w O
i1 O
, O
• O
• O
• O
, O
w O
i O
M O
} O

as O
the O
result O
and O
human O
reaction O
of O
the O
implied O
social O
event O
behind O
s O
i O
. O
We O
then O
concatenate O
them O
to O
form O
its O
event O
- O
augmented O
version O
. O
For O
semantic O
coherence O
, O
we O
further O
leverage O
two O
linkers O
l O
r O
and O
l O
h O
, O
where O
l O
r O
denotes O
" O
then O
may O
" O
for O
e O
r O
i O
and O
l O
r O
denotes O
" O
and O
I O
feel O
" O
for O
e O
h O
i O
. O
Accordingly O
, O
the O
final O
event O
- O
augmented O
sample O
is O
formed O
by O

s O
e O
i O
= O
s O
i O
⊕ O
l O
e O
⊕ O
e O
r O
i O
⊕ O
l O
h O
⊕ O
e O
h O
i O
, O

Masked O
Graph O
- O
based O
Encoder O

Given O
event O
- O
augmented O
samples O
{ O
s O
e O
i O
} O
N O
i=1 O
, O
we O
suggest O
a O
masked O
graph O
- O
based O
encoder O
to O
induce O
their O
embeddings O
{ O
z O
i O
} O
N O
i=1 O
. O

Constructing O
Graphs O
of O
Samples O

By O
analogy O
to O
( O
Lou O
et O
al O
. O
, O
2021 O
; O
Liang O
et O
al O
. O
, O
2022 O
) O
, O
we O
assume O
that O
the O
syntactic O
information O
of O
eventaugmented O
samples O
can O
intuitively O
imply O
the O
incongruity O
of O
sarcasm O
. O
Accordingly O
, O
we O
transform O
each O
s O
e O
i O
into O
an O
undirected O
graph O
G O
i O
= O
{ O
V O
i O
, O
E O
i O
} O
with O
the O
off O
- O
the O
- O
shelf O
dependency O
parsing O
tool O
, O
3 O
where O
V O
i O
is O
the O
set O
of O
nodes O
, O
i.e. O
, O
the O
tokens O
occurring O
in O
s O
e O
i O
, O
and O
E O
i O
is O
the O
set O
of O
edges O
computed O
by O
dependency O
parsing O
. O
Define O
A O
i O
∈ O
{ O
0 O
, O
1 O
} O
M O
e O
×M O
e O
as O
its O
corresponding O
adjacency O
matrix O
, O
and O
1 O
/ O
0 O
denotes O
the O
component O
corresponds O
to O
an O
edge O
or O
not O
. O
Besides O
, O
each O
node O
is O
with O
self O
- O
loop O
. O

Initializing O
Node O
Embeddings O

For O
each O
G O
i O
, O
we O
initialize O
its O
node O
embeddings O

H O
( O
0 O
) O
i O
= O
[ O
h O
( O
0 O
) O
i1 O
, O
• O
• O
• O
, O
h O
( O
0 O
) O

iM O
e O
] O
⊤ O
by O
leveraging O
a O
singlelayer O
Bi O
- O
LSTM O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O
Specifically O
, O
we O
represent O
the O
nodes O

X O
i O
= O
[ O
x O
i1 O
, O
• O
• O
• O
, O
x O
iM O
e O
] O

⊤ O
by O
the O
pre O
- O
trained O
GloVe O
word O
embeddings O
, O
and O
then O
feed O
X O
i O
into O
the O
Bi O
- O
LSTM O
as O
follows O
: O

H O
( O
0 O
) O
i O
= O
Bi O
- O
LSTM O
( O
X O
i O
; O
W O
b O
) O
, O
( O
1 O
) O

where O
W O
b O
is O
the O
trainable O
parameter O
of O
Bi O
- O
LSTM O
. O

Learning O
Sample O
Embeddings O
with O
Dynamic O
Masking O

Given O
each O
pair O
{ O
G O
i O
, O
H O

i O
} O
, O
we O
optimize O
the O
node O
embeddings O
H O

( O
l O
) O
i O
= O
[ O
h O
( O
l O
) O
i1 O
, O
• O
• O
• O
, O
h O
( O
l O
) O

iM O
e O
] O
⊤ O
by O
a O
Llayer O
graph O
neural O
network O
encoder O
with O
dynamic O
masking O
( O
Yang O
et O
al O
. O
, O
2021 O
) O
, O
and O
then O
form O
the O
final O
sample O
embedding O
z O
i O
by O
leveraging O
the O
readout O
operator O
with O
H O
i O
. O

To O
be O
specific O
, O
the O
learning O
process O
of O
node O
embeddings O
for O
each O
layer O
is O
formulated O
below O
: O

h O
( O
l O
) O
ij O
= O
ReLU O
W O
( O
l O
) O
n O
m O
( O
l O
) O
ij O
h O
( O
l−1 O
) O
ij O
⊕ O
h O
( O
l−1 O
) O
N O
( O
ij O
) O
, O
j O
= O
1 O
, O
• O
• O
• O
, O
M O
e O
, O
l O
= O
1 O
, O
• O
• O
• O
, O
L O
, O
( O
2 O
) O

where O

W O
n O
= O
{ O
W O
( O
l O
) O

n O
} O
L O
l=1 O
are O
the O
trainable O
parameters O
; O
m O
ij O
∈ O
[ O
0 O
, O
1 O
] O
is O
the O
mask O
weight O
of O
the O
j O
- O
th O
node O
, O
used O
to O
capture O
the O
possible O
noisy O
e O
r O
i O
and O
e O
h O
i O
inferred O
by O
COMET O
; O
N O
( O
ij O
) O
denotes O
the O
neighbor O
set O
of O
the O
j O
- O
th O
node O
; O
and O
h O

( O
l−1 O
) O
N O
( O
ij O
) O
= O
k∈N O
( O
ij O
) O
m O
( O
l−1 O
) O
ik O
h O
( O
l−1 O
) O
ik O

is O
the O
weighted O
sum O
of O
the O
neighbors O
of O
the O
j O
- O
th O
node O
. O

The O
update O
process O
of O
the O
mask O
weights O
for O
each O
layer O
is O
formulated O
below O
: O

m O
( O
l O
) O
ij O
= O
Sigmoid O
W O
( O
l O
) O
mĥ O
( O
l−1 O
) O
ij O
⊕ O
W O
( O
l O
) O
f O
h O
( O
l−1 O
) O
N O
( O
ij O
) O
j O
= O
1 O
, O
• O
• O
• O
, O
M O
e O
, O
l O
= O
1 O
, O
• O
• O
• O
, O
L O
, O
( O
3 O
) O
where O
W O
m O
= O
{ O
W O
( O
l O
) O
m O
} O
L O
l=1 O
and O
W O
f O
= O
{ O
W O
( O
l O
) O
f O
} O
L O
l=1 O

are O
the O
trainable O
parameters O
; O
andĥ O

( O
l−1 O
) O
ij O
= O
m O
( O
l−1 O
) O
ij O
h O
( O
l−1 O
) O
ij O
. O

After O
obtaining O
the O
node O
embeddings O
H O
( O
L O
) O
i O
of O
the O
last O
layer O
, O
we O
can O
form O
the O
sample O
embedding O
z O
i O
by O
leveraging O
the O
readout O
operator O
as O
follows O
: O

z O
i O
= O
1 O
M O
e O
M O
e O
i=1 O
h O
( O
L O
) O
i O
( O
4 O
) O

Sarcastic O
Classifier O
and O
Training O
Objective O

Given O
the O
sample O
embeddings O
{ O
z O
i O
} O
N O
i=1 O
, O
we O
employ O
a O
single O
- O
layer O
MLP O
as O
the O
sarcastic O
classifier O
. O
For O
each O
z O
i O
, O
we O
predict O
its O
category O
labelŷ O
i O
by O
the O
following O
equation O
: O

y O
i O
= O
Softmax O
( O
W O
c O
z O
i O
) O
, O
( O
5 O
) O

where O
W O
c O
is O
the O
trainable O
parameter O
of O
the O
sarcastic O
classifier O
. O

Consider O
N O
training O
pairs O
{ O
( O
z O
i O
, O
y O
i O
) O
} O
N O
i=1 O
, O
we O
can O
formulate O
the O
full O
objective O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
with O
respect O
to O
all O
trainable O
parameters O

W O
= O
{ O
W O
b O
, O
W O
n O
, O
W O
m O
, O
W O
f O
, O
W O
c O
} O
: O
L O
( O
W O
) O
= O
N O
i=1 O
L O
CE O
( O
y O
i O
, O
ŷ O
i O
) O
+ O
λ∥W∥ B-HyperparameterName
2 O
, O
( O
6 O
) O

where O
L O
CE O
is O
the O
cross O
- O
entropy O
loss O
; O
∥ O
• O
∥ O
denotes O
the O
ℓ O
2 O
-norm O
; O
and O
λ B-HyperparameterName
∈ O
[ O
0 O
, O
1 O
] O
is O
the O
regularization O
coefficient O
. O

Experiment O

Experimental O
Settings O

Datasets O
. O
To O
thoroughly O
evaluate O
the O
performance O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
, O
we O
conduct O
experiments O
on O
four O
publicly O
available O
SD B-TaskName
datasets O
with O
different O
scales O
. O
Their O
statistics O
of O
are O
shown O
in O
Table O
3 O
, O
and O
they O
are O
briefly O
described O
below O
: O

• O
SemEval18 B-DatasetName
is O
collected O
in O
SemEval O
2018 O
Task O
3 O
Subtask O
A O
( O
Van O
Hee O
et O
al O
. O
, O
2018 O
) O
. O

• O
iSarcasm B-DatasetName
( O
Oprea O
and O
Magdy O
, O
2020 O
) O
consists O
of O
tweets O
written O
by O
participants O
of O
an O
online O
survey O
and O
thus O
is O
for O
intented O
sarcasm B-TaskName
detection I-TaskName
. O

• O
Ghosh B-DatasetName
( O
Ghosh O
and O
Veale O
, O
2016 O
) O
is O
collected O
from O
Twitter O
and O
leverages O
hashtag O
to O
automatically O
annotate O
samples O
. O

• O
IAC B-DatasetName
- I-DatasetName
V2 I-DatasetName
( O
Abbott O
et O
al O
. O
, O
2016 O
) O
is O
sourced O
from O
online O
political O
debates O
forum O
. O
4 O
Compared O
with O
other O
datasets O
, O
the O
samples O
of O
IAC B-DatasetName
- I-DatasetName
V2 I-DatasetName
are O
relatively O
longer O
and O
more O
normative O
. O
Baselines O
. O
We O
select O
a O
number O
of O
recent O
baseline O
methods O
for O
comparison O
. O
They O
are O
briefly O
described O
below O
: O

• O
NBOW B-MethodName
: O
A O
traditional O
SD B-TaskName
method O
that O
represents O
samples O
by O
the O
averages O
of O
word O
embeddings O
. O

• O
Bi B-MethodName
- I-MethodName
LSTM I-MethodName
: O
A O
SD B-TaskName
method O
that O
sequentially O
encodes O
sarcastic O
texts O
with O
Bi O
- O
LSTM O
. O

• O
SIARN B-MethodName
and O
MIARN B-MethodName
( O
Tay O
et O
al O
. O
, O
2018 O
) O
: O
Two O
RNN O
- O
based O
SD B-TaskName
methods O
that O
capture O
the O
incongruity O
by O
using O
the O
single O
- O
dimensional O
and O
multi O
- O
dimensional O
intra O
- O
sentence O
attentions O
, O
respectively O
. O
We O
implement O
in O
- O
house O
codes O
. O

• O
SAWS B-MethodName
5 O
( O
Pan O
et O
al O
. O
, O
2020 O
) O
: O
A O
CNN O
- O
based O
SD B-TaskName
method O
that O
cuts O
each O
text O
sample O
into O
snippets O
and O
uses O
self O
- O
attention O
to O
re O
- O
weight O
them O
. O

• O
ADGCN B-MethodName
6 O
( O
Lou O
et O
al O
. O
, O
2021 O
) O
: O
A O
GCN O
- O
based O
SD B-TaskName
method O
that O
builds O
affective O
and O
dependency O
graphs O
with O
SenticNet O
to O
capture O
the O
incongruity O
in O
a O
long O
distance O
. O

• O
DC B-MethodName
- I-MethodName
Net I-MethodName
7 O
: O
A O
BERT O
- O
based O
SD B-TaskName
method O
that O
respectively O
encodes O
literal O
5 O
https O
: O
/ O
/ O
github.com O
/ O
marvel2120 O
/ O
SAWS O
6 O
https O
: O
/ O
/ O
github.com O
/ O
HLT-HITSZ O
/ O
ADGCN O
7 O
https O
: O
/ O
/ O
github.com O
/ O
yiyi-ict O
/ O
dual O
- O
channel O
- O
for O
- O
sarcasm O
meanings O
and O
implied O
meanings O
by O
the O
external O
sentiment O
lexicon O
. O

• O
SarDeCK B-MethodName
8 O
( O
Li O
et O
al O
. O
, O
2021b O
) O
A O
BERT O
- O
based O
SD B-TaskName
method O
that O
uses O
the O
COMET O
to O
derive O
dynamic O
commonsense O
knowledge O
and O
fuses O
the O
knowledge O
to O
enrich O
the O
contexts O
with O
attention O
. O

Implementation O
details O
. O
In O
the O
experiments O
, O
except O
the O
BERT O
- O
based O
methods O
, O
we O
apply O
the O
300dimensional O
GloVe O
embeddings O
9 O
to O
represent O
the O
words O
initially O
. O
The O
dimension B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
Bi I-HyperparameterName
- I-HyperparameterName
LSTM I-HyperparameterName
output I-HyperparameterName
is O
set O
to O
300 B-HyperparameterValue
, O
and O
the O
layer B-HyperparameterName
number I-HyperparameterName
of O
the O
masked O
graph O
- O
based O
encoder O
is O
set O
to O
3 B-HyperparameterValue
. O
For O
all O
neural O
network O
- O
based O
methods O
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
. O
We O
take O
Adam O
as O
the O
optimizer B-HyperparameterName
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
0.001 B-HyperparameterValue
. O
The O
regularization O
coefficient O
λ B-HyperparameterName
is O
set O
to O
0.01 B-HyperparameterValue
. O
Besides O
, O
we O
use O
the O
Xavier O
Uniform O
to O
initialize O
the O
parameters O
. O
For O
the O
BERTbased O
methods O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
epochs I-HyperparameterName
is O
set O
to O
6 B-HyperparameterValue
, O
while O
for O
other O
methods O
, O
the O
epoch B-HyperparameterName
number I-HyperparameterName
is O
fixed O
to O
100 B-HyperparameterValue
with O
an O
early O
stopping O
mechanism O
( O
Lou O
et O
al O
. O
, O
2021 O
) O
. O
In O
terms O
of O
all O
datasets O
, O
the O
splitting O
of O
training O
and O
testing O
is O
shown O
in O
Table O
3 O
. O
We O
independently O
run O
all O
comparing O
methods O
5 O
times O
and O
report O
the O
average O
results O
. O

Results O
and O
Analysis O

The O
main O
results O
of O
all O
comparing O
methods O
are O
reported O
in O
Table O
4 O
, O
and O
we O
draw O
the O
following O
observations O
: O
( O
1 O
) O
First O
, O
it O
can O
be O
clearly O
seen O
that O
our O
SD B-MethodName
- I-MethodName
APRR I-MethodName
can O
achieve O
the O
highest O
scores O
of O
both O
Accuracy B-MetricName
and O
Macro B-MetricName
- I-MetricName
F1 I-MetricName
in O
most O
settings O
, O
where O
it O
ranks O
the O
first O
over O
SemEval18 B-DatasetName
, O
iSarcasm B-DatasetName
, O
and O
IAC B-DatasetName
- I-DatasetName
V2 I-DatasetName
, O
and O
ranks O
the O
second O
over O
Ghosh B-DatasetName
. O

( O
2 O
) O
Second O
, O
we O
observe O
that O
SD B-MethodName
- I-MethodName
APRR I-MethodName
mostly O
outperforms O
the O
recent O
strong O
baseline O
SarDeCK B-MethodName
, O
which O
also O
employs O
COMET O
to O
generate O
auxiliary O
commonsense O
relations O
. O
A O
major O
difference O
between O
SarDeCK B-MethodName
and O
SD B-MethodName
- I-MethodName
APRR I-MethodName
is O
that O
the O
former O
integrates O
training O
samples O
with O
their O
corresponding O
commonsense O
results O
of O
COMET O
at O
the O
embedding O
level O
, O
while O
the O
latter O
treats O
the O
augmentations O
of O
raw O
training O
texts O
and O
inferred O
commonsense O
results O
of O
COMET O
as O
the O
whole O
raw O
texts O
. O
So O
the O
improvements O
to O
SarDeCK B-MethodName
indirectly O
indicate O
that O
the O
direct O
access O
view O
may O
be O
a O
better O
perspective O
for O
SD B-TaskName
. O
( O
3 O
) O
Third O
, O
compared O
with O
ADGCN B-MethodName
that O
is O
also O
based O
on O
graph O
neural O
networks O
, O
our O
SD B-MethodName
- I-MethodName
APRR I-MethodName
achieves O
significant O
improvements O
over O
all O
datasets O
. O
This O
indicates O
that O
leveraging O
contextually O
inferred O
results O
and O
reactions O
can O
be O
a O
more O
efficient O
way O
for O
SD B-TaskName
than O
leveraging O
context O
- O
free O
affective O
lexicons O
in O
a O
static O
way O
. O
( O
4 O
) O
Finally O
, O
SD B-MethodName
- I-MethodName
APRR I-MethodName
, O
ADGCN B-MethodName
, O
DC B-MethodName
- I-MethodName
Net I-MethodName
, O
and O
SarDeCK B-MethodName
consistently O
perform O
better O
than O
NBOW B-MethodName
, O
Bi B-MethodName
- I-MethodName
LSTM I-MethodName
, O
SIARN B-MethodName
, O
MIARN B-MethodName
, O
and O
SAWS B-MethodName
, O
the O
methods O
without O
external O
resources O
. O
The O
results O
support O
the O
previous O
statement O
that O
understanding O
sarcasm O
heavily O
relies O
on O
human O
background O
information O
. O

Ablation O
Study O

We O
conduct O
ablation O
studies O
to O
examine O
the O
effectiveness O
of O
the O
augmentations O
of O
results O
, O
augmentations O
of O
human O
reactions O
, O
and O
the O
denoising O
module O
. O

The O
results O
are O
reported O
in O
Table O
5 O
. O
Overall O
, O
when O
removing O
the O
results O
( O
w O
/ O
o O
Result O
) O
and O
the O
reactions O
( O
w O
/ O
o O
Reaction O
) O
, O
the O
performance O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
show O
a O
decline O
on O
all O
datasets O
. O
This O
indicates O
that O
the O
potential O
results O
enable O
the O
SD B-MethodName
- I-MethodName
APRR I-MethodName
to O
have O
extra O
explainable O
contexts O
to O
understand O
the O
negativity O
inside O
the O
negative O
situations O
. O
Meanwhile O
, O
human O
reactions O
provide O
explicit O
emotional O
clues O
that O
can O
be O
related O
to O
the O
negative O
situations O
during O
graph O
learning O
. O
However O
, O
when O
removing O
the O
denoising O
module O
( O
w O
/ O
o O
Masking O
) O
, O
the O
performance O
of O
SD B-MethodName
- I-MethodName
APRR I-MethodName
decreases O
across O
the O
IAC B-DatasetName
- I-DatasetName
V2 I-DatasetName
dataset O
. O
This O
is O
because O
samples O
in O
the O
Ghosh B-DatasetName
are O
short O
texts O
, O
and O
their O
syntactical O
information O
may O
not O
be O
accurately O
captured O
, O
leading O
the O
masked O
graph O
- O
based O
encoder O
skips O
nodes O
related O
to O
the O
sarcasm O
by O
mistake O
. O
Additionally O
, O
we O
replace O
the O
masked O
graphbased O
encoder O
with O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
further O
compare O
this O
BERT B-MethodName
- I-MethodName
based I-MethodName
version I-MethodName
of I-MethodName
SD I-MethodName
- I-MethodName
APRR I-MethodName
with O
its O
ablative O
versions O
( O
w O
/ O
o O
Result O
and O
w O
/ O
o O
Reaction O
) O
. O
Due O
to O
the O
space O
limitation O
, O
we O
report O
the O
results O
on O
two O
datasets O
, O
i.e. O
, O
Ghosh B-DatasetName
with O
relatively O
more O
training O
samples O
and O
IAC B-DatasetName
- I-DatasetName
V2 I-DatasetName
with O
longer O
text O
lengths O
. O
The O
results O
are O
shown O
in O
Table O
6 O
. O
We O
can O
observe O
that O
the O
full O
version O
performs O
the O
best O
compared O
with O
the O
ablative O
versions O
. O
These O
results O
further O
indicate O
the O
augmentations O
of O
results O
and O
human O
reactions O
inferred O
by O
COMET O
can O
improve O
the O
classification O
performance O
even O
with O
a O
different O
encoder O
. O

The O
Impact O
of O
Layer B-HyperparameterName
Numbers I-HyperparameterName
of O
the O
Masked O
Graph O
- O
based O
Encoder O

We O
now O
investigate O
the O
impact O
of O
the O
layer O
number O
L B-HyperparameterName
of O
the O
masked O
graph O
- O
based O
encoder O
across O
benchmark O
datasets O
. O
We O
present O
the O
results O
with O
different O
values O
of O
L B-HyperparameterName
∈ O
{ O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
} O
in O
Fig O
2 O
. O
We O
can O
observe O
that O
SD B-MethodName
- I-MethodName
APRR I-MethodName
performs O
the O
best O
results O
across O
the O
SemEval18 B-DatasetName
dataset O
when O
L B-HyperparameterName
= O
1 B-HyperparameterValue
, O
while O
achieving O
the O
best O
results O
across O
the O
other O
datasets O
when O
L B-HyperparameterName
= O
3 B-HyperparameterValue
. O
The O
reason O
may O
be O
that O
the O
positive O
surfaces O
and O
the O
negative O
situations O
in O
the O
SemEval18 B-DatasetName
dataset O
are O
close O
to O
each O
other O
on O
the O
dependency O
graph O
, O
so O
the O
two O
terms O
can O
be O
associated O
together O
through O
low O
- O
order O
message O
- O
passing O
. O
While O
for O
the O
other O
three O
datasets O
, O
SD B-MethodName
- I-MethodName
APRR I-MethodName
requires O
higher O
- O
order O
message O
passing O
to O
model O
the O
incongruity O
between O
the O
two O
terms O
. O
In O
practice O
, O
we O
suggest O
L B-HyperparameterName
= O
3 B-HyperparameterValue
as O
the O
default O
setting O
. O

Visualization O
of O
Mask O
Weights O
. O

To O
qualitatively O
visualize O
the O
impact O
of O
mask O
weights O
, O
we O
randomly O
select O
several O
examples O
and O
show O
the O
words O
with O
lower O
mask O
weights O
of O
the O
final O
layer O
of O
the O
masked O
graph O
- O
based O
encoder O
. O
The O
visualization O
is O
shown O
in O
Table O
7 O
. O
We O
use O
the O
red O
color O
to O
demonstrate O
the O
word O
tokens O
with O
lower O
mask O
weights O
. O
From O
the O
table O
, O
we O
observe O
that O
the O
encoder O
can O
effectively O
eliminate O
semantically O
irrelevant O
tokens O
, O
such O
as O
" O
gets O
fired O
" O
and O
" O
see O
doctor O
" O
, O
and O
wrong O
speakers O
' O
reactions O
, O
such O
as O
the O
term O
of O
" O
happy O
" O
in O
the O
second O
and O
the O
third O
cases O
. O

Besides O
, O
we O
observe O
that O
some O
sarcasm O
- O
irrelevant O
parts O
in O
the O
original O
texts O
can O
also O
be O
captured O
, O
e.g. O
, O
the O
stop O
words O
" O
on O
" O
, O
" O
to O
" O
, O
" O
is O
" O
. O

Conclusion O
and O
Limitations O

In O
this O
paper O
, O
we O
propose O
a O
novel O
SD B-TaskName
method O
, O
entitled O
SD B-MethodName
- I-MethodName
APRR I-MethodName
, O
which O
expresses O
negative O
situations O
of O
sarcasm O
by O
the O
potential O
results O
and O
human O
reactions O
of O
the O
associated O
events O
. O
We O
employ O
the O
COMET O
to O
estimate O
the O
results O
and O
human O
reactions O
, O
and O
form O
event O
- O
augmented O
samples O
with O
them O
. O
We O
employ O
those O
augmented O
samples O
as O
the O
whole O
sarcastic O
texts O
from O
the O
direct O
access O
view O
. O
We O
suggest O
a O
masked O
graph O
- O
based O
encoder O
, O
enabling O
to O
generate O
discriminative O
sample O
embeddings O
. O
Experimental O
results O
demonstrate O
that O
our O
SD B-MethodName
- I-MethodName
APRR I-MethodName
can O
achieve O
competitive O
performance O
compared O
with O
the O
existing O
baseline O
methods O
. O

We O
demonstrate O
two O
limitations O
: O
( O
1 O
) O
The O
datasets O
used O
in O
this O
work O
are O
mostly O
collected O
from O
social O
media O
. O
In O
the O
future O
, O
we O
plan O
to O
collect O
sarcastic O
texts O
from O
various O
sources O
, O
such O
as O
the O
literature O
and O
films O
, O
and O
conduct O
more O
experiments O
with O
them O
. O
( O
2 O
) O
Our O
exploration O
of O
sarcasm O
theories O
still O
has O
some O
space O
to O
improve O
. O
Though O
the O
incongruity O
theory O
is O
the O
mainstream O
in O
the O
community O
, O
there O
are O
other O
theories O
worthy O
to O
investigate O
in O
the O
future O
. O
dation O
of O
China O
( O
No.62076046 O
) O
, O
and O
the O
Young O
Scientists O
Fund O
of O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No.62006034 O
) O
. O

Acknowledgment O

We O
would O
like O
to O
acknowledge O
support O
for O
this O
project O
from O
the O
National O
Natural O
Science O
Foun- O

B2 O
. O
Did O
you O
discuss O
the O
license O
or O
terms O
for O
use O
and O
/ O
or O
distribution O
of O
any O
artifacts O
? O
Left O
blank O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O

Left O
blank O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Left O
blank O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Left O
blank O
. O

B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O

Left O
blank O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Left O
blank O
. O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Left O
blank O
. O

BLINK B-MethodName
with I-MethodName
Elasticsearch I-MethodName
for O
Efficient O
Entity B-TaskName
Linking I-TaskName
in O
Business O
Conversations O

An O
Entity B-TaskName
Linking I-TaskName
system O
aligns O
the O
textual O
mentions O
of O
entities O
in O
a O
text O
to O
their O
corresponding O
entries O
in O
a O
knowledge O
base O
. O
However O
, O
deploying O
a O
neural O
entity O
linking O
system O
for O
efficient O
real O
- O
time O
inference O
in O
production O
environments O
is O
a O
challenging O
task O
. O
In O
this O
work O
, O
we O
present O
a O
neural O
entity O
linking O
system O
that O
connects O
the O
product O
and O
organization O
type O
entities O
in O
business O
conversations O
to O
their O
corresponding O
Wikipedia O
and O
Wikidata O
entries O
. O
The O
proposed O
system O
leverages O
Elasticsearch O
to O
ensure O
inference O
efficiency O
when O
deployed O
in O
a O
resource O
limited O
cloud O
machine O
, O
and O
obtains O
significant O
improvements O
in O
terms O
of O
inference O
speed O
and O
memory O
consumption O
while O
retaining O
high O
accuracy O
. O

Introduction O

Companies O
that O
offer O
VoIP O
telephony O
products O
with O
built O
- O
in O
speech O
and O
natural O
language O
processing O
features O
aim O
to O
assist O
the O
customer O
support O
agents O
with O
information O
relevant O
to O
the O
content O
of O
their O
conversations O
with O
the O
customers O
. O
To O
be O
useful O
, O
such O
assistance O
should O
be O
provided O
in O
near O
realtime O
of O
the O
triggering O
utterance O
. O
In O
this O
paper O
, O
we O
demonstrate O
how O
we O
build O
a O
near O
real O
- O
time O
entity O
linking O
system O
at O
Dialpad O
1 O
to O
link O
the O
entities O
in O
business O
phone O
transcripts O
to O
a O
knowledge O
base O
to O
provide O
more O
semantically O
- O
informed O
assistance O
. O

The O
entity O
linking O
task O
is O
usually O
comprised O
of O
three O
steps O
: O
( O
i O
) O
detect O
the O
mentions O
in O
the O
given O
text O
, O
( O
ii O
) O
generate O
a O
list O
of O
candidate O
entities O
relevant O
to O
each O
mention O
, O
and O
finally O
( O
iii O
) O
link O
each O
mention O
to O
its O
most O
relevant O
entry O
in O
the O
knowledge O
base O
( O
Ravi O
et O
al O
. O
, O
2021 O
) O
. O
Note O
that O
entity O
linking O
systems O
used O
in O
production O
should O
provide O
the O
optimum O
performance O
in O
terms O
of O
both O
inference O
speed O
and O
memory O
consumption O
while O
being O
used O
within O
a O
limited O
computational O
budget O
. O
Since O
there O
are O
millions O
of O
entities O
stored O
in O
a O
knowledge O
base O
, O
the O
1 O
https O
: O
/ O
/ O
www.dialpad.com O
/ O
scaling O
issue O
is O
a O
major O
concern O
while O
developing O
a O
real O
- O
time O
entity O
linking O
system O
. O

The O
goal O
of O
this O
research O
is O
to O
develop O
a O
neural O
entity O
linking O
system O
to O
efficiently O
link O
product O
and O
organization O
type O
entities O
in O
business O
phone O
conversations O
to O
their O
respective O
entries O
in O
a O
knowledge O
base O
for O
information O
extraction O
. O
For O
that O
purpose O
, O
we O
present O
an O
extended O
version O
of O
the O
stateof O
- O
the O
- O
art O
neural O
entity O
linker O
, O
the O
BLINK B-MethodName
model O
( O
Wu O
et O
al O
. O
, O
2020 O
) O
. O
Though O
BLINK B-MethodName
was O
originally O
proposed O
for O
entity O
linking O
on O
Wikipedia O
, O
we O
extend O
it O
for O
entity O
linking O
on O
Wikidata O
2 O
since O
unlike O
Wikipedia O
, O
the O
Wikidata O
knowledge O
base O
contains O
information O
related O
to O
the O
entities O
in O
a O
structured O
way O
. O
Thus O
, O
it O
allows O
effective O
extraction O
of O
relevant O
information O
for O
each O
entity O
. O
More O
importantly O
, O
for O
production O
deployment O
, O
we O
also O
introduce O
several O
new O
techniques O
that O
significantly O
reduce O
the O
memory O
requirements O
, O
computational O
resource O
usage O
, O
and O
the O
inference O
speed O
of O
BLINK B-MethodName
. O
More O
concretely O
, O
our O
major O
contributions O
are O
stated O
below O
: O

• O
We O
tackle O
the O
computational O
complexities O
in O
BLINK B-MethodName
by O
saving O
all O
pre O
- O
trained O
entity O
embeddings O
in O
Elasticsearch O
3 O
and O
propose O
a O
word O
matching O
technique O
to O
retrieve O
the O
candidate O
entities O
faster O
. O
We O
also O
present O
an O
approach O
to O
pre O
- O
compute O
the O
linking O
between O
the O
Wikipedia O
page O
of O
each O
entity O
to O
its O
respective O
Wikidata O
page O
to O
reduce O
the O
runtime O
latency O
. O

• O
Extensive O
experiments O
show O
that O
our O
entity O
linking O
system O
significantly O
reduces O
the O
inference O
time O
and O
memory O
requirements O
while O
retaining O
high O
accuracy O
in O
a O
computationally O
inexpensive O
machine O
. O
We O
also O
successfully O
deploy O
our O
entity O
linking O
system O
in O
a O
10 O
GB O
RAM O
machine O
( O
without O
GPU O
) O
whereas O
the O
original O
model O
requires O
a O
machine O
in O
our O
server O
having O
60 O
GB O
RAM O
for O
inference O
. O

Figure O
1 O
: O
The O
Proposed O
Entity B-TaskName
Linking I-TaskName
System O
. O
First O
, O
our O
Internal O
NER O
model O
detects O
the O
mention O
in O
the O
given O
text O
. O
Then O
we O
retrieve O
a O
list O
of O
candidate O
entities O
with O
their O
embeddings O
from O
Elasticsearch O
. O
At O
the O
same O
time O
, O
we O
generate O
the O
contextualized O
representation O
of O
the O
input O
text O
using O
the O
pre O
- O
trained O
BLINK O
embeddings O
. O
Afterward O
, O
we O
utilize O
the O
pre O
- O
trained O
BLINK O
Bi O
- O
Encoder O
to O
determine O
the O
entity O
that O
is O
the O
most O
relevant O
among O
the O
candidates O
and O
finally O
we O
extract O
information O
related O
to O
that O
entity O
from O
our O
knowledge O
base O
in O
Elasticsearch O
. O

Related O
Work O

Prior O
work O
on O
entity O
linking O
mostly O
focused O
on O
linking O
named O
entities O
to O
unstructured O
knowledge O
bases O
like O
Wikipedia O
, O
whereas O
the O
amount O
of O
work O
that O
used O
a O
structured O
knowledge O
base O
like O
Wikidata O
is O
very O
limited O
( O
Shen O
et O
al O
. O
, O
2014 O
; O
Sakor O
et O
al O
. O
, O
2020 O
) O
. O

Though O
other O
knowledge O
bases O
like O
DBpedia O
( O
Auer O
et O
al O
. O
, O
2007 O
) O
or O
YAGO O
( O
Fabian O
et O
al O
. O
, O
2007 O
) O
have O
also O
been O
studied O
, O
the O
utilization O
of O
Wikidata O
as O
the O
knowledge O
base O
to O
extract O
relevant O
information O
has O
gained O
lots O
of O
attention O
recently O
( O
Lin O
et O
al O
. O
, O
2021 O
; O
Möller O
et O
al O
. O
, O
2021 O
) O
. O
Detecting O
mentions O
( O
i.e. O
, O
entities O
) O
in O
the O
given O
text O
( O
Huang O
et O
al O
. O
, O
2015 O
; O
Akbik O
et O
al O
. O
, O
2018 O
) O
is O
an O
important O
step O
for O
entity O
linking O
. O
In O
recent O
years O
, O
utilizing O
the O
neural O
network O
architecture O
for O
mention O
detection O
has O
been O
extensively O
studied O
( O
Wu O
et O
al O
. O
, O
2020 O
; O
Onoe O
and O
Durrett O
, O
2020a O
) O
. O
More O
recently O
, O
the O
impressive O
success O
of O
the O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Yamada O
et O
al O
. O
, O
2020 O
) O
in O
a O
wide O
range O
of O
natural O
language O
processing O
tasks O
has O
also O
inspired O
researchers O
to O
apply O
transformer O
models O
for O
the O
entity O
recognition O
( O
Lin O
et O
al O
. O
, O
2021 O
) O
step O
in O
entity O
linking O
( O
Ravi O
et O
al O
. O
, O
2021 O
) O
, O
which O
results O
in O
obtaining O
superior O
performance O
over O
the O
previously O
used O
recurrent O
neural O
network O
- O
based O
models O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

For O
the O
candidate O
generation O
step O
in O
entity O
linking O
, O
early O
work O
mostly O
utilized O
various O
non O
- O
neural O
network O
approaches O
such O
as O
TF O
- O
IDF O
or O
alias O
tables O
( O
Wu O
et O
al O
. O
, O
2020 O
) O
, O
whereas O
more O
recent O
work O
utilized O
dense O
embeddings O
learnt O
via O
pre O
- O
trained O
transformers O
to O
retrieve O
the O
relevant O
candidates O
( O
Wu O
et O
al O
. O
, O
2020 O
; O
Onoe O
and O
Durrett O
, O
2020b O
) O
. O
However O
, O
there O
is O
an O
important O
limitation O
while O
generating O
the O
candidates O
via O
pre O
- O
trained O
embeddings O
. O
For O
instance O
, O
the O
state O
- O
of O
- O
the O
- O
art O
neural O
entity O
linking O
model O
BLINK B-MethodName
( O
Wu O
et O
al O
. O
, O
2020 O
) O
loads O
the O
pre O
- O
trained O
embeddings O
of O
all O
entities O
in O
Wikipedia O
into O
memory O
. O
Thus O
, O
it O
becomes O
inapplicable O
for O
deployment O
in O
production O
scenarios O
where O
the O
requirement O
is O
to O
ensure O
lower O
memory O
consumption O
. O
In O
this O
paper O
, O
we O
address O
this O
issue O
via O
storing O
the O
pre O
- O
trained O
embeddings O
in O
Elasticsearch O
. O
Moreover O
, O
we O
introduce O
new O
techniques O
that O
pre O
- O
compute O
the O
linking O
between O
Wikipedia O
and O
Wikidata O
to O
ensure O
efficient O
information O
retrieval O
, O
while O
also O
optimize O
the O
pre O
- O
trained O
models O
to O
meet O
the O
goal O
of O
deploying O
the O
proposed O
system O
in O
a O
limited O
computational O
resource O
setting O
. O

System O
Overview O

To O
develop O
the O
entity B-TaskName
linking I-TaskName
system O
, O
we O
adopt O
BLINK B-MethodName
, O
a O
neural O
entity O
linker O
that O
uses O
the O
transformer O
- O
based O
BERT B-MethodName
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
trains O
it O
on O
Wikipedia O
. O
BLINK B-MethodName
connects O
each O
mention O
in O
a O
given O
text O
with O
its O
respective O
Wikipedia O
page O
based O
on O
the O
overall O
context O
. O
Since O
Wikipedia O
contains O
textual O
data O
in O
an O
unstructured O
format O
, O
it O
is O
difficult O
to O
extract O
information O
from O
it O
. O
Thus O
, O
we O
connect O
BLINK B-MethodName
with O
a O
structured O
knowledge O
base O
, O
Wikidata O
, O
to O
extract O
information O
about O
product O
and O
organization O
type O
entities O
. O
Note O
that O
we O
store O
our O
knowledge O
base O
as O
well O
as O
the O
embedding O
representation O
of O
each O
entity O
in O
Elasticsearch O
. O
Moreover O
, O
we O
replace O
the O
Flair O
Named O
Entity O
Recognition O
( O
NER O
) O
model O
( O
Akbik O
et O
al O
. O
, O
2019 O
) O
originally O
used O
by O
BLINK O
with O
an O
NER O
model O
( O
we O
denote O
it O
as O
Internal O
NER O
) O
trained O
on O
transcripts O
of O
business O
phone O
conversations O
using O
DistilBERT B-MethodName
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
. O

We O
show O
our O
entity O
linking O
system O
in O
Figure O
1 O
. O
At O
first O
, O
the O
input O
text O
is O
processed O
by O
the O
NER O
model O
to O
detect O
the O
mention O
. O
Then O
, O
we O
generate O
the O
representation O
for O
the O
input O
text O
using O
the O
pretrained O
BLINK O
embeddings O
, O
while O
we O
retrieve O
the O
relevant O
candidates O
with O
their O
embeddings O
from O
Elasticsearch O
using O
the O
Multi O
Match O
Query O
4 O
feature O
of O
Elasticsearch O
. O
Finally O
, O
the O
embedding O
representations O
of O
the O
input O
text O
and O
the O
candidates O
are O
sent O
to O
the O
pre O
- O
trained O
BLINK B-MethodName
Bi I-MethodName
- I-MethodName
Encoder I-MethodName
to O
select O
the O
most O
relevant O
candidate O
. O
Below O
, O
we O
first O
demonstrate O
our O
proposed O
entity O
linking O
system O
: O
BLINK B-MethodName
with I-MethodName
Elasticsearch I-MethodName
, O
followed O
by O
describing O
how O
we O
deploy O
our O
proposed O
system O
in O
production O
. O

BLINK B-MethodName
with I-MethodName
Elasticsearch I-MethodName

The O
original O
BLINK B-MethodName
model O
requires O
about O
25 O
GB O
RAM O
to O
load O
all O
pretrained O
embeddings O
into O
memory O
. O
In O
our O
proposed O
system O
, O
we O
instead O
store O
these O
embeddings O
in O
an O
external O
database O
. O
To O
do O
so O
, O
we O
store O
all O
entity O
embeddings O
as O
dense O
vectors O
5 O
in O
our O
knowledge O
base O
in O
a O
remote O
Elasticsearch O
server O
along O
with O
saving O
textual O
information O
, O
such O
as O
Wikipedia O
title O
, O
description O
, O
URL O
, O
and O
etc O
. O
of O
each O
entity O
. O
This O
allows O
the O
model O
to O
only O
load O
the O
top O
K O
candidate O
embeddings O
into O
the O
memory O
that O
are O
most O
relevant O
to O
the O
mention O
in O
a O
given O
utterance O
. O
As O
mentioned O
earlier O
, O
the O
BLINK B-MethodName
model O
was O
trained O
over O
Wikipedia O
, O
while O
our O
goal O
is O
to O
utilize O
Wikidata O
for O
information O
extraction O
. O
Thus O
, O
we O
need O
to O
map O
the O
Wikipedia O
URL O
of O
each O
entity O
to O
its O
Wikidata O
URL O
such O
that O
we O
can O
utilize O
Wikidata O
to O
extract O
relevant O
information O
. O
Below O
, O
we O
first O
describe O
how O
we O
add O
Wikidata O
URL O
of O
each O
entity O
to O
our O
knowledge O
base O
. O
Then O
, O
we O
demonstrate O
how O
we O
retrieve O
the O
relevant O
candidates O
from O
our O
knowledge O
base O
. O

Pre O
- O
computing O
Wikipedia O
to O
Wikidata O
Linking O

We O
pre O
- O
compute O
the O
mapping O
between O
Wikipedia O
and O
Wikidata O
using O
the O
Wikimapper O
6 O
API O
and O
add O
the O
Wikidata O
URL O
of O
each O
entity O
to O
our O
knowledge O
base O
in O
Elasticsearch O
( O
see O
Figure O
2 O
) O
. O
This O
allows O
our O
entity O
linking O
system O
to O
reduce O
the O
runtime O
latency O
. O
Note O
that O
during O
the O
pre O
- O
computation O
step O
, O
other O
information O
from O
Wikidata O
for O
each O
entity O
can O
also O
be O
added O
to O
the O
knowledge O
base O
( O
for O
our O
case O
, O
we O
add O
the O
instance O
of O
property O
as O
the O
entity O
type O
) O
. O

Multi O
Match O
Query O
for O
Candidate O
Retrieval O

We O
find O
that O
the O
whole O
word O
or O
subword O
( O
s O
) O
in O
the O
product O
or O
organization O
type O
entity O
names O
usually O
appear O
in O
the O
Wikipedia O
title O
and O
description O
fields O
. O
Thus O
, O
to O
retrieve O
the O
most O
relevant O
candidates O
, O
we O
utilize O
the O
multi O
match O
query O
feature O
of O
Elasticsearch O
for O
each O
entity O
mention O
in O
the O
input O
text O
and O
apply O
it O
to O
the O
title O
and O
description O
fields O
in O
our O
knowledge O
base O
( O
see O
Figure O
3 O
) O
. O
For O
multi O
match O
query O
, O
we O
give O
more O
weight O
to O
the O
title O
field O
to O
make O
it O
two O
times O
more O
important O
than O
the O
description O
field O
. O
In O
this O
way O
, O
we O
retrieve O
the O
top O
k O
= O
250 O
candidates O
from O
Elasticsearch O
and O
send O
to O
the O
BLINK O
Bi O
- O
Encoder O
to O
select O
the O
most O
relevant O
entity O
. O

Model O
Deployment O

We O
deploy O
our O
entity O
linking O
system O
in O
containers O
7 O
in O
a O
Kubernetes O
8 O
cluster O
with O
2 O
CPUs O
and O
10 O
GB O
RAM O
. O
The O
deployed O
system O
architecture O
is O
shown O
in O
Figure O
4 O
. O
For O
production O
deployment O
, O
we O
also O
apply O
some O
optimization O
techniques O
to O
reduce O
the O
size O
of O
the O
pre O
- O
trained O
Bi O
- O
Encoder O
, O
as O
well O
as O
our O
knowledge O
base O
. O
We O
describe O
these O
below O
. O

Pre O
- O
trained O
Bi O
- O
Encoder O
Optimization O

We O
noticed O
that O
the O
binary O
file O
of O
the O
pre O
- O
trained O
BLINK O
Bi O
- O
Encoder O
had O
two O
type O
of O
tensors O
: O
one O
for O
context O
encoding O
( O
for O
the O
input O
representation O
) O
, O
and O
the O
other O
for O
the O
candidate O
encoding O
. O
However O
, O
the O
candidate O
encoding O
is O
only O
required O
during O
the O
training O
phase O
and O
it O
is O
not O
required O
during O
the O
inference O
stage O
since O
all O
the O
candidate O
embeddings O
are O
already O
stored O
in O
our O
knowledge O
base O
in O
Elasticsearch O
. O
Thus O
, O
we O
remove O
the O
unnecessary O
candidate O
encoding O
tensors O
from O
the O
binary O
file O
which O
results O
in O
reducing O
the O
file O
size O
from O
2.5 O
GB O
to O
1.2 O
GB O
( O
50 O
% O
reduced O
space O
) O
to O
improve O
memory O
efficiency O
. O

Knowledge O
Base O
Optimization O

The O
original O
version O
of O
the O
pre O
- O
trained O
BLINK B-MethodName
model O
( O
Wu O
et O
al O
. O
, O
2020 O
) O
learns O
the O
embedding O
representations O
of O
59,03,527 O
Wikipedia O
entities O
. O
In O
total O
, O
the O
size O
of O
these O
pre O
- O
computed O
embeddings O
is O
about O
23 O
GB O
. O
As O
our O
goal O
is O
to O
detect O
the O
Product O
and O
Organization O
type O
entities O
in O
business O
conversational O
data O
, O
we O
apply O
some O
filtering O
techniques O
to O
optimize O
the O
knowledge O
base O
such O
that O
it O
mostly O
contains O
the O
entities O
that O
are O
relevant O
to O
our O
NER O
system O
. O
In O
order O
to O
do O
that O
, O
we O
utilize O
the O
Instance O
Of O
property O
in O
Wikidata O
of O
each O
entity O
and O
remove O
entities O
that O
are O
of O
Person O
, O
Disambiguation O
, O
Location O
, O
etc O
. O
In O
this O
way O
, O
the O
size O
of O
the O
Knowledge O
base O
is O
reduced O
from O
23 O
GB O
to O
12 O
GB O
( O
about O
50 O
% O
reduced O
space O
) O
, O
while O
the O
total O
number O
of O
entities O
has O
been O
reduced O
from O
59,03,527 O
to O
27,84,042 O
. O

Experimental O
Details O

In O
this O
section O
, O
we O
demonstrate O
the O
datasets O
used O
in O
our O
experiments O
and O
the O
implementation O
details O
. O

Datasets O

To O
demonstrate O
the O
effectiveness O
of O
our O
proposed O
approach O
, O
we O
conduct O
a O
series O
of O
experiments O
on O
seven O
academic O
datasets O
as O
well O
as O
on O
a O
sample O
of O
287 O
utterances O
collected O
from O
business O
conversation O
data O
. O
Below O
, O
we O
describe O
these O
datasets O
. O

Business O
Conversation O
Dataset O

As O
our O
goal O
is O
to O
develop O
an O
entity O
linking O
system O
that O
can O
link O
entities O
in O
conversational O
data O
from O
business O
domains O
, O
we O
sample O
some O
real O
world O
business O
phone O
conversation O
transcripts O
. O
After O
data O
collection O
, O
we O
use O
domain O
experts O
( O
in O
- O
house O
scientists O
) O
to O
annotate O
the O
utterances O
to O
label O
the O
mentions O
( O
i.e. O
, O
product O
and O
organization O
type O
entities O
) O
. O
Our O
annotated O
business O
conversation O
data O
consists O
of O
287 O
utterances O
that O
we O
use O
in O
our O
experiment O
for O
evaluation O
. O

Academic O
Datasets O

Since O
our O
goal O
is O
to O
develop O
an O
entity O
linking O
system O
to O
extract O
information O
for O
product O
and O
organization O
type O
entities O
, O
at O
first O
we O
pre O
- O
process O
the O
academic O
datasets O
such O
that O
our O
model O
only O
links O
product O
and O
organization O
type O
entities O
during O
experiments O
. O
Similar O
to O
the O
original O
BLINK B-MethodName
model O
( O
Wu O
et O
al O
. O
, O
2020 O
) O
, O
we O
also O
did O
not O
leverage O
the O
training O
data O
and O
only O
used O
the O
test O
data O
of O
each O
dataset O
for O
zero O
- O
shot O
entity O
linking O
. O
In O
our O
experiment O
, O
we O
use O
the O
AIDA B-DatasetName
- I-DatasetName
YAGO2 I-DatasetName
- I-DatasetName
CONLL I-DatasetName
dataset O
( O
testa O
and O
testb O
) O
from O
Hoffart O
et O
al O
. O
( O
2011 O
) O
that O
contains O
newswire O
articles O
from O
the O
Reuters O
Corpus O
; O
the O
ACE B-DatasetName
2004 I-DatasetName
, O
AQUAINT B-DatasetName
, O
and O
MSNBC B-DatasetName
datasets O
from O
( O
Guo O
and O
Barbosa O
, O
2018 O
) O
that O
were O
constructed O
from O
news O
articles O
; O
and O
the O
WNED B-DatasetName
- I-DatasetName
CWEB I-DatasetName
( O
Guo O
and O
Barbosa O
, O
2018 O
) O
and O
the O
WNED B-DatasetName
- I-DatasetName
WIKI I-DatasetName
( O
Gabrilovich O
et O
al O
. O
, O
2013 O
) O
datasets O
that O
were O
constructed O
from O
CWEB O
and O
Wikipedia O
respectively O
. O

Implementation O

Recall O
that O
instead O
of O
using O
the O
Flair B-MethodName
NER I-MethodName
model O
( O
Akbik O
et O
al O
. O
, O
2019 O
) O
used O
by O
the O
original O
BLINK B-MethodName
model O
, O
we O
train O
an O
NER O
model O
on O
phone O
transcripts O
as O
our O
goal O
is O
to O
build O
the O
entity O
linking O
model O
for O
real O
world O
business O
conversation O
data O
. O
For O
this O
purpose O
, O
we O
adopt O
the O
pre O
- O
trained O
DistilBERT B-MethodName
model O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
and O
fine O
- O
tune O
it O
on O
a O
business O
conversational O
dataset O
collected O
from O
some O
phone O
transcripts O
in O
Dialpad O
that O
contains O
516124 O
training O
samples O
( O
16124 O
instances O
were O
annotated O
by O
humans O
while O
500k O
instances O
were O
pseudo O
labels O
generated O
by O
the O
pre O
- O
trained O
LUKE B-MethodName
NER I-MethodName
model O
( O
Yamada O
et O
al O
. O
, O
2020 O
) O
) O
. O
There O
were O
also O
2292 O
human O
annotated O
samples O
in O
the O
validation O
set O
while O
4497 O
human O
annotated O
samples O
in O
the O
test O
set O
. O
We O
use O
the O
HuggingFace O
9 O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
to O
implement O
the O
distilbert O
- O
base O
- O
cased O
10 O
model O
and O
utilize O
it O
for O
the O
sequence O
labeling O
task O
with O
the O
following O
hyperparameters O
: O
learning B-HyperparameterName
rate I-HyperparameterName
= O
2e-5 B-HyperparameterValue
, O
total O
number O
of O
epoch B-HyperparameterName
= O
15 B-HyperparameterValue
, O
and O
batch B-HyperparameterName
size I-HyperparameterName
= O
32 B-HyperparameterValue
. O

To O
implement O
the O
BLINK B-MethodName
model O
for O
inference O
, O
we O
use O
its O
original O
source O
code O
11 O
. O

Results O
and O
Discussions O

We O
denote O
our O
entity O
linking O
model O
that O
utilizes O
Multi O
Match O
Query O
( O
MMQ O
) O
on O
Elasticsearch O
( O
ES O
) O
as O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
MMQ I-MethodName
. O
Here O
, O
we O
first O
discuss O
its O
performance O
on O
our O
business O
conversation O
data O
. O
Then O
we O
conduct O
experiments O
on O
some O
academic O
datasets O
to O
demonstrate O
its O
generalized O
effectiveness O
. O

Performance O
on O
Business O
Conversation O
Data O

Below O
, O
we O
present O
some O
baselines O
that O
we O
use O
to O
compare O
the O
performance O
of O
our O
proposed O
model O
. O
For O
this O
experiment O
, O
we O
use O
the O
following O
evaluation O
metrics O
, O
( O
i O
) O
average O
inference O
time O
: O
it O
refers O
to O
how O
much O
time O
it O
takes O
on O
average O
per O
utterance O
for O
entity O
linking O
, O
( O
ii O
) O
accuracy O
: O
it O
computes O
the O
correctness O
of O
linking O
the O
named O
entities O
to O
the O
Wikidata O
knowledge O
base O
, O
( O
iii O
) O
memory O
: O
it O
refers O
to O
the O
RAM O
configuration O
of O
the O
Machine O
that O
had O
to O
be O
used O
to O
run O
the O
model O
in O
Google O
Cloud O
Platform O
( O
GCP O
) O
13 O
. O

Since O
the O
utilization O
of O
GPUs O
significantly O
increases O
the O
computational O
cost O
, O
we O
did O
not O
leverage O
any O
GPU O
in O
our O
experiments O
to O
mimic O
the O
production O
environment O
. O
We O
show O
our O
experimental O
results O
in O
Table O
1 O
and O
find O
that O
our O
proposed O
model O
significantly O
reduces O
the O
inference O
time O
while O
achieving O
high O
accuracy O
. O
Moreover O
, O
we O
were O
able O
to O
run O
our O
proposed O
model O
in O
GCP O
on O
an O
n1 O
- O
standard-4 O
machine O
having O
15 O
GB O
RAM O
with O
4 O
CPUs O
whereas O
BLINK B-MethodName
models O
with O
Pywikibot O
had O
to O
be O
run O
on O
an O
n1 O
- O
standard-16 O
machine O
having O
60 O
GB O
RAM O
with O
16 O
CPUs O
( O
we O
failed O
to O
run O
the O
model O
for O
inference O
due O
to O
memory O
leaks O
in O
other O
n1 O
- O
standard O
machines O
in O
GCP O
that O
had O
less O
RAM O
) O
. O

From O
Table O
1 O
, O
we O
also O
observe O
that O
the O
performance O
of O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
CS I-MethodName
model O
is O
the O
poorest O
among O
all O
models O
. O
One O
possible O
explanation O
behind O
this O
could O
be O
because O
the O
BLINK B-MethodName
model O
did O
not O
leverage O
cosine O
similarity O
during O
its O
training O
phase O
and O
so O
zero O
- O
shot O
cosine O
similarity O
between O
the O
embedding O
of O
the O
candidate O
entity O
and O
the O
input O
embedding O
for O
candidate O
entity O
retrieval O
led O
to O
poorer O
accuracy O
. O
Moreover O
, O
we O
observe O
that O
the O
cosine O
similarity O
between O
embeddings O
is O
also O
very O
slow O
in O
comparison O
to O
MMQ O
. O
Furthermore O
, O
we O
find O
that O
our O
Internal O
NER O
is O
more O
effective O
than O
the O
Flair B-MethodName
NER I-MethodName
( O
about O
46 O
% O
) O
and O
combining O
it O
with O
the O
MMQ O
leads O
to O
the O
highest O
accuracy B-MetricName
score O
of O
93.03 B-MetricValue
. O

Performance O
on O
Academic O
Datasets O

In O
this O
section O
, O
we O
further O
analyze O
the O
performance O
of O
our O
proposed O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
MMQ I-MethodName
model O
via O
conducting O
experiments O
on O
seven O
academic O
datasets O
. O
We O
particularly O
conduct O
this O
experiment O
to O
investigate O
the O
generalized O
effectiveness O
of O
multi O
match O
query O
. O
For O
this O
analysis O
, O
we O
use O
the O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
CS I-MethodName
model O
as O
the O
baseline O
where O
cosine O
similar- O
ity O
has O
been O
used O
instead O
of O
multi O
match O
query O
. O
As O
our O
goal O
is O
to O
deploy O
our O
model O
in O
a O
limited O
computational O
resource O
setting O
to O
ensure O
less O
memory O
consumption O
, O
we O
only O
use O
the O
models O
in O
this O
experiment O
that O
can O
be O
run O
in O
a O
machine O
that O
do O
not O
require O
more O
than O
16 O
GB O
RAM O
. O
For O
this O
reason O
, O
we O
use O
the O
models O
that O
leverage O
Elasticsearch O
instead O
of O
Pywikibot O
( O
we O
have O
already O
demonstrated O
in O
our O
previous O
experiment O
on O
business O
conversation O
data O
how O
our O
proposed O
method O
is O
more O
effective O
in O
terms O
of O
both O
accuracy O
and O
efficiency O
than O
other O
baseline O
models O
that O
utilized O
Pywikbot O
) O
. O

We O
show O
the O
results O
of O
our O
experiments O
in O
Table O
2 O
to O
find O
that O
in O
5 O
out O
of O
7 O
datasets O
, O
our O
proposed O
method O
that O
uses O
multi O
match O
query O
instead O
of O
cosine O
similarity O
outperforms O
its O
counterparts O
. O
The O
only O
two O
datasets O
where O
our O
model O
could O
not O
outperform O
the O
baseline O
are O
the O
AIDA B-DatasetName
- I-DatasetName
YAGO2 I-DatasetName
- I-DatasetName
CONLL I-DatasetName
dataset O
( O
testa O
) O
and O
the O
AQUAINT B-DatasetName
dataset O
where O
cosine B-MetricName
similarity I-MetricName
outperforms O
multi O
match O
query O
by O
7.94 B-MetricValue
% I-MetricValue
and O
0.65 B-MetricValue
% I-MetricValue
respectively O
. O
In O
other O
datasets O
, O
our O
proposed O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
MMQ I-MethodName
model O
outperforms O
the O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
CS I-MethodName
model O
by O
2.98 B-MetricValue
% I-MetricValue
, O
10.42 B-MetricValue
% I-MetricValue
, O
10.52 B-MetricValue
% I-MetricValue
, O
11.22 B-MetricValue
% I-MetricValue
, O
and O
5.75 B-MetricValue
% I-MetricValue
in O
AIDA B-DatasetName
- I-DatasetName
YAGO2 I-DatasetName
- I-DatasetName
CONLL I-DatasetName
( O
testb O
) O
, O
ACE B-DatasetName
2004 I-DatasetName
, O
MSNBC B-DatasetName
, O
WNED B-DatasetName
- I-DatasetName
CWEB I-DatasetName
, O
and O
WNED B-DatasetName
- I-DatasetName
WIKI I-DatasetName
datasets O
respectively O
. O
Furthermore O
, O
we O
find O
during O
our O
experiments O
that O
our O
proposed O
method O
outperforms O
its O
counterpart O
in O
terms O
of O
inference O
speed O
in O
all O
7 O
datasets O
( O
on O
average O
, O
8 O
times O
faster O
) O
. O
These O
findings O
further O
validate O
the O
effectiveness O
of O
our O
proposed O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
MMQ I-MethodName
model O
for O
real O
world O
deployment O
in O
computationally O
limited O
resource O
settings O
. O
So O
far O
, O
we O
discuss O
the O
effectiveness O
of O
our O
entity O
linking O
system O
in O
terms O
of O
both O
accuracy O
and O
efficiency O
based O
on O
extensive O
experiments O
in O
business O
conversation O
data O
, O
as O
well O
as O
in O
benchmark O
academic O
datasets O
. O
Below O
, O
we O
conduct O
a O
case O
study O
to O
analyze O
how O
the O
top O
K O
candidates O
retrieval O
from O
Elasticsearch O
impacts O
the O
overall O
performance O
. O

Case O
Study O

For O
the O
case O
study O
( O
see O
Table O
3 O
) O
, O
we O
conduct O
experiments O
with O
some O
additional O
values O
of O
K O
for O
candidate O
retrieval O
to O
investigate O
its O
effect O
on O
accuracy O
and O
inference O
speed O
. O
For O
that O
purpose O
, O
in O
addition O
to O
the O
original O
value O
of O
K O
= O
250 O
for O
the O
BLINK B-MethodName
+ I-MethodName
ES I-MethodName
MMQ I-MethodName
model O
, O
we O
use O
the O
following O
values O
: O
K O
= O
100 O
and O
K O
= O
500 O
. O
We O
find O
that O
even O
though O
reducing O
the O
value O
of O
K O
to O
100 O
for O
candidate O
retrieval O
leads O
to O
a O
faster O
inference O
speed O
, O
the O
accuracy B-MetricName
is O
decreased O
by O
3.74 B-MetricValue
% I-MetricValue
. O
Moreover O
, O
increasing O
the O
value O
of O
K O
to O
500 O
provides O
an O
opposite O
impact O
, O
as O
it O
improves O
the O
accuracy B-MetricName
by O
1.13 B-MetricValue
% I-MetricValue
but O
makes O
the O
candidate O
retrieval O
speed O
slower O
by O
taking O
more O
than O
2 O
seconds O
per O
utterance O
. O
This O
trade O
- O
off O
implies O
that O
the O
retrieval O
value O
for O
K O
can O
be O
tuned O
based O
on O
the O
requirement O
. O

Ablation O
Study O

To O
further O
investigate O
the O
effectiveness O
of O
our O
proposed O
approach O
of O
combining O
BLINK B-MethodName
with I-MethodName
Elasticsearch I-MethodName
via O
leveraging O
MMQ O
for O
candidate O
retrieval O
, O
we O
do O
an O
ablation O
test O
. O
In O
our O
ablation O
test O
, O
we O
remove O
BLINK B-MethodName
and O
only O
utilize O
the O
MMQ O
of O
Elasticsearch O
to O
retrieve O
the O
most O
relevant O
candidate O
. O
In O
this O
way O
, O
only O
one O
top O
matched O
candidate O
entity O
is O
retrieved O
from O
Elasticsearch O
. O
The O
result O
of O
our O
experiment O
is O
given O
in O
Table O
4 O
. O

From O
Table O
4 O
, O
we O
observe O
that O
even O
though O
removing O
BLINK B-MethodName
led O
to O
a O
great O
improvement O
in O
terms O
of O
the O
inference O
speed O
, O
there O
is O
a O
significant O
drop O
in O
accuracy B-MetricName
( O
by O
20.22 B-MetricValue
% I-MetricValue
) O
. O
This O
makes O
the O
model O
without O
BLINK B-MethodName
inapplicable O
in O
production O
scenarios O
where O
the O
requirement O
is O
to O
ensure O
high O
accuracy O
. O

Conclusion O

In O
this O
paper O
, O
we O
introduce O
an O
efficient O
, O
scalable O
version O
of O
the O
BLINK B-MethodName
model O
and O
extend O
it O
for O
entity O
linking O
on O
Wikidata O
. O
With O
extensive O
experiments O
, O
we O
show O
that O
our O
proposed O
system O
is O
usable O
for O
production O
environments O
within O
a O
limited O
budget O
setting O
since O
it O
significantly O
reduces O
memory O
requirements O
, O
computing O
resource O
usage O
, O
as O
well O
as O
the O
inference O
time O
while O
retaining O
high O
accuracy O
. O
We O
also O
effectively O
deploy O
our O
proposed O
entity O
linking O
system O
in O
a O
10 O
GB O
RAM O
machine O
without O
using O
any O
GPU O
for O
near O
real O
- O
time O
inference O
. O
In O
the O
future O
, O
we O
will O
investigate O
how O
to O
make O
our O
entity O
linking O
system O
more O
efficient O
such O
that O
it O
can O
give O
inference O
in O
real O
- O
time O
( O
e.g. O
, O
within O
one O
second O
) O
. O
Moreover O
, O
we O
will O
study O
how O
different O
BERT B-MethodName
- O
based O
( O
Sanh O
et O
al O
. O
, O
2019 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Lan O
et O
al O
. O
, O
2019 O
) O
sentence O
similarity O
models O
( O
Garg O
et O
al O
. O
, O
2019 O
; O
Laskar O
et O
al O
. O
, O
2020aLaskar O
et O
al O
. O
, O
, O
b O
, O
2021 O
for O
candidate O
retrieval O
can O
impact O
the O
performance O
, O
while O
also O
exploring O
different O
techniques O
such O
as O
dimensionality O
reduction O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
to O
optimize O
the O
space O
used O
in O
Elasticsearch O
as O
well O
as O
the O
computing O
resource O
requirements O
. O

Ethics O
Statement O

The O
business O
phone O
conversational O
data O
used O
for O
entity O
linking O
experiments O
is O
annotated O
by O
the O
inhouse O
Scientists O
for O
which O
the O
annotations O
were O
acquired O
for O
individual O
utterances O
. O
Whereas O
to O
annotate O
the O
conversation O
dataset O
to O
train O
our O
internal O
NER O
model O
, O
Appen O
was O
used O
( O
https O
: O
/ O
/ O
appen O
. O
com O
/ O
) O
for O
data O
annotation O
and O
the O
annotators O
were O
provided O
with O
adequate O
compensation O
( O
above O
minimum O
wages O
) O
. O
There O
is O
a O
data O
retention O
policy O
available O
for O
all O
users O
so O
that O
data O
will O
not O
be O
collected O
if O
the O
user O
is O
not O
consent O
to O
data O
collection O
. O
To O
protect O
user O
privacy O
, O
sensitive O
data O
such O
as O
personally O
identifiable O
information O
( O
e.g. O
, O
credit O
card O
number O
, O
phone O
number O
) O
were O
removed O
while O
collecting O
the O
data O
. O
Since O
our O
model O
is O
doing O
classification O
to O
link O
the O
named O
entities O
to O
their O
corresponding O
entries O
in O
a O
publicly O
available O
knowledge O
base O
for O
information O
extraction O
, O
incorrect O
predictions O
will O
not O
cause O
any O
harm O
to O
the O
user O
besides O
an O
unsatisfactory O
experience O
. O
We O
also O
maintain O
the O
licensing O
requirements O
accordingly O
while O
using O
different O
tools O
, O
such O
as O
Wikidata O
, O
WikiMapper O
, O
PyWikiBot O
, O
Elasticsearch O
, O
HuggingFace O
, O
BLINK O
, O
etc O
. O

Acknowledgements O

We O
gratefully O
appreciate O
the O
reviewers O
for O
their O
excellent O
review O
comments O
that O
helped O
us O
to O
improve O
the O
quality O
of O
this O
paper O
. O

Discovering O
Differences O
in O
the O
Representation O
of O
People O
using O
Contextualized B-MethodName
Semantic I-MethodName
Axes I-MethodName

A O
common O
paradigm O
for O
identifying O
semantic O
differences O
across O
social O
and O
temporal O
contexts O
is O
the O
use O
of O
static O
word O
embeddings O
and O
their O
distances O
. O
In O
particular O
, O
past O
work O
has O
compared O
embeddings O
against O
" O
semantic O
axes O
" O
that O
represent O
two O
opposing O
concepts O
. O
We O
extend O
this O
paradigm O
to O
BERT B-MethodName
embeddings O
, O
and O
construct O
contextualized O
axes O
that O
mitigate O
the O
pitfall O
where O
antonyms O
have O
neighboring O
representations O
. O
We O
validate O
and O
demonstrate O
these O
axes O
on O
two O
people O
- O
centric O
datasets O
: O
occupations B-DatasetName
from I-DatasetName
Wikipedia I-DatasetName
, O
and O
multi B-DatasetName
- I-DatasetName
platform I-DatasetName
discussions I-DatasetName
in I-DatasetName
extremist I-DatasetName
, O
men O
's O
communities O
over O
fourteen O
years O
. O
In O
both O
studies O
, O
contextualized O
semantic O
axes O
can O
characterize O
differences O
among O
instances O
of O
the O
same O
word O
type O
. O
In O
the O
latter O
study O
, O
we O
show O
that O
references O
to O
women O
and O
the O
contexts O
around O
them O
have O
become O
more O
detestable O
over O
time O
. O

Introduction O

Warning O
: O
This O
paper O
contains O
content O
that O
may O
be O
offensive O
or O
upsetting O
. O

Quantifying O
and O
describing O
the O
nature O
of O
language O
differences O
is O
key O
to O
measuring O
the O
impact O
of O
social O
and O
cultural O
factors O
on O
text O
. O
Past O
work O
has O
compared O
English O
embeddings O
for O
people O
to O
adjectives O
or O
concepts O
( O
Garg O
et O
al O
. O
, O
2018 O
; O
Mendelsohn O
et O
al O
. O
, O
2020 O
; O
Charlesworth O
et O
al O
. O
, O
2022 O
) O
, O
or O
projected O
embeddings O
against O
axes O
representing O
contrasting O
attributes O
( O
Turney O
and O
Littman O
, O
2003 O
; O
Kozlowski O
et O
al O
. O
, O
2019 O
; O
Field O
and O
Tsvetkov O
, O
2019 O
; O
Mathew O
et O
al O
. O
, O
2020 O
; O
Kwak O
et O
al O
. O
, O
2021 O
; O
Lucy O
and O
Bamman O
, O
2021b O
; O
Fraser O
et O
al O
. O
, O
2021 O
; O
Grand O
et O
al O
. O
, O
2022 O
) O
. O
Static O
representations O
for O
the O
same O
word O
can O
also O
be O
juxtaposed O
across O
corpora O
that O
reflect O
different O
time O
periods O
( O
Gonen O
et O
al O
. O
, O
2020 O
; O
Hamilton O
et O
al O
. O
, O
2016 O
) O
. O
This O
paradigm O
of O
using O
embedding O
distances O
to O
uncover O
socially O
meaningful O
patterns O
has O
also O
transferred O
over O
to O
studies O
that O
measure O
biases O
in O
contextualized O
embeddings O
, O
such O
as O
Wolfe O
and O
Caliskan O
( O
2021 O
) O
's O
finding O
that O
BERT B-MethodName
beautiful O
ugly O
… O
gorgeous O
equipage O
, O
ornately O
attired O
… O
… O
a O
grotesque O
, O
monster O
- O
like O
costume O
… O
… O
there O
are O
beautiful O
women O
who O
are O
willing O
… O
… O
literally O
filled O
with O
garbage O
women O
… O
Figure O
1 O
: O
An O
axis O
is O
constructed O
using O
embeddings O
of O
adjectives O
in O
selected O
contexts O
. O
These O
contexts O
are O
predictive O
of O
synonyms O
, O
but O
not O
antonyms O
, O
of O
the O
target O
adjective O
during O
masked O
language O
modeling O
. O
Tokenlevel O
embeddings O
for O
people O
are O
then O
projected O
onto O
this O
axis O
. O

embeddings O
of O
less O
frequent O
minority O
names O
are O
closer O
to O
words O
related O
to O
unpleasantness O
. O
The O
use O
of O
" O
semantic O
axes O
" O
is O
enticing O
in O
that O
it O
offers O
an O
interpretable O
measurement O
of O
word O
differences O
beyond O
a O
single O
similarity O
value O
( O
Turney O
and O
Littman O
, O
2003 O
; O
Kozlowski O
et O
al O
. O
, O
2019 O
; O
Kwak O
et O
al O
. O
, O
2021 O
) O
. O
Words O
are O
projected O
onto O
axes O
where O
the O
poles O
represent O
antonymous O
concepts O
( O
such O
as O
beautiful O
- O
ugly O
) O
, O
and O
the O
projected O
embedding O
's O
location O
along O
the O
axis O
indicates O
how O
similar O
it O
is O
to O
either O
concept O
. O
Semantic O
axes O
constructed O
using O
static O
, O
type O
- O
based O
embeddings O
have O
been O
used O
to O
analyze O
socially O
meaningful O
differences O
, O
such O
as O
words O
' O
associations O
with O
class O
( O
Kozlowski O
et O
al O
. O
, O
2019 O
) O
, O
or O
gender O
stereotypes O
in O
narratives O
( O
Huang O
et O
al O
. O
, O
2021 O
; O
Lucy O
and O
Bamman O
, O
2021b O
) O
. O

Our O
work O
investigates O
the O
extension O
and O
application O
of O
semantic O
axes O
to O
contextualized O
embeddings O
. O
We O
present O
a O
novel O
approach O
for O
constructing B-MethodName
semantic I-MethodName
axes I-MethodName
with I-MethodName
English I-MethodName
BERT I-MethodName
embeddings I-MethodName
( O
Figure O
1 O
) O
. O
These O
axes O
are O
built O
to O
encourage O
selfconsistency O
, O
where O
antonymous O
poles O
are O
less O
conflated O
with O
each O
other O
. O
They O
are O
able O
to O
capture O
semantic O
differences O
across O
word O
types O
as O
well O
as O
variation O
in O
a O
single O
word O
across O
contexts O
. O
Their O
ability O
to O
differentiate O
contexts O
makes O
them O
suitable O
for O
studying O
how O
a O
word O
changes O
across O
domains O
or O
across O
individual O
sentences O
. O
These O
axes O
are O
also O
more O
self O
- O
consistent O
and O
coherent O
than O
ones O
created O
using O
GloVe B-MethodName
and O
other O
baseline O
approaches O
. O

We O
demonstrate O
the O
use O
of O
contextualized O
axes O
on O
two O
datasets O
: O
occupations B-DatasetName
from I-DatasetName
Wikipedia I-DatasetName
, O
and O
people O
discussed O
in O
misogynistic O
online O
communities O
. O
We O
use O
the O
former O
as O
a O
case O
where O
terms O
appear O
in O
definitional O
contexts O
, O
and O
characteristics O
of O
people O
are O
well O
- O
known O
. O
In O
the O
latter O
longitudinal O
, O
cross O
- O
platform O
case O
study O
, O
we O
examine O
lexical O
choices O
made O
by O
communities O
whose O
attitudes O
towards O
women O
tend O
to O
be O
salient O
and O
extreme O
. O
We O
chose O
this O
set O
of O
online O
communities O
as O
a O
substantive O
use O
case O
of O
our O
method O
, O
in O
light O
of O
recent O
attention O
in O
web O
science O
on O
analyzing O
online O
extremism O
and O
hate O
at O
scale O
( O
e.g. O
Ribeiro O
et O
al O
. O
, O
2021b O
, O
a O
; O
Aliapoulios O
et O
al O
. O
, O
2021 O
) O
. O
There O
, O
we O
analyze O
language O
change O
and O
variation O
along O
axes O
through O
a O
sociolinguistic O
lens O
, O
emphasizing O
that O
speakers O
use O
language O
that O
reflects O
their O
social O
identities O
and O
beliefs O
( O
CH O
- O
Wang O
and O
Jurgens O
, O
2021 O
; O
Huffaker O
and O
Calvert O
, O
2017 O
; O
Card O
et O
al O
. O
, O
2016 O
; O
Lakoff O
and O
Ferguson O
, O
2006 O
) O
. O

Our O
code O
, O
vocabularies O
, O
and O
other O
resources O
can O
be O
found O
in O
our O
Github O
repo O
: O
https O
: O
/ O
/ O
github.c O
om O
/ O
lucy3 O
/ O
context_semantic_axes O
. O

Constructing O
semantic O
axes O

Static O
embeddings O
. O
Several O
formulae O
for O
calculating O
the O
similarity O
of O
a O
target O
word O
to O
two O
sets O
of O
pole O
words O
have O
been O
proposed O
in O
prior O
work O
on O
static O
semantic O
axes O
. O
These O
differ O
in O
whether O
they O
take O
the O
difference O
between O
a O
target O
word O
's O
similarities O
to O
each O
pole O
( O
Turney O
and O
Littman O
, O
2003 O
) O
, O
calculate O
a O
target O
word O
's O
similarity O
to O
the O
difference O
between O
pole O
averages O
Kwak O
et O
al O
. O
, O
2021 O
) O
, O
or O
calculate O
a O
target O
word O
's O
similarity O
to O
the O
average O
of O
several O
word O
pair O
differences O
that O
represent O
the O
same O
antonymous O
relationship O
( O
Kozlowski O
et O
al O
. O
, O
2019 O
) O
. O
We O
build O
on O
the O
approach O
of O
and O
Kwak O
et O
al O
. O
( O
2021 O
) O
, O
because O
it O
does O
not O
require O
us O
to O
curate O
multiple O
paired O
antonyms O
for O
each O
axis O
, O
and O
it O
draws O
out O
the O
difference O
between O
two O
concepts O
before O
a O
target O
word O
is O
compared O
to O
them O
, O
rather O
than O
after O
. O
We O
define O
an O
axis O
V O
containing O
antonymous O
sets O
of O
adjective O
vectors O
, O
S O
l O
= O
{ O
l O
1 O
, O
l O
2 O
, O
l O
3 O
, O
... O
, O
l O
n O
} O
and O
S O
r O
= O
{ O
r O
1 O
, O
r O
2 O
, O
r O
3 O
, O
... O
, O
r O
m O
} O
, O
as O
the O
following O
: O

V O
= O
1 O
n O
n O
i=1 O
l O
i O
− O
1 O
m O
m O
j=1 O
r O
j O
. O

Relying O
on O
single O
- O
word O
poles O
for O
axes O
can O
be O
unstable O
to O
the O
choice O
of O
each O
word O
Antoniak O
and O
Mimno O
, O
2021 O
) O
. O
creates O
a O
pole O
's O
set O
of O
words O
using O
the O
nearest O
neighbors O
of O
a O
seed O
word O
, O
which O
may O
risk O
conflating O
unintended O
meanings O
or O
antonymous O
neighbors O
( O
Mrkšić O
et O
al O
. O
, O
2016 O
; O
Sedoc O
et O
al O
. O
, O
2017 O
) O
. O
For O
example O
, O
one O
axis O
uses O
the O
opposite O
seed O
words O
green O
and O
experienced O
, O
but O
green O
's O
nearest O
neighbors O
include O
red O
rather O
than O
inexperienced O
. O
Instead O
of O
using O
this O
nearest O
neighbors O
approach O
, O
we O
construct O
poles O
using O
WordNet O
antonym O
relations O
. O
Each O
end O
of O
an O
axis O
aggregates O
synonymous O
and O
similar O
lemmas O
in O
WordNet O
synsets O
, O
which O
are O
expanded O
using O
the O
similar O
to O
relation O
( O
Miller O
, O
1992 O
) O
. O

Our O
type O
- O
based O
embedding O
baseline O
, O
GLOVE B-MethodName
, O
uses O
300 B-HyperparameterValue
- O
dimensional B-HyperparameterName
GloVe B-MethodName
vectors O
pretrained O
on O
Wikipedia B-DatasetName
and O
Gigaword B-DatasetName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O
We O
only O
keep O
poles O
where O
both O
sides O
have O
at O
least O
three B-HyperparameterValue
adjectives O
that O
appear O
in O
the O
GloVe B-MethodName
vocabulary O
, O
and O
we O
also O
exclude O
acronyms O
, O
which O
are O
often O
more O
ambiguous O
in O
meaning O
. O
We O
start O
with O
723 B-HyperparameterValue
axes B-HyperparameterName
, O
where O
poles O
have O
on O
average O
9.63 O
adjectives O
each O
. O

Contextualized O
embeddings O
. O
Static O
embeddings O
, O
however O
, O
present O
a O
number O
of O
limitations O
. O
Such O
embeddings O
can O
not O
easily O
handle O
polysemy O
or O
homonymy O
( O
Wiedemann O
et O
al O
. O
, O
2019 O
) O
, O
and O
even O
when O
they O
are O
trained O
on O
different O
social O
or O
temporal O
contexts O
, O
they O
require O
additional O
steps O
to O
be O
aligned O
( O
Gonen O
et O
al O
. O
, O
2020 O
) O
. O
Context O
- O
specific O
embeddings O
also O
need O
enough O
training O
examples O
of O
target O
words O
to O
create O
usable O
representations O
. O
These O
limitations O
prevent O
the O
analysis O
of O
token O
- O
based O
semantic O
variation O
, O
such O
as O
measuring O
how O
one O
mention O
of O
a O
word O
is O
more O
or O
less O
beautiful O
than O
another O
. O
Our O
main O
contribution O
of O
contextualized O
axes O
uses O
the O
same O
WordNet O
- O
based O
formulation O
as O
our O
GloVe B-MethodName
baseline O
. O
Rather O
than O
each O
word O
in O
S O
l O
or O
S O
r O
being O
represented O
by O
a O
single O
GloVe B-MethodName
embedding O
, O
we O
obtain O
BERT B-MethodName
embeddings O
over O
multiple O
occurrences O
of O
each O
adjective O
. O
We O
use O
BERT B-MethodName
- I-MethodName
base I-MethodName
, O
as O
this O
model O
is O
small O
enough O
for O
efficient O
application O
on O
large O
datasets O
and O
is O
popular O
in O
previous O
work O
on O
semantic O
change O
and O
differences O
( O
e.g. O
Hu O
et O
al O
. O
, O
2019 O
; O
Lucy O
and O
Bamman O
, O
2021a O
; O
Giulianelli O
et O
al O
. O
, O
2020 O
; O
Zhou O
et O
al O
. O
, O
2022 O
; O
Coll O
Ardanuy O
et O
al O
. O
, O
2020 O
; O
Martinc O
et O
al O
. O
, O
2020 O
) O
. O
It O
is O
also O
used O
in O
tutorials O
for O
researchers O
outside O
of O
NLP O
, O
which O
means O
it O
has O
high O
potential O
use O
in O
computational O
social O
science O
and O
cultural O
analytics O
( O
Mimno O
et O
al O
. O
, O
2022 O
) O
. O

For O
contextualized O
axes O
, O
we O
obtain O
a O
potential O
pool O
of O
contexts O
for O
adjectives O
sampled O
over O
all O
of O
Wikipedia B-DatasetName
from O
December O
21 O
, O
2021 O
, O
preprocessed O
using O
Attardi O
( O
2015 O
) O
's O
text O
extractor O
. O
This O
sample O
contains O
up O
to O
1000 O
sentences O
, O
or O
contexts O
, O
that O
contain O
each O
adjective O
, O
and O
we O
avoid O
contexts O
that O
are O
too O
short O
( O
over O
10 B-HyperparameterValue
tokens O
) O
or O
too O
long O
( O
over O
150 B-HyperparameterValue
tokens O
) O
. O
1 O
We O
experiment O
with O
two O
methods O
of O
obtaining O
contextualized O
BERT B-MethodName
embeddings O
for O
each O
adjective O
: O
a O
random O
" O
default O
" O
( O
BERT B-MethodName
- I-MethodName
DEFAULT I-MethodName
) O
and O
one O
where O
contexts O
are O
picked O
based O
on O
word O
probabilities O
( O
BERT B-MethodName
- I-MethodName
PROB I-MethodName
) O
. O
For O
BERT B-MethodName
- I-MethodName
DEFAULT I-MethodName
, O
we O
take O
a O
random O
sample O
of O
100 B-HyperparameterValue
contextualized O
embeddings O
across O
the O
adjectives O
in O
each O
pole O
. O
Since O
words O
can O
be O
nearest O
neighbors O
with O
their O
antonyms O
in O
semantic O
space O
( O
Mrkšić O
et O
al O
. O
, O
2016 O
; O
Sedoc O
et O
al O
. O
, O
2017 O
) O
, O
our O
main O
approach O
, O
BERT B-MethodName
- I-MethodName
PROB I-MethodName
, O
aggregates O
word O
embeddings O
over O
contexts O
that O
highlight O
contrasting O
meanings O
of O
axes O
' O
poles O
. O

To O
select O
contexts O
, O
we O
mask O
out O
the O
target O
adjective O
in O
each O
of O
its O
1000 O
sentences O
, O
and O
have O
BERT B-MethodName
- I-MethodName
base I-MethodName
predict O
the O
probabilities O
of O
synonyms O
and O
antonyms O
for O
that O
masked O
token O
. O
We O
remove O
contexts O
where O
the O
average O
probability O
of O
antonyms O
is O
greater O
than O
that O
of O
synonyms O
, O
sort O
by O
average O
synonym O
probability O
, O
and O
take O
the O
top O
100 B-HyperparameterValue
contexts O
. O
One O
limitation O
of O
our O
approach O
is O
that O
predictions O
are O
restricted O
to O
adjectives O
that O
can O
be O
represented O
by O
one O
wordpiece O
token O
. O
If O
none O
of O
the O
words O
on O
a O
pole O
of O
an O
axis O
appear O
in O
BERT B-MethodName
's O
vocabulary O
, O
we O
backoff O
to O
BERT B-MethodName
- I-MethodName
DEFAULT I-MethodName
to O
represent O
that O
axis O
. O

For O
each O
axis O
type O
, O
we O
also O
have O
versions O
where O
words O
' O
embeddings O
are O
z O
- O
scored O
, O
which O
has O
been O
shown O
to O
improve O
BERT B-MethodName
's O
alignment O
with O
humans O
' O
word O
similarity O
judgements O
( O
Timkey O
and O
van O
Schijndel O
, O
2021 O
) O
. O
For O
z O
- O
scoring O
, O
we O
calculate O
mean O
and O
standard O
deviation O
BERT B-MethodName
embeddings O
from O
a O
sample O
of O
around O
370k B-HyperparameterValue
whole O
words O
from O
Wikipedia O
. O
As O
recommended O
by O
Bommasani O
et O
al O
. O
( O
2020 O
) O
, O
we O
use O
mean O
pooling O
over O
wordpieces O
to O
produce O
word O
representations O
when O
necessary O
, O
and O
we O
extend O
this O
approach O
to O
create O
bigram O
representations O
as O
well O
. O
These O
embeddings O
are O
a O
concatenation O
of O
the O
last O
four O
layers O
of O
BERT B-MethodName
, O
as O
these O
tend O
to O
capture O
more O
context O
- O
specific O
information O
( O
Ethayarajh O
, O
2019 O
) O
. O

Internal O
validation O

We O
internally O
validate O
our O
axes O
for O
self B-MetricName
- I-MetricName
consistency I-MetricName
. O

For O
each O
axis O
, O
we O
remove O
one O
adjective O
's O
embeddings O
from O
either O
side O
, O
and O
compute O
its O
cosine O
similarity O
to O
the O
axis O
constructed O
from O
the O
remaining O
adjectives O
. O
For O
BERT B-MethodName
approaches O
, O
we O
average O
the O
adjective O
's O
multiple O
embeddings O
to O
produce O
only O
one O
before O
computing O
its O
similarity O
to O
the O
axis O
. O
In O
a O
" O
consistent O
" O
axis O
, O
a O
left O
- O
out O
adjective O
should O
be O
closer O
to O
the O
pole O
it O
belongs O
to O
. O
That O
is O
, O
if O
it O
belongs O
to O
S O
l O
, O
its O
similarity O
to O
the O
axis O
should O
be O
positive O
. O
We O
average O
these O
leave O
- O
one O
- O
out O
similarities O
for O
each O
pole O
, O
negating O
the O
score O
when O
the O
adjective O
belongs O
to O
S O
r O
, O
to O
produce O
a O
consistency B-MetricName
metric O
, O
C B-MetricName
. O

Table O
1 O
shows O
C B-MetricName
for O
different O
axis O
- O
building O
methods O
. O
2 O
An O
axis O
is O
" O
consistent O
" O
if O
both O
of O
its O
poles O
have O
C B-MetricName
≥ O
0 B-MetricValue
. O
GLOVE O
's O
most O
inconsistent O
axis O
poles O
often O
involve O
directions O
, O
such O
as O
east O
↔ O
west O
, O
left O
- O
handed O
↔ O
right O
- O
handed O
, O
and O
right O
↔ O
left O
. O
These O
concepts O
may O
be O
difficult O
to O
learn O
from O
text O
without O
grounding O
. O
We O
find O
that O
the O
various O
BERT B-MethodName
approaches O
' O
most O
inconsistent O
axes O
include O
direction O
- O
related O
ones O
as O
well O
, O
but O
they O
also O
struggle O
to O
separate O
concepts O
such O
as O
lower O
- O
class O
↔ O
upper O
- O
class O
. O

The O
best O
method O
for O
producing O
consistent O
axes O
is O
z B-MethodName
- I-MethodName
scored I-MethodName
BERT I-MethodName
- I-MethodName
PROB I-MethodName
, O
with O
a O
significant O
difference O
in O
C B-MetricName
from O
z B-MethodName
- I-MethodName
scored I-MethodName
BERT I-MethodName
- I-MethodName
DEFAULT I-MethodName
and O
GLOVE B-MethodName
( O
Mann O
- O
Whitney O
U O
- O
test O
, O
p O
< O
0.001 O
) O
. O
It O
also O
produces O
the O
highest O
number O
of O
consistent O
axes O
. O
GLOVE B-MethodName
presents O
itself O
as O
a O
formidable O
baseline O
, O
3 O
and O
BERT B-MethodName
- I-MethodName
DEFAULT I-MethodName
struggles O
in O
comparison O
to O
it O
. O

External O
validation O

Previous O
work O
on O
static O
semantic O
axes O
validates O
them O
using O
sentiment O
lexicons O
, O
exploratory O
anal- O
yses O
, O
and O
human O
- O
reported O
associations O
Kwak O
et O
al O
. O
, O
2021 O
; O
Kozlowski O
et O
al O
. O
, O
2019 O
) O
. O
We O
perform O
external O
validation O
of O
self O
- O
consistent O
axes O
on O
a O
dataset O
where O
people O
appear O
in O
a O
variety O
of O
well O
- O
defined O
and O
known O
contexts O
: O
occupations O
from O
Wikipedia B-DatasetName
. O
We O
conduct O
two O
main O
experiments O
. O
In O
the O
first O
, O
we O
test O
whether O
contextualized O
axes O
can O
detect O
differences O
across O
occupation O
terms O
, O
and O
in O
the O
second O
, O
we O
investigate O
whether O
they O
can O
detect O
differences O
across O
contexts O
. O

Data O

We O
For O
each O
occupation O
's O
singular O
form O
, O
we O
extract O
sentences O
in O
its O
page O
that O
contains O
it O
. O
In O
total O
, O
we O
have O
3,015 O
sentences O
for O
300 O
occupations O
. O

Term O
- O
level O
experiment O
( O
occupations O
) O

Each O
occupation O
is O
represented O
by O
a O
pre O
- O
trained O
GloVe B-MethodName
embedding O
or O
a O
BERT B-MethodName
embedding O
averaged O
over O
all O
occurrences O
on O
its O
page O
. O
If O
an O
axis O
uses O
z O
- O
scored O
adjective O
embeddings O
, O
we O
also O
z O
- O
score O
the O
occupation O
embeddings O
compared O
to O
it O
. O
We O
assign O
poles O
to O
occupations O
based O
on O
which O
side O
of O
the O
axis O
they O
are O
closer O
to O
via O
cosine O
similarity O
. O
embeddings O
' O
proximity O
can O
reflect O
any O
type O
of O
semantic O
association O
, O
not O
just O
that O
a O
person O
actually O
has O
the O
attributes O
of O
an O
adjective O
. O
For O
example O
, O
adjectives O
related O
to O
unhealthy O
are O
highly O
associated O
with O
Health O
occupations O
, O
which O
can O
be O
explained O
by O
doctors O
working O
in O
environments O
where O
unhealthiness O
is O
prominent O
. O
Therefore O
, O
embedding O
distances O
only O
provide O
a O
foggy O
window O
into O
the O
nature O
of O
words O
, O
and O
this O
ambiguity O
should O
be O
considered O
when O
interpreting O
word O
similarities O
and O
their O
implications O
. O
This O
limitation O
applies O
to O
both O
static O
embeddings O
and O
their O
contextualized O
counterparts O
. O

We O
conduct O
human O
evaluation O
on O
this O
task O
of O
using O
semantic O
axes O
to O
differentiate O
and O
characterize O
occupations O
. O
Three O
student O
annotators O
examined O
the O
top O
three O
poles O
retrieved O
by O
each O
axisbuilding O
approach O
and O
ranked O
these O
outputs O
based O
on O
semantic O
relatedness O
to O
occupation O
categories O
( O
Appendix O
B O
) O
. O
These O
annotators O
had O
fair O
agreement O
, O
with O
an O
average O
Kendall O
's O
W O
of O
0.629 O
across O
categories O
and O
experiments O
. O
Though O
GLOVE B-MethodName
is O
a O
competitive O
baseline O
, O
z B-MethodName
- I-MethodName
scored I-MethodName
BERT I-MethodName
- I-MethodName
PROB I-MethodName
is O
the O
highest O
- O
ranked O
approach O
overall O
( O
Table O
3 O
) O
. O
This O
suggests O
that O
more O
self O
- O
consistent O
axes O
also O
produce O
measurements O
that O
better O
reflect O
human O
judgements O
of O
occupations O
' O
general O
meaning O
. O

Context O
- O
level O
experiment O
( O
person O
) O

The O
identity O
of O
a O
word O
, O
and O
prior O
associations O
learned O
from O
BERT B-MethodName
's O
training O
data O
, O
have O
the O
potential O
to O
overpower O
its O
in O
- O
context O
use O
( O
Field O
and O
Tsvetkov O
, O
2019 O
) O
. O
Thus O
, O
we O
may O
want O
to O
discount O
word O
associations O
originally O
learned O
by O
BERT B-MethodName
when O
we O
examine O
the O
use O
of O
a O
target O
word O
in O
a O
narrower O
context O
. O
Prior O
work O
has O
shown O
that O
words O
with O
higher O
frequency O
in O
BERT B-MethodName
's O
training O
data O
tend O
to O
encode O
more O
context O
- O
specific O
information O
in O
their O
embeddings O
( O
Ethayarajh O
, O
2019 O
; O
Zhou O
et O
al O
. O
, O
2021 O
; O
Wolfe O
and O
Caliskan O
, O
2021 O
) O
. O
To O
investigate O
whether O
contextualized O
axes O
can O
measure O
context O
changes O
for O
people O
, O
we O
replace O
all O
occupation O
bigrams O
and O
unigrams O
with O
person O
, O
a O
very O
common O
word O
. O
This O
also O
makes O
contexts O
across O
different O
words O
comparable O
to O
each O
other O
, O
a O
property O
which O
we O
will O
leverage O
later O
in O
Section O
5.4 O
. O

Each O
person O
embedding O
is O
averaged O
over O
one O
occupation O
's O
contexts O
. O
The O
identity O
of O
person O
tends O
to O
overpower O
its O
similarity O
to O
axes O
across O
contexts O
, O
in O
that O
the O
top O
- O
ranked O
poles O
are O
similar O
across O
occupation O
categories O
. O
So O
, O
in O
contrast O
to O
the O
previous O
occupation O
experiment O
, O
additional O
steps O
are O
needed O
to O
draw O
out O
meaningful O
differences O
in O
how O
person O
is O
used O
in O
one O
group O
of O
contexts O
from O
its O
typical O
use O
. O
To O
do O
this O
, O
we O
estimate O
the O
average O
cosine O
similarity O
to O
axes O
of O
n O
person O
embeddings O
in O
occupational O
contexts O
using O
1000 O
bootstrapped O
samples O
, O
where O
n O
is O
the O
number O
of O
terms O
in O
an O
occupation O
category O
. O
We O
take O
the O
axes O
with O
the O
highest O
statistically O
significant O
( O
p O
< O
0.001 O
, O
one O
- O
sample O
t O
- O
test O
) O
difference O
in O
cosine O
similarity O
. O

We O
assume O
that O
occupations O
' O
Wikipedia O
pages O
mention O
them O
within O
definitional O
contexts O
, O
so O
topranked O
poles O
should O
reflect O
the O
original O
occupation O
replaced O
by O
person O
. O
These O
top O
poles O
are O
less O
intuitive O
than O
those O
outputted O
by O
the O
earlier O
term O
- O
level O
experiment O
( O
Table O
2 O
) O
. O
Still O
, O
in O
some O
cases O
, O
such O
as O
for O
Government O
and O
Math O
& O
Statistics O
occupations O
, O
we O
uncover O
relative O
differences O
that O
distinguish O
one O
category O
from O
others O
. O
We O
only O
show O
three O
adjectives O
in O
the O
top O
two O
poles O
in O
Table O
2 O
due O
to O
space O
considerations O
, O
but O
moving O
further O
down O
the O
list O
for O
z B-MethodName
- I-MethodName
scored I-MethodName
BERT I-MethodName
- I-MethodName
PROB I-MethodName
uncovers O
additional O
meaningful O
poles O
. O
For O
example O
, O
the O
pole O
spry O
, O
gymnastic O
, O
sporty O
is O
the O
third O
most O
prominent O
shift O
and O
highest O
similarity O
increase O
( O
+ O
) O
in O
the O
person O
experiment O
for O
Sports O
occupations O
. O
In O
addition O
, O
human O
evaluators O
preferred O
BERT B-MethodName
- I-MethodName
PROB I-MethodName
over O
other O
approaches O
( O
Table O
3 O
, O
Appendix O
B O
) O
. O

Measuring O
change O
and O
variation O

Now O
that O
we O
have O
contextualized O
semantic O
axes O
that O
can O
measure O
differences O
across O
words O
and O
contexts O
, O
we O
apply O
them O
onto O
a O
domain O
that O
can O
showcase O
salient O
and O
socially O
meaningful O
variation O
. O
NLP O
research O
on O
harmful O
language O
often O
employs O
methods O
that O
focus O
on O
the O
target O
group O
, O
such O
as O
measuring O
their O
association O
with O
other O
words O
( O
Zannettou O
et O
al O
. O
, O
2020 O
; O
Garg O
et O
al O
. O
, O
2018 O
; O
Tahmasbi O
et O
al O
. O
, O
2021 O
; O
Field O
and O
Tsvetkov O
, O
2019 O
) O
, O
or O
with O
biases O
in O
models O
( O
Wolfe O
and O
Caliskan O
, O
2021 O
; O
Ghosh O
et O
al O
. O
, O
2021 O
) O
. O
We O
illustrate O
the O
application O
of O
self O
- O
consistent O
z B-MethodName
- I-MethodName
scored I-MethodName
BERT I-MethodName
- I-MethodName
PROB I-MethodName
axes O
onto O
the O
manosphere O
, O
which O
is O
a O
collection O
of O
communities O
with O
mostly O
male O
users O
who O
hold O
alternative O
beliefs O
around O
relationships O
and O
gender O
. O
We O
use O
the O
same O
axes O
we O
presented O
earlier O
, O
which O
were O
created O
using O
Wikipedia B-DatasetName
data O
, O
because O
Wikipedia B-DatasetName
provides O
more O
normative O
coverage O
of O
a O
variety O
of O
adjectives O
than O
topic O
- O
specific O
communities O
. O
This O
way O
, O
we O
examine O
how O
entities O
in O
the O
manosphere O
orient O
themselves O
against O
typical O
adjectival O
uses O
and O
meanings O
. O

The O
manosphere O
has O
been O
linked O
to O
acts O
of O
violence O
in O
the O
physical O
world O
( O
Hoffman O
et O
al O
. O
, O
2020 O
) O
, O
and O
most O
members O
believe O
that O
men O
are O
systemically O
disadvantaged O
in O
society O
( O
Van O
Valkenburgh O
, O
2021 O
; O
Marwick O
and O
Caplan O
, O
2018 O
; O
Lin O
, O
2017 O
; O
Ging O
, O
2019 O
) O
. O
These O
communities O
focus O
on O
heterosexual O
relationships O
and O
masculinity O
, O
and O
feature O
a O
dynamic O
linguistic O
landscape O
. O
Much O
prior O
work O
on O
the O
manosphere O
has O
been O
qualitative O
, O
such O
as O
ethnographies O
( O
Lin O
, O
2017 O
; O
Lumsden O
, O
2019 O
; O
Van O
Valkenburgh O
, O
2021 O
) O
. O
There O
have O
been O
a O
few O
quantitative O
analyses O
of O
their O
language O
, O
usually O
focusing O
on O
phrase O
and O
word O
frequencies O
in O
a O
few O
communities O
( O
Farrell O
et O
al O
. O
, O
2019 O
; O
Gothard O
et O
al O
. O
, O
2021 O
; O
LaViolette O
and O
Hogan O
, O
2019 O
; O
Jaki O
et O
al O
. O
, O
2019 O
) O
. O
As O
an O
example O
involving O
word O
vectors O
, O
Farrell O
et O
al O
. O
( O
2020 O
) O
uses O
static O
embeddings O
identify O
the O
meanings O
of O
incels O
' O
neologisms O
by O
inspecting O
words O
' O
nearest O
neighbors O
. O

Our O
case O
study O
extends O
beyond O
prior O
work O
with O
its O
methodology O
and O
scale O
. O
We O
use O
contextualized O
semantic O
axes O
to O
tackle O
one O
question O
: O
how O
have O
references O
to O
women O
and O
contexts O
around O
them O
changed O
over O
fourteen O
years O
? O

Data O

We O
use O
a O
taxonomy O
of O
subreddits O
and O
external O
forums O
described O
by O
Ribeiro O
et O
al O
. O
( O
2021a O
) O
, O
who O
show O
that O
the O
manosphere O
began O
with O
ideologies O
such O
as O
pick O
- O
up O
artists O
( O
PUA O
) O
and O
Men O
's O
Rights O
Activists O
( O
MRA O
) O
, O
and O
evolved O
into O
more O
extreme O
ones O
such O
as O
The O
Red O
Pill O
( O
TRP O
) O
, O
incels O
( O
short O
for O
involuntary O
celibate O
) O
and O
Men O
Who O
Go O
Their O
Own O
Way O
( O
MGTOW O
) O
, O
with O
users O
moving O
from O
older O
to O
newer O
ideologies O
. O
We O
call O
this O
dataset O
EXTREME_REL B-DatasetName
, O
because O
it O
contains O
extreme O
views O
of O
relationships O
. O

We O
use O
Reddit O
posts O
and O
comments O
from O
March O
2008 O
to O
December O
2019 O
from O
subreddits O
listed O
in O
Ribeiro O
et O
al O
. O
( O
2021a O
) O
's O
study O
, O
downloaded O
from O
Pushshift O
( O
Baumgartner O
et O
al O
. O
, O
2020 O
) O
. O
We O
slightly O
modify O
their O
taxonomy O
by O
separating O
out O
incel O
subreddits O
where O
the O
intended O
userbase O
are O
women O
( O
femcels O
) O
, O
and O
also O
include O
a O
newer O
set O
of O
subreddits O
focused O
on O
" O
Female O
Dating O
Strategy O
" O
( O
FDS O
) O
, O
a O
women O
- O
led O
community O
analogous O
to O
TRP O
( O
Holden O
, O
2020 O
; O
Clark O
- O
Flory O
, O
2021 O
) O
. O
Therefore O
, O
we O
have O
60 O
subreddits O
in O
seven O
ideological O
categories O
: O
Incels O
, O
MGTOW O
, O
PUA O
, O
MRA O
, O
TRP O
, O
FDS O
, O
and O
Femcels O
4 O
( O
Appendix O
C O
) O
. O
This O
Reddit O
subset O
of O
EX B-DatasetName
- I-DatasetName
TREME_REL I-DatasetName
contains O
over O
1.3 O
billion O
tokens O
. O

We O
also O
include O
seven O
external O
forums O
provided O
by O
Ribeiro O
et O
al O
. O
( O
2021a O
) O
. O
These O
public O
forums O
include O
A O
Voice O
for O
Men O
( O
AVFM O
) O
, O
Master O
Pick O
- O
up O
Artist O
( O
MPUA O
) O
Forum O
, O
The O
Attraction O
, O
incels.co O
, O
MGTOW O
Forum O
, O
RooshV O
, O
and O
Red O
Pill O
Talk O
. O
5 O
This O
forum O
subset O
of O
EXTREME_REL B-DatasetName
contains O
over O
800 O
million O
tokens O
spanning O
November O
2005 O
to O
June O
2019 O
, O
and O
we O
remove O
duplicates O
and O
quoted O
text O
from O
posts O
. O
Some O
experiments O
use O
a O
subset O
of O
Reddit O
that O
shares O
a O
similar O
topical O
focus O
as O
EXTREME_REL B-DatasetName
, O
but O
may O
have O
more O
mainstream O
views O
of O
women O
and O
relationships O
. O

We O
use O
a O
list O
6 O
of O
common O
" O
Relationship O
" O
subreddits O
: O
r O
/ O
relationships O
, O
r O
/ O
dating O
, O
r O
/ O
relationship_advice O
, O
r O
/ O
dating_advice O
, O
and O
r O
/ O
breakups O
. O

We O
call O
this O
dataset O
GEN B-DatasetName
- I-DatasetName
ERAL_REL I-DatasetName
, O
and O
it O
contains O
1.2 O
billion O
tokens O
from O
September O
2009 O
to O
December O
2019 O
. O
For O
Reddit O
data O
, O
we O
do O
not O
use O
posts O
and O
comments O
written O
by O
usernames O
who O
have O
bot O
- O
like O
behavior O
, O
which O
we O
define O
as O
repeating O
any O
10 B-HyperparameterValue
- O
gram O
more O
than O
100 B-HyperparameterValue
times O
. O

Vocabulary O

We O
use O
a O
mix O
of O
NER O
, O
online O
glossaries O
, O
and O
manual O
inspection O
to O
curate O
a O
unique O
vocabulary O
of O
people O
( O
details O
in O
Appendix O
D O
) O
. O
This O
vocabulary O
has O
2,434 O
unigrams O
and O
4,179 O
bigrams O
, O
tokenized O
using O
BERT B-MethodName
's O
tokenizer O
without O
splitting O
words O
into O
wordpieces O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
These O
terms O
appear O
at O
least O
500 O
times O
in O
EXTREME_REL B-DatasetName
. O

Since O
gender O
is O
central O
to O
the O
manosphere O
, O
we O
infer O
these O
labels O
based O
on O
terms O
' O
social O
gender O
in O
a O
dataset O
. O
For O
example O
, O
accuser O
is O
not O
semantically O
gendered O
like O
girl O
and O
woman O
, O
but O
its O
social O
gender O
, O
estimated O
using O
pronouns O
, O
is O
more O
feminine O
in O
EXTREME_REL B-DatasetName
than O
GENERAL_REL B-DatasetName
. O
We O
use O
two O
stages O
of O
gender O
inference O
to O
account O
for O
pronoun O
sparsity O
and O
noise O
. O
First O
, O
we O
use O
a O
list O
of O
semantically O
gendered O
nouns O
, O
and O
second O
, O
we O
use O
feminine O
and O
masculine O
pronouns O
linked O
to O
terms O
via O
coreference O
resolution O
( O
details O
in O
Appendix O
E O
) O
. O
We O
label O
each O
vocabulary O
term O
based O
on O
its O
fraction O
of O
cooccurring O
feminine O
pronouns O
in O
EXTREME_REL B-DatasetName
and O
GENERAL_REL B-DatasetName
, O
separately O
. O
We O
are O
able O
to O
label O
72.5 O
% O
of O
the O
vocabulary O
in O
EXTREME_REL B-DatasetName
and O
67.0 O
% O
of O
it O
in O
GENERAL_REL B-DatasetName
. O

Term O
- O
level O
change O

Contextualized O
semantic O
axes O
can O
reveal O
how O
word O
and O
phrase O
types O
change O
over O
time O
. O
Here O
, O
our O
analyses O
focus O
on O
1,482 O
feminine O
( O
genderleaning O
> O
0.75 O
) O
terms O
in O
EXTREME_REL B-DatasetName
. O
To O
capture O
broad O
snapshots O
of O
words O
' O
use O
, O
we O
randomly O
sample O
up O
to O
500 B-HyperparameterValue
sentence B-HyperparameterName
- I-HyperparameterName
level I-HyperparameterName
occurrences I-HyperparameterName
of I-HyperparameterName
each I-HyperparameterName
term I-HyperparameterName
in I-HyperparameterName
each I-HyperparameterName
platform I-HyperparameterName
and I-HyperparameterName
ideology I-HyperparameterName
( O
e.g. O
a O
specific O
forum O
or O
Reddit O
category O
) O
in O
each O
year O
. O
Overall O
z B-MethodName
- I-MethodName
scored I-MethodName
BERT I-MethodName
embeddings O
for O
each O
vocab O
word O
are O
averages O
over O
this O
stratified O
sample O
of O
its O
contexts O
. O

The O
history O
of O
the O
manosphere O
is O
characterized O
by O
waves O
of O
different O
ideological O
communities O
( O
Ribeiro O
et O
al O
. O
, O
2021a O
) O
. O
To O
reflect O
this O
characterization O
through O
language O
, O
we O
segment O
our O
vocabulary O
based O
on O
when O
terms O
peak O
in O
popularity O
. O
We O
cluster O
normalized O
frequency O
time O
series O
7 O
for O
each O
term O
using O
K O
- O
Spectral O
Centroid O
clustering O
( O
KSC O
) O
( O
Yang O
and O
Leskovec O
, O
2011 O
) O
. O
We O
use O
their O
default O
parameters O
, O
including O
K B-HyperparameterName
= O
6 B-HyperparameterValue
. O
In O
contrast O
to O
their O
original O
approach O
, O
our O
symmetric O
distance O
measure O
d O
is O
invariant O
to O
scaling O
by O
α O
but O
not O
to O
the O
translation O
of O
the O
time O
series O
, O
so O
that O
peaks O
earlier O
in O
time O
are O
not O
clustered O
with O
those O
later O
in O
time O
: O

d O
( O
x O
, O
y O
) O
= O
||x O
− O
αy|| O
||x|| O
, O

where O
α O
= O
x O
T O
y O
/ O
||y|| O
2 O
. O
" O
Waves O
" O
of O
term O
types O
for O
people O
correspond O
to O
ideological O
change O
. O
Figure O
2 O
shows O
examples O
of O
feminine O
terms O
, O
but O
the O
top O
masculine O
terms O
are O
often O
labels O
of O
ideological O
groups O
, O
such O
as O
mgtow O
and O
incels O
, O
which O
we O
use O
to O
estimate O
which O
clusters O
align O
with O
ideological O
up O
and O
downturns O
. O
8 O
Cluster O
A O
and O
cluster O
D O
tend O
to O
have O
terms O
that O
have O
widespread O
use O
. O

We O
examine O
the O
shifts O
of O
high O
variance O
, O
substantive O
axes O
across O
temporal O
clusters O
. O
High O
variance O
axes O
include O
those O
related O
to O
gender O
, O
appearance O
, O
and O
desirability O
( O
Table O
4 O
) O
. O
For O
example O
, O
the O
lovable O
versus O
detestable O
pole O
contrasts O
beautiful O
girls O
with O
degenerate O
whores O
. O
As O
another O
example O
, O
the O
axis O
for O
clean O
versus O
dirty O
contrasts O
loyal O
wife O
with O
harlots O
. O
Prior O
studies O
using O
toxicity O
detection O
and O
lexicon O
- O
based O
approaches O
found O
that O
hate O
and O
misogyny O
rose O
with O
the O
arrival O
of O
later O
MG O
- O
TOW O
and O
incel O
communities O
( O
Farrell O
et O
al O
. O
, O
2019 O
; O
Ribeiro O
et O
al O
. O
, O
2021a O
) O
. O
Similarly O
, O
we O
find O
that O
lexical O
choices O
for O
women O
are O
more O
detestable O
and O
dirty O
in O
later O
waves O
associated O
with O
MGTOW O
and O
incels O
( O
Figure O
3 O
) O
. O
Often O
, O
low O
and O
high O
frequency O
words O
share O
similar O
patterns O
in O
each O
wave O
. O

Context O
- O
level O
change O

Contextualized B-MethodName
semantic I-MethodName
axes I-MethodName
can O
reveal O
how O
the O
contexts O
around O
people O
have O
changed O
over O
time O
. O
Women O
in O
online O
communities O
can O
be O
referenced O
in O
a O
variety O
of O
ways O
( O
Figure O
2 O
) O
. O
To O
compare O
overall O
changes O
around O
women O
between O
mainstream O
and O
extremist O
communities O
, O
we O
examine O
the O
contexts O
around O
feminine O
( O
gender B-MetricName
- I-MetricName
leaning I-MetricName
> O
0.75 B-MetricValue
) O
words O
. O
We O
use O
instances O
of O
287 B-HyperparameterValue
unigram O
types O
, O
since O
bigrams O
can O
include O
modifiers O
that O
would O
be O
considered O
" O
context O
" O
. O
As O
discussed O
earlier O
, O
word O
identities O
impact O
measurements O
of O
contextual O
changes O
across O
them O
( O
Section O
4.3 O
) O
. O
We O
replace O
each O
target O
word O
with O
person O
or O
people O
depending O
on O
whether O
it O
is O
singular O
or O
plural O
, O
estimated O
through O
the O
Python O
INFLECT O
package O
. O
We O
choose O
Figure O
4 O
: O
Contexts O
around O
singular O
( O
person O
) O
or O
plural O
( O
people O
) O
feminine O
words O
over O
time O
in O
EXTREME_REL B-DatasetName
and O
GENERAL_REL B-DatasetName
along O
three O
axes O
. O
Time O
series O
include O
95 B-HyperparameterValue
% I-HyperparameterValue
CI B-HyperparameterName
, O
and O
dotted O
lines O
mark O
the O
peak O
of O
major O
ideological O
communities O
( O
gray O
labels O
) O
. O
These O
vertical O
lines O
are O
months O
that O
have O
the O
highest O
normalized O
frequencies O
of O
words O
used O
to O
refer O
to O
their O
members O
: O
puas O
, O
mras O
, O
trpers O
, O
mgtows O
, O
and O
incels O
. O

replacements O
to O
respect O
singular O
/ O
plural O
forms O
to O
ensure O
ecological O
validity O
and O
not O
perturb O
BERT B-MethodName
's O
sensitivity O
to O
grammaticality O
( O
Yin O
et O
al O
. O
, O
2020 O
) O
. O
We O
use O
reservoir O
sampling O
to O
obtain O
up O
to O
1000 O
occurrences O
of O
person O
- O
or O
people O
- O
replaced O
feminine O
words O
in O
each O
month O
on O
EXTREME_REL B-DatasetName
and O
GEN B-DatasetName
- I-DatasetName
ERAL_REL I-DatasetName
. O

In O
comparison O
to O
GENERAL_REL B-DatasetName
, O
EX B-DatasetName
- I-DatasetName
TREME_REL I-DatasetName
has O
more O
detestable O
, O
sickening O
, O
and O
dirty O
contexts O
for O
women O
( O
Figure O
4 O
) O
. O
Both O
GENERAL_REL B-DatasetName
and O
EXTREME_REL B-DatasetName
discuss O
relationship O
issues O
, O
but O
contextualized O
axes O
reveal O
how O
contrasting O
and O
changing O
attitudes O
toward O
women O
can O
influence O
context O
. O
Negative O
associations O
especially O
peak O
during O
the O
height O
of O
the O
incels O
' O
movement O
around O
late O
2017 O
to O
mid O
2019 O
. O
These O
persist O
despite O
Reddit O
's O
ban O
of O
r O
/ O
incels O
in O
November O
2017 O
and O
the O
quarantine O
of O
r O
/ O
braincels O
and O
r O
/ O
theredpill O
in O
September O
2018 O
. O
Thus O
, O
the O
widespread O
efficacy O
of O
community O
- O
level O
moderation O
is O
worthy O
of O
closer O
study O
( O
e.g. O
Copland O
, O
2020 O
; O
Ribeiro O
et O
al O
. O
, O
2021b O
) O
. O
An O
advantage O
of O
computing O
scores O
at O
the O
token O
- O
level O
rather O
than O
at O
the O
type O
- O
level O
is O
interpretability O
. O
That O
is O
, O
one O
can O
see O
which O
contexts O
land O
at O
the O
extreme O
ends O
of O
axes O
( O
as O
illustrated O
in O
Table O
5 O
) O
. O

Contextualized B-MethodName
semantic I-MethodName
axes I-MethodName
can O
also O
illuminate O
differences O
among O
lexical O
variables O
, O
or O
different O
linguistic O
forms O
that O
share O
the O
same O
referential O
meaning O
( O
Nguyen O
et O
al O
. O
, O
2021 O
; O
Labov O
, O
1972 O
) O
. O

As O
prominent O
examples O
, O
men O
- O
led O
communities O
use O
the O
lexical O
innovations O
femoids O
and O
foids O
, O
which O
are O
shortenings O
of O
female O
humanoids O
, O
as O
dehumanizing O
words O
for O
all O
women O
( O
Chang O
, O
2020 O
; O
Prażmo O
, O
2020 O
FDS O
, O
use O
moids O
as O
an O
analogous O
way O
to O
refer O
to O
men O
. O
Prior O
work O
studying O
three O
manosphere O
subreddits O
showed O
that O
the O
lemmas O
woman O
and O
girl O
are O
constructed O
negatively O
as O
immoral O
, O
deceptive O
, O
incapable O
and O
insignificant O
( O
Krendel O
, O
2020 O
) O
. O
We O
hypothesize O
that O
the O
contexts O
of O
community O
- O
specific O
variants O
should O
have O
even O
more O
dehumanizing O
connotations O
along O
similar O
dimensions O
. O
In O
this O
experiment O
, O
we O
replace O
all O
terms O
( O
men O
, O
moids O
, O
foids O
, O
femoids O
, O
and O
women O
) O
with O
people O
. O

We O
sample O
up O
to O
100 B-HyperparameterValue
occurrences O
of O
each O
variant O
in O
each O
platform O
and O
ideology O
per O
year O
, O
limiting O
time O
ranges O
to O
when O
domain O
- O
specific O
variants O
are O
widely O
used O
by O
their O
home O
community O
. O
We O
examine O
the O
use O
of O
variants O
for O
men O
by O
Femcels O
and O
FDS O
in O
2018 O
- O
2019 O
, O
and O
the O
use O
of O
variants O
for O
women O
by O
all O
other O
communities O
in O
EXTREME_REL B-DatasetName
in O
2017 O
- O
2019 O
. O
Unlike O
in O
the O
person O
experiment O
for O
occupations O
, O
we O
have O
substantial O
pools O
of O
occurrences O
to O
compare O
. O
Thus O
, O
to O
find O
axes O
that O
distinguish O
one O
variant O
from O
another O
, O
we O
use O
axis O
scores O
as O
features O
in O
random O
forest O
classifiers O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
, O
and O
perform O
binary O
classification O
of O
word O
identity O
: O
women O
versus O
foids O
or O
femoids O
, O
and O
men O
versus O
moids O
( O
Appendix O
G O
) O
. O
We O
rank B-MetricName
axes O
based O
on O
their O
feature O
importance O
, O
and O
select O
three O
highly O
ranked O
and O
relevant O
axes O
to O
show O
in O
Figure O
5 O
. O
Shifts O
along O
these O
axes O
confirm O
our O
hypothesis O
that O
communityspecific O
variants O
are O
more O
dehumanized O
than O
their O
widely O
- O
used O
counterparts O
. O

Conclusion O

In O
this O
work O
, O
we O
examine O
the O
capability O
of O
contextualized O
embeddings O
for O
discovering O
differences O
among O
words O
and O
contexts O
. O
Our O
method O
uses O
predicted O
word O
probabilities O
to O
pinpoint O
which O
contexts O
to O
include O
when O
aggregating O
BERT B-MethodName
embeddings O
to O
construct O
axes O
. O
This O
approach O
creates O
more O
self O
- O
consistent O
axes O
that O
better O
fit O
different O
occupation O
categories O
, O
in O
comparison O
to O
baselines O
. O
We O
further O
demonstrate O
the O
use O
of O
these O
axes O
in O
a O
longitudinal O
, O
cross O
- O
platform O
case O
study O
. O
Overall O
, O
contextualized O
embeddings O
offer O
more O
flexibility O
and O
granularity O
compared O
to O
static O
ones O
for O
the O
analysis O
of O
content O
across O
time O
and O
communities O
. O
That O
is O
, O
rather O
than O
train O
static O
word O
embeddings O
for O
various O
subsets O
of O
data O
, O
we O
can O
characterize O
change O
and O
variation O
at O
the O
token O
- O
level O
. O

Though O
we O
focus O
on O
analyzing O
associations O
between O
adjectives O
and O
people O
, O
our O
approach O
can O
generalize O
to O
other O
types O
of O
entities O
as O
well O
. O
Measuring O
and O
comparing O
the O
contexts O
of O
other O
entity O
types O
should O
include O
many O
of O
the O
same O
considerations O
we O
did O
, O
such O
as O
reducing O
the O
conflation O
of O
antonyms O
, O
controlling O
for O
word O
identity O
by O
replacing O
target O
words O
with O
a O
shared O
hypernym O
, O
and O
experimenting O
with O
z O
- O
scoring O
. O
Future O
work O
includes O
understanding O
why O
some O
opposing O
concepts O
are O
conflated O
in O
large O
language O
models O
, O
and O
how O
a O
word O
embed O
- O
ding O
's O
identity O
influences O
its O
encoding O
of O
contexts O
. O

Limitations O

Aside O
from O
computing O
power O
requirements O
( O
Appendix O
H O
) O
, O
we O
outline O
a O
few O
additional O
limitations O
of O
our O
methodology O
and O
its O
application O
not O
discussed O
in O
the O
main O
text O
. O
Domain O
shift O
. O
The O
use O
of O
pretrained O
BERT B-MethodName
on O
a O
niche O
set O
of O
communities O
makes O
our O
approaches O
susceptible O
to O
domain O
shift O
, O
such O
as O
rare O
words O
having O
less O
robust O
embeddings O
( O
Zhou O
et O
al O
. O
, O
2022 O
( O
Zhou O
et O
al O
. O
, O
, O
2021 O
, O
or O
target O
words O
carrying O
over O
learned O
associations O
from O
a O
broader O
corpus O
that O
are O
less O
applicable O
in O
a O
narrower O
one O
. O
Domain O
shift O
is O
difficult O
to O
avoid O
without O
retraining O
or O
further O
pretraining O
BERT B-MethodName
, O
which O
is O
resource O
- O
intensive O
, O
may O
risk O
catastrophic O
forgetting O
, O
and O
inaccessible O
to O
some O
disciplines O
in O
computational O
social O
science O
( O
Gururangan O
et O
al O
. O
, O
2020 O
; O
Ramponi O
and O
Plank O
, O
2020 O
; O
Goodfellow O
et O
al O
. O
, O
2014 O
) O
. O
Also O
, O
training O
a O
large O
language O
model O
on O
text O
with O
toxic O
and O
misogynistic O
origins O
introduces O
additional O
risk O
of O
dual O
use O
( O
Kurenkov O
, O
2022 O
) O
. O
We O
suggest O
some O
potential O
workarounds O
that O
lessen O
the O
severity O
of O
domain O
shift O
, O
such O
as O
replacing O
target O
words O
with O
common O
ones O
for O
context O
- O
focused O
analyses O
. O

WordNet O
. O
WordNet O
is O
a O
popular O
lexical O
resource O
for O
NLP O
, O
but O
its O
senses O
for O
words O
can O
be O
overly O
finegrained O
( O
Pilehvar O
and O
Camacho O
- O
Collados O
, O
2019 O
) O
and O
not O
suitable O
for O
all O
domains O
. O
We O
use O
WordNet O
version O
3.0 O
, O
which O
is O
included O
in O
NLTK O
, O
and O
this O
version O
was O
last O
updated O
in O
2006 O
. O
Since O
English O
is O
constantly O
changing O
, O
some O
synonym O
and O
antonym O
relations O
may O
be O
outdated O
. O

Errors O
. O
Our O
method O
for O
drawing O
out O
differences O
in O
words O
is O
better O
than O
common O
baselines O
yet O
still O
imperfect O
, O
and O
some O
of O
the O
opposing O
concepts O
in O
embedding O
space O
that O
BERT B-MethodName
struggles O
to O
separate O
may O
be O
important O
for O
an O
application O
domain O
. O
Therefore O
, O
domain O
expertise O
is O
needed O
to O
recognize O
spurious O
patterns O
from O
real O
ones O
and O
fill O
these O
gaps O
. O

In O
the O
main O
text O
we O
mention O
that O
embeddings O
offer O
a O
" O
foggy O
window O
" O
into O
how O
two O
concepts O
may O
be O
associated O
or O
related O
, O
and O
the O
exact O
type O
of O
relation O
is O
not O
always O
clear O
. O
For O
example O
, O
if O
contexts O
for O
women O
are O
closer O
to O
unpleasant O
, O
does O
it O
mean O
that O
the O
text O
discusses O
unpleasant O
events O
that O
affect O
women O
, O
or O
that O
the O
writers O
believe O
that O
women O
are O
unpleasant O
, O
or O
both O
? O
Some O
of O
this O
uncertainty O
could O
be O
resolved O
qualitatively O
by O
inspecting O
sentences O
at O
poles O
' O
extremes O
. O
We O
compare O
embeddings O
for O
people O
to O
axes O
, O
but O
it O
is O
also O
possible O
to O
include O
relation O
- O
based O
approaches O
such O
as O
dependency O
parsing O
and O
compare O
words O
that O
share O
specific O
relations O
with O
people O
to O
axes O
( O
e.g. O
Lucy O
and O
Bamman O
, O
2021b O
) O
. O
One O
trade O
- O
off O
of O
doing O
this O
is O
that O
informative O
verbs O
and O
adjectives O
connected O
to O
mentions O
of O
target O
groups O
can O
be O
sparse O
. O
Our O
method O
is O
able O
to O
find O
that O
mathematician O
replaced O
with O
person O
is O
highly O
similar O
to O
calculable O
in O
a O
variety O
of O
sentence O
structures O
, O
such O
as O
this O
one O
modified O
off O
Wikipedia O
: O
A O
person O
is O
someone O
who O
uses O
an O
extensive O
knowledge O
of O
mathematics O
in O
their O
work O
, O
typically O
to O
solve O
mathematical O
problems O
. O

Ethical O
considerations O

User O
privacy O
. O
Online O
data O
opens O
many O
doors O
for O
research O
, O
but O
its O
use O
raises O
concerns O
around O
user O
privacy O
. O
For O
our O
use O
case O
, O
we O
believe O
that O
the O
benefits O
of O
our O
work O
outweigh O
privacy O
- O
related O
harms O
. O
Consent O
is O
infeasible O
to O
obtain O
for O
large O
datasets O
( O
Buchanan O
, O
2017 O
) O
, O
and O
in O
the O
manosphere O
, O
it O
is O
unlikely O
that O
users O
would O
give O
consent O
, O
especially O
if O
the O
researchers O
using O
their O
data O
believe O
that O
their O
ideologies O
are O
harmful O
and O
wrong O
. O
Obtaining O
consent O
would O
pose O
risks O
to O
the O
safety O
of O
the O
researcher O
( O
Conway O
, O
2021 O
; O
Doerfler O
et O
al O
. O
, O
2021 O
) O
. O

All O
online O
discussions O
included O
in O
our O
work O
were O
public O
when O
downloaded O
by O
their O
original O
curators O
, O
mainly O
Baumgartner O
et O
al O
. O
( O
2020 O
) O
and O
Ribeiro O
et O
al O
. O
( O
2021a O
) O
. O
Some O
forums O
and O
online O
glossaries O
were O
relocated O
, O
shutdown O
, O
banned O
, O
or O
made O
private O
later O
on O
. O
A O
user O
's O
" O
right O
to O
be O
forgotten O
" O
confronts O
researchers O
who O
have O
interests O
in O
documenting O
and O
studying O
the O
histories O
of O
communities O
. O
We O
truncate O
the O
examples O
shown O
in O
our O
paper O
rather O
than O
use O
them O
in O
full O
verbatim O
( O
Bruckman O
, O
2002 O
) O
. O

Communities O
may O
expect O
their O
posts O
to O
stay O
within O
their O
in O
- O
group O
, O
but O
the O
content O
in O
our O
work O
was O
posted O
on O
public O
platforms O
. O
This O
publicness O
and O
increased O
visibility O
plays O
a O
key O
role O
in O
how O
this O
content O
impacts O
others O
, O
such O
as O
those O
who O
view O
this O
information O
and O
propagate O
it O
elsewhere O
, O
or O
those O
who O
are O
direct O
targets O
of O
hate O
. O
Common O
targets O
such O
as O
women O
and O
people O
of O
color O
carry O
a O
bigger O
burden O
when O
participating O
in O
online O
spaces O
( O
Hoffmann O
and O
Jonas O
, O
2017 O
) O
, O
and O
our O
broader O
research O
agenda O
aims O
to O
mitigate O
this O
issue O
. O

Social O
biases O
in O
models O
and O
resources O
. O
We O
use O
WordNet O
to O
group O
similar O
adjectives O
into O
semantic O
axes O
, O
but O
we O
observe O
some O
socially O
harmful O
asso O
- O
ciations O
in O
this O
resource O
. O
For O
example O
, O
gross O
and O
fat O
are O
listed O
as O
similar O
lemmas O
. O
As O
another O
example O
, O
WordNet O
conflates O
gender O
and O
sexuality O
when O
androgynous O
and O
bisexual O
are O
also O
listed O
as O
similar O
lemmas O
. O
The O
BERT B-MethodName
language O
model O
, O
like O
all O
large O
, O
pretrained O
models O
, O
is O
also O
susceptible O
to O
social O
biases O
in O
its O
training O
data O
( O
Bender O
et O
al O
. O
, O
2021 O
) O
. O

Gender O
inference O
. O
In O
this O
paper O
's O
main O
case O
study O
, O
we O
perform O
gender O
inference O
for O
word O
and O
phrase O
types O
. O
This O
step O
was O
necessary O
to O
study O
how O
women O
are O
portrayed O
over O
time O
, O
which O
is O
a O
key O
question O
due O
to O
the O
centrality O
of O
misogyny O
in O
these O
communities O
. O
However O
, O
perfect O
prediction O
of O
each O
word O
's O
perceived O
gender O
in O
our O
dataset O
using O
pronouns O
is O
impossible O
( O
Cao O
and O
Daumé O
III O
, O
2021 O
) O
. O
Not O
all O
mentions O
of O
people O
co O
- O
occur O
with O
pronouns O
, O
pronouns O
do O
not O
equate O
gender O
, O
and O
coreference O
resolution O
systems O
can O
produce O
errors O
. O
So O
, O
we O
approximate O
the O
social O
gender O
of O
terms O
by O
aggregating O
coreference O
patterns O
over O
all O
instances O
of O
that O
term O
. O
Since O
it O
is O
difficult O
to O
separate O
noisy O
errors O
from O
meaningful O
word O
- O
level O
pronoun O
variation O
at O
scale O
, O
we O
had O
to O
use O
a O
score O
threshold O
to O
pinpoint O
what O
words O
were O
feminine O
- O
leaning O
enough O
to O
be O
included O
in O
our O
analyses O
. O

Restricting O
pronouns O
to O
the O
traditional O
binary O
of O
feminine O
and O
masculine O
is O
limiting O
, O
since O
individuals O
use O
other O
pronouns O
as O
well O
. O
They O
/ O
them O
pronouns O
are O
predominantly O
used O
to O
reference O
plural O
terms O
in O
this O
dataset O
, O
and O
the O
coreference O
model O
we O
use O
does O
not O
handle O
neopronouns O
. O
The O
manosphere O
and O
the O
typical O
framing O
under O
which O
it O
is O
studied O
is O
heavily O
cisheteronormative O
. O
We O
use O
a O
frequency B-HyperparameterName
cutoff I-HyperparameterName
to O
determine O
our O
vocabulary O
( O
Appendix O
D O
) O
, O
so O
references O
to O
transgender O
and O
nonbinary O
people O
may O
be O
filtered O
out O
. O
Vocab O
terms O
retained O
for O
transgender O
people O
are O
outdated O
or O
typically O
offensive O
terms O
such O
as O
transsexuals O
and O
transgenders O
, O
and O
no O
vocab O
term O
includes O
non O
- O
binary O
, O
nb O
, O
or O
nonbinary O
. O

A O
Wikipedia B-DatasetName
page O
titles O

Table O
6 O
lists O
the O
categories O
of O
occupations O
, O
the O
titles O
of O
Wikipedia O
pages O
that O
list O
them O
, O
and O
the O
number O
of O
terms O
in O
each O
category O
. O
These O
lists O
were O
retrieved O
in O
February O
2022 O
. O

B O
Human O
evaluation O
for O
occupations O

We O
recruited O
three O
student O
volunteers O
with O
familiarity O
with O
NLP O
coursework O
and O
tasks O
to O
rank O
the O
top O
poles O
provided O
by O
each O
axis O
- O
building O
method O
for O
our O
occupation O
and O
person O
experiments O
. O
We O
used O
Qualtrics O
to O
design O
and O
launch O
the O
survey O
. O
Since O
we O
were O
not O
asking O
about O
personal O
opinions O
but O
rather O
evaluating O
models O
, O
we O
were O
determined O
exempt O
from O
IRB O
review O
by O
the O
appropriate O
office O
at O
our O
institution O
. O
Each O
question O
pertains O
to O
a O
specific O
occupation O
category O
, O
and O
within O
each O
experiment O
, O
question O
order O
and O
answer O
option O
order O
are O
randomly O
shuffled O
. O
Each O
model O
option O
is O
presented O
with O
its O
top O
three O
poles O
, O
in O
order O
of O
most O
to O
less O
Hi O
! O
Thank O
you O
so O
much O
for O
volunteering O
to O
evaluate O
the O
performance O
of O
NLP O
models O
. O

Please O
read O
these O
instructions O
carefully O
. O

In O
this O
task O
, O
you O
will O
judge O
how O
much O
lists O
of O
adjectives O
from O
WordNet O
outputted O
by O
models O
are O
semantically O
related O
to O
occupational O
differences O
described O
in O
Wikipedia O
. O
These O
models O
make O
predictions O
based O
on O
a O
large O
collection O
of O
sentences O
, O
of O
which O
you O
will O
see O
a O
few O
examples O
to O
help O
you O
make O
your O
decision O
. O
The O
purpose O
is O
to O
see O
whether O
NLP O
models O
capture O
semantic O
, O
or O
meaning O
, O
differences O
in O
the O
contexts O
around O
people O
in O
sentences O
. O
These O
occupations O
fall O
under O
several O
categories O
, O
ranging O
from O
scientists O
to O
entertainers O
. O

You O
are O
deciding O
which O
models O
' O
outputs O
are O
typically O
more O
related O
to O
occupations O
, O
which O
may O
not O
reflect O
your O
personal O
opinions O
about O
occupations O
. O

There O
are O
two O
sets O
of O
questions O
, O
and O
11 O
questions O
in O
each O
set O
. O

As O
a O
toy O
example O
: O

Examples O
of O
occupations O
in O
Fairytales O
include O
fairy O
godmothers O
, O
prince O
charming O
, O
evil O
villains O
, O
and O
wizards O
. O

You O
are O
given O
the O
sets O
of O
adjectives O
below O
. O
Adjective O
sets O
include O
" O
MORE O
" O
and O
" O
LESS O
" O
labels O
based O
on O
how O
people O
in O
the O
category O
above O
are O
more O
or O
less O
related O
to O
them O
, O
in O
comparison O
to O
other O
people O
who O
work O
as O
artists O
, O
government O
workers O
, O
and O
scientists O
: O
The O
above O
three O
models O
are O
ranked O
from O
most O
related O
to O
the O
occupation O
category O
to O
least O
related O
. O
That O
is O
, O
Model O
A O
is O
higher O
than O
Model O
B O
because O
even O
though O
they O
both O
agree O
that O
fairytale O
jobs O
are O
very O
related O
to O
" O
more O
mythical O
/ O
legendary O
/ O
fantastical O
" O
, O
Model O
B O
incorrectly O
lists O
" O
less O
noisy O
/ O
clamorous O
/ O
creaky O
" O
as O
its O
second O
set O
of O
adjectives O
. O
Model O
C O
is O
ranked O
last O
because O
its O
first O
two O
sets O
of O
adjectives O
are O
not O
related O
to O
fairytale O
jobs O
. O

Try O
to O
be O
consistent O
in O
your O
rankings O
. O
That O
is O
, O
in O
the O
example O
above O
, O
you O
should O
not O
rank O
Model O
C O
after O
A O
and O
before O
B O
because O
A O
and O
B O
agree O
on O
the O
first O
set O
and O
overall O
share O
two O
valid O
adjective O
sets O
. O
Model O
C O
is O
more O
of O
an O
outlier O
, O
with O
only O
one O
valid O
third O
adjective O
set O
. O
relevant O
. O
Figure O
6 O
shows O
screenshots O
of O
instructions O
. O
In O
the O
toy O
example O
, O
the O
options O
are O
labeled O
with O
" O
Model O
A O
" O
, O
" O
Model O
B O
" O
, O
" O
Model O
C O
" O
, O
to O
allow O
explanation O
clarity O
, O
but O
in O
the O
actual O
task O
questions O
, O
options O
are O
not O
labeled O
with O
model O
letters O
to O
avoid O
biasing O
the O
evaluators O
towards O
a O
specific O
model O
. O
Some O
annotators O
expressed O
that O
the O
task O
was O
difficult O
, O
and O
for O
some O
occupations O
, O
different O
approaches O
output O
similar O
axes O
, O
just O
in O
different O
order O
. O

C O
Reddit O
communities O

We O
used O
a O
list O
of O
subreddits O
9 O
for O
the O
manosphere O
provided O
by O
( O
Ribeiro O
et O
al O
. O
, O
2021a O
) O
in O
their O
detailed O
, O
data O
- O
driven O
sketch O
of O
the O
manosphere O
. O
Five O
of O
the O
subreddits O
included O
in O
Ribeiro O
et O
al O
. O
( O
2021a O
) O
's O
taxonomy O
of O
the O
Reddit O
manosphere O
( O
r O
/ O
malecels O
, O
r O
/ O
lonelynonviolentmen O
, O
r O
/ O
1ncels O
, O
r O
/ O
incelbrotherhood O
, O
r O
/ O
incelspurgatory O
) O
were O
not O
on O
Pushshift O
's O
dump O
of O
Reddit O
. O
We O
curated O
the O
list O
of O
communities O
for O
our O
new O
ideological O
category O
, O
Female O
Dating O
Strategy O
( O
FDS O
) O
, O
using O
a O
now O
removed O
list O
of O
FDS O
's O
" O
sister O
communities O
" O
on O
the O
subreddit O
r O
/ O
FemaleDatingStrategy O
's O
sidebar O
: O

r O
/ O
PinkpillFeminism O
, O
r O
/ O
AskFDS O
, O
r O
/ O
FDSSuperFans O
, O
r O
/ O
PornFreeRelationships O
, O
and O
r O
/ O
FemaleLevelUpStrategy O
. O
The O
Femcels O
set O
of O
subreddits O
include O
: O
r O
/ O
Trufemcels O
, O
r O
/ O
TheGlowUp O
, O
and O
r O
/ O
AskTruFemcels O
. O
Though O
the O
main O
user O
base O
of O
the O
manosphere O
are O
men O
, O
there O
are O
also O
small O
populations O
of O
women O
in O
other O
ideologies O
as O
well O
, O
such O
as O
r O
/ O
redpillwomen O
. O
We O
mainly O
portion O
out O
FDS O
and O
Femcels O
due O
to O
their O
role O
in O
Section O
5.4 O
's O
lexical O
variant O
experiment O
as O
communities O
who O
use O
moids O
. O

In O
total O
we O
have O
12 O
subreddits O
in O
TRP O
, O
11 O
in O
MRA O
, O
7 O
in O
PUA O
, O
22 O
in O
Incels O
, O
3 O
in O
MGTOW O
, O
4 O
in O
Femcels O
, O
and O
6 O
in O
FDS O
. O
The O
complete O
list O
of O
subreddits O
and O
their O
categories O
is O
also O
in O
our O
Github O
repo O
. O

D O
Vocabulary O
creation O

First O
, O
we O
extract O
nominal O
and O
proper O
persons O
using O
NER O
, O
keeping O
ones O
that O
are O
popular O
( O
occur O
at O
least O
500 B-HyperparameterValue
times O
in O
EXTREME_REL B-DatasetName
) O
, O
and O
unambiguous O
, O
where O
at O
least O
20 B-HyperparameterValue
% I-HyperparameterValue
of O
its O
instances O
in O
these O
datasets O
are O
tagged O
as O
a O
person O
. O
Gathering O
a O
substantial O
number O
of O
labels O
from O
our O
domain O
to O
train O
an O
indomain O
NER O
system O
from O
scratch O
is O
outside O
the O
scope O
of O
our O
work O
, O
so O
we O
experimented O
with O
three O
models O
trained O
on O
other O
labeled O
datasets O
: O
ACE O
, O
contemporary O
literature O
, O
and O
a O
combination O
of O
both O
. O
We O
evaluated O
these O
models O
on O
a O
small O
set O
of O
posts O
and O
comments O
labeled O
by O
one O
author O
, O
after O
retrieving O
25 O
examples O
per O
forum O
or O
Reddit O
ideological O
category O
using O
reservoir O
sampling O
. O
The O
annotator O
only O
labeled O
spans O
for O
nominal O
and O
named O
PERSON O
entities O
. O
Table O
7 O
shows O
the O
performance O
of O
each O
model O
on O
EXTREME_REL O
. O
Based O
on O
these O
evaluation O
results O
, O
we O
chose O
to O
use O
the O
model O
trained O
on O
contemporary O
literature O
. O

We O
extract O
bigrams O
and O
unigrams O
from O
detected O
spans O
, O
excluding O
determiners O
and O
possessives O
whose O
heads O
are O
the O
root O
of O
the O
span O
. O
Named O
entities O
that O
refer O
to O
types O
of O
people O
rather O
than O
specific O
individuals O
were O
estimated O
through O
their O
co O
- O
occurrence O
with O
the O
determiner O
a O
, O
e.g. O
a O
Chad O
. O

Then O
, O
one O
author O
consulted O
community O
glossaries O
and O
examined O
in O
- O
context O
use O
of O
words O
to O
manually O
correct O
the O
list O
of O
automatically O
extracted O
terms O
. O
We O
include O
additional O
popular O
and O
unambiguous O
words O
not O
tagged O
sufficiently O
often O
enough O
by O
NER O
, O
but O
defined O
as O
people O
in O
prior O
work O
and O
online O
resources O
. O

Table O
8 O
lists O
the O
sources O
and O
glossaries O
for O
vocabulary O
words O
and O
the O
ideologies O
they O
include O
. O
Some O
of O
these O
sources O
, O
such O
as O
the O
Shedding O
of O
the O
Ego O
, O
are O
created O
by O
insiders O
in O
the O
community O
, O
while O
some O
, O
such O
as O
academic O
papers O
and O
news O
articles O
, O
are O
by O
outsiders O
. O
For O
each O
of O
these O
glossaries O
and O
lists O
of O
terms O
, O
we O
manually O
separated O
them O
into O
two O
categories O
: O
269 O
people O
( O
singular O
and O
plural O
forms O
) O
and O
1776 O
non O
- O
people O
. O
Two O
of O
these O
sites O
, O
Shedding O
of O
the O
Ego O
and O
Pualingo O
, O
no O
longer O
exists O
, O
but O
were O
publicly O
available O
until O
at O
least O
late O
2020 O
. O
We O
include O
93 O
terms O
for O
people O
that O
were O
initially O
filtered O
out O
in O
our O
NER O
pipeline O
in O
our O
final O
vocabulary O
, O
excluding O
ambiguous O
ones O
that O
also O
occur O
often O
as O
non O
- O
human O
entities O
, O
such O
as O
tool O
( O
a O
fool O
who O
is O
taken O
advantage O
of O
) O
and O
jaw O
( O
short O
for O
just O
another O
wannabe O
) O
. O

The O
resulting O
vocabulary O
contains O
niche O
language O
, O
where O
20.7 O
% O
of O
unigrams O
are O
not O
found O
in O
WordNet O
, O
and O
85.1 O
% O
of O
those O
missing O
are O
also O
not O
in O
the O
Internet O
resource O
Urban O
Dictionary O
. O
10 O
The O
full O
list O
is O
also O
available O
in O
our O
Github O
repo O
. O

E O
Gender O
inference O

This O
section O
includes O
additional O
details O
around O
our O
gender O
inference O
process O
. O

Our O
list O
of O
semantically O
gendered O
terms O
, O
or O
words O
gendered O
by O
definition O
, O
expands O
upon O
the O
one O
used O
by O
Hoyle O
et O
al O
. O
( O
2019 O
) O
We O
include O
the O
following O
additional O
semantically O
gendered O
terms O
: O
male O
, O
males O
, O
dude O
, O
dudes O
, O
guy O
, O
guys O
, O
boyfriend O
, O
boyfriends O
, O
bf O
, O
female O
, O
females O
, O
chick O
, O
chicks O
, O
girlfriend O
, O
girlfriends O
, O
gf O
, O
gal O
, O
gals O
, O
bro O
, O
transmen O
, O
transwomen O
, O
she O
, O
he O
. O

We O
check O
if O
any O
of O
the O
above O
words O
appear O
in O
a O
unigram O
or O
bigram O
vocabularly O
term O
. O
Around O
29.9 O
% O
of O
our O
vocabulary O
in O
EXTREME_REL B-DatasetName
is O
gendered O
through O
this O
word O
list O
approach O
. O

To O
infer O
gender O
for O
the O
remaining O
words O
using O
pronouns O
, O
we O
ran O
coreference O
resolution O
on O
EX B-DatasetName
- I-DatasetName
TREME_REL I-DatasetName
, O
and O
extracted O
all O
pronouns O
that O
are O
clustered O
in O
coreference O
chains O
with O
terms O
in O
our O
vocabulary O
( O
Clark O
and O
Manning O
, O
2016 O
) O
. O
We O
label O
the O
masculine O
to O
feminine O
leaning O
of O
vocab O
terms O
by O
calculating O
the O
proportion O
of O
feminine O
pronouns O
( O
she O
, O
her O
, O
hers O
, O
herself O
) O
over O
the O
sum O
of O
feminine O
and O
masculine O
pronouns O
( O
he O
, O
him O
, O
his O
, O
himself O
) O
. O
We O
only O
consider O
a O
word O
to O
have O
a O
usable O
gender O
signal O
if O
it O
appears O
in O
at O
least O
10 B-HyperparameterValue
coreference O
clusters O
with O
feminine O
or O
masculine O
pronouns O
. O
Since O
plural O
words O
do O
not O
usually O
appear O
with O
he O
/ O
she O
pronouns O
, O
we O
have O
plural O
words O
take O
on O
the O
gender O
leaning O
of O
their O
singular O
forms O
. O
We O
pair O
plural O
and O
singular O
forms O
using O
the O
Python O
INFLECT O
package O
. O
11 O
We O
also O
transfer O
unigrams O
' O
gender O
to O
bigrams O
, O
after O
examining O
the O
modifiers O
( O
the O
first O
token O
) O
in O
bigram O
terms O
to O
check O
that O
they O
are O
not O
differently O
and O
semantically O
gendered O
. O
Around O
20.9 O
% O
of O
our O
vocabulary O
in O
EXTREME_REL B-DatasetName
is O
gendered O
through O
pronouns O
alone O
, O
an O
additional O
12.6 O
% O
is O
gendered O
through O
plural O
to O
singular O
mapping O
, O
and O
an O
additional O
9.1 O
% O
is O
gendered O
through O
bigram O
to O
unigram O
mapping O
. O

F O
High O
variance O
axes O

Table O
9 O
shows O
the O
top O
vocabulary O
terms O
that O
correspond O
to O
the O
poles O
of O
high O
variance O
axes O
. O

G O
Classification O
of O
lexical O
variants O

Our O
main O
goal O
here O
is O
to O
tease O
out O
which O
axes O
differentiate O
the O
contexts O
of O
lexical O
variants O
, O
rather O
than O
find O
the O
best O
model O
that O
performs O
well O
on O
a O
classification O
task O
. O
Therefore O
, O
we O
choose O
to O
use O
a O
random O
forest O
classifier O
for O
its O
interpretability O
: O
it O
outputs O
weights O
that O
indicate O
what O
features O
were O
most O
important O
across O
its O
decisions O
. O
We O
use O
scikitlearn O
's O
implementation O
, O
and O
perform O
randomized O
search O
with O
5 B-HyperparameterValue
- O
fold B-HyperparameterName
cross O
validation O
and O
weighted O
F1 B-MetricName
scoring O
to O
select O
model O
parameters O
( O
Table O
10 O
) O
. O
Table O
11 O
shows O
the O
most O
important O
axis O
features O
of O
these O
models O
. O
In O
general O
, O
the O
set O
of O
most O
important O
features O
did O
not O
change O
much O
with O
parameter O
choices O
and O
roughly O
aligns O
with O
axes O
that O
showcase O
the O
largest O
mean O
differences O
between O
each O
pair O
of O
variants O
. O
That O
is O
, O
the O
three O
axes O
we O
show O
in O
the O
main O
text O
in O
Figure O
5 O
are O
also O
among O
the O
top O
ten O
ordered O
by O
mean O
difference O
for O
men O
vs. O
moids O
and O
women O
vs. O
femoids O
. O

H O
Runtime O
and O
infrastructure O

We O
only O
use O
BERT B-MethodName
- I-MethodName
base I-MethodName
for O
inference O
, O
but O
the O
overall O
runtime O
cost O
is O
high O
due O
to O
the O
size O
of O
our O
corpora O
: O
English O
Wikipedia O
and O
social O
media O
discussions O
. O
We O
use O
one O
Titan O
XP O
GPU O
with O
8 O
CPU O
cores O
for O
most O
of O
the O
paper O
, O
and O
occasionally O
expanded O
to O
multiple O
machines O
with O
1080ti O
and O
K80 O
GPUs O
in O
parallel O
when O
handling O
social O
media O
data O
. O
We O
use O
BERT B-MethodName
for O
two O
main O
purposes O
: O
predicting O
word O
probabilities O
to O
select O
contexts O
for O
constructing O
axes O
, O
and O
obtaining O
word O
embeddings O
. O
On O
one O
10 O
: O
Parameter O
choices O
for O
random O
forest O
classification O
. O
Symbols O
mark O
selected O
parameters O
for O
each O
task O
, O
where O
† O
refers O
to O
men O
vs. O
moids O
, O
‡ O
refers O
to O
women O
vs. O
femoids O
, O
and O
* O
refer O
to O
women O
vs. O
foids O
. O
These O
models O
had O
weighted B-MetricName
F1 I-MetricName
scores I-MetricName
of O
0.670 B-MetricValue
, O
0.759 B-MetricValue
, O
and O
0.781 B-MetricValue
, O
respectively O
. O
Titan O
XP O
GPU O
, O
the O
former O
takes O
∼1 O
hour O
for O
one O
million O
sentences O
containing O
one O
masked O
target O
word O
each O
, O
and O
the O
latter O
takes O
∼2.5 O
hours O
for O
one O
million O
sentences O
, O
including O
wordpiece O
aggregation O
. O

Acknowledgements O

We O
thank O
Manoel O
Horta O
Ribeiro O
for O
sharing O
his O
dataset O
and O
materials O
for O
our O
case O
study O
, O
and O
Sam O
Robertson O
, O
Alexus O
Lopez O
, O
and O
Harold O
Cha O
for O
evaluating O
model O
outputs O
. O
In O
addition O
, O
we O
are O
grateful O
for O
feedback O
provided O
by O
Nicholas O
Tomlin O
, O
Kaitlyn O
Zhou O
, O
and O
our O
anonymous O
reviewers O
. O
This O
research O
was O
supported O
by O
funding O
from O
the O
National O
Science O
Foundation O
( O
DGE-1752814 O
, O
IIS-1813470 O
, O
and O
IIS-1942591 O
) O
. O

Title2Event B-DatasetName
: O
Benchmarking O
Open B-TaskName
Event I-TaskName
Extraction I-TaskName
with O
a O
Large O
- O
scale O
Chinese O
Title O
Dataset O

Event B-TaskName
extraction I-TaskName
( O
EE B-TaskName
) O
is O
crucial O
to O
downstream O
tasks O
such O
as O
new O
aggregation O
and O
event O
knowledge O
graph O
construction O
. O
Most O
existing O
EE B-TaskName
datasets O
manually O
define O
fixed O
event O
types O
and O
design O
specific O
schema O
for O
each O
of O
them O
, O
failing O
to O
cover O
diverse O
events O
emerging O
from O
the O
online O
text O
. O
Moreover O
, O
news O
titles O
, O
an O
important O
source O
of O
event O
mentions O
, O
have O
not O
gained O
enough O
attention O
in O
current O
EE B-TaskName
research O
. O
In O
this O
paper O
, O
We O
present O
Title2Event B-DatasetName
, O
a O
large O
- O
scale O
sentence O
- O
level O
dataset O
benchmarking O
Open B-TaskName
Event I-TaskName
Extraction I-TaskName
without O
restricting O
event O
types O
. O
Title2Event B-DatasetName
contains O
more O
than O
42,000 O
news O
titles O
in O
34 O
topics O
collected O
from O
Chinese O
web O
pages O
. O
To O
the O
best O
of O
our O
knowledge O
, O
it O
is O
currently O
the O
largest O
manuallyannotated O
Chinese O
dataset O
for O
open B-TaskName
event I-TaskName
extraction I-TaskName
. O
We O
further O
conduct O
experiments O
on O
Ti B-DatasetName
- I-DatasetName
tle2Event I-DatasetName
with O
different O
models O
and O
show O
that O
the O
characteristics O
of O
titles O
make O
it O
challenging O
for O
event B-TaskName
extraction I-TaskName
, O
addressing O
the O
significance O
of O
advanced O
study O
on O
this O
problem O
. O
The O
dataset O
and O
baseline O
codes O
are O
available O
at O
https O
: O
/ O
/ O
open-event-hub.github.io O
/ O
title2event O
. O

Introduction O

Event B-TaskName
extraction I-TaskName
( O
EE B-TaskName
) O
is O
an O
essential O
task O
in O
information O
extraction O
( O
IE O
) O
, O
aiming O
to O
extract O
structured O
event O
information O
from O
unstructured O
plain O
text O
. O
Extracting O
events O
from O
news O
plays O
an O
important O
role O
in O
tracking O
and O
analyzing O
social O
media O
trending O
, O
and O
facilitates O
various O
downstream O
tasks O
including O
information O
retrieval O
( O
Basile O
et O
al O
. O
, O
2014 O
) O
, O
news O
recommendation O
system O
( O
Raza O
and O
Ding O
, O
2020 O
) O
and O
event O
knowledge O
graph O
construction O
( O
Gottschalk O
and O
Demidova O
, O
2018 O
; O
. O
Figure O
1 O
shows O
an O
example O
of O
extracting O
events O
from O
multiple O
news O
titles O
. O
Based O
on O
the O
extracted O
events O
, O
news O
reporting O
the O
same O
event O
could O
be O
aggregated O
and O
sent O
to O
users O
to O
provide O
comprehensive O
views O
from O
different O
sources O
. O
Event B-TaskName
extraction I-TaskName
can O
be O
categorized O
into O
two O
levels O
: O
sentence O
- O
level O
EE B-TaskName
and O
document O
- O
level O
EE B-TaskName
. O
Sentence O
- O
level O
EE B-TaskName
identifies O
event O
entities O
and O
attributes O
in O
a O
single O
sentence O
( O
Ahn O
, O
2006 O
) O
, O
while O
document O
- O
level O
EE B-TaskName
aims O
to O
extract O
entities O
of O
the O
same O
event O
scattered O
across O
an O
article O
( O
Sundheim O
, O
1992 O
) O
. O
In O
scenarios O
such O
as O
news O
aggregation O
, O
human O
- O
written O
news O
titles O
often O
preserve O
the O
core O
information O
of O
the O
news O
event O
, O
while O
news O
articles O
may O
contain O
too O
many O
trivial O
details O
. O
Therefore O
, O
performing O
sentence O
- O
level O
EE B-TaskName
on O
news O
titles O
is O
more O
efficient O
than O
document O
- O
level O
EE B-TaskName
on O
news O
articles O
to O
aggregate O
relevant O
news O
. O

However O
, O
most O
EE B-TaskName
models O
trained O
on O
traditional O
sentence O
- O
level O
datasets O
could O
not O
reach O
ideal O
performance O
when O
extracting O
events O
from O
titles O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Nguyen O
and O
Nguyen O
, O
2019 O
; O
Wadden O
et O
al O
. O
, O
2019 O
; O
Du O
and O
Cardie O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020a O
; O
Lu O
et O
al O
. O
, O
2021 O
; O
Lou O
et O
al O
. O
, O
2022 O
) O
. O
On O
the O
one O
hand O
, O
these O
models O
request O
predefined O
event O
types O
and O
a O
specific O
schema O
for O
each O
of O
them O
. O
Each O
event O
schema O
consists O
of O
manually O
designed O
argument O
roles O
such O
as O
event O
trigger O
, O
person O
, O
time O
, O
and O
location O
. O
Then O
the O
extraction O
of O
events O
will O
The O
first O
event O
is O
actually O
the O
subject O
of O
the O
second O
event O
, O
which O
indicates O
these O
two O
events O
are O
associated O
. O
be O
decomposed O
into O
sub O
- O
tasks O
of O
extracting O
each O
argument O
role O
separately O
. O
Despite O
the O
success O
in O
traditional O
EE B-TaskName
, O
the O
manual O
design O
of O
specific O
event O
schema O
is O
costly O
and O
time O
- O
consuming O
, O
and O
the O
limited O
predefined O
event O
types O
could O
not O
handle O
a O
great O
variety O
of O
events O
emerging O
from O
the O
Internet O
where O
most O
news O
titles O
nowadays O
are O
derived O
from O
. O
On O
the O
other O
hand O
, O
extracting O
events O
from O
Chinese O
titles O
could O
be O
more O
challenging O
than O
traditional O
sentence O
- O
level O
EE B-TaskName
such O
as O
the O
ACE O
2005 O
benchmark O
. O
1 O
This O
is O
because O
some O
unique O
writing O
styles O
are O
observed O
in O
news O
titles O
on O
Chinese O
social O
media O
, O
as O
shown O
in O
Figure O
2 O
. O
First O
, O
the O
writing O
of O
many O
titles O
does O
not O
strictly O
obey O
the O
correct O
grammar O
. O
For O
example O
, O
some O
titles O
will O
omit O
the O
agent O
when O
describing O
an O
action O
for O
brevity O
, O
while O
others O
may O
place O
the O
action O
before O
the O
first O
mention O
of O
the O
agent O
for O
emphasis O
. O
Second O
, O
the O
role O
overlap O
problem O
, O
i.e. O
, O
the O
same O
entity O
may O
play O
different O
roles O
in O
multiple O
events O
, O
usually O
occurs O
when O
the O
events O
in O
the O
text O
have O
certain O
associations O
with O
each O
other O
. O

Requirement O
of O
Domain O
Knowledge O

Although O
there O
are O
about O
10 O
% O
events O
in O
ACE B-DatasetName
2005 I-DatasetName
having O
this O
problem O
, O
it O
has O
not O
gained O
enough O
research O
attention O
for O
quite O
a O
long O
time O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
the O
role O
overlap O
problem O
is O
much O
We O
write O
detailed O
annotation O
guidelines O
and O
conducted O
two O
rounds O
of O
expert O
review O
for O
quality O
control O
. O
To O
the O
best O
of O
our O
knowledge O
, O
Title2Event B-DatasetName
is O
currently O
the O
largest O
manually O
annotated O
Chinese O
dataset O
for O
OpenEE B-TaskName
. O

3 O
. O
It O
is O
the O
first O
sentence O
- O
level O
dataset O
with O
a O
special O
focus O
on O
titles O
with O
its O
unique O
values O
and O
challenges O
that O
little O
attention O
has O
been O
paid O
to O
. O
We O
believe O
Title2Event B-DatasetName
could O
further O
facilitate O
current O
EE B-TaskName
research O
in O
real O
- O
world O
scenarios O
. O

We O
experiment O
with O
different O
methods O
on O
Ti B-DatasetName
- I-DatasetName
tle2Event I-DatasetName
and O
analyze O
their O
performance O
to O
address O
the O
challenges O
of O
this O
task O
. O

Related O
Work O

Event B-TaskName
Extraction I-TaskName
Datasets O
. O
Automatic B-DatasetName
Content I-DatasetName
Extraction I-DatasetName
( O
ACE B-DatasetName
2005 I-DatasetName
) O
( O
Doddington O
et O
al O
. O
, O
2004 O
) O
is O
one O
of O
the O
most O
widely O
- O
used O
corpora O
in O
event O
extraction O
. O
It O
contains O
599 O
documents O
with O
8 O
event O
types O
, O
33 O
event O
subtypes O
, O
and O
35 O
argument O
roles O
in O
English O
, O
Arabic O
and O
Chinese O
( O
Li O
et O
al O
. O
, O
2021b O
) O
. O
TAC B-DatasetName
KBP I-DatasetName
2017 I-DatasetName
2 O
is O
a O
dataset O
of O
the O
event O
tracking O
task O
in O
KBP O
which O
contains O
8 O
event O
types O
and O
18 O
event O
subtypes O
in O
English O
, O
Chinese O
and O
Spanish O
. O
MAVEN B-DatasetName
( O
Wang O
et O
al O
. O
, O
2020 O
) O
collects O
4,480 O
Wikipedia O
documents O
, O
118,732 O
event O
mention O
instances O
and O
constructs O
168 O
event O
types O
. O
Despite O
the O
large O
scale O
, O
MAVEN B-DatasetName
merely O
focuses O
on O
event O
triggers O
without O
annotating O
event O
arguments O
. O
All O
of O
the O
above O
datasets O
manually O
define O
event O
types O
and O
schema O
, O
struggling O
to O
handle O
newly O
emerging O
event O
types O
in O
real O
- O
world O
applications O
. O

Open O
Information O
Extraction O
. O
Open O
information O
extraction O
( O
OpenIE O
) O
aims O
to O
extract O
facts O
in O
the O
form O
of O
relational O
tuples O
from O
unstructured O
text O
without O
restricting O
target O
relations O
, O
relieving O
human O
labor O
of O
designing O
complex O
domaindependent O
schema O
( O
Niklaus O
et O
al O
. O
, O
2018 O
) O
. O
Due O
to O
the O
release O
of O
large O
- O
scale O
OpenIE O
benchmarks O
such O
as O
OIE2016 O
( O
Stanovsky O
and O
Dagan O
, O
2016 O
) O
and O
CaRB O
( O
Bhardwaj O
et O
al O
. O
, O
2019 O
) O
, O
neural O
OpenIE O
approaches O
become O
popular O
( O
Zhou O
et O
al O
. O
, O
2022 O
) O
. O
Existing O
neural O
OpenIE O
models O
can O
be O
categorized O
into O
sequence O
tagging O
models O
( O
Stanovsky O
et O
al O
. O
, O
2018 O
; O
Kolluru O
et O
al O
. O
, O
2020a O
; O
Zhan O
and O
Zhao O
, O
2020 O
) O
and O
generative O
sequence O
- O
to O
- O
sequence O
models O
( O
Cui O
et O
al O
. O
, O
2018 O
; O
Kolluru O
et O
al O
. O
, O
2020b O
) O
. O
We O
adopt O
the O
formulation O
of O
OpenIE O
and O
represent O
events O
as O
triplets O
since O
the O
event O
mentions O
in O
news O
titles O
tend O
to O
be O
brief O
without O
complex O
substructures O
. O

Chinese O
Event B-TaskName
Extraction I-TaskName
. O
Chinese O
event B-TaskName
extraction I-TaskName
can O
be O
regarded O
as O
a O
special O
case O
of O
EE B-TaskName
due O
to O
its O
unique O
linguistic O
properties O
and O
challenges O
( O
Li O
et O
al O
. O
, O
2021b O
) O
. O
However O
, O
the O
resources O
of O
Chinese O
EE B-TaskName
data O
are O
relatively O
scarce O
and O
lack O
sufficient O
coverage O
comparing O
with O
EE B-TaskName
data O
in O
English O
, O
which O
greatly O
hinders O
existing O
research O
( O
Zeng O
et O
al O
. O
, O
2016 O
; O
Lin O
et O
al O
. O
, O
2018 O
; O
Ding O
et O
al O
. O
, O
2019 O
; O
Xiangyu O
et O
al O
. O
, O
2019 O
; O
Cui O
et O
al O
. O
, O
2020 O
) O
. O
Apart O
from O
multilingual O
datasets O
with O
Chinese O
corpora O
such O
as O
ACE B-DatasetName
2005 I-DatasetName
and O
TAC B-DatasetName
KBP I-DatasetName
2017 I-DatasetName
, O
Chinese B-DatasetName
Emergency I-DatasetName
Corpus I-DatasetName
( O
CEC B-DatasetName
) O
3 O
collects O
6 O
types O
of O
common O
emergency O
events O
. O
Doc2EDAG B-DatasetName
and O
FEED B-DatasetName
( O
Li O
et O
al O
. O
, O
2021a O
) O
are O
two O
Chinese O
financial O
EE B-TaskName
datasets O
built O
upon O
distant O
supervision O
. O
DuEE B-DatasetName
( O
Li O
et O
al O
. O
, O
2020b O
) O
is O
a O
document O
- O
level O
EE B-TaskName
dataset O
with O
19,640 O
events O
categorized O
into O
65 O
event O
types O
, O
collected O
from O
news O
articles O
on O
Chinese O
social O
media O
. O
Compared O
with O
DuEE B-DatasetName
, O
our O
Ti B-DatasetName
- I-DatasetName
tle2Event I-DatasetName
dataset O
is O
larger O
in O
scale O
and O
does O
not O
restrict O
event O
types O
. O

Dataset O
Construction O

This O
section O
describes O
the O
process O
of O
data O
collection O
and O
annotation O
details O
. O

Data O
Collection O

We O
broadly O
collect O
Chinese O
web O
pages O
from O
January O
to O
March O
2022 O
using O
the O
web O
crawler O
logs O
of O
the O
search O
engine O
of O
Tencent O
as O
well O
as O
a O
proven O
business O
tool O
to O
select O
web O
pages O
containing O
event O
mentions O
( O
most O
of O
them O
are O
from O
news O
websites O
) O
. O
Afterwards O
, O
the O
titles O
of O
the O
selected O
web O
pages O
are O
extracted O
and O
automatically O
tagged O
with O
our O
predefined O
topics O
, O
and O
titles O
containing O
toxic O
contents O
are O
all O
removed O
. O
To O
ensure O
the O
diversity O
of O
events O
, O
we O
conduct O
data O
sampling O
every O
ten O
days O
during O
the O
crawling O
period O
, O
reducing O
the O
occurrence O
of O
events O
belonging O
to O
the O
top O
frequently O
appeared O
topics O
to O
make O
the O
distribution O
of O
topics O
more O
balanced O
. O
Eventually O
, O
around O
43,000 O
instances O
are O
collected O
. O

Annotation O
Framework O

Annotation O
Standard O
. O
We O
summarize O
some O
essential O
parts O
of O
our O
annotation O
standard O
. O
In O
general O
, O
we O
expect O
each O
event O
could O
be O
represented O
by O
a O
( O
Subject O
, O
Predicate O
, O
Object O
) O
triplet O
where O
the O
subject O
and O
object O
could O
be O
viewed O
as O
the O
argument O
roles O
of O
the O
event O
triggered O
by O
the O
predicate O
. O
Multiple O
event O
triplets O
may O
be O
extracted O
from O
a O
single O
title O
, O
and O
they O
may O
have O
some O
overlaps O
. O
However O
, O
the O
predicate O
of O
an O
triplet O
is O
considered O
as O
the O
unique O
identifier O
of O
an O
event O
, O
thus O
multiple O
triplets O
of O
a O
single O
title O
will O
not O
share O
the O
same O
predicate O
. O
Some O
important O
specifications O
are O
listed O
below O
: O

1 O
) O
We O
define O
event O
as O
an O
action O
or O
a O
state O
of O
change O
which O
occurs O
in O
the O
real O
world O
. O
Some O
statements O
such O
as O
policy O
notifications O
or O
some O
subjective O
opinions O
are O
not O
considered O
as O
events O
. O
Also O
, O
if O
an O
title O
is O
not O
clearly O
expressed O
, O
or O
is O
concatenated O
by O
several O
unrelated O
events O
( O
e.g. O
news O
round O
- O
up O
) O
, O
then O
it O
should O
be O
labeled O
as O
" O
invalid O
" O
by O
annotators O
. O

2 O
) O
We O
find O
the O
identification O
of O
predicates O
in O
Chinese O
is O
complex O
, O
so O
we O
specify O
some O
rules O
to O
unify O
them O
. O
First O
, O
if O
an O
event O
tends O
to O
emphasize O
the O
state O
change O
of O
the O
subject O
, O
e.g. O
" O
南阳大桥通 O
车 O
" O
( O
Nanyang O
Bridge O
opens O
to O
traffic O
) O
, O
then O
the O
predicate O
will O
be O
labeled O
as O
" O
通车 O
" O
( O
open O
- O
to O
- O
traffic O
) O
instead O
of O
" O
通 O
" O
with O
" O
车 O
" O
as O
the O
object O
. O
Second O
, O
for O
phrases O
with O
serialized O
verbs O
and O
dual O
objects O
, O
we O
integrate O
the O
direct O
target O
of O
the O
action O
( O
i.e. O
the O
Patient O
) O
into O
the O
predicate O
expression O
while O
taking O
the O
indirect O
patient O
( O
i.e. O
the O
Affectee O
) O
( O
Thompson O
, O
1973 O
) O
as O
the O
object O
of O
the O
event O
. O
For O
example O
, O
in O
" O
送孩子去学校 O
" O
( O
send O
kids O
to O
school O
) O
the O
predicate O
will O
be O
labeled O
as O
" O
送去学校 O
" O
( O
send O
- O
toschool O
) O
with O
" O
孩子 O
" O
( O
kids O
) O
as O
the O
object O
. O
Moreover O
, O
we O
find O
the O
colon O
( O
" O
： O
" O
) O
frequently O
plays O
the O
role O
of O
predicate O
in O
titles O
, O
representing O
the O
meaning O
of O
" O
say O
" O
, O
" O
announce O
" O
or O
" O
require O
" O
, O
etc O
. O
We O
view O
this O
as O
a O
feature O
of O
news O
titles O
and O
allow O
annotators O
to O
label O
it O
as O
the O
predicate O
. O

3 O
) O
We O
expect O
the O
fine O
- O
grained O
annotations O
of O
argument O
roles O
, O
which O
are O
intact O
yet O
not O
redundant O
. O
All O
determiners O
and O
modifiers O
of O
entities O
are O
kept O
only O
if O
they O
largely O
affect O
the O
understanding O
of O
events O
. O
All O
triplets O
are O
required O
to O
have O
a O
subject O
and O
a O
predicate O
, O
while O
the O
object O
could O
be O
omitted O
as O
in O
the O
original O
text O
. O

Crowdsourced O
Annotation O
. O
We O
cooperate O
with O
crowdsourcing O
companies O
to O
hire O
human O
annotators O
. O
After O
multi O
- O
rounds O
of O
training O
in O
three O
weeks O
, O
27 O
annotators O
are O
selected O
. O
We O
pay O
them O
￥1 O
per O
instance O
. O
Meanwhile O
, O
four O
experts O
are O
participated O
in O
two O
rounds O
of O
annotation O
checking O
for O
quality O
control O
. O
For O
each O
instance O
, O
a O
human O
annotator O
is O
asked O
to O
write O
all O
expected O
event O
triplets O
independently O
. O
To O
reduce O
the O
annotation O
difficulty O
, O
we O
provide O
some O
auxiliary O
information O
along O
with O
the O
raw O
title O
, O
including O
the O
tokenization O
outputs O
, O
to O
help O
annotators O
quickly O
capture O
the O
entities O
and O
concepts O
present O
in O
the O
titles O
. O
Note O
that O
we O
do O
not O
force O
annotators O
to O
strictly O
obey O
the O
tokenization O
outputs O
, O
as O
we O
find O
that O
many O
of O
them O
do O
not O
match O
the O
desired O
granularity O
of O
triplet O
elements O
under O
our O
criteria O
. O
Instead O
, O
the O
annotation O
is O
conducted O
in O
a O
< O
text O
, O
label O
> O
pair O
paradigm O
rather O
than O
a O
token O
- O
level O
tagging O
paradigm O
. O
Moreover O
, O
we O
provide O
automatic O
extraction O
outputs O
as O
references O
. O
During O
the O
initial O
phase O
, O
we O
design O
an O
unsupervised O
model O
to O
extract O
triplets O
. O
After O
20,000 B-HyperparameterValue
labeled O
instances O
are O
collected O
, O
we O
train O
a O
better O
sequence O
tagging O
model O
for O
the O
rest O
of O
annotation O
process O
. O
Both O
models O
are O
introduced O
in O
Section O
5 O
. O
Meanwhile O
, O
as O
titles O
often O
contain O
some O
domain O
knowledge O
which O
the O
annotators O
may O
not O
be O
familiar O
with O
, O
we O
allow O
them O
to O
refer O
to O
search O
engines O
. O
To O
ensure O
the O
quality O
, O
we O
also O
allow O
them O
to O
label O
an O
instance O
as O
" O
not O
sure O
" O
if O
they O
are O
not O
confident O
enough O
. O
The O
crowdsourced O
annotation O
is O
conducted O
in O
batches O
. O
Every O
batch O
of O
annotated O
instances O
undergoes O
two O
rounds O
of O
quality O
checking O
before O
being O
integrated O
into O
the O
final O
version O
of O
our O
dataset O
. O
We O
also O
develop O
a O
browserbased O
web O
application O
to O
accelerate O
the O
annotation O
process O
, O
see O
Appendix O
A O
. O

First O
- O
round O
Checking O
. O
Each O
time O
the O
crowdsourced O
annotation O
of O
a O
batch O
is O
completed O
, O
it O
is O
sent O
to O
four O
experts O
to O
check O
whether O
they O
meet O
the O
requirements O
of O
our O
annotation O
standard O
. O
Instances O
which O
do O
not O
pass O
the O
quality O
check O
will O
be O
sent O
back O
for O
revision O
, O
attached O
with O
the O
specific O
reasons O
for O
rejection O
. O
This O
process O
repeats O
until O
the O
acceptance B-HyperparameterName
rate I-HyperparameterName
reaches O
90 B-HyperparameterValue
% I-HyperparameterValue
. O

Second O
- O
round O
Checking O
. O
Each O
batch O
of O
annotated O
instances O
passing O
the O
first O
- O
round O
checking O
is O
sent O
to O
the O
authors O
for O
dual O
check O
. O
The O
authors O
will O
randomly O
check O
30 O
% O
of O
the O
instances O
and O
send O
unqualified O
instances O
back O
to O
the O
experts O
along O
with O
the O
reasons O
for O
rejection O
. O
Slight O
adjustments O
on O
annotation O
standard O
also O
take O
place O
in O
this O
phase O
. O
This O
process O
repeats O
until O
the O
acceptance B-HyperparameterName
rate I-HyperparameterName
reaches O
95 B-HyperparameterValue
% I-HyperparameterValue
. O

Our O
annotation O
process O
encourages O
positive O
interactions O
among O
the O
authors O
, O
the O
experts O
and O
the O
crowdsourced O
annotators O
, O
which O
effectively O
helps O
the O
annotators O
to O
understand O
the O
annotation O
standard O
and O
provide O
timely O
feedback O
. O

Data O
Analysis O
on O
Title2Event B-DatasetName

This O
section O
describes O
the O
statistics O
and O
characteristics O
of O
Title2Event B-DatasetName
from O
various O
perspectives O
. O
Table O
1 O
shows O
the O
overview O
of O
the O
dataset O
. O

Topic O
Distribution O
. O
The O
titles O
in O
the O
dataset O
can O
be O
categorized O
into O
34 O
topics O
, O
24 O
of O
which O
contain O
more O
than O
100 O
instances O
. O
Figure O
3 O
Event O
Distribution O
. O
As O
shown O
in O
Table O
1 O
, O
most O
of O
the O
titles O
contain O
more O
than O
one O
event O
, O
and O
the O
maximum O
number O
of O
events O
per O
title O
is O
six O
. O
We O
further O
investigate O
the O
distribution O
of O
instances O
containing O
different O
numbers O
of O
triggers O
( O
i.e. O
predicates O
for O
Title2Event B-DatasetName
) O
, O
and O
compare O
our O
dataset O
with O
the O
ACE2005 B-DatasetName
Chinese O
dataset O
( O
denoted O
as O
ACE05 B-DatasetName
- I-DatasetName
zh I-DatasetName
) O
4 O
as O
shown O
in O
Figure O
4 O
. O
It O
can O
be O
observed O
that O
the O
phenomenon O
of O
multiple O
events O
per O
instance O
is O
more O
common O
in O
Title2Event B-DatasetName
compared O
with O
ACE05 B-DatasetName
- I-DatasetName
zh I-DatasetName
, O
which O
brings O
additional O
challenges O
in O
event O
extraction O
. O

Predicate O
Distribution O
. O
We O
also O
investigate O
the O
distribution O
of O
predicates O
in O
Title2Event B-DatasetName
. O
Figure O
5 O
shows O
the O
distribution O
of O
the O
30 O
most O
frequent O
predicates O
in O
the O
dataset O
. O

Challenge O
Distribution O
. O
We O
further O
analyze O
to O
what O
extent O
are O
the O
observed O
challenges O
described O
in O
Figure O
2 O
covered O
in O
Title2Event B-DatasetName
. O
To O
do O
this O
, O
we O
randomly O
sample O
1,000 B-HyperparameterValue
instances O
and O
manually O
annotate O
1 O
) O
Whether O
the O
instance O
omits O
or O
inverts O
some O
event O
arguments O
which O
makes O
itself O
not O
strictly O
obeying O
the O
grammatical O
norms O
. O

2 O
) O
Whether O
there O
's O
a O
text O
span O
appearing O
at O
multiple O
events O
of O
the O
instance O
. O
( O
3 O
) O
Whether O
some O
domain O
knowledge O
is O
crucial O
in O
understanding O
the O
instance O
that O
without O
these O
knowledge O
one O
might O
not O
correctly O
identify O
the O
event O
arguments O
. O
The O
annotation O
result O
shows O
that O
9.70 O
% O
of O
sampled O
instances O
are O
observed O
with O
unconventional O
writing O
, O
21.50 O
% O
instances O
have O
role O
overlap O
problem O
( O
10 O
% O
for O
ACE B-DatasetName
2005 I-DatasetName
for O
comparison O
) O
, O
and O
2.80 O
% O
instances O
requires O
domain O
knowledge O
for O
correct O
event O
understating O
. O
We O
believe O
such O
statistics O
are O
a O
good O
identification O
of O
the O
challenging O
nature O
of O
Title2Event B-DatasetName
. O

Methods O

Formally O
, O
given O
a O
sequence O
of O
tokens O
S O
= O
< O
w O
1 O
, O
w O
2 O
, O
. O
. O
. O
, O
w O
n O
> O
, O
Open O
EE B-TaskName
aims O
to O
output O
a O
list O
of O
triplets O
T O
= O
< O
t O
1 O
, O
t O
2 O
, O
. O
. O
. O
, O
t O
m O
> O
where O
each O
triplet O
t O
i O
= O
< O
s O
i O
, O
p O
i O
, O
o O
i O
> O
represents O
an O
event O
occurred O
in O
S O
and O
s O
i O
, O
p O
i O
, O
o O
i O
denote O
the O
subject O
, O
predicate O
and O
object O
of O
the O
event O
respectively O
. O
The O
object O
of O
an O
event O
could O
be O
empty O
, O
and O
the O
total O
number O
of O
events O
per O
sentence O
m O
is O
not O
fixed O
. O
Open B-TaskName
EE I-TaskName
can O
also O
be O
aligned O
with O
traditional O
EE B-TaskName
task O
formulation O
by O
considering O
the O
predicate O
as O
the O
event O
trigger O
as O
well O
as O
a O
unique O
event O
type O
, O
while O
the O
subjects O
and O
objects O
both O
taken O
as O
event O
arguments O
. O

Based O
on O
the O
task O
formulation O
, O
we O
first O
implement O
an O
unsupervised O
method O
using O
an O
existing O
toolkit O
. O
Then O
, O
we O
split O
the O
task O
into O
trigger O
extraction O
and O
argument O
extraction O
, O
and O
implement O
different O
supervised O
methods O
on O
them O
. O

Unsupervised B-MethodName
Method I-MethodName

Since O
the O
formulation O
of O
Open B-TaskName
EE I-TaskName
is O
similar O
to O
some O
traditional O
tasks O
such O
as O
dependency O
parsing O
( O
DP O
) O
and O
semantic O
role O
labeling O
( O
SRL O
) O
, O
we O
investigate O
the O
performance O
of O
existing O
triplet O
extraction O
methods O
on O
Open B-TaskName
EE I-TaskName
. O
Each O
title O
will O
be O
segmented O
and O
tokenized O
first O
, O
then O
the O
extraction O
is O
conducted O
as O
a O
token O
- O
wise O
sequence O
- O
labeling O
task O
. O
Each O
token O
will O
first O
be O
labeled O
by O
a O
SRL O
module O
on O
whether O
it O
belongs O
to O
a O
semantic O
role O
which O
appears O
in O
one O
of O
the O
S O
- O
P O
- O
O O
, O
S O
- O
P O
, O
P O
- O
O O
semantic O
tuples O
. O
If O
not O
, O
it O
will O
be O
relabeled O
by O
a O
DP O
module O
on O
whether O
it O
appears O
in O
a O
syntactical O
tuple O
of O
the O
above O
structures O
. O
The O
entire O
method O
is O
implemented O
using O
the O
LTP O
toolkit O
( O
Che O
et O
al O
. O
, O
2020 O
) O
. O

Trigger O
Extraction O

Since O
the O
number O
of O
triggers O
per O
sentence O
is O
neither O
fixed O
nor O
given O
as O
input O
, O
we O
adopt O
a O
token O
- O
level O
sequence O
tagging O
model O
to O
extract O
all O
event O
triggers O
in O
a O
given O
sentence O
based O
on O
the O
inductive O
bias O
that O
event O
triggers O
( O
i.e. O
, O
predicates O
) O
will O
not O
overlap O
with O
each O
other O
( O
see O
Section O
3.2 O
) O
. O
Sequence O
tagging O
model O
requires O
a O
set O
of O
tags O
where O
each O
tag O
, O
aligned O
with O
a O
token O
, O
represents O
a O
part O
of O
an O
event O
element O
( O
i.e. O
, O
a O
triplet O
element O
) O
or O
a O
non O
- O
event O
element O
. O
Then O
, O
the O
model O
learns O
the O
probability O
distributions O
of O
tags O
for O
each O
given O
sentence O
, O
and O
outputs O
triplets O
based O
on O
the O
predicted O
tags O
. O
We O
adopt O
the O
BIO O
tagging O
scheme O
where O
a O
token O
is O
tagged O
B O
- O
trg O
i O
( O
I O
- O
trg O
i O
) O
if O
it O
is O
at O
the O
beginning O
of O
( O
inside O
) O
the O
i O
th O
trigger O
, O
or O
O O
if O
it O
is O
outside O
any O
trigger O
. O
The O
subscript O
is O
used O
to O
distinguish O
between O
different O
triggers O
as O
they O
might O
be O
discontinuous O
tokens O
. O
Since O
Title2Event B-DatasetName
is O
not O
annotated O
on O
tokenlevel O
( O
see O
3.2 O
) O
, O
we O
perform O
automatic O
tagging O
by O
locating O
each O
annotated O
event O
element O
at O
the O
source O
sentence O
to O
get O
its O
offset O
. O
We O
use O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
the O
sentence O
encoder O
to O
get O
the O
contextualized O
representations O
of O
tokens O
, O
and O
each O
token O
representation O
will O
be O
fed O
to O
a O
classification O
layer O
to O
compute O
the O
probability O
distribution O
of O
the O
tags O
. O

Argument O
Extraction O

Argument O
extraction O
models O
take O
the O
source O
sentence O
and O
the O
given O
triggers O
as O
input O
and O
output O
the O
arguments O
of O
each O
given O
trigger O
respectively O
. O
Due O
to O
the O
role O
overlap O
problem O
, O
a O
token O
might O
appear O
in O
multiple O
event O
arguments O
and O
thus O
has O
multiple O
tags O
, O
which O
does O
not O
match O
the O
common O
setting O
of O
sequence O
tagging O
task O
. O
Therefore O
, O
we O
iterate O
over O
the O
extracted O
triggers O
and O
extract O
the O
arguments O
of O
each O
event O
trigger O
separately O
. O
We O
implement O
three O
methods O
for O
argument O
extraction O
. O

Sequence B-MethodName
Tagging I-MethodName
. O
The O
first O
method O
is O
a O
tokenlevel O
sequence O
tagging O
model O
similar O
to O
the O
trigger O
extraction O
model O
, O
which O
also O
uses O
BIO O
tagging O
scheme O
for O
subject O
and O
object O
tokens O
. O
During O
each O
forward O
process O
, O
to O
specify O
the O
current O
trigger O
, O
we O
adopt O
the O
method O
proposed O
by O
Yang O
et O
al O
. O
( O
2019 O
) O
. O
Specifically O
, O
the O
input O
of O
BERT B-MethodName
encoder O
is O
the O
sum O
of O
WordPiece O
embeddings O
, O
position O
embeddings O
and O
segment O
embeddings O
, O
and O
we O
set O
the O
segment O
ids O
of O
current O
trigger O
tokens O
being O
one O
while O
others O
being O
zero O
to O
explicitly O
encode O
the O
current O
trigger O
. O
Span B-MethodName
MRC I-MethodName
. O
The O
second O
method O
is O
a O
span O
- O
level O
tagging O
model O
which O
formulates O
argument O
extraction O
as O
a O
machine O
reading O
comprehension O
( O
MRC O
) O
task O
, O
inspired O
from O
Du O
and O
Cardie O
( O
2020 O
) O
and O
. O
For O
each O
given O
sentence O
as O
well O
as O
a O
specified O
trigger O
, O
the O
subject O
and O
object O
are O
extracted O
separately O
by O
prepending O
a O
question O
, O
e.g. O
" O
动作 O
< O
trigger O
> O
的主体是 O
？ O
" O
( O
What O
is O
the O
subject O
of O
< O
trigger O
> O
) O
? O
, O
into O
the O
sentence O
to O
form O
a O
context O
like O
" O
[ O
CLS O
] O
question O
[ O
SEP O
] O
sentence O
[ O
SEP O
] O
" O
, O
then O
the O
model O
is O
asked O
to O
extract O
the O
answer O
span O
from O
the O
context O
for O
the O
given O
question O
by O
predicting O
a O
start O
position O
and O
an O
end O
position O
. O
We O
also O
use O
BERT B-MethodName
as O
the O
context O
encoder O
. O

Seq2Seq B-MethodName
MRC I-MethodName
. O
The O
third O
method O
is O
a O
sequenceto B-MethodName
- I-MethodName
sequence I-MethodName
MRC I-MethodName
model O
with O
same O
the O
question O
design O
as O
Span B-MethodName
MRC I-MethodName
. O
However O
, O
instead O
of O
extracting O
the O
answer O
spans O
from O
the O
context O
, O
it O
directly O
generates O
a O
sequence O
of O
tokens O
as O
the O
output O
with O
the O
given O
context O
by O
maximizing O
the O
conditional O
probability O
P O
( O

Y O
| O
S O
) O
= O
m O
i=1 O
p O
( O
y O
i O
| O
y O
1 O
, O
y O
2 O
, O
. O
. O
. O
, O
y O
i−1 O
; O
S O
) O

, O
where O
Y O
= O
< O
y O
1 O
, O
. O
. O
. O
, O
y O
m O
> O
is O
the O
golden O
answer O
. O
We O
adopt O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2021 O
) O
, O
a O
multilingual O
text O
- O
to O
- O
text O
transformer O
model O
as O
the O
context O
encoder O
as O
well O
as O
the O
answer O
decoder O
. O

Experiments O

We O
conduct O
experiments O
on O
Title2Event B-DatasetName
with O
the O
methods O
described O
in O
Section O
5 O
and O
analyze O
their O
performance O
. O

Evaluation O
Metrics O

We O
adapt O
the O
evaluation O
metrics O
used O
in O
previous O
works O
on O
traditional O
EE B-TaskName
tasks O
( O
Li O
et O
al O
. O
, O
2021b O
) O
to O
Open B-TaskName
EE I-TaskName
. O
We O
first O
define O
the O
matching O
criteria O
: O
an O
event O
trigger O
or O
argument O
is O
correctly O
identified O
if O
it O
exactly O
matches O
the O
golden O
answer O
, O
and O
an O
event O
triplet O
is O
correctly O
identified O
only O
if O
all O
of O
its O
elements O
are O
correctly O
identified O
. O
We O
then O
compute O
the O
precision B-MetricName
( O
P B-MetricName
) O
, O
recall B-MetricName
( O
R B-MetricName
) O
, O
and O
F1 B-MetricName
- I-MetricName
score I-MetricName
( O
F1 B-MetricName
) O
for O
trigger O
extraction O
, O
argument O
extraction O
and O
triplet O
extraction O
respectively O
. O

Evaluation O
Model O

We O
summarize O
all O
the O
models O
we O
implement O
for O
experiments O
here O
: O
Unsuper B-MethodName
. O
The O
unsupervised O
triplet O
extraction O
method O
implemented O
by O
the O
LTP O
toolkit O
using O
the O
Chinese O
- O
ELECTRA O
- O
small O
( O
Cui O
et O
al O
. O
, O
2021 O
) O
model O
. O
SeqTag B-MethodName
. O
A O
pipeline O
tagging O
- O
based O
model O
consisting O
of O
a O
trigger O
extractor O
and O
an O
argument O
extractor O
, O
both O
are O
based O
on O
the O
token O
- O
level O
sequence B-MethodName
tagging I-MethodName
model O
using O
BERT B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
Chinese I-MethodName
as O
the O
encoder O
, O
and O
are O
trained O
separately O
. O
During O
inference O
, O
the O
argument O
extractor O
predicts O
the O
arguments O
based O
on O
the O
triggers O
predicted O
by O
the O
trigger O
extractor O
. O
ST B-MethodName
- I-MethodName
SpanMRC I-MethodName
. O
A O
pipeline O
model O
using O
a O
tokenlevel O
sequence O
tagging O
model O
as O
the O
trigger O
extractor O
, O
and O
a O
span B-MethodName
- I-MethodName
level I-MethodName
MRC I-MethodName
model O
as O
the O
argument O
extractor O
, O
both O
are O
based O
on O
BERT B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
Chinese I-MethodName
. O
ST B-MethodName
- I-MethodName
Seq2SeqMRC I-MethodName
. O
A O
pipeline O
model O
which O
replaces O
the O
argument O
extractor O
with O
a O
sequence B-MethodName
- I-MethodName
tosequence I-MethodName
MRC I-MethodName
model O
using O
mT5 B-MethodName
- I-MethodName
base I-MethodName
. O

Overall O
Experimental O
Results O

Table O
2 O
shows O
the O
results O
of O
all O
Open B-TaskName
EE I-TaskName
methods O
experimented O
on O
Title2Event B-DatasetName
. O
It O
can O
be O
observed O
that O
: O
1 O
) O
For O
trigger O
extraction O
, O
the O
sequence B-MethodName
tagging I-MethodName
model I-MethodName
significantly O
outperforms O
the O
unsupervised B-MethodName
model O
. O
2 O
) O
For O
argument O
extraction O
and O
triplet O
extraction O
, O
ST B-MethodName
- I-MethodName
Seq2SeqMRC I-MethodName
outperforms O
the O
other O
tagging O
- O
based O
models O
. O
A O
large O
part O
of O
the O
reason O
is O
that O
the O
unconventional O
writing O
styles O
of O
titles O
make O
it O
difficult O
to O
locate O
token O
- O
level O
tags O
or O
span O
offsets O
in O
the O
source O
text O
, O
while O
sequence O
- O
to O
- O
sequence O
models O
are O
free O
from O
these O
restrictions O
. O

Analysis O
on O
Error O
Propagation O

Table O
3 O
shows O
the O
results O
of O
argument O
extraction O
with O
predicted O
triggers O
and O
with O
golden O
triggers O
. O
All O
three O
models O
' O
performance O
improve O
by O
approximately O
20 B-MetricValue
% I-MetricValue
if O
provided O
with O
golden O
triggers O
, O
indicating O
the O
huge O
impact O
of O
correct O
triggers O
on O
argument O
extraction O
and O
the O
urgent O
need O
to O
alleviate O
the O
propagating O
error O
brought O
by O
pipeline O
architecture O
in O
future O
works O
. O

Analysis O
on O
Multiple O
Event B-TaskName
Extraction I-TaskName

Figure O
4 O
shows O
that O
containing O
multiple O
events O
per O
instance O
is O
an O
important O
feature O
of O
Title2Event B-DatasetName
, O
thus O
we O
further O
investigate O
the O
models O
' O
performance O
on O
multiple O
event B-TaskName
extraction I-TaskName
, O
as O
shown O
in O
Figure O
6 O
. O
We O
can O
see O
that O
as O
the O
number O
of O
events O
per O
instance O
increases O
, O
all O
models O
on O
trigger O
extraction O
, O
argument O
extraction O
, O
and O
triplet O
extraction O
show O
a O
decrease O
in O
performance O
, O
which O
indicates O
that O
multiple O
events O
per O
instance O
brings O
additional O
challenges O
to O
open O
event O
extraction O
. O

Analysis O
on O
Different O
Topics O

We O
also O
investigate O
the O
results O
of O
trigger O
extraction O
and O
argument O
extraction O
on O
different O
topics O
of O
Ti B-DatasetName
- I-DatasetName
tle2Event I-DatasetName
, O
see O
Appendix O
A O
for O
details O
. O
It O
can O
be O
observed O
that O
the O
F1 B-MetricName
- I-MetricName
scores I-MetricName
of O
" O
Weather O
" O
are O
higher O
than O
other O
topics O
, O
probably O
because O
news O
titles O
on O
weather O
( O
forecast O
) O
usually O
have O
a O
fixed O
template O
which O
makes O
extraction O
easier O
. O

Analysis O
on O
Error O
Cases O

We O
summarize O
three O
typical O
challenges O
observed O
in O
Title2Event B-DatasetName
in O
Section O
1 O
. O
Here O
, O
we O
analyze O
some O
error O
cases O
of O
the O
model O
outputs O
to O
further O
demonstrate O
the O
issues O
. O
Figure O
7 O
( O
a O
) O
shows O
an O
error O
output O
in O
trigger O
extraction O
, O
where O
the O
given O
title O
is O
unconventionally O
written O
by O
concatenating O
two O
predicates O
. O
As O
a O
result O
, O
SegTag B-MethodName
is O
unable O
to O
distinguish O
the O
two O
different O
predicates O
. O
Figure O
7 O
( O
b O
) O
shows O
an O
instance O
with O
multiple O
events O
and O
all O
the O
models O
mix O
up O
the O
argument O
roles O
. O
Figure O
7 O
( O
c O
) O
shows O
a O
sport O
news O
title O
, O
without O
the O
background O
that O
Real O
Madrid O
and O
PSG O
are O
both O
football O
clubs O
, O
none O
of O
the O
models O
properly O
understand O
the O
event O
that O
PSG O
is O
defeated O
by O
Real O
Madrid O
. O
All O
of O
the O
above O
cases O
clearly O
address O
the O
challenges O
present O
in O
Ti B-DatasetName
- I-DatasetName
tle2Event I-DatasetName
, O
which O
are O
also O
common O
in O
real O
- O
world O
scenarios O
, O
and O
require O
advanced O
study O
to O
be O
better O
solved O
. O

Conclusions O

In O
this O
paper O
, O
we O
present O
Title2Event B-DatasetName
, O
a O
Chinese O
title O
dataset O
benchmarking O
the O
task O
of O
open B-TaskName
event I-TaskName
extraction I-TaskName
. O
To O
the O
best O
of O
our O
knowledge O
, O
Ti B-DatasetName
- I-DatasetName
tle2Event I-DatasetName
is O
the O
largest O
manually O
- O
annotated O
Chinese O
dataset O
for O
sentence O
- O
level O
event O
extraction O
. O
We O
experiment O
with O
different O
methods O
and O
conduct O
detailed O
analysis O
to O
address O
the O
challenges O
observed O
in O
Title2Event B-DatasetName
, O
which O
are O
rather O
scarce O
in O
existing O
datasets O
yet O
common O
in O
real O
- O
world O
scenarios O
. O
We O
believe O
Title2Event B-DatasetName
could O
further O
facilitate O
advanced O
research O
in O
event O
extraction O
. O

Limitations O

We O
summarize O
the O
limitations O
of O
Title2Event B-DatasetName
as O
follows O
: O
Evaluation O
Metrics O
. O
We O
make O
Title2Event B-DatasetName
a O
benchmark O
for O
open B-TaskName
event I-TaskName
extraction I-TaskName
with O
a O
hope O
that O
it O
could O
evaluate O
the O
performance O
of O
domaingeneral O
EE B-TaskName
models O
. O
We O
adapt O
the O
formulation O
of O
Open O
IE O
and O
represent O
events O
in O
a O
universal O
triplet O
format O
while O
adopting O
traditional O
EE B-TaskName
metrics O
which O
is O
based O
on O
exact O
match O
. O
However O
, O
we O
observe O
that O
the O
narrative O
of O
events O
in O
Chinese O
titles O
are O
extremely O
diverse O
. O
To O
unify O
them O
into O
the O
triplet O
format O
without O
losing O
the O
core O
event O
information O
, O
we O
design O
detailed O
annotation O
guidelines O
which O
results O
in O
the O
fact O
that O
the O
a O
large O
amount O
of O
triplet O
elements O
are O
text O
spans O
instead O
of O
one O
or O
two O
tokens O
which O
is O
common O
in O
traditional O
EE B-TaskName
datasets O
such O
as O
ACE B-DatasetName
2005 I-DatasetName
. O
Therefore O
, O
using O
exact O
match O
in O
Title2Event B-DatasetName
might O
be O
too O
strict O
for O
model O
outputs O
which O
are O
just O
one O
or O
two O
tokens O
different O
from O
the O
golden O
text O
span O
. O
We O
leave O
the O
design O
of O
finegrained O
evaluation O
approaches O
to O
future O
work O
. O

Methods O
. O
Some O
characteristics O
of O
Title2Event B-DatasetName
such O
as O
unfixed O
number O
of O
events O
per O
instance O
and O
the O
role O
overlap O
problem O
bring O
difficulties O
to O
the O
model O
design O
. O
We O
adopt O
a O
pipeline O
architecture O
which O
suffers O
from O
the O
error O
propagation O
problem O
as O
discussed O
in O
Section O
6.4 O
. O
We O
also O
adapt O
some O
end O
- O
to O
- O
end O
models O
in O
traditional O
EE B-TaskName
such O
as O
TEXT2EVENT O
proposed O
by O
Lu O
et O
al O
. O
( O
2021 O
) O
to O
our O
Open B-TaskName
EE I-TaskName
benchmark O
, O
but O
find O
the O
performance O
is O
unexpectedly O
poor O
. O
We O
conduct O
preliminary O
analysis O
and O
find O
that O
the O
length O
of O
text O
span O
in O
triplets O
( O
as O
mentioned O
above O
) O
as O
well O
as O
the O
relatively O
complex O
linearized O
event O
structures O
( O
largely O
due O
to O
the O
multiple O
events O
per O
instance O
issue O
) O
are O
the O
potential O
factors O
of O
the O
limited O
performance O
. O
Therefore O
, O
we O
do O
not O
provide O
a O
good O
end O
- O
to O
- O
end O
model O
as O
baseline O
, O
which O
might O
make O
the O
model O
comparison O
in O
Section O
6 O
less O
comprehensive O
. O
However O
, O
we O
hope O
that O
future O
works O
could O
pay O
more O
attention O
to O
the O
design O
of O
text O
- O
to O
- O
structure O
models O
except O
from O
traditional O
tagging O
- O
based O
models O
. O

Ethics O
Statement O

As O
Title2Event B-DatasetName
is O
an O
Open B-TaskName
EE I-TaskName
dataset O
which O
broadly O
collects O
contents O
of O
various O
categories O
on O
the O
Internet O
, O
keeping O
the O
corpus O
without O
bias O
is O
extremely O
difficult O
. O
However O
, O
we O
put O
large O
efforts O
in O
cleaning O
the O
toxicity O
of O
data O
. O
First O
, O
all O
crawled O
web O
paged O
are O
automatically O
removed O
if O
they O
contain O
toxic O
contents O
using O
an O
existing O
system O
. O
During O
annotation O
, O
all O
instances O
will O
be O
dual O
checked O
by O
the O
human O
annotators O
and O
manually O
deleted O
if O
not O
passing O
the O
check O
. O
Moreover O
, O
in O
our O
annotation O
standard O
, O
we O
ask O
annotators O
to O
label O
only O
factual O
events O
while O
ignoring O
all O
subjective O
opinions O
, O
as O
we O
hope O
Title2Event B-DatasetName
could O
be O
factual O
and O
unbiased O
. O

A O
Appendix O
Annotation O
Tool O
. O
Figure O
8 O
shows O
a O
screenshot O
of O
our O
annotation O
web O
page O
. O
The O
raw O
title O
are O
given O
with O
auxiliary O
information O
, O
the O
annotators O
will O
first O
determine O
whether O
to O
abandon O
this O
case O
as O
well O
as O
is O
this O
case O
easy O
to O
annotate O
or O
not O
. O
Then O
, O
they O
will O
type O
all O
plausible O
events O
in O
the O
text O
boxes O
following O
our O
annotation O
guidelines O
. O
Results O
on O
Different O
Topics O
. O
Figure O
9 O
shows O
the O
F1 B-MetricName
- I-MetricName
scores I-MetricName
of O
trigger O
extraction O
( O
using O
SeqTag B-MethodName
model O
) O
and O
argument O
extraction O
with O
golden O
triggers O
( O
using O
SeqTag B-MethodName
, O
SpanMRC B-MethodName
, O
and O
Seq2SeqMRC B-MethodName
models O
) O
on O
the O
top-10 O
topics O
in O
Title2Event B-DatasetName
. O

Hyper O
- O
parameter O
Settings O
in O
Training O
. O
For O
all O
models O
, O
we O
use O
the O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
train O
them O
for O
30 B-HyperparameterValue
epochs B-HyperparameterName
on O
the O
training O
set O
of O
Title2Event B-DatasetName
. O
All O
models O
are O
trained O
on O
a O
single O
Tesla O
A100 O
GPU O
. O
We O
use O
the O
linear O
learning O
rate O
scheduler O
and O
AdamW O
as O
the O
optimizer O
. O
For O
models O
based O
on O
Bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
Chinese I-MethodName
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
5e-5 B-HyperparameterValue
; O
For O
models O
based O
on O
mT5 B-MethodName
- I-MethodName
base I-MethodName
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
1e-4 B-HyperparameterValue
. O
All O
supervised O
models O
are O
implemented O
using O
the O
Huggingface O
- O
transformers O
library O
. O

Summarizing O
Community O
- O
based O
Question O
- O
Answer O
Pairs O

Community O
- O
based O
Question O
Answering O
( O
CQA O
) O
, O
which O
allows O
users O
to O
acquire O
their O
desired O
information O
, O
has O
increasingly O
become O
an O
essential O
component O
of O
online O
services O
in O
various O
domains O
such O
as O
E O
- O
commerce O
, O
travel O
, O
and O
dining O
. O
However O
, O
an O
overwhelming O
number O
of O
CQA O
pairs O
makes O
it O
difficult O
for O
users O
without O
particular O
intent O
to O
find O
useful O
information O
spread O
over O
CQA O
pairs O
. O
To O
help O
users O
quickly O
digest O
the O
key O
information O
, O
we O
propose O
the O
novel O
CQA B-TaskName
summarization I-TaskName
task O
that O
aims O
to O
create O
a O
concise O
summary O
from O
CQA O
pairs O
. O
To O
this O
end O
, O
we O
first O
design O
a O
multi O
- O
stage O
data O
annotation O
process O
and O
create O
a O
benchmark O
dataset O
, O
CO B-DatasetName
- I-DatasetName
QASUM I-DatasetName
, O
based O
on O
the O
Amazon B-DatasetName
QA I-DatasetName
corpus O
. O
We O
then O
compare O
a O
collection O
of O
extractive O
and O
abstractive O
summarization O
methods O
and O
establish O
a O
strong O
baseline O
approach O
DedupLED B-MethodName
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O
Our O
experiment O
further O
confirms O
two O
key O
challenges O
, O
sentencetype O
transfer O
and O
deduplication O
removal O
, O
towards O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O
Our O
data O
and O
code O
are O
publicly O
available O
. O
1 O
* O
Work O
done O
while O
at O
Megagon O
Labs O
. O
1 O
https O
: O
/ O
/ O
github.com O
/ O
megagonlabs O
/ O
qa O
- O
summarization O
Q O
: O
Is O
this O
actually O
a O
rigid O
board O
or O
more O
of O
a O
floppy O
mat O
? O
A O
: O
It O
is O
rigid.the O
main O
board O
is O
rigid O
, O
the O
two O
sides O
are O
semi O
. O
A O
: O
The O
main O
area O
is O
very O
sturdy O
. O
Then O
there O
are O
two O
work O
area O
pads O
that O
are O
more O
flexible O
so O
when O
moving O
those O
I O
keep O
two O
hands O
on O
them O
. O
A O
: O
16 O
inches O
wide O
( O
there O
are O
two O
) O
.A O
: O
most O
certainly O
will O
. O
… O
… O
( O
omitted O
27 O
QAs O
) O
Summary O
: O
This O
puzzle O
board O
comes O
with O
a O
rigid O
main O
board O
. O
You O
can O
arrange O
pieces O
in O
the O
middle O
and O
on O
two O
side O
pieces O
, O
and O
then O
pick O
up O
those O
side O
pieces O
to O
place O
them O
atop O
the O
middle O
area O
before O
folding O
the O
wings O
in O
. O
The O
dimension O
of O
the O
puzzle O
space O
is O
32 O
" O
x21.75 O
" O
. O
The O
closed O
unit O
is O
almost O
the O
same O
size O
as O
the O
puzzle O
workspace O
( O
32 O
" O
x21.75 O
" O
) O
. O
There O
are O
two O
16 O
" O
wide O
side O
inserts O
. O
The O
mat O
holds O
most O
1000 O
pieces O
puzzles O
. O
It O
is O
too O
big O
to O
use O
on O
you O
lap O
and O
definitely O
needs O
a O
table O
. O
( O
a O
) O
. O
QAs O
for O
a O
puzzle O
board O
product O
( O
Input O
) O
( O
b O
) O
. O
Summary O
of O
QAs O
( O
Output O
) O
Q O
: O
what O
is O
the O
storage O
size O
when O
case O
is O
fully O
closed O
for O
storage O
? O
A O
: O
Closed O
size O
is O
32.25 O
x O
22.75 O
" O
. O
Q O
: O
What O
size O
is O
the O
closed O
unit O
? O
A O
: O
Closed O
is O
almost O
the O
same O
size O
as O
the O
puzzle O
workspace O
. O
32.25 O
x O
22 O
. O

Introduction O

Community O
- O
based O
Question O
Answering O
( O
CQA O
) O
enables O
users O
to O
post O
their O
questions O
and O
obtain O
answers O
from O
other O
users O
. O
With O
the O
increase O
in O
online O
services O
, O
CQA O
has O
become O
essential O
for O
various O
purposes O
, O
including O
online O
shopping O
, O
hotel O
/ O
restaurant O
booking O
, O
and O
job O
searching O
. O
Many O
online O
platforms O
implement O
CQA O
features O
to O
help O
users O
acquire O
additional O
information O
about O
entities O
( O
e.g. O
, O
products O
, O
hotels O
, O
restaurants O
, O
and O
companies O
) O
of O
their O
interests O
. O
CQA O
complements O
customer O
reviewsanother O
type O
of O
user O
- O
generated O
content O
, O
which O
provide O
additional O
information O
about O
the O
entity O
but O
mostly O
focusing O
on O
user O
experiences O
and O
their O
opinions O
. O
While O
CQA O
greatly O
benefits O
users O
in O
decisionmaking O
, O
digesting O
information O
from O
original O
question O
and O
answer O
pairs O
( O
QA O
pairs O
2 O
) O
also O
has O
become O
increasingly O
harder O
. O
Due O
to O
the O
community O
- O
based O
nature O
, O
CQA O
tends O
to O
have O
a O
large O
number O
of O
heavily O
repetitive O
QA O
pairs O
, O
which O
make O
it O
difficult O
for O
users O
, O
especially O
those O
who O
do O
not O
have O
specific O
intent O
( O
i.e. O
, O
questions O
) O
, O
to O
find O
and O
digest O
key O
information O
. O

Existing O
summarization O
efforts O
for O
CQA O
( O
Liu O
et O
al O
. O
, O
2008 O
; O
Deng O
et O
al O
. O
, O
2020a O
; O
Fabbri O
et O
al O
. O
, O
2021b O
) O
primarily O
focus O
on O
summarizing O
answers O
for O
a O
given O
question O
, O
which O
still O
assumes O
that O
the O
user O
has O
a O
certain O
intent O
. O
We O
believe O
that O
information O
spread O
over O
QA O
pairs O
can O
be O
summarized O
into O
a O
more O
concise O
text O
, O
which O
helps O
any O
users O
grasp O
the O
key O
points O
of O
discussions O
about O
a O
target O
entity O
. O
Therefore O
, O
we O
take O
a O
step O
beyond O
the O
scope O
of O
answer O
summarization O
and O
propose O
a O
novel O
task O
of O
CQA B-TaskName
summarization I-TaskName
, O
which O
aims O
to O
summarize O
a O
collection O
of O
QA O
pairs O
about O
a O
single O
entity O
into O
a O
concise O
summary O
in O
declarative O
sentences O
( O
shown O
in O
Figure O
1 O
) O
. O

The O
CQA B-TaskName
summarization I-TaskName
task O
has O
the O
following O
two O
unique O
challenges O
. O
First O
, O
CQA B-TaskName
summarization I-TaskName
needs O
to O
solve O
sentence O
- O
type O
transfer O
as O
questions O
in O
interrogative O
sentences O
have O
to O
be O
converted O
into O
declarative O
sentences O
to O
make O
a O
concise O
summary O
. O
This O
challenge O
is O
not O
trivial O
as O
existing O
summarization O
tasks O
assume O
that O
input O
and O
output O
are O
both O
written O
in O
declarative O
sentences O
. O
Second O
, O
CQA O
contains O
duplicated O
questions O
and O
answers O
. O
That O
is O
, O
different O
users O
can O
post O
similar O
questions O
. O
A O
question O
can O
have O
multiple O
answers O
, O
many O
of O
which O
contain O
duplicate O
information O
. O
Also O
, O
unlike O
questionanswering O
forums O
( O
e.g. O
, O
Quora O
) O
, O
CQA O
in O
online O
services O
is O
less O
incentivized O
to O
remove O
such O
redundancy O
. O
Slightly O
different O
questions O
/ O
answers O
can O
provide O
detailed O
and O
useful O
information O
not O
mentioned O
in O
other O
questions O
/ O
answers O
. O
Having O
more O
similar O
answers O
supports O
the O
information O
is O
more O
reliable O
. O
Those O
properties O
make O
existing O
summarization O
solutions O
unsuitable O
for O
CQA B-TaskName
summarization I-TaskName
. O

To O
enable O
further O
study O
of O
the O
CQA B-TaskName
summarization I-TaskName
task O
, O
we O
create O
a O
corpus O
COQASUM B-DatasetName
by O
collecting O
reference O
summaries O
on O
QA O
pairs O
from O
the O
Amazon B-DatasetName
QA I-DatasetName
dataset O
( O
Wan O
and O
McAuley O
, O
2016 O
; O
McAuley O
and O
Yang O
, O
2016 O
) O
. O
Reference O
summary O
annotation O
is O
challenging O
for O
CQA B-TaskName
summarization I-TaskName
, O
as O
a O
single O
entity O
( O
i.e. O
, O
a O
product O
for O
the O
dataset O
) O
can O
have O
so O
many O
questions O
and O
answers O
that O
the O
annotator O
can O
not O
write O
a O
summary O
directly O
from O
them O
. O
Furthermore O
, O
the O
sentence O
- O
type O
difference O
( O
i.e. O
, O
interrogative O
vs. O
declarative O
) O
obstructs O
summary O
writing O
. O
To O
make O
this O
annotation O
feasible O
, O
we O
designed O
a O
multi O
- O
stage O
annotation O
framework O
. O
Sampled O
seed O
QA O
pairs O
are O
given O
to O
the O
annotator O
to O
convert O
into O
declarative O
sentences O
, O
which O
are O
then O
rewritten O
into O
gold O
- O
standard O
summaries O
by O
expert O
writers O
. O
At O
the O
last O
step O
, O
we O
collected O
semantically O
similar O
QA O
pairs O
to O
to O
make O
the O
annotated O
corpus O
more O
realistic O
. O

We O
conduct O
a O
comprehensive O
experiment O
that O
compares O
a O
collection O
of O
extractive O
and O
abstractive O
summarization O
solutions O
and O
establish O
a O
strong O
baseline O
approach O
, O
DedupLED B-MethodName
, O
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O
Specifically O
, O
DedupLED B-MethodName
finetunes O
the O
entire O
LED B-MethodName
model O
for O
summary O
generation O
while O
additional O
classifier O
attached O
to O
the O
encoder O
is O
optimized O
to O
extract O
representative O
QA O
pairs O
. O
Leveraging O
the O
strengths O
of O
both O
abstractive O
and O
extractive O
summarization O
objectives O
, O
as O
well O
as O
the O
pre O
- O
trained O
language O
model O
checkpoints O
, O
Dedu B-MethodName
- I-MethodName
pLED I-MethodName
significantly O
outperforms O
the O
other O
alternative O
methods O
. O
Our O
experiment O
also O
confirms O
that O
DedupLED B-MethodName
is O
suitable O
for O
CQA B-TaskName
summarization I-TaskName
, O
as O
the O
model O
implements O
the O
functionality O
for O
both O
( O
1 O
) O
sentence O
- O
type O
transfer O
and O
( O
2 O
) O
duplicate O
removal O
. O

Our O
contributions O
of O
the O
paper O
are O
as O
follows O
: O

• O
We O
propose O
the O
novel O
task O
of O
CQA B-TaskName
summarization I-TaskName
, O
which O
takes O
QA O
pairs O
about O
a O
single O
entity O
as O
input O
and O
make O
a O
summary O
written O
in O
declarative O
sentences O
( O
Section O
2 O
) O
. O
• O
We O
designed O
a O
multi O
- O
stage O
annotation O
framework O
and O
collected O
reference O
summaries O
to O
build O
the O
first O
benchmark O
corpus O
for O
CQA B-TaskName
summarization I-TaskName
. O
The O
corpus O
is O
based O
on O
the O
Amazon B-DatasetName
QA I-DatasetName
corpus O
( O
Wan O
and O
McAuley O
, O
2016 O
) O
and O
consists O
of O
reference O
summaries O
for O
1 O
, O
440 O
entities O
with O
39 O
, O
485 O
QA O
pairs O
from O
17 O
product O
categories O
. O
( O
Section O
3 O
) O
. O
• O
We O
conduct O
comprehensive O
experiments O
on O
a O
collection O
of O
extractative O
and O
abstractive O
summarization O
methods O
and O
develop O
a O
strong O
baseline O
DedupLED B-MethodName
, O
which O
implements O
key O
characteristics O
of O
sentence O
- O
type O
transfer O
and O
duplication O
removal O
functions O
. O
( O
Section O
4 O
and O
Section O
5 O
) O
. O

Problem O
definition O

Let O
D O
denote O
a O
dataset O
of O
questions O
and O
answers O
on O
individual O
entities O
{ O
e O
1 O
, O
e O
2 O
, O
... O
, O
e O
|D| O
} O
( O
e.g. O
, O
products O
or O
hotels O
) O
. O
For O
every O
entity O
e O
, O
we O
define O
a O
set O
of O
question O
- O
answer O
pairs O
QA O
e O
= O
{ O
( O
q O
i O
, O
a O
i O
) O
} O
|QAe| O
i=1 O
, O
where O
the O
question O
q O
i O
and O
the O
answer O
a O
i O
are O
sequences O
of O
tokens O
q O
i O
= O
( O
w O
1 O
, O
... O
, O
w O
n O
) O
and O
a O
i O
= O
( O
a O
1 O
, O
... O
, O
a O
m O
) O
3 O
. O
Given O
a O
set O
of O
QA O
pairs O
for O
an O
entity O
e O
, O
the O
CQA B-TaskName
summarization I-TaskName
task O
is O
to O
generate O
a O
natural O
language O
summary O
S O
e O
from O
QA O
e O
. O

The O
COQASUM B-DatasetName
Corpus O

We O
first O
describe O
the O
multi O
- O
stage O
annotation O
framework O
to O
collect O
gold O
- O
standard O
reference O
summaries O
from O
input O
QA O
pairs O
and O
then O
describe O
our O
benchmark O
dataset O
COQASUM B-DatasetName
. O

A O
Multi O
- O
stage O
Annotation O
Framework O

Reading O
and O
summarizing O
a O
set O
of O
QA O
pairs O
is O
challenging O
and O
error O
- O
prone O
for O
three O
reasons O
: O
( O
1 O
) O
a O
large O
number O
of O
QA O
pairs O
, O
( O
2 O
) O
the O
heavy O
repetition O
and O
noise O
in O
both O
questions O
answers O
, O
and O
( O
3 O
) O
the O
difficulty O
of O
converting O
questions O
and O
answers O
into O
declarative O
summaries O
. O
Thus O
, O
it O
is O
infeasible O
to O
collect O
high O
- O
quality O
reference O
summaries O
by O
simply O
showing O
a O
set O
of O
QA O
pairs O
and O
asking O
annotators O
to O
write O
a O
summary O
. O
In O
this O
paper O
, O
we O
design O
a O
multistage O
annotation O
framework O
that O
first O
simplifies O
this O
complex O
annotation O
task O
into O
more O
straightforward O
annotation O
tasks O
and O
then O
enriches O
the O
collected O
annotations O
. O

Figure O
2 O
depicts O
the O
schematic O
procedure O
of O
the O
multi O
- O
stage O
annotation O
framework O
. O
For O
each O
entity O
and O
its O
corresponding O
QA O
pairs O
in O
the O
original O
corpus O
, O
we O
first O
select O
representative O
seed O
QA O
pairs O
and O
ask O
annotators O
to O
rewrite O
them O
into O
declarative O
sentences O
, O
which O
are O
then O
concatenated O
into O
a O
raw O
summary O
. O
Next O
, O
we O
ask O
highly O
- O
skilled O
annotators O
to O
polish O
the O
raw O
summary O
into O
a O
more O
fluent O
summary O
. O
In O
the O
last O
step O
, O
we O
enrich O
the O
seed O
QA O
pairs O
by O
selecting O
semantically O
similar O
QA O
pairs O
from O
the O
original O
corpus O
. O

Step O
1 O
: O
QA O
Pair O
Selection O
and O
Rewriting O

In O
this O
step O
, O
we O
use O
a O
drastic O
strategy O
to O
remove O
duplicate O
QA O
pairs O
and O
simplify O
the O
annotation O
task O
for O
human O
annotators O
. O
A O
natural O
way O
to O
deduplicate O
QA O
pairs O
is O
by O
manually O
comparing O
existing O
QA O
pairs O
' O
semantics O
and O
only O
keeping O
the O
unique O
ones O
. O
However O
, O
we O
found O
this O
approach O
less O
practical O
because O
asking O
human O
annotators O
to O
perform O
the O
comparison O
is O
extremely O
expensive O
. O
It O
is O
also O
hard O
to O
validate O
the O
quality O
because O
selecting O
a O
representative O
QA O
from O
a O
set O
of O
semantically O
similar O
ones O
is O
a O
subjective O
process O
. O

Thus O
, O
we O
use O
a O
heuristic O
- O
based O
strategy O
to O
select O
representative O
QA O
pairs O
from O
the O
original O
corpus O
. O
Specifically O
, O
we O
use O
the O
following O
two O
rules O
to O
filter O
out O
QA O
pairs O
that O
are O
not O
suitable O
for O
creating O
reference O
summaries O
: O
( O
1 O
) O
length B-HyperparameterName
rule O
: O
QA O
pairs O
with O
less O
than O
5 B-HyperparameterValue
or O
more O
than O
150 B-HyperparameterValue
tokens O
; O
( O
2 O
) O
pronoun O
rule O
: O
QA O
pairs O
that O
include O
first O
- O
person O
pronouns O
. O
We O
found O
that O
long O
questions O
/ O
answers O
tend O
to O
contain O
their O
background O
information O
( O
e.g. O
, O
personal O
stories O
) O
, O
which O
is O
irrelevant O
to O
the O
entity O
. O
First O
- O
person O
pronouns O
are O
also O
a O
strong O
indicator O
for O
such O
questions O
/ O
answers O
. O
After O
the O
filtering O
, O
we O
randomly O
sample O
k B-HyperparameterName
seed O
QA O
pairs O
from O
the O
remaining O
ones O
. O
In O
addition O
, O
to O
avoid O
redundancy O
, O
we O
only O
sample O
seed O
QA O
pairs O
of O
different O
questions O
. O
4 O
For O
each O
of O
the O
k B-HyperparameterName
seed O
QA O
pairs O
, O
we O
ask O
human O
annotators O
to O
rewrite O
them O
into O
declarative O
sentences O
. O
We O
recruited O
three O
crowd O
workers O
from O
Amazon O
Mechanical O
Turk O
5 O
to O
annotate O
every O
QA O
pair O
and O
chose O
the O
highest O
- O
quality O
annotation O
based O
on O
6 O
criteria O
: O
( O
1 O
) O
length O
of O
LCS O
against O
the O
original O
QA O
pair O
, O
( O
2 O
) O
use O
of O
yes O
/ O
no O
, O
( O
3 O
) O
use O
of O
interrogative O
determiner O
( O
e.g. O
, O
What O
) O
, O
( O
4 O
) O
use O
of O
first O
- O
person O
pronouns O
, O
( O
5 O
) O
use O
of O
the O
item O
name O
at O
the O
beginning O
, O
( O
6 O
) O
the O
ignorance O
of O
the O
question O
information O
. O
We O
also O
blocked O
MTurk O
workers O
with O
consistently O
low O
- O
quality O
annotations O
to O
ensure O
the O
quality O
of O
annotations O
. O

Step O
2 O
: O
Summary O
Writing O

We O
form O
a O
raw O
summary O
by O
concatenating O
annotations O
( O
i.e. O
, O
declarative O
sentences O
rewritten O
from O
QA O
pairs O
) O
obtained O
in O
the O
first O
step O
for O
each O
entity O
. O
The O
raw O
summaries O
are O
not O
necessarily O
fluent O
and O
coherent O
as O
different O
pieces O
are O
annotated O
independently O
. O
They O
may O
also O
contain O
redundant O
information O
. O
To O
address O
these O
issues O
, O
we O
use O
another O
annotation O
task O
to O
polish O
and O
write O
a O
summary O
from O
the O
raw O
summary O
. O
To O
ensure O
the O
quality O
, O
we O
hired O
highly O
- O
skilled O
writers O
from O
Upwork O
6 O
by O
conducting O
screening O
interviews O
for O
this O
annotation O
task O
. O
For O
each O
entity O
, O
we O
show O
annotators O
the O
raw O
summary O
and O
ask O
them O
to O
write O
a O
fluent O
and O
concise O
summary O
. O

Step O
3 O
: O
Enriching O
Input O
QA O
Pairs O
Recall O
that O
in O
Step O
1 O
, O
we O
select O
k B-HyperparameterName
seed O
QA O
pairs O
for O
each O
entity O
. O
The O
seed O
QA O
pairs O
are O
less O
redundant O
because O
of O
the O
filtering O
and O
sampling O
strategy O
. O
This O
does O
not O
reflect O
the O
real O
- O
world O
scenario O
, O
where O
similar O
questions O
are O
asked O
multiple O
times O
, O
and O
each O
question O
often O
contains O
several O
answers O
. O

To O
align O
the O
benchmark O
with O
more O
realistic O
settings O
, O
we O
enrich O
the O
input O
QA O
pairs O
in O
Step O
3 O
. O
In O
Sampled O
SeedQA O
① O
SeedQA O
Selection O
and O
Rewriting O
Q O
: O
how O
wide O
is O
each O
Side O
piece O
? O
" O
A O
: O
Each O
side O
piece O
is O
16 O
inches O
wide O
( O
there O
are O
two O
) O
. O

Q O
: O
Can O
this O
be O
used O
in O
your O
lap O
while O
seated O
in O
a O
chair O
or O
sofa O
? O
A O
: O
Its O
too O
big O
to O
use O
on O
your O
lap O
. O
Definitely O
needs O
a O
table O
. O
Q O
: O
will O
this O
mat O
hold O
1000 O
piece O
puzzle O
? O
A O
: O
most O
certainly O
will O
. O

There O
are O
two O
side O
pieces O
, O
each O
one O
is O
16 O
inches O
wide O
. O
This O
product O
is O
too O
big O
to O
use O
on O
your O
lap O
while O
seated O
in O
a O
chair O
or O
sofa O
. O
You O
need O
a O
table O
for O
it O
. O
This O
puzzle O
mat O
can O
hold O
1000 O
puzzles O
. O

Rewritten O
SeedQA O

( O
in O
declarative O
language O
) O
There O
are O
two O
side O
pieces O
, O
each O
one O
is O
16 O
inches O
wide O
. O
This O
product O
is O
too O
big O
to O
use O
on O
your O
lap O
while O
seated O
in O
a O
chair O
or O
sofa O
. O
You O
need O
a O
table O
for O
it O
. O
This O
puzzle O
mat O
can O
hold O
1000 O
puzzles O
. O
This O
puzzle O
mat O
can O
hold O
500 O
puzzles O
. O

② O
Summary B-TaskName
Writing I-TaskName

Raw O
summary O

( O
concatenated O
from O
rewritten O
QAs O
) O
particular O
, O
we O
add O
all O
answers O
to O
every O
question O
in O
the O
seed O
QA O
pairs O
. O
Besides O
, O
we O
retrieve O
questions O
that O
are O
semantically O
similar O
to O
seed O
questions O
and O
add O
all O
the O
answers O
to O
the O
input O
QA O
pairs O
. O
For O
semantic O
similarity O
calculation O
, O
we O
use O
BERT O
embeddings O
and O
word O
overlap O
to O
find O
the O
candidates O
, O
followed O
by O
an O
additional O
crowd O
- O
sourcing O
task O
using O
Appen O
7 O
for O
manual O
validation O
. O
The O
validation O
step O
ensures O
that O
our O
reference O
summaries O
can O
be O
created O
from O
the O
enriched O
input O
QA O
pairs O
. O

Dataset O
Statistics O

Using O
the O
multi O
- O
stage O
annotation O
framework O
, O
we O
created O
the O
COQASUM B-DatasetName
benchmark O
based O
on O
the O
Amazon B-DatasetName
QA I-DatasetName
dataset O
( O
Wan O
and O
McAuley O
, O
2016 O
; O
McAuley O
and O
Yang O
, O
2016 O
) O
Table O
1 O
shows O
the O
statistics O
of O
COQASUM B-DatasetName
. O
We O
7 O
https O
: O
/ O
/ O
appen.com O
/ O
confirm O
that O
the O
average O
word O
count O
of O
input O
QA O
pairs O
/ O
raw O
summaries O
/ O
reference O
summaries O
is O
consistent O
for O
different O
categories O
. O
The O
novel O
n O
- O
gram O
distributions O
also O
confirm O
that O
COQASUM B-DatasetName
offers O
a O
fairly O
abstractive O
summarization O
task O
. O
Some O
product O
categories O
such O
as O
" O
Office O
Products O
" O
and O
" O
Patio O
Lawn O
and O
Garden O
" O
have O
lower O
novel O
n O
- O
gram O
ratios O
, O
indicating O
that O
the O
task O
becomes O
relatively O
extractive O
. O
The O
word O
count O
difference O
between O
the O
raw O
summary O
and O
the O
reference O
summary O
supports O
the O
value O
and O
quality O
of O
the O
summary O
writing O
task O
in O

Step O
2 O
, O
indicating O
that O
the O
raw O
summary O
still O
contains O
some O
redundant O
information O
. O

Models O

To O
examine O
the O
feasibility O
and O
explore O
the O
challenges O
of O
CQA B-TaskName
summarization I-TaskName
, O
we O
tested O
several O
summarization O
solutions O
on O
COQASUM B-DatasetName
. O
The O
models O
are O
grouped O
into O
Extractive O
, O
Extractive O
- O
Abstractive O
and O
Abstractive O
methods O
. O

Extractive O

Extractive O
methods O
extract O
salient O
sentences O
from O
input O
QA O
pairs O
as O
the O
output O
summary O
. O
We O
consider O
unsupervised O
( O
LexRank B-MethodName
) O
and O
supervised O
( O
Bert- B-MethodName
SumExt I-MethodName
) O
models O
in O
addition O
to O
a O
simple O
rule B-MethodName
- I-MethodName
based I-MethodName
baseline I-MethodName
that O
filters O
the O
original O
seed O
input O
QA O
. O
We O
evaluate O
those O
methods O
to O
understand O
how O
well O
selecting O
sentences O
without O
sentence O
- O
type O
transfer O
performs O
on O
the O
task O
. O

SeedQAs B-MethodName
: O
This O
method O
concatenates O
the O
seed O
QA O
pairs O
used O
in O
the O
first O
annotation O
task O
of O
the O
multi O
- O
stage O
annotation O
framework O
. O
This O
is O
an O
oracle O
method O
as O
we O
can O
not O
tell O
which O
QA O
pairs O
were O
used O
as O
seed O
QA O
pairs O
for O
annotation O
. O
We O
use O
this O
method O
to O
verify O
the O
performance O
of O
simply O
extracting O
QA O
pairs O
. O

LexRank B-MethodName
( O
Erkan O
and O
Radev O
, O
2004 O
) O
: O
This O
is O
an O
unsupervised O
extractive O
method O
, O
which O
uses O
the O
similarity O
between O
words O
to O
build O
a O
sentence O
graph O
and O
compute O
the O
centrality O
of O
sentences O
for O
selecting O
top O
- O
ranked O
sentences O
as O
summary O
. O

BertSumExt B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
: O
Bert B-MethodName
- I-MethodName
SumExt I-MethodName
is O
a O
supervised O
model O
, O
which O
fine O
- O
tunes O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
to O
extract O
sentences O
by O
solving O
multiple O
sentence O
- O
level O
classifications O
. O

In O
our O
experiment O
, O
we O
use O
BertSumExt B-MethodName
to O
extract O
salient O
QA O
pairs O
from O
the O
input O
, O
where O
the O
goldstandard O
labels O
are O
acquired O
by O
greedily O
select O
QA O
pairs O
that O
maximize O
the O
ROUGE B-MetricName
scores O
to O
the O
goldstandard O
summary O
8 O
. O

Extractive O
- O
Abstractive O

While O
extractive O
methods O
can O
remove O
duplication O
from O
the O
input O
, O
they O
can O
not O
transfer O
interrogative O
8 O
https O
: O
/ O
/ O
github.com O
/ O
nlpyang O
/ O
BertSum O
sentences O
( O
i.e. O
, O
questions O
) O
into O
declarative O
sentences O
. O
To O
handle O
this O
better O
, O
we O
combine O
extractive O
and O
abstractive O
models O
to O
implement O
two O
- O
stage O
solutions O
. O
We O
also O
test O
an O
existing O
two O
- O
stage O
algorithm O
in O
addition O
to O
another O
summarization O
model O
that O
learns O
to O
extract O
and O
rewrite O
in O
an O
end O
- O
to O
- O
end O
manner O
. O

LexRank+LED B-MethodName
This O
method O
is O
a O
select O
- O
thenrewrite O
hybrid O
model O
. O
Using O
a O
sentence O
- O
type O
transfer O
model O
, O
the O
model O
rewrites O
each O
of O
the O
QA O
pairs O
extracted O
by O
LexRank B-MethodName
into O
declarative O
sentences O
, O
which O
are O
then O
concatenated O
as O
an O
output O
summary O
. O
For O
the O
sentence O
- O
type O
transfer O
model O
, O
we O
fine O
- O
tune O
the O
LED B-MethodName
model O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
on O
the O
seed O
QA O
pairs O
and O
their O
rewritten O
texts O
collected O
in O
Step O
1 O
of O
the O
multi O
- O
stage O
annotation O
pipeline O
( O
Section O
3.1 O
) O
. O

LED+LexRank B-MethodName
This O
method O
is O
a O
rewrite O
- O
thenselect O
hybrid O
model O
that O
swaps O
the O
steps O
of O
LexRank+LED B-MethodName
. O
It O
uses O
LexRank B-MethodName
to O
extract O
salient O
sentences O
from O
input O
QA O
pairs O
rewritten O
by O
the O
same O
sentence O
- O
type O
transfer O
model O
. O

Bert B-MethodName
- I-MethodName
SingPairMix I-MethodName
( O
Lebanoff O
et O
al O
. O
, O
2019 O
) O
Bert B-MethodName
- I-MethodName
SingPairMix I-MethodName
is O
a O
select O
- O
then O
- O
rewrite O
- O
style O
model O
that O
first O
selects O
salient O
sentences O
from O
the O
input O
and O
then O
summarizes O
the O
selected O
sentences O
into O
the O
summary O
. O
In O
our O
experiment O
, O
we O
use O
our O
goldstandard O
summaries O
to O
train O
both O
the O
content O
selection O
model O
and O
the O
abstractive O
summarizer O
. O
FastAbstractiveSum B-MethodName
( O
Chen O
and O
Bansal O
, O
2018 O
) O
FastAbstractiveSum B-MethodName
also O
implements O
select O
- O
thenrewrite O
summarization O
via O
reinforcement O
learning O
. O

The O
model O
learns O
to O
select O
representative O
sentences O
with O
the O
extractor O
and O
rewrite O
the O
selected O
sentences O
with O
the O
abstractor O
. O
We O
train O
a O
FastAbstractiveSum B-MethodName
model O
on O
the O
gold O
- O
standard O
summaries O
. O

Abstractive O

As O
the O
final O
group O
of O
models O
, O
we O
explore O
abstractive O
models O
that O
directly O
summarize O
input O
QA O
pairs O
. O
Specifically O
, O
we O
use O
LED B-MethodName
and O
its O
variants O
, O
which O
can O
take O
long O
- O
document O
as O
input O
. O
Our O
DedupLED B-MethodName
is O
a O
variant O
of O
LED B-MethodName
and O
falls O
into O
this O
group O
. O
HierLED B-MethodName
( O
Zhu O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
Hierarchical B-MethodName
LED I-MethodName
( O
HierLED B-MethodName
) O
is O
a O
variant O
of O
LED B-MethodName
, O
which O
has O
two O
encoders O
for O
token O
- O
level O
and O
QAlevel O
inputs O
to O
handle O
the O
structure O
of O
QA O
pairs O
better O
. O
We O
use O
the O
same O
architecture O
as O
Hierarchical O
T5 O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
, O
replacing O
T5 O
with O
LED B-MethodName
. O
We O
fine O
- O
tune O
the O
model O
in O
the O
same O
manner O
as O
LED B-MethodName
. O

DedupLED B-MethodName
While O
pre O
- O
trained O
encoder O
- O
decoder O
models O
, O
including O
LED B-MethodName
, O
are O
known O
to O
be O
powerful O
summarization O
solutions O
, O
they O
do O
not O
explicitly O
implement O
deduplication O
functionality O
. O
Inspired O
by O
BertSumExt B-MethodName
, O
we O
consider O
incorporating O
a O
classifier O
layer O
optimized O
to O
extract O
the O
original O
seed O
QA O
pairs O
into O
an O
LED B-MethodName
model O
and O
fine O
- O
tuning O
the O
LED B-MethodName
model O
via O
multi O
- O
task O
learning O
, O
which O
we O
refer O
to O
as O
DedupLED B-MethodName
. O
Figure O
3 O
depicts O
the O
model O
architecture O
. O
The O
classifier O
layer O
is O
trained O
to O
select O
the O
original O
seed O
QA O
pairs O
, O
so O
the O
shared O
encoder O
learns O
to O
detect O
duplicate O
information O
while O
the O
decoder O
is O
optimized O
to O
generate O
a O
summary O
. O
In O
the O
training O
time O
, O
DedupLED B-MethodName
uses O
the O
original O
seed O
QA O
pair O
information O
in O
addition O
to O
the O
gold O
- O
standard O
summaries O
in O
the O
training O
data O
. O
We O
would O
like O
to O
note O
that O
DedupLED B-MethodName
does O
not O
require O
any O
additional O
information O
other O
than O
input O
QA O
pairs O
in O
the O
summary O
generation O
phase O
. O

Evaluation O

We O
conduct O
comparative O
experiments O
to O
evaluate O
those O
models O
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
on O
the O
COQASUM B-DatasetName
dataset O
. O
We O
randomly O
split O
the O
data O
into O
train B-HyperparameterName
/ O
validation B-HyperparameterName
/ O
test B-HyperparameterName
sets O
, O
which O
consist O
of O
1152 B-HyperparameterValue
/ O
144 B-HyperparameterValue
/ O
144 B-HyperparameterValue
entities O
, O
respectively O
. O
For O
LexRank B-MethodName
, O
we O
limit O
the O
output O
length O
based O
on O
the O
average B-HyperparameterName
reference I-HyperparameterName
summary I-HyperparameterName
length I-HyperparameterName
in O
the O
training O
set O
. O
For O
LED B-MethodName
and O
its O
variations O
, O
we O
fine O
- O
tuned O
the O
allenai O
/ O
led O
- O
base-16384 O
checkpoint O
using O
the O
Hugging O
Face O
Transformers O
library O
. O
9 O
We O
report O
the O
performance O
of O
the O
best O
epoch B-HyperparameterName
( O
based O
on O
ROUGE-1 B-MetricName
F1 I-MetricName
) O
chosen O
on O
the O
validation O
set O
for O
all O
the O
supervised O
models O
. O

Automatic O
Evaluation O

For O
automatic O
evaluation O
, O
we O
use O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
F1 B-MetricName
10 O
and O
BERTScore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
F1 B-MetricName
11 O
with O
the O
default O
configuration O
. O
The O
performance O
and O
required O
supervision O
of O
all O
models O
described O
in O
Section O
4 O
are O
shown O
in O
Table O
2 O
. O

Extractive O
: O
SeedQAs B-MethodName
, O
which O
simply O
selects O
the O
original O
QA O
pairs O
, O
performs O
badly O
. O
This O
is O
expected O
because O
while O
with O
high O
recall B-MetricName
( O
88.45 B-MetricValue
R1 B-MetricName
- I-MetricName
recall I-MetricName
) O
, O
the O
Oracle O
method O
suffers O
badly O
from O
low O
precision O
, O
largely O
due O
to O
the O
sentence O
- O
type O
inconsistency O
( O
i.e. O
, O
interrogative O
vs. O
declarative O
) O
and O
duplication O
in O
input O
QA O
pairs O
. O
LexRank B-MethodName
, O
the O
unsupervised O
summarization O
baseline O
, O
performs O
slightly O
better O
than O
SeedQAs B-MethodName
thanks O
to O
its O
ability O
to O
select O
more O
concise O
QAs O
for O
the O
output O
summary O
. O
BertSumExt B-MethodName
, O
while O
leveraging O
gold O
- O
standard O
summaries O
, O
achieves O
similar O
performance O
with O
LexRank B-MethodName
. O
We O
believe O
the O
discrepancy O
between O
interrogative O
and O
declarative O
sentences O
in O
input O
QA O
pairs O
and O
gold O
- O
standard O
summaries O
is O
the O
primary O
cause O
of O
the O
performance O
. O

Extractive O
- O
Abstractive O
: O
Extractive O
- O
abstractive O
models O
achieve O
better O
performance O
than O
extractive O
models O
. O
The O
sentence O
- O
type O
transfer O
helps O
LexRank+LED B-MethodName
/ O
LED+LexRank B-MethodName
achieve O
a O
much O
higher O
R1 B-MetricName
score O
while O
comparative O
R2 B-MetricName
/ O
RL B-MetricName
/ O
BS B-MetricName
scores O
against O
the O
original O
LexRank B-MethodName
. O
This O
implies O
the O
limitation O
of O
sentence O
selection O
before O
/ O
after O
sentence O
- O
type O
transfer O
. O
Also O
, O
the O
sentence O
- O
type O
transfer O
model O
was O
trained O
on O
seed O
QA O
pairs O
and O
their O
corresponding O
declarative O
sentences O
, O
not O
the O
gold O
- O
standard O
summaries O
. O
Thus O
, O
another O
factor O
may O
be O
the O
difference O
between O
the O
rewritten O
QA O
pairs O
and O
the O
gold O
- O
standard O
summaries O
. O
Both O
FastAbstractiveSum B-MethodName
and O
BERT B-MethodName
- I-MethodName
SingPairMix I-MethodName
, O
which O
are O
directly O
supervised O
by O
the O
gold O
- O
standard O
summaries O
, O
show O
significantly O
better O
performance O
than O
the O
extractive O
models O
. O
The O
results O
confirm O
that O
those O
models O
can O
learn O
to O
perform O
both O
sentence O
- O
style O
transfer O
and O
duplication O
removal O
directly O
from O
gold O
- O
standard O
summaries O
. O

Abstractive O
: O
All O
three O
models O
achieve O
strong O
performance O
on O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O
The O
vanilla O
LED B-MethodName
outperforms O
extractive O
/ O
extractiveabstractive O
models O
. O
By O
incorporating O
the O
hierarchical O
structure O
into O
the O
model O
, O
HierLED B-MethodName
improves O
the O
performance O
against O
the O
vanilla O
LED B-MethodName
. O
Furthermore O
, O
DedupLED B-MethodName
achieves O
the O
best O
performance O
for O
all O
the O
evaluation O
metrics O
. O
This O
confirms O
that O
by O
adding O
an O
auxiliary O
objective O
and O
using O
another O
supervision O
( O
i.e. O
, O
seed O
QA O
pair O
selection O
) O
, O
DedupLED B-MethodName
appropriately O
learns O
to O
deduplicate O
while O
learning O
to O
summarize O
input O
QA O
pairs O
. O
Takeaway O
: O
From O
the O
results O
, O
we O
confirm O
that O
both O
sentence O
- O
style O
transfer O
and O
duplication O
removal O
are O
crucial O
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O
In O
addition O
, O
fine O
- O
tuning O
pre O
- O
trained O
language O
models O
using O
the O
gold O
- O
standard O
summaries O
offers O
strong O
performance O
, O
better O
than O
manually O
- O
crafted O
twostage O
summarization O
models O
. O
Finally O
, O
by O
incorporating O
the O
duplication O
removal O
functionality O
into O
the O
model O
via O
multi O
- O
task O
learning O
, O
we O
show O
that O
DedupLED B-MethodName
establishes O
a O
strong O
baseline O
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O

Human O
evaluation O

We O
further O
conducted O
human O
evaluation O
to O
judge O
the O
quality O
of O
generated O
summaries O
by O
different O
models O
. O
For O
every O
entity O
in O
the O
test O
set O
, O
we O
showed O
summaries O
generated O
by O
four O
models O
( O
LexRank B-MethodName
, O
FastAbstractiveSum B-MethodName
, O
BERT B-MethodName
- I-MethodName
SinglePairMix I-MethodName
, O
and O
DedupLED B-MethodName
) O
to O
three O
human O
judges O
12 O
to O
choose O
the O
best O
and O
worst O
summaries O
for O
three O
criteria O
: O
informativeness B-MetricName
( O
Inf B-MetricName
. O
) O
, O
coherence B-MetricName
( O
Coh B-MetricName
. O
) O
, O
and O
conciseness B-MetricName
( O
Con B-MetricName
. O
) O
. O
Then O
, O
we O
computed O
the O
performance O
of O
the O
models O
using O
the O
Best O
- O
Worst O
Scaling O
( O
Louviere O
et O
al O
. O
, O
2015 O
) O
. O
Table O
3 O
shows O
that O
DedupLED B-MethodName
consistently O
achieves O
the O
best O
performance O
in O
all O
three O
criteria O
. O
On O
the O
other O
hand O
, O
LexRank B-MethodName
, O
as O
expected O
, O
performs O
the O
worst O
among O
all O
the O
methods O
we O
tested O
. O
The O
human O
evaluation O
performance O
trend O
aligns O
with O
the O
automatic O
evaluation O
performance O
, O
validating O
the O
quality O
of O
COQASUM B-DatasetName
as O
a O
Inf B-MetricName
. O

Coh B-MetricName
. O

Con B-MetricName
. O
benchmark O
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O

6 O
Analysis O

Choice O
of O
Pre O
- O
trained O
Language O
Models O

To O
justify O
our O
observation O
that O
pre O
- O
trained O
language O
models O
have O
strong O
abilities O
we O
test O
and O
compare O
three O
additional O
pre O
- O
trained O
language O
models O
on O
COQASUM B-DatasetName
: O
PEGASUS B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O
We O
confirm O
that O
all O
models O
perform O
better O
than O
the O
extractive O
and O
extractive O
- O
abstractive O
models O
. O
While O
PEGASUS B-MethodName
and O
T5 B-MethodName
show O
similar O
( O
24.81 B-MetricValue
and O
24.61 B-MetricValue
RL B-MetricName
, O
respectively O
) O
, O
they O
are O
less O
effective O
than O
BART B-MethodName
and O
LED B-MethodName
( O
26.89 B-MetricValue
and O
26.01 B-MetricValue
RL B-MetricName
, O
respectively O
) O
. O

Learning O
Curve O
Analysis O

Since O
collecting O
reference O
summaries O
is O
costly O
and O
time O
- O
consuming O
, O
we O
investigate O
the O
models O
' O
performance O
with O
limited O
training O
data O
. O
We O
tested O
the O
models O
' O
performance O
when O
trained O
with O
20 B-HyperparameterValue
% I-HyperparameterValue
, O
40 B-HyperparameterValue
% I-HyperparameterValue
, O
. O
. O
. O
, O
100 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
data O
. O
Figure O
4 O
shows O
the O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
F1 I-MetricName
scores O
of O
DedupLED B-MethodName
and O
FastAb B-MethodName
- I-MethodName
stractiveSum I-MethodName
when O
trained O
on O
different O
size B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
data I-HyperparameterName
. O
By O
leveraging O
a O
pre O
- O
trained O
checkpoint O
, O
DedupLED B-MethodName
performs O
consistently O
and O
substantially O
better O
than O
FastAbstractiveSum B-MethodName
, O
which O
is O
trained O
from O
scratch O
. O
DedupLED B-MethodName
also O
shows O
a O
faster O
learning O
curve O
and O
reaches O
the O
plateau O
in O
performance O
when O
trained O
with O
60 B-HyperparameterValue
% I-HyperparameterValue
and O
more O
data O
. O
This O
supports O
that O
the O
annotation O
size O
of O
COQASUM B-DatasetName
is O
sufficient O
for O
fine O
- O
tuning O
pre O
- O
trained O
language O
models O
, O
while O
it O
may O
be O
insufficient O
for O
non O
- O
pre O
- O
trained O
models O
. O
it O
on O
the O
five O
categories O
. O
For O
each O
category O
, O
we O
split B-HyperparameterName
entities O
into O
train O
/ O
dev O
/ O
test O
sets O
in O
0.8 B-HyperparameterValue
/ O
0.1 B-HyperparameterValue
/ O
0.1 B-HyperparameterValue
ratios B-HyperparameterName
. O

Related O
Work O

Opinion O
summarization O
( O
Amplayo O
et O
al O
. O
, O
2022 O
) O
aims O
to O
create O
a O
summary O
from O
multiple O
customer O
reviews O
. O
While O
opinion O
summarization O
is O
relevant O
to O
CQA B-TaskName
summarization I-TaskName
as O
it O
summarizes O
consumergenerated O
text O
, O
customer O
reviews O
are O
significantly O
different O
from O
QA O
pairs O
in O
CQA O
as O
they O
are O
selfcontained O
and O
tend O
to O
contain O
more O
subjective O
information O
. O
Recent O
opinion O
summarization O
models O
have O
adopted O
pre O
- O
trained O
LMs O
( O
LED B-MethodName
) O
for O
summarizing O
multiple O
reviews O
( O
Oved O
and O
Levy O
, O
2021 O
; O
Iso O
et O
al O
. O
, O
2022 O
) O
. O

A O
line O
of O
work O
studies O
on O
summarizing O
answers O
in O
CQA O
, O
which O
can O
be O
categorized O
into O
extractive O
models O
( O
Liu O
et O
al O
. O
, O
2008 O
; O
Chan O
et O
al O
. O
, O
2012 O
; O
Deng O
et O
al O
. O
, O
2020a O
, O
b O
) O
and O
abstractive O
models O
( O
Fabbri O
et O
al O
. O
, O
2021b O
; O
Chowdhury O
et O
al O
. O
, O
2021 O
) O
. O
Among O
them O
, O
Chowdhury O
and O
Chakraborty O
( O
2019 O
) O
created O
a O
benchmark O
by O
selecting O
the O
best O
answer O
as O
the O
reference O
summary O
, O
and O
Fabbri O
et O
al O
. O
( O
2021b O
) O
has O
collected O
professionally O
written O
reference O
summaries O
for O
answer O
summarization O
. O
Our O
CQA O
summarization O
differs O
from O
answer O
summarization O
as O
we O
con- O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
, O
customer O
support O
conversations O
( O
Feigenblat O
et O
al O
. O
, O
2021 O
) O
, O
conversations O
in O
multiple O
domains O
( O
Fabbri O
et O
al O
. O
, O
2021a O
) O
, O
and O
forum O
discussions O
( O
Khalman O
et O
al O
. O
, O
2021 O
) O
. O
CQA B-TaskName
summarization I-TaskName
is O
similar O
to O
those O
tasks O
in O
creating O
abstractive O
summaries O
from O
multiple O
turntaking O
conversations O
between O
more O
than O
one O
user O
. O
Meanwhile O
, O
we O
also O
found O
that O
CQA B-TaskName
summarization I-TaskName
tends O
to O
contain O
more O
duplication O
in O
the O
input O
by O
nature O
as O
the O
compression O
ratio O
( O
i.e. O
, O
input O
length O
/ O
summary O
length O
) O
of O
COQASUM B-DatasetName
is O
10.88 O
% O
, O
which O
is O
smaller O
than O
EmailSum O
( O
29.38 O
% O
) O
and O
Fo O
- O
rumSum O
( O
11.85 O
% O
) O
. O
We O
also O
tested O
HierLED B-MethodName
, O
a O
variant O
of O
the O
strongest O
baseline O
for O
E O
- O
mail O
thread O
summarization O
, O
and O
confirmed O
that O
DedupLED B-MethodName
performs O
better O
than O
HierLED B-MethodName
, O
indicating O
that O
CQA B-TaskName
summarization I-TaskName
offers O
unique O
challenges O
that O
are O
not O
in O
E O
- O
mail O
summarization O
. O

Conclusion O

We O
propose O
the O
CQA B-TaskName
summarization I-TaskName
task O
to O
summarize O
QA O
pairs O
in O
Community O
- O
based O
Question O
Answering O
. O
We O
develope O
a O
multi O
- O
stage O
annotation O
framework O
and O
created O
a O
benchmark O
COQASUM B-DatasetName
for O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O
Our O
multi O
- O
stage O
annotation O
framework O
decomposes O
a O
complex O
annotation O
task O
into O
three O
much O
simpler O
ones O
, O
thus O
allows O
higher O
annotation O
quality O
. O
We O
further O
compare O
a O
collection O
of O
extractive O
and O
abstractive O
summarization O
methods O
and O
establish O
a O
strong O
baseline O
method O
DedupLED B-MethodName
for O
the O
CQA B-TaskName
summarization I-TaskName
task I-TaskName
. O
Our O
experiment O
also O
confirms O
two O
key O
challenges O
, O
sentence O
- O
type O
transfer O
and O
duplication O
removal O
, O
towards O
the O
CQA B-TaskName
summarization I-TaskName
task O
. O

Limitations O

As O
we O
propose O
and O
tackle O
a O
challenging O
summarization O
task O
, O
the O
paper O
has O
certain O
limitations O
. O
First O
, O
our O
benchmark O
is O
in O
a O
single O
domain O
( O
E O
- O
commerce O
) O
in O
a O
single O
language O
( O
English O
) O
, O
which O
not O
necessarily O
ensuring O
the O
generalizability O
for O
other O
domains O
and O
languages O
. O
Second O
, O
the O
quality O
of O
our O
annotations O
relies O
on O
the O
initial O
selection O
of O
seed O
QA O
pairs O
. O
As O
we O
discussed O
in O
the O
paper O
, O
we O
filtered O
high O
- O
quality O
seed O
QA O
pairs O
to O
minimize O
the O
risk O
. O
Nevertheless O
, O
it O
may O
not O
accurately O
replicate O
the O
summarization O
procedure O
by O
experts O
. O
Third O
, O
we O
use O
rules O
and O
heuristics O
to O
ensure O
the O
quality O
of O
the O
freetext O
annotation O
. O
Despite O
being O
able O
to O
detect O
and O
eliminate O
a O
significant O
ratio O
of O
low O
- O
quality O
annotation O
, O
our O
rules O
and O
heuristics O
do O
not O
provide O
perfect O
guarantee O
, O
meaning O
that O
COQASUM B-DatasetName
may O
still O
contain O
noisy O
and O
low O
- O
quality O
annotations O
. O
With O
those O
limitations O
, O
we O
still O
believe O
that O
the O
paper O
and O
the O
benchmark O
are O
benefitial O
for O
the O
community O
to O
take O
a O
step O
beyond O
the O
scope O
of O
existing O
summarization O
tasks O
. O

Ethics O
Statement O

For O
the O
annotation O
tasks O
, O
we O
paid O
$ O
10 O
hourly O
wage O
for O
the O
crowd O
workers O
on O
MTurk O
and O
Appen O
( O
Steps O
1 O
and O
3 O
) O
and O
$ O
15 O
to O
$ O
30 O
hourly O
wage O
for O
the O
Upwork O
contractors O
( O
Step O
2 O
) O
, O
making O
sure O
to O
pay O
higher O
than O
the O
minimum O
wage O
in O
the O
U.S. O
( O
i.e. O
, O
$ O
7.25 O
per O
hour O
) O
. O
Our O
COQASUM B-DatasetName
is O
based O
on O
the O
publicly O
available O
Amazon B-DatasetName
QA I-DatasetName
dataset O
. O
To O
our O
knowledge O
, O
the O
dataset O
does O
not O
contain O
any O
harmful O
content O
. O

TAGPRIME B-MethodName
: O
A O
Unified O
Framework O
for O
Relational B-TaskName
Structure I-TaskName
Extraction I-TaskName

Many O
tasks O
in O
natural O
language O
processing O
require O
the O
extraction O
of O
relationship O
information O
for O
a O
given O
condition O
, O
such O
as O
event B-TaskName
argument I-TaskName
extraction I-TaskName
, O
relation B-TaskName
extraction I-TaskName
, O
and O
taskoriented B-TaskName
semantic I-TaskName
parsing I-TaskName
. O
Recent O
works O
usually O
propose O
sophisticated O
models O
for O
each O
task O
independently O
and O
pay O
less O
attention O
to O
the O
commonality O
of O
these O
tasks O
and O
to O
have O
a O
unified O
framework O
for O
all O
the O
tasks O
. O
In O
this O
work O
, O
we O
propose O
to O
take O
a O
unified O
view O
of O
all O
these O
tasks O
and O
introduce O
TAGPRIME B-MethodName
to O
address O
relational O
structure O
extraction O
problems O
. O
TAGPRIME B-MethodName
is O
a O
sequence O
tagging O
model O
that O
appends O
priming O
words O
about O
the O
information O
of O
the O
given O
condition O
( O
such O
as O
an O
event O
trigger O
) O
to O
the O
input O
text O
. O
With O
the O
self O
- O
attention O
mechanism O
in O
pre O
- O
trained O
language O
models O
, O
the O
priming O
words O
make O
the O
output O
contextualized O
representations O
contain O
more O
information O
about O
the O
given O
condition O
, O
and O
hence O
become O
more O
suitable O
for O
extracting O
specific O
relationships O
for O
the O
condition O
. O
Extensive O
experiments O
and O
analyses O
on O
three O
different O
tasks O
that O
cover O
ten O
datasets O
across O
five O
different O
languages O
demonstrate O
the O
generality O
and O
effectiveness O
of O
TAGPRIME B-MethodName
. O

Introduction O

There O
are O
many O
tasks O
in O
natural O
language O
processing O
( O
NLP O
) O
that O
require O
extracting O
relational O
structures O
from O
texts O
. O
For O
example O
, O
the O
event B-TaskName
argument I-TaskName
extraction I-TaskName
task O
aims O
to O
identify O
event O
arguments O
and O
their O
corresponding O
roles O
for O
a O
given O
event O
trigger O
( O
Huang O
et O
al O
. O
, O
2022 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
In O
entity B-TaskName
relation I-TaskName
extraction I-TaskName
, O
the O
model O
identifies O
the O
tail O
- O
entities O
and O
head O
- O
entities O
that O
forms O
specific O
relations O
Yu O
et O
al O
. O
, O
2020 O
) O
. O
In O
taskoriented B-TaskName
semantic I-TaskName
parsing I-TaskName
, O
the O
model O
predicts O
the O
slots O
and O
their O
semantic O
roles O
for O
a O
given O
intent O
in O
an O
* O
The O
authors O
contribute O
equally O
. O

utterance O
( O
Tür O
et O
al O
. O
, O
2010 O
; O
. O
These O
tasks O
are O
beneficial O
to O
a O
wide O
range O
of O
applications O
, O
such O
as O
dialog O
systems O
, O
question O
answering O
( O
Yasunaga O
et O
al O
. O
, O
2021 O
) O
, O
and O
narrative O
generation O
( O
Chen O
et O
al O
. O
, O
2019a O
) O
. O
Prior O
works O
usually O
design O
models O
to O
specifically O
address O
each O
of O
the O
tasks O
Miwa O
and O
Bansal O
, O
2016 O
; O
Fu O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
less O
attention O
is O
paid O
to O
the O
commonality O
among O
these O
tasks O
and O
having O
a O
unified O
framework O
to O
deal O
with O
them O
and O
provide O
a O
strong O
baseline O
for O
every O
task O
. O

In O
this O
work O
, O
we O
take O
a O
unified O
view O
of O
these O
NLP O
tasks O
. O
We O
call O
them O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
( O
RSE B-TaskName
) O
tasks O
and O
formulate O
them O
as O
a O
unified O
task O
that O
identifies O
arguments O
to O
a O
given O
condition O
and O
classifies O
their O
relationships O
. O
The O
condition O
could O
be O
a O
textual O
span O
, O
such O
as O
an O
event O
trigger O
for O
event B-TaskName
argument I-TaskName
extraction I-TaskName
, O
or O
a O
concept O
, O
such O
as O
an O
intent O
for O
task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
. O

We O
present O
TAGPRIME B-MethodName
, O
a O
simple O
, O
unified O
, O
and O
strong O
model O
, O
which O
follows O
a O
sequence O
tagging O
paradigm O
with O
a O
priming O
technique O
, O
which O
is O
proposed O
by O
Fincke O
et O
al O
. O
( O
2022 O
) O
. O
TAGPRIME B-MethodName
inherits O
the O
strength O
of O
sequence O
tagging O
models O
to O
unifiedly O
address O
RSE B-TaskName
by O
converting O
the O
relational O
structure O
into O
a O
sequence O
of O
predictions O
by O
sequentially O
labeling O
tokens O
in O
the O
input O
passage O
. O
TAGPRIME B-MethodName
further O
improves O
this O
framework O
's O
performance O
by O
better O
incorporating O
information O
about O
the O
given O
condition O
via O
priming O
. O
Traditional O
sequence O
tagging O
models O
usually O
leverage O
learnable O
feature O
embeddings O
to O
incorporate O
information O
about O
the O
given O
condition O
before O
the O
tags O
are O
assigned O
, O
as O
illustrated O
in O
Figure O
1 O
( O
a O
) O
. O
With O
the O
priming O
mechanism O
, O
TAGPRIME B-MethodName
augments O
the O
input O
text O
with O
condition O
- O
specific O
contexts O
, O
as O
illustrated O
in O
Figure O
1 O
A O
conventional O
sequence O
tagging O
model O
that O
embeds O
conditional O
information O
by O
adding O
learnable O
features O
to O
the O
output O
representation O
from O
a O
pre O
- O
trained O
language O
model O
. O
In O
the O
shown O
example O
, O
the O
conditional O
features O
contain O
two O
parts O
: O
one O
is O
the O
token O
embedding O
representing O
the O
conditional O
word O
" O
military O
" O
, O
and O
the O
other O
is O
an O
entity O
type O
embedding O
. O
( O
b O
) O
TAGPRIME B-MethodName
with O
condition O
priming O
: O
The O
conditional O
information O
is O
further O
applied O
to O
the O
input O
sequence O
to O
induce O
the O
output O
representation O
from O
the O
pre O
- O
trained O
language O
model O
to O
become O
condition O
- O
aware O
. O
( O
c O
) O
TAGPRIME B-MethodName
with O
condition O
& O
relationship O
priming O
: O
Our O
approach O
that O
further O
append O
the O
verbalized O
relationship O
to O
TAGPRIME B-MethodName
with O
condition O
priming O
model O
. O
For O
this O
case O
, O
the O
goal O
of O
the O
tagging O
model O
is O
to O
make O
predictions O
specific O
to O
the O
relationship O
type O
in O
the O
input O
. O
We O
omit O
CRF O
layers O
after O
MLP O
layers O
in O
this O
figure O
for O
better O
readability O
. O

priming O
technique O
comes O
from O
the O
nature O
of O
the O
self O
- O
attention O
mechanism O
in O
pre O
- O
trained O
language O
models O
. O
Augmenting O
input O
text O
with O
conditionspecific O
contexts O
makes O
the O
sentence O
representations O
condition O
- O
specific O
directly O
. O
Thus O
, O
it O
unlocks O
the O
capability O
of O
sequence O
tagging O
methods O
for O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
better O
than O
the O
commonly O
used O
feature O
embedding O
approach O
, O
as O
shown O
in O
Section O
5 O
. O

Our O
contributions O
can O
be O
summarized O
as O
follows O
. O

( O
1 O
) O
We O
take O
a O
unified O
view O
of O
NLP O
tasks O
that O
requires O
extracting O
relational O
structures O
, O
including O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
, O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
relation I-TaskName
extraction I-TaskName
, O
and O
task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
. O
Then O
, O
we O
present O
TAGPRIME B-MethodName
, O
a O
unified O
sequence O
tagging O
model O
with O
priming O
that O
can O
serve O
as O
a O
strong O
baseline O
to O
various O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
problems O
. O
( O
2 O
) O
Thorough O
experiments O
on O
three O
different O
tasks O
show O
that O
TAGPRIME B-MethodName
achieves O
competitive O
performance O
than O
the O
current O
state O
- O
of O
- O
the O
- O
art O
on O
ten O
datasets O
in O
five O
different O
languages O
. O
( O
3 O
) O
We O
propose O
a O
novel O
efficient O
approximation O
to O
speed O
up O
TAG B-MethodName
- I-MethodName
PRIME I-MethodName
during O
inference O
time O
without O
sacrificing O
too O
much O
performance O
. O

Our O
code O
will O
be O
publicly O
accessible O
at O
https O
: O
/ O
/ O
github.com O
/ O
PlusLabNLP O
/ O
TagPrime B-MethodName
. O

Related O
Work O

Many O
natural O
language O
processing O
applications O
require O
extracting O
relational O
structures O
, O
including O
event B-TaskName
extraction I-TaskName
, O
relation B-TaskName
extraction I-TaskName
, O
coreference O
resolution O
, O
etc O
. O
The O
prevalence O
of O
these O
applications O
makes O
us O
hard O
to O
exhaustively O
list O
them O
in O
this O
short O
summary O
, O
hence O
, O
we O
mainly O
focus O
on O
related O
works O
for O
the O
applications O
we O
experiment O
on O
. O

Event B-TaskName
extraction I-TaskName
. O
Early O
works O
in O
event O
extraction O
mostly O
consider O
a O
pipelined O
approach O
( O
Nguyen O
and O
Grishman O
, O
2015 O
; O
Wang O
et O
al O
. O
, O
2019 O
; O
to O
deal O
with O
event O
extraction O
. O
Some O
followup O
works O
argue O
that O
pipelined O
design O
leads O
to O
error O
propagation O
issues O
and O
hence O
propose O
end O
- O
to O
- O
end O
approaches O
to O
better O
capture O
dependencies O
between O
each O
prediction O
Li O
et O
al O
. O
, O
2013 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Hsu O
et O
al O
. O
, O
2022b O
; O
Lu O
et O
al O
. O
, O
2021 O
; O
Huang O
and O
Peng O
, O
2021 O
) O
. O
However O
, O
recently O
, O
some O
empirical O
studies O
( O
Hsu O
et O
al O
. O
, O
2022b O
; O
Zhong O
and O
Chen O
, O
2021 O
; O
Fincke O
et O
al O
. O
, O
2022 O
) O
also O
show O
that O
when O
an O
abundant O
amount O
of O
data O
is O
used O
to O
learn O
representations O
for O
each O
pipelined O
task O
, O
it O
is O
hard O
to O
conclude O
that O
joint O
learning O
approaches O
always O
provide O
a O
stronger O
result O
. O
This O
aligns O
with O
our O
discovery O
in O
experiments O
-even O
though O
we O
apply O
a O
pipelined O
approach O
with O
a O
simple O
sequence O
tagging O
framework O
on O
event B-TaskName
extraction I-TaskName
, O
with O
the O
help O
of O
priming O
to O
learn O
more O
condition O
- O
aware O
contextualized O
representation O
, O
we O
can O
still O
achieve O
very O
strong O
performance O
on O
multiple O
datasets O
. O

Relation B-TaskName
extraction I-TaskName
. O
End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
relation I-TaskName
extraction I-TaskName
can O
usually O
be O
solved O
using O
two O
categories O
of O
approaches O
. O
The O
first O
one O
is O
to O
directly O
perform O
joint O
inference O
on O
named O
entities O
and O
their O
relation O
( O
s O
) O
( O
Zheng O
et O
al O
. O
, O
2017 O
; O
Wang O
and O
Lu O
, O
2020 O
; O
Katiyar O
and O
Cardie O
, O
2017 O
; O
Miwa O
and O
Bansal O
, O
2016 O
; O
Fu O
et O
al O
. O
, O
2019 O
) O
. O
The O
second O
category O
is O
to O
perform O
a O
pipeline O
that O
first O
extracts O
named O
entities O
, O
and O
then O
performs O
relation O
classification O
( O
Wu O
and O
He O
, O
2019 O
; O
Hsu O
et O
al O
. O
, O
2022a O
; O
Lyu O
and O
Chen O
, O
2021 O
; O
Peng O
et O
al O
. O
, O
2020 O
; O
Zhou O
and O
Chen O
, O
2021a O
; O
Lu O
et O
al O
. O
, O
2022 O
) O
, O
which O
assumes O
that O
both O
the O
head O
- O
entity O
and O
tail O
- O
entity O
are O
given O
. O
Yet O
, O
in O
our O
unified O
formulation O
for O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
tasks O
, O
we O
extract O
tail O
- O
entities O
and O
their O
corresponding O
relation O
types O
for O
a O
given O
head O
- O
entity O
, O
which O
is O
more O
similar O
to O
a O
less O
frequently O
studied O
framework O
called O
cascading O
approaches O
Yu O
et O
al O
. O
, O
2020 O
) O
. O
Despite O
being O
a O
less O
popular O
formulation O
to O
deal O
with O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
relation I-TaskName
extraction I-TaskName
, O
TAGPRIME B-MethodName
presents O
a O
strong O
performance O
compared O
to O
prior O
studies O
, O
showcasing O
the O
practicality O
and O
effectiveness O
of O
our O
unified O
formulation O
. O
Task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
. O
Task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
, O
which O
focuses O
on O
intent O
classification O
and O
slot O
filling O
, O
has O
a O
long O
history O
of O
development O
( O
Tür O
et O
al O
. O
, O
2010 O
; O
Gupta O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2018 O
; O
Louvan O
and O
Magnini O
, O
2020 O
) O
. O
Recently O
, O
some O
more O
advanced O
neural O
network O
- O
based O
approaches O
have O
been O
proposed O
, O
such O
as O
MLP O
- O
mixer O
( O
Fusco O
et O
al O
. O
, O
2022 O
) O
or O
sequence O
- O
to O
- O
sequence O
formulation O
( O
Desai O
et O
al O
. O
, O
2021 O
) O
. O
Among O
them O
, O
JointBERT B-MethodName
( O
Chen O
et O
al O
. O
, O
2019b O
) O
, O
a O
sequence O
- O
tagging O
- O
based O
model O
that O
is O
trained O
to O
jointly O
predict O
intent O
and O
extract O
slots O
, O
serves O
as O
a O
widely O
- O
used O
baseline O
due O
to O
its O
simplicity O
. O
Our O
approach O
benefits O
from O
the O
same O
simplicity O
as O
JointBERT B-MethodName
and O
can O
further O
improve O
its O
performance O
. O

Method O

We O
first O
introduce O
our O
view O
to O
unify O
RSE B-TaskName
problems O
and O
then O
discuss O
how O
TAGPRIME B-MethodName
approaches O
this O
problem O
under O
a O
unified O
framework O
of O
sequence O
tagging O
model O
with O
priming O
. O
1 O
, O
r O
c O
2 O
, O
... O
, O
r O
c O
l O
] O
towards O
the O
condition O
c O
, O
where O
r O
c O
i O
∈ O
A O
and O
A O
is O
the O
set O
of O
all O
possible O
relationships O
or O
attributes O
. O
Many O
NLP O
tasks O
can O
be O
formulated O
as O
an O
RSE B-TaskName
task O
. O
We O
showcase O
how O
this O
formulation O
can O
be O
applied O
to O
event B-TaskName
extraction I-TaskName
, O
entity B-TaskName
relation I-TaskName
extraction I-TaskName
, O
and O
taskoriented B-TaskName
semantic I-TaskName
parsing I-TaskName
below O
. O

A O
Unified O
Formulation O
of O
RSE B-TaskName

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
. O
End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
aims O
to O
extract O
events O
from O
given O
texts O
( O
Ma O
et O
al O
. O
, O
2020 O
; O
Hsu O
et O
al O
. O
, O
2022b O
; O
) O
. O
An O
event O
contains O
a O
trigger O
, O
which O
is O
the O
textual O
span O
that O
best O
represents O
the O
occurrence O
of O
an O
event O
, O
and O
several O
arguments O
, O
which O
are O
the O
participants O
involved O
in O
the O
event O
with O
different O
argument O
roles O
. O
We O
consider O
a O
pipeline O
solutionafter O
the O
event O
triggers O
are O
identified O
, O
an O
argument O
extraction O
model O
extracts O
the O
event O
arguments O
and O
their O
corresponding O
roles O
for O
each O
given O
event O
trigger O
. O
Under O
the O
RSE B-TaskName
formulation O
, O
the O
condition O
c O
is O
the O
given O
event O
trigger O
, O
and O
the O
target O
spans O
s O
c O
and O
the O
relationships O
r O
c O
are O
the O
arguments O
and O
their O
argument O
roles O
, O
respectively O
. O

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
relation I-TaskName
extraction I-TaskName
. O
Relation O
extraction O
identifies O
entities O
and O
their O
relations O
from O
texts O
, O
and O
it O
is O
usually O
solved O
by O
pipeline O
approachesfirst O
extracting O
named O
entities O
and O
then O
predicting O
relations O
for O
each O
entity O
- O
pair O
( O
Wu O
and O
He O
, O
2019 O
; O
Zhong O
and O
Chen O
, O
2021 O
) O
. O
Under O
the O
new O
formulation O
, O
an O
RSE B-TaskName
model O
is O
used O
to O
predict O
tail O
- O
entities O
and O
the O
relations O
for O
each O
extracted O
named O
entity O
that O
serves O
as O
the O
head O
- O
entity O
. O
For O
example O
, O
in O
Figure O
1 O
( O
b O
) O
, O
we O
extract O
the O
tail O
- O
entities O
( O
" O
Iraqi O
" O
and O
" O
base O
" O
) O
and O
their O
relation O
( O
" O
Part O
- O
Whole O
" O
and O
" O
ART O
" O
) O
for O
the O
head O
- O
entity O
, O
" O
military O
" O
. O
In O
this O
way O
, O
each O
given O
head O
- O
entity O
is O
the O
condition O
c O
, O
and O
the O
extracted O
tail O
- O
entities O
are O
s O
c O
, O
with O
relations O
, O
r O
c O
. O
Task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
. O
Task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
aims O
to O
classify O
the O
intent O
and O
parse O
the O
semantic O
slots O
in O
an O
utterance O
( O
to O
a O
taskoriented O
dialog O
system O
) O
Gupta O
et O
al O
. O
, O
2018 O
) O
. O
To O
fit O
into O
our O
formulation O
, O
we O
first O
predict O
the O
intent O
and O
then O
use O
a O
relational O
structure O
extraction O
model O
to O
predict O
the O
slots O
( O
s O
c O
) O
as O
well O
as O
their O
semantic O
roles O
( O
r O
c O
) O
for O
the O
given O
intent O
( O
c O
) O
. O

Sequence O
Tagging O
Model O
for O
RSE B-TaskName

We O
hereby O
introduce O
the O
typical O
way O
of O
applying O
a O
sequence O
tagging O
model O
to O
unifiedly O
solve O
relational O
structure O
extraction O
. O
The O
goal O
of O
our O
sequence O
tagging O
model O
for O
relational O
structure O
extraction O
is O
to O
predict O
the O
BIO O
- O
tag O
sequence O
y O
= O
[ O
y O
1 O
, O
y O
2 O
, O
... O
, O
y O
n O
] O
, O
where O
each O
y O
i O
is O
the O
corresponding O
tag O
for O
each O
token O
x O
i O
in O
the O
input O
text O
. O
The O
BIOtag O
sequence O
can O
then O
be O
decoded O
to O
represent O
the O
extracted O
spans O
s O
c O
( O
and O
their O
relationships O
r O
c O
) O
. O

Specifically O
, O
given O
an O
input O
text O
, O
we O
obtain O
the O
contextualized O
representation O
z O
i O
for O
each O
token O
x O
i O
by O
passing O
the O
passage O
to O
a O
pre O
- O
trained O
language O
model O
. O
1 O
To O
embed O
the O
information O
of O
the O
condition O
c O
, O
one O
commonly O
- O
used O
technique O
is O
to O
add O
conditional O
features O
to O
z O
i O
( O
Ma O
et O
al O
. O
, O
2020 O
; O
Yu O
et O
al O
. O
, O
2020 O
) O
, O
as O
shown O
in O
Figure O
1 O
( O
a O
) O
. O
For O
example O
, O
in O
Ma O
et O
al O
. O
( O
2020 O
) O
, O
they O
use O
a O
token O
embedding O
of O
the O
given O
event O
trigger O
word O
and O
a O
learnable O
event O
type O
feature O
as O
the O
conditional O
features O
for O
the O
task O
of O
event B-TaskName
argument I-TaskName
extraction I-TaskName
. O
In O
such O
case O
, O
the O
feature O
of O
c O
will O
contain O
the O
contextualized O
word O
representation O
z O
j O
, O
if O
x O
j O
is O
the O
token O
that O
represents O
the O
given O
condition O
, O
i.e. O
, O
event O
trigger O
. O
In O
our O
experimental O
setup O
, O
if O
the O
given O
condition O
can O
be O
represented O
as O
an O
input O
span O
, O
we O
will O
include O
the O
span O
embeddings O
as O
the O
conditional O
features O
together O
with O
the O
type O
embeddings O
, O
such O
as O
the O
cases O
for O
event B-TaskName
extraction I-TaskName
and O
relation B-TaskName
extraction I-TaskName
. O
If O
the O
condition O
is O
only O
a O
concept O
, O
such O
as O
the O
task B-TaskName
- I-TaskName
oriented I-TaskName
semantic I-TaskName
parsing I-TaskName
case I-TaskName
, O
the O
conditional O
features O
will O
only O
contain O
type O
embeddings O
. O
Augmented O
with O
these O
conditional O
features O
, O
the O
final O
representation O
for O
token O
x O
i O
is O
further O
fed O
into O
multi O
- O
layer O
perceptrons O
and O
a O
conditional O
random O
field O
( O
CRF O
) O
layer O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
to O
predict O
the O
BIO O
- O
tag O
sequence O
y O
, O
as O
illustrated O
in O
Figure O
1 O
( O
a O
) O
. O

TAGPRIME B-MethodName

TAGPRIME B-MethodName
follows O
the O
sequence O
tagging O
paradigm O
but O
utilizes O
the O
priming O
technique O
for O
better O
leverage O
information O
about O
the O
input O
condition O
. O

Condition O
Priming O
. O
Motivated O
by O
previous O
work O
( O
Fincke O
et O
al O
. O
, O
2022 O
) O
, O
we O
consider O
priming O
to O
inject O
the O
information O
of O
the O
condition O
c O
to O
further O
improve O
the O
sequence O
tagging O
model O
. O
The O
priming O
mechanism O
informs O
the O
model O
of O
the O
conditional O
information O
by O
directly O
appending O
conditional O
information O
to O
the O
input O
text O
. O
However O
, O
unlike O
Fincke O
et O
al O
. O
( O
2022 O
) O
that O
uses O
an O
integer O
string O
to O
represent O
features O
in O
a O
categorical O
style O
, O
we O
use O
a O
naturallanguage O
- O
styled O
indicator O
to O
better O
exploit O
the O
semantics O
of O
the O
condition O
. O
The O
indicators O
can O
be O
obtained O
by O
verbalizing O
the O
conditional O
information O
. O

Take O
Figure O
1 O
( O
b O
) O
as O
an O
example O
, O
when O
extracting O
the O
tail O
- O
entities O
and O
the O
relationships O
for O
the O
" O
military O
" O
head O
- O
entity O
( O
condition O
c O
) O
, O
we O
first O
verbalize O
the O
entity O
type O
of O
" O
military O
" O
, O
i.e. O
, O
from O
" O
Org O
" O
to O
" O
Organization O
" O
. O
Then O
, O
the O
string O
" O
military O
" O
and O
" O
Organization O
" O
are O
appended O
to O
the O
input O
text O
, O
which O
serves O
as O
the O
information O
about O
the O
condition O
c O
. O

The O
priming O
technique O
leverages O
the O
selfattention O
mechanism O
in O
pre O
- O
trained O
language O
models O
and O
makes O
the O
token O
representation O
z O
i O
conditionaware O
. O
Hence O
, O
the O
representation O
of O
every O
z O
i O
is O
more O
task O
- O
specific O
than O
the O
one O
in O
the O
model O
described O
in O
Section O
3.2 O
. O
More O
precisely O
, O
for O
tagging O
models O
without O
priming O
, O
the O
representation O
z O
i O
usually O
captures O
more O
general O
information O
that O
focuses O
on O
the O
context O
of O
input O
text O
. O
For O
models O
with O
priming O
, O
the O
representation O
z O
i O
is O
affected O
by O
the O
additional O
verbalized O
words O
when O
computing O
attention O
. O
Hence O
, O
z O
i O
becomes O
more O
task O
- O
specific O
and O
more O
suitable O
for O
addressing O
the O
task O
( O
Zheng O
and O
Lapata O
, O
2022 O
; O
Zhong O
and O
Chen O
, O
2021 O
) O
. O
Additionally O
, O
the O
priming O
method O
can O
be O
easily O
combined O
with O
conditional O
features O
described O
in O
Section O
3.2 O
. O
More O
discussion O
on O
this O
will O
be O
shown O
in O
Section O
5 O
. O

Relationship O
Priming O
. O
The O
same O
idea O
of O
condition O
priming O
can O
also O
be O
extended O
to O
relationship O
. O
Specifically O
, O
we O
decompose O
a O
relational O
structure O
extraction O
task O
into O
several O
extraction O
subtasks O
, O
each O
of O
them O
only O
focusing O
on O
one O
single O
relationship O
r O
( O
r O
∈ O
A O
) O
. O
Similar O
to O
the O
condition O
priming O
, O
we O
verbalize O
the O
relationship O
information O
and O
append O
related O
strings O
to O
the O
input O
text O
as O
well O
. O
Therefore O
, O
the O
representation O
z O
i O
is O
aware O
of O
the O
relation O
- O
ship O
r O
and O
specific O
for O
predicting O
spans O
with O
relationship O
r O
to O
the O
condition O
c O
. O

For O
example O
, O
in O
Figure O
1 O
( O
c O
) O
, O
for O
the O
given O
relationship O
" O
Part O
- O
Whole O
" O
, O
we O
first O
verbalized O
it O
into O
" O
is O
part O
of O
" O
. O
Then O
, O
the O
string O
" O
is O
part O
of O
" O
is O
appended O
to O
the O
input O
text O
together O
with O
the O
condition O
priming O
strings O
. O
The O
BIO O
- O
tag O
sequence O
can O
be O
decoded O
into O
those O
tail O
- O
entities O
s O
c O
that O
form O
" O
Part O
- O
Whole O
" O
relationship O
( O
s O
) O
with O
the O
given O
head O
- O
entity O
" O
military O
" O
. O

Discussion O
. O
A O
similar O
idea O
of O
appending O
tokens O
in O
the O
pre O
- O
trained O
language O
model O
's O
input O
to O
affect O
the O
output O
text O
representation O
has O
also O
been O
leveraged O
in O
Zhou O
and O
Chen O
( O
2021b O
) O
; O
Zhong O
and O
Chen O
( O
2021 O
) O
. O
Yet O
, O
different O
from O
their O
works O
that O
only O
focus O
on O
relation O
classification O
and O
apply O
instancespecific O
information O
, O
our O
TAGPRIME B-MethodName
with O
relationship O
priming O
method O
focuses O
on O
using O
task O
- O
specific O
information O
, O
because O
we O
decompose O
relational B-TaskName
extraction I-TaskName
into O
sub O
- O
tasks O
. O
We O
want O
that O
different O
taskspecific O
representation O
can O
be O
learned O
for O
different O
sub O
- O
tasks O
, O
hence O
proposing O
relationship O
priming O
. O

An O
underlying O
advantage O
of O
TAGPRIME B-MethodName
with O
relationship O
priming O
is O
its O
ability O
to O
handle O
cases O
containing O
multi O
- O
relationships O
. O
After O
we O
decompose O
a O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
task O
into O
several O
extraction O
subtasks O
, O
we O
do O
not O
perform O
any O
filtering O
to O
address O
conflict O
relationship O
predictions O
between O
the O
same O
condition O
and O
extracted O
span O
. O
This O
is O
to O
enlarge O
our O
model O
's O
generality O
to O
different O
scenarios O
. O

Experiments O

To O
study O
the O
effectiveness O
of O
TAGPRIME B-MethodName
, O
we O
consider O
three O
NLP O
tasks O
: O
( O
1 O
) O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
, O
( O
2 O
) O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
relation I-TaskName
extraction I-TaskName
, O
and O
( O
3 O
) O
taskoriented B-TaskName
semantic I-TaskName
parsing I-TaskName
. O
All O
the O
results O
are O
the O
average O
of O
five O
runs O
with O
different O
random O
seeds O
. O

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
End I-TaskName
Event I-TaskName
Extraction I-TaskName

Datasets O
. O
We O
consider O
the O
two O
most O
widely O
- O
used O
event B-TaskName
extraction I-TaskName
datasets O
, O
ACE-2005 B-DatasetName
( O
Doddington O
et O
al O
. O
, O
2004 O
) O
and O
ERE B-DatasetName
( O
Song O
et O
al O
. O
, O
2015 O
) O
. O
For O
ACE-2005 B-DatasetName
( O
ACE05 B-DatasetName
- I-DatasetName
E I-DatasetName
) O
, O
we O
experiment O
on O
the O
English O
and O
Chinese O
portions O
and O
keep O
33 O
event O
types O
and O
22 O
roles O
, O
as O
suggested O
in O
previous O
works O
( O
Wadden O
et O
al O
. O
, O
2019 O
; O
Hsu O
et O
al O
. O
, O
2022b O
) O
. O
For O
ERE B-DatasetName
, O
we O
consider O
the O
English O
and O
Spanish O
annotations O
and O
follow O
the O
preprocessing O
of O
to O
keep O
38 O
event O
types O
and O
21 O
roles O
. O

Baselines O
. O
We O
consider O
the O
following O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
models O
, O
including O
DyGIE++ B-MethodName
( O
Wad O
- O
den O
et O
al O
. O
, O
2019 O
) O
, O
TANL B-MethodName
( O
Paolini O
et O
al O
. O
, O
2021 O
) O
, O
Text2Event B-MethodName
( O
Lu O
et O
al O
. O
, O
2021 O
) O
, O
OneIE B-MethodName
, O
and O
DEGREE B-MethodName
( O
Hsu O
et O
al O
. O
, O
2022b O
) O
. O
Since O
TAGPRIME B-MethodName
requires O
trigger O
predictions O
, O
we O
simply O
take O
the O
trigger O
predictions O
made O
by O
a O
simple O
sequence O
tagging O
model O
trained O
with O
multi O
- O
tasking O
on O
trigger O
detection O
and O
named O
entity O
recognition O
. O

For O
TAGPRIME B-MethodName
, O
DyGIE++ B-MethodName
, O
and O
OneIE B-MethodName
, O
we O
consider O
BERT O
- O
large O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
for O
ACE05 B-DatasetName
- I-DatasetName
E I-DatasetName
( O
en O
) O
and O
ERE B-DatasetName
( O
en O
) O
, O
and O
consider O
XLM O
- O
RoBERTalarge O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
for O
ACE05 B-DatasetName
- I-DatasetName
E I-DatasetName
( O
zh O
) O
and O
ERE B-DatasetName
( O
es O
) O
. O
For O
generation O
- O
based O
models O
, O
we O
consider O
BART O
- O
large O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
for O
DEGREE B-MethodName
, O
T5 O
- O
base O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
for O
TANL B-MethodName
, O
and O
T5large O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
for O
Text2Event B-MethodName
, O
as O
suggested O
by O
their O
original O
papers O
. O
language O
models O
with O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
being O
0.2 B-HyperparameterValue
. O
We O
use O
AdamW O
optimizer B-HyperparameterName
. O
For O
parameters O
that O
are O
not O
pre O
- O
trained O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
10 B-HyperparameterValue
−3 I-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
10 B-HyperparameterValue
−3 I-HyperparameterValue
. O
For O
parameters O
that O
are O
not O
pre O
- O
trained O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
10 B-HyperparameterValue
−5 I-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
10 B-HyperparameterValue
−5 I-HyperparameterValue
. O
We O
consider O
the O
linear B-HyperparameterValue
scheduler B-HyperparameterName
with O
a O
warm O
- O
up O
, O
where O
the O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
epoch I-HyperparameterName
is O
5 B-HyperparameterValue
. O
The O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
is O
90 B-HyperparameterValue
. O
The O
training O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
6 B-HyperparameterValue
. O
For O
conditional O
token O
features O
and O
learnable O
features O
, O
the O
dimension B-HyperparameterName
is O
set O
to O
100 B-HyperparameterValue
. O
It O
takes O
around O
6 O
hours O
to O
train O
with O
a O
NVIDIA O
RTX O
A6000 O
with O
48 O
GB O
memory O
. O

Evaluation O
metrics O
. O
Following O
previous O
works O
( O
Wadden O
et O
al O
. O
, O
2019 O
; O
, O
we O
measure O
the O
correctness O
of O
arguments O
based O
on O
whether O
the O
offsets O
of O
the O
argument O
span O
match O
or O
not O
. O
We O
Table O
1 O
: O
Results O
of O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
. O
All O
values O
are O
micro O
F1 B-MetricName
- O
score O
, O
and O
we O
highlight O
highest O
scores O
with O
boldface O
. O
TAGPRIME B-MethodName
with O
conditional O
and O
relationship O
priming O
achieves O
more O
than O
1.4 B-MetricValue
Arg B-MetricName
- I-MetricName
C I-MetricName
F1 B-MetricName
- O
score O
improvements O
in O
three O
out O
of O
four O
datasets O
. O
* O
We O
reproduce O
the O
results O
using O
their O
released O
code O
. O
consider O
argument O
identification O
F1 B-MetricName
- O
score O
( O
Arg B-MetricName
- I-MetricName
I I-MetricName
) O
, O
which O
cares O
about O
only O
the O
offset O
correctness O
, O
and O
argument O
classification O
F1 B-MetricName
- O
score O
( O
Arg B-MetricName
- I-MetricName
C I-MetricName
) O
, O
which O
cares O
about O
both O
offsets O
and O
the O
role O
types O
. O
We O
also O
report O
trigger O
classification O
F1 B-MetricName
- O
score O
( O
Tri B-MetricName
- I-MetricName
C I-MetricName
) O
, O
although O
it O
is O
not O
our O
main O
focus O
as O
the O
triggers O
are O
provided O
via O
other O
models O
and O
we O
just O
use O
their O
predictions O
to O
simulate O
the O
end O
- O
to O
- O
end O
scenarios O
. O

Results O
. O
Table O
1 O
shows O
the O
results O
of O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
event I-TaskName
extraction I-TaskName
on O
various O
datasets O
and O
languages O
. O
Although O
simple O
, O
TAGPRIME B-MethodName
surprisingly O
has O
decent O
performance O
and O
achieves O
better O
results O
than O
the O
state O
- O
of O
- O
the O
- O
art O
models O
in O
terms O
of O
argument O
F1scores B-MetricName
. O
We O
attribute O
the O
good O
performance O
to O
the O
design O
of O
priming O
, O
which O
leverages O
the O
semantics O
of O
the O
condition O
and O
makes O
the O
representations O
more O
task O
- O
specific O
. O
It O
is O
worth O
noting O
that O
considering O
relationship O
priming O
further O
improves O
the O
results O
, O
which O
again O
shows O
the O
importance O
of O
task O
- O
specific O
representations O
. O

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
End I-TaskName
Relation I-TaskName
Extraction I-TaskName

Datasets O
. O
We O
consider O
two O
popular O
end O
- O
toend O
relation O
extraction O
datasets O
, O
ACE04 B-DatasetName
and O
ACE05 B-DatasetName
( O
Doddington O
et O
al O
. O
, O
2004 O
) O
, O
denoted O
as O
ACE04 B-DatasetName
- I-DatasetName
R I-DatasetName
and O
ACE05 B-DatasetName
- I-DatasetName
R. I-DatasetName
Both O
datasets O
consider O
7 O
named O
entity O
types O
and O
6 O
different O
relations O
. O
We O
follow O
the O
same O
procedure O
in O
Zhong O
and O
Chen O
( O
2021 O
) O
to O
preprocess O
the O
data O
and O
split O
the O
datasets O
. O
We O
refer O
readers O
to O
their O
papers O
for O
more O
details O
about O
the O
datasets O
. O

Baselines O
. O
We O
compare O
to O
the O
following O
end B-TaskName
- I-TaskName
toend I-TaskName
relation I-TaskName
extraction I-TaskName
models O
: O
Table B-MethodName
- I-MethodName
Sequence I-MethodName
( O
Wang O
and O
Lu O
, O
2020 O
) O
, O
PFN B-MethodName
, O
and O
Cascade B-MethodName
- I-MethodName
SRN I-MethodName
( O
both O
late O
fusion O
and O
early O
fusion O
) O
( O
Wang O
et O
al O
. O
, O
2022 O
) O
. O
Additionally O
, O
we O
consider O
PURE B-MethodName
( O
Zhong O
and O
Chen O
, O
2021 O
) O
, O
which O
also O
takes O
a O
pipelined O
approach O
to O
solve O
end O
- O
to O
- O
end O
relation O
extraction O
. O
To O
fairly O
compare O
with O
prior O
works O
, O
we O
use O
PURE B-MethodName
's O
named O
entity O
predictions O
on O
the O
test O
set O
for O
TAGPRIME B-MethodName
to O
perform O
relational O
structure O
extraction O
. O
4 O
In O
order O
to O
be O
consistent O
with O
our O
other O
tasks O
, O
we O
adopt O
the O
single O
sentence O
setting O
( O
Zhong O
and O
Chen O
, O
2021 O
) O
for O
our O
model O
. O
However O
, O
we O
also O
list O
baselines O
with O
cross O
- O
sentence O
settings O
, O
such O
as O
PURE B-MethodName
's O
and O
UniRE B-MethodName
( O
Wang O
et O
al O
. O
, O
2021 O
) O
's O
results O
with O
cross O
- O
sentence O
context O
as O
input O
. O
All O
the O
models O
use O
ALBERT O
- O
xxlarge O
- O
v1 O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
as O
the O
pre O
- O
trained O
language O
models O
. O
Implementation O
details O
. O
The O
followings O
are O
the O
training O
details O
for O
all O
baselines O
: O

• O
Table B-MethodName
- I-MethodName
Sequence I-MethodName
( O
Wang O
and O
Lu O
, O
2020 O
) O
: O
we O
report O
the O
numbers O
from O
the O
original O
paper O
. O

• O
Cascade B-MethodName
- I-MethodName
SRN I-MethodName
( O
Wang O
et O
al O
. O
, O
2022 O
) O
: O
we O
report O
the O
numbers O
from O
the O
original O
paper O
. O

• O
PURE B-MethodName
( O
Zhong O
and O
Chen O
, O
2021 O
) O
: O
we O
report O
the O
numbers O
from O
the O
original O
paper O
. O

• O
PFN B-MethodName
: O
we O
report O
the O
numbers O
from O
the O
original O
paper O
. O

• O
UniRE B-MethodName
( O
Wang O
et O
al O
. O
, O
2021 O
) O
: O
we O
report O
the O
numbers O
from O
the O
original O
paper O
. O

• O
TAGPRIME B-MethodName
( O
ours O
) O
: O
We O
fine O
- O
tune O
pre O
- O
trained O
language O
models O
with O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
being O
0.2 B-HyperparameterValue
. O
We O
use O
AdamW O
optimizer B-HyperparameterName
. O
For O
parameters O
that O
are O
not O
pre O
- O
trained O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
10 B-HyperparameterValue
−3 I-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
10 B-HyperparameterValue
−3 I-HyperparameterValue
. O
For O
parameters O
that O
are O
not O
pre O
- O
trained O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
10 B-HyperparameterValue
−5 I-HyperparameterValue
. O
We O
consider O
the O
linear O
scheduler B-HyperparameterName
with O
a O
warm O
- O
up O
, O
where O
the O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
epoch I-HyperparameterName
is O
5 B-HyperparameterValue
. O
The O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
is O
30 B-HyperparameterValue
. O
The O
training O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
. O
For O
conditional O
token O
features O
and O
learnable O
features O
, O
the O
dimension B-HyperparameterName
is O
set O
to O
100 B-HyperparameterValue
. O
It O
takes O
around O
20 O
hours O
to O
train O
with O
a O
NVIDIA O
RTX O
A6000 O
with O
48 O
GB O
memory O
. O

Model O
ACE05 B-MethodName
- I-MethodName
R I-MethodName
ACE04 B-MethodName
- I-MethodName
R I-MethodName
Ent O
Rel O
Rel+ O
Ent O
Rel O
Rel+ O

Evaluation O
metrics O
. O
We O
follow O
the O
standard O
evaluation O
setting O
with O
prior O
works O
( O
Bekoulis O
et O
al O
. O
, O
2018 O
; O
Zhong O
and O
Chen O
, O
2021 O
) O
and O
use O
micro B-MetricName
F1score I-MetricName
as O
the O
evaluation O
metric O
. O
For O
named B-TaskName
entity I-TaskName
recognition I-TaskName
, O
a O
predicted O
entity O
is O
considered O
as O
a O
correct O
prediction O
if O
its O
span O
and O
the O
entity O
type O
are O
both O
correct O
. O
We O
denote O
the O
score O
as O
" O
Ent B-MetricName
" O
and O
report O
the O
scores O
even O
though O
it O
is O
not O
our O
main O
focus O
for O
evaluation O
. O
For O
relation B-TaskName
extraction I-TaskName
, O
two O
evaluation O
metrics O
are O
considered O
: O
( O
1 O
) O
Rel B-MetricName
: O
a O
predicted O
relation O
is O
considered O
as O
correct O
when O
the O
boundaries O
of O
head O
- O
entity O
span O
and O
tail O
- O
entity O
span O
are O
correct O
and O
the O
predicted O
relation O
type O
is O
correct O
; O
( O
2 O
) O
Rel+ B-MetricName
: O
a O
stricter O
evaluation O
of O
Rel B-MetricName
, O
where O
they O
additionally O
required O
that O
the O
entity O
types O
of O
head O
- O
entity O
span O
and O
tail O
- O
entity O
must O
also O
be O
correct O
. O

Results O
. O
The O
results O
of O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
relation I-TaskName
extraction I-TaskName
are O
presented O
in O
Table O
2 O
. O
From O
the O
table O
, O
we O
observe O
that O
TAGPRIME B-MethodName
has O
the O
best O
performance O
on O
ACE05 B-DatasetName
- I-DatasetName
R I-DatasetName
and O
outperforms O
most O
baselines O
on O
ACE04 B-DatasetName
- I-DatasetName
R. I-DatasetName
This O
shows O
the O
effectiveness O
of O
TAG B-MethodName
- I-MethodName
PRIME I-MethodName
. O
Similar O
to O
the O
results O
of O
event O
extraction O
, O
considering O
relationship O
priming O
makes O
the O
representations O
more O
relationship O
- O
aware O
and O
leads O
to O
performance O
improvement O
. O

Task B-TaskName
- I-TaskName
Oriented I-TaskName
Semantic I-TaskName
Parsing I-TaskName

Datasets O
. O
We O
choose O
MTOP B-DatasetName
, O
a O
multilingual O
dataset O
on O
semantic B-TaskName
parsing I-TaskName
for O
taskoriented O
dialog O
systems O
. O
We O
specifically O
consider O
data O
in O
English O
( O
en O
) O
, O
Spanish O
( O
es O
) O
, O
French O
( O
fr O
) O
, O
and O
German O
( O
de O
) O
. O

Baselines O
. O
We O
consider O
JointBERT B-MethodName
( O
Chen O
et O
al O
. O
, O
2019b O
) O
, O
the O
commonly O
used O
baseline O
for O
task O
- O
oriented O
semantic O
parsing O
. O
We O
directly O
use O
the O
predicted O
intents O
by O
JointBERT B-MethodName
as O
the O
condition O
of O
TAGPRIME B-MethodName
for O
a O
fair O
comparison O
. O
Both O
TAGPRIME B-MethodName
and O
JointBERT B-MethodName
are O
trained O
with O
XLM O
- O
RoBERTalarge O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
. O
Unlike O
event B-TaskName
extraction I-TaskName
and O
relation B-TaskName
extraction I-TaskName
, O
the O
condition O
of O
taskoriented B-TaskName
semantics I-TaskName
parsing I-TaskName
( O
intent O
) O
does O
not O
include O
the O
word O
span O
, O
therefore O
, O
only O
a O
type O
feature O
embedding O
is O
contained O
in O
the O
conditional O
features O
for O
TAGPRIME B-MethodName
in O
this O
experiment O
. O

Implementation O
details O
. O
The O
followings O
are O
the O
training O
details O
for O
all O
baselines O
: O

• O
JointBERT B-MethodName
( O
Chen O
et O
al O
. O
, O
2019b O
) O
: O
we O
use O
the O
training O
script O
5 O
with O
the O
default O
parameters O
. O

• O
TAGPRIME B-MethodName
( O
ours O
) O
: O
We O
fine O
- O
tune O
pre O
- O
trained O
language O
models O
with O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
being O
0.2 B-HyperparameterValue
. O
We O
use O
AdamW O
optimizer B-HyperparameterName
. O
For O
parameters O
that O
are O
not O
pre O
- O
trained O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
10 B-HyperparameterValue
−3 I-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
10 B-HyperparameterValue
−3 I-HyperparameterValue
. O
For O
parameters O
that O
are O
not O
pre O
- O
trained O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
10 B-HyperparameterValue
−5 I-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
10 B-HyperparameterValue
−5 I-HyperparameterValue
. O
We O
consider O
the O
linear O
scheduler B-HyperparameterName
with O
warm O
- O
up O
, O
where O
the O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
epoch I-HyperparameterName
is O
5 B-HyperparameterValue
. O
The O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
is O
90 B-HyperparameterValue
. O
The O
training O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
6 B-HyperparameterValue
. O
For O
conditional O
token O
features O
and O
learnable O
features O
, O
the O
dimension B-HyperparameterName
is O
set O
to O
100 B-HyperparameterValue
. O
It O
takes O
around O
4 O
hours O
to O
train O
with O
a O
NVIDIA O
RTX O
A6000 O
with O
48 O
GB O
memory O
. O

Evaluation O
metrics O
. O
We O
following O
MTOP B-DatasetName
to O
consider O
slot B-MetricName
identification I-MetricName
( O
Slot B-MetricName
- I-MetricName
I I-MetricName
) O
and O
slot B-MetricName
classification I-MetricName
( O
Slot B-MetricName
- I-MetricName
C I-MetricName
) O
F1 O
- O
scores O
. O
Even O
though O
we O
focus O
on O
the O
performance O
of O
slot O
filling O
, O
we O
also O
report O
the O
intent O
classification B-MetricName
accuracy I-MetricName
. O
Table O
4 O
: O
The O
ablation O
study O
results O
for O
three O
different O
tasks O
. O
The O
average O
column O
calculates O
the O
average O
scores O
of O
the O
stricter O
evaluation O
metrics O
( O
i.e O
, O
Arg B-MetricName
- I-MetricName
C I-MetricName
, O
Slot B-MetricName
- I-MetricName
C I-MetricName
, O
and O
Rel+ B-MetricName
) O
for O
each O
dataset O
. O
From O
the O
table O
, O
we O
demonstrate O
priming O
technique O
is O
the O
key O
attribute O
that O
make O
our O
sequence O
tagging O
model O
stronger O
than O
models O
with O
learnable O
features O
, O
which O
is O
the O
typical O
way O
of O
using O
sequence O
tagging O
models O
for O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
. O

Results O
. O
As O
demonstrated O
in O

achieves O
a O
better O
performance O
than O
the O
baselines O
. O
Again O
, O
considering O
relationship O
priming O
leads O
to O
further O
improvement O
. O
It O
is O
worth O
noting O
that O
TAG B-MethodName
- I-MethodName
PRIME I-MethodName
is O
effective O
for O
different O
languages O
, O
which O
shows O
the O
generality O
of O
TAGPRIME B-MethodName
. O

Summary O

We O
show O
the O
superiority O
of O
TAGPRIME B-MethodName
on O
three O
different O
tasks O
( O
including O
ten O
different O
datasets O
across O
five O
different O
languages O
) O
. O
Although O
being O
a O
unified O
and O
simple O
model O
, O
the O
results O
suggest O
that O
TAG B-MethodName
- I-MethodName
PRIME I-MethodName
can O
achieve O
competitive O
results O
for O
tasks O
requiring O
extracting B-TaskName
relational I-TaskName
structures I-TaskName
. O

Analysis O

In O
this O
section O
, O
we O
study O
two O
questions O
: O
( O
1 O
) O
What O
is O
the O
effectiveness O
of O
priming O
techniques O
compared O
to O
learnable O
features O
? O
( O
2 O
) O
Relationship O
priming O
boosts O
the O
performance O
of O
TAGPRIME B-MethodName
, O
but O
the O
task O
decomposition O
could O
slightly O
slow O
down O
the O
inference O
speed O
. O
Can O
we O
mitigate O
this O
issue O
? O

To O
answer O
the O
first O
question O
, O
we O
conduct O
ablation O
experiments O
on O
sequence O
tagging O
models O
using O
different O
combinations O
of O
learnable O
features O
or O
/ O
and O
adding O
information O
through O
the O
priming O
technique O
( O
Section O
5.1 O
) O
. O
For O
the O
second O
question O
, O
we O
propose O
a O
simple O
modification O
to O
TAGPRIME B-MethodName
so O
that O
we O
can O
flexibly O
control O
the O
number O
of O
layers O
to O
fuse O
priming O
information O
to O
contextualized O
representations O
. O

The O
modified B-MethodName
TAGPRIME I-MethodName
can O
serve O
as O
an O
efficient O
approximation O
of O
TAGPRIME B-MethodName
( O
Section O
5.2 O
) O
. O

Ablation O
Study O

We O
focus O
on O
the O
setting O
where O
we O
alter O
the O
choices O
on O
how O
to O
include O
the O
type O
information O
of O
the O
condition O
c O
and O
the O
relationship O
information O
r. O
Table O
4 O
demonstrates O
our O
experimental O
results O
. O

Comparing O
the O
first O
four O
cases O
in O
Table O
4 O
, O
we O
observe O
that O
the O
addition O
of O
type O
features O
is O
useful O
in O
general O
, O
and O
using O
the O
priming O
technique O
is O
a O
more O
effective O
way O
to O
incorporate O
conditional O
information O
. O
For O
models O
in O
case B-MethodName
5 I-MethodName
to O
case B-MethodName
8 I-MethodName
, O
the O
relationship O
decomposition O
formulation O
described O
in O
Section O
3.3 O
is O
applied O
. O
Comparing O
case B-MethodName
2 I-MethodName
to O
case B-MethodName
5 I-MethodName
, O
we O
can O
see O
that O
simply O
applying O
the O
relationship O
decomposition O
formulation O
for O
solving O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
does O
not O
lead O
to O
improvements O
if O
the O
way O
to O
embed O
the O
relationship O
r O
is O
only O
through O
learnable O
features O
. O
However O
, O
comparing O
case B-MethodName
3 I-MethodName
to O
case B-MethodName
6 I-MethodName
and O
case B-MethodName
4 I-MethodName
to O
case B-MethodName
7 I-MethodName
, O
we O
show O
that O
the O
relationship O
priming O
approach O
makes O
the O
representation O
z O
i O
well O
capture O
the O
attribute O
of O
the O
queried O
relationship O
, O
thus O
, O
better O
exploiting O
the O
advantage O
of O
the O
relationship O
decomposition O
formulation O
and O
gaining O
improvements O
. O
Note O
that O
we O
conducted O
preliminary O
experiments O
that O
use O
pretrained O
language O
models O
' O
representations O
of O
the O
same O
verbalized O
token O
to O
be O
the O
initialization O
of O
the O
learnable O
type O
feature O
embedding O
, O
but O
the O
method O
shows O
similar O
results O
with O
the O
random O
initialization O
, O
hence O
, O
we O
stick O
to O
random O
initialization O
on O
the O
learnable O
type O
features O
. O

Efficient O
approximation O
of O
TAGPRIME B-MethodName

To O
make O
TAGPRIME B-MethodName
to O
inference O
faster O
, O
we O
perform O
two O
modifications O
to O
TAGPRIME B-MethodName
: O
( O
1 O
) O
We O
first O
separate O
the O
pre O
- O
trained O
language O
model O
, O
which O
contains O
L O
layers O
, O
into O
two O
halves O
-one O
with O
the O
first O
k O
layers O
, O
the O
other O
one O
is O
the O
remaining O
layers O
. O

( O
2 O
) O
We O
copy O
the O
first O
half O
of O
the O
language O
model O
to O
another O
module O
. O
When O
an O
input O
passage O
is O
fed O
into O
the O
model O
. O
We O
use O
the O
original O
first O
half O
to O
encode O
the O
input O
text O
as O
well O
as O
the O
verbalized O
condition O
, O
and O
we O
use O
the O
copied O
first O
half O
to O
encode O
the O
verbalized O
relation O
. O
Finally O
, O
the O
encoded O
representations O
will O
be O
fed O
into O
the O
second O
half O
layers O
, O
as O
illustrated O
in O
Figure O
2 O
. O
The O
value O
of O
k O
is O
adjustable O
, O
where O
when O
k O
= O
0 O
, O
it O
represents O
the O
TAGPRIME B-MethodName
with O
condition O
and O
relationship O
priming O
, O
and O
when O
k O
= O
L O
, O
it O
is O
TAGPRIME B-MethodName
with O
condition O
priming O
. O
Since O
the O
encoding O
stage O
of O
the O
input O
text O
and O
the O
verbalized O
relationship O
is O
separated O
, O
we O
can O
accelerate O
the O
inference O
time O
of O
our O
modified B-MethodName
TAG I-MethodName
- I-MethodName
PRIME I-MethodName
through O
parallel O
encoding O
. O
More O
precisely O
, O
our O
modified B-MethodName
TAGPRIME I-MethodName
can O
aggregate O
instances O
that O
share O
the O
same O
passage O
and O
verbalized O
condition O
. O
For O
those O
instances O
, O
TAGPRIME B-MethodName
only O
needs O
to O
perform O
the O
encoding O
once O
on O
their O
input O
passage O
part O
, O
6 O
and O
paired O
with O
several O
separated O
embedded O
verbalized O
relationships O
, O
which O
could O
be O
parallelly O
encoded O
together O
. O

We O
conduct O
experiments O
on O
the O
ACE05 B-DatasetName
- I-DatasetName
E I-DatasetName
( O
en O
) O
dataset O
to O
test O
our O
modification O
. O
In O
order O
to O
better O
analyze O
the O
results O
and O
isolate O
the O
influence O
from O
the O
pipelined O
errors O
, O
we O
report O
the O
results O
on O
the O
event O
argument O
extraction O
when O
gold O
event O
triggers O
are O
given O
. O
The O
experimental O
results O
are O
shown O
in O
Figure O
3 O
. O
First O
, O
we O
investigate O
the O
performance O
influence O
of O
our O
modification O
. O
We O
find O
that O
when O
k B-HyperparameterName
≤ O
10 B-HyperparameterValue
, O
the O
performance O
of O
our O
modified B-MethodName
TAGPRIME I-MethodName
is O
strong O
in O
general O
and O
is O
comparable O
with O
TAGPRIME B-MethodName
with O
the O
condition O
and O
relationship O
priming O
. O
To O
compare O
the O
efficiency O
of O
the O
model O
, O
we O
benchmark O
the O
inference O
time O
by O
performing O
inference O
on O
the O
whole O
testing O
dataset O
fifty O
times O
and O
calculate O
the O
average B-MetricName
speed I-MetricName
, O
which O
is O
measured O
by O
checking O
how O
many O
instances O
can O
be O
processed O
per O
second O
. O
The O
red O
line O
in O
Figure O
3 O
shows O
the O
results O
. O
We O
observe O
that O
for O
our O
modified B-MethodName
TAGPRIME I-MethodName
with O
k B-HyperparameterName
= O
10 B-HyperparameterValue
, O
its O
inference O
speed O
is O
around O
30 B-MetricValue
% I-MetricValue
faster O
than O
the O
TAGPRIME B-MethodName
with O
the O
condition O
and O
relationship O
priming O
, O
but O
they O
perform O
similarly O
. O

Conclusion O

In O
this O
work O
, O
we O
take O
a O
unified O
view O
of O
tasks O
requiring O
extracting B-TaskName
relational I-TaskName
structures I-TaskName
and O
present O
TAG B-MethodName
- I-MethodName
PRIME I-MethodName
, O
a O
simple O
, O
unified O
, O
effective O
, O
and O
general O
sequence O
tagging O
model O
. O
The O
key O
idea O
is O
applying O
priming O
, O
a O
small O
trick O
to O
make O
the O
representations O
task O
- O
specific O
by O
appending O
condition O
- O
related O
and O
relationship O
- O
related O
strings O
to O
the O
input O
text O
. O
Our O
experimental O
results O
demonstrate O
that O
TAGPRIME B-MethodName
is O
general O
to O
different O
tasks O
in O
various O
languages O
and O
can O
serve O
as O
a O
strong O
baseline O
for O
future O
research O
on O
relational B-TaskName
structure I-TaskName
extraction I-TaskName
. O

Acknowledgments O

We O
thank O
anonymous O
reviewers O
for O
their O
helpful O
feedback O
. O

Limitations O

As O
we O
point O
out O
in O
Section O
5 O
, O
one O
of O
the O
limitations O
in O
TAGPRIME B-MethodName
is O
the O
inference O
speed O
. O
When O
we O
perform O
TAGPRIME B-MethodName
with O
condition O
and O
relationship O
priming O
, O
we O
requires O
more O
turns O
of O
sequence O
tagging O
processes O
than O
typical O
sequence O
tagging O
models O
. O
Observing O
this O
, O
we O
propose O
a O
simple O
way O
to O
mitigate O
such O
issue O
and O
increase O
the O
inference O
speed O
with O
only O
a O
small O
performance O
drop O
. O
Despite O
such O
effort O
, O
it O
is O
still O
slightly O
slower O
than O
the O
model O
requires O
only O
one O
pass O
of O
sequence O
labeling O
. O

The O
other O
potential O
limitation O
of O
our O
method O
is O
that O
we O
assume O
the O
condition O
and O
relationship O
can O
be O
verbalized O
. O
However O
, O
in O
practice O
, O
there O
could O
be O
cases O
that O
the O
verbalization O
is O
hard O
to O
be O
done O
. O
Considering O
this O
, O
we O
do O
conduct O
preliminary O
experiments O
of O
applying O
TAGPRIME B-MethodName
with O
special O
tokens O
priming O
rather O
than O
verbalized O
tokens O
. O
However O
, O
our O
preliminary O
results O
show O
that O
such O
method O
's O
performance O
is O
less O
stable O
and O
weaker O
than O
we O
can O
achieve O
with O
TAGPRIME B-MethodName
. O

Ethics O
Considerations O

TAGPRIME B-MethodName
fine O
- O
tunes O
the O
pre O
- O
trained O
language O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Lan O
et O
al O
. O
, O
2020 O
) O
. O
There O
have O
been O
works O
showing O
the O
potential O
bias O
in O
pretrained O
language O
models O
. O
Although O
with O
a O
low O
possibility O
, O
especially O
after O
our O
finetuning O
, O
it O
is O
possible O
for O
our O
model O
to O
make O
counterfactual O
, O
and O
biased O
predictions O
, O
which O
may O
cause O
ethical O
concerns O
. O
We O
suggest O
carefully O
examining O
those O
potential O
issues O
before O
deploying O
the O
model O
in O
any O
real O
- O
world O
applications O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Section O
4 O
B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Not O
applicable O
. O
Left O
blank O
. O

A O
Detailed O
Results O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Not O
applicable O
. O
Left O
blank O
. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Section O
4 O
C O
Did O
you O
run O
computational O
experiments O
? O
Section O
4 O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Section O
4 O
and O
Appendix O
A O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Multimodal B-TaskName
Intent I-TaskName
Discovery I-TaskName
from I-TaskName
Livestream I-TaskName
Videos I-TaskName

Individuals O
, O
educational O
institutions O
, O
and O
businesses O
are O
prolific O
at O
generating O
instructional O
video O
content O
such O
as O
" O
how O
- O
to O
" O
and O
tutorial O
guides O
. O
While O
significant O
progress O
has O
been O
made O
in O
basic O
video O
understanding O
tasks O
, O
identifying O
procedural O
intent O
within O
these O
instructional O
videos O
is O
a O
challenging O
and O
important O
task O
that O
remains O
unexplored O
but O
essential O
to O
video O
summarization O
, O
search O
, O
and O
recommendations O
. O
This O
paper O
introduces O
the O
problem O
of O
instructional B-TaskName
intent I-TaskName
identification I-TaskName
and I-TaskName
extraction I-TaskName
from I-TaskName
software I-TaskName
instructional I-TaskName
livestreams I-TaskName
. O
We O
construct O
and O
present O
a O
new O
multimodal O
dataset O
consisting O
of O
software O
instructional O
livestreams O
and O
containing O
manual O
annotations O
for O
both O
detailed O
and O
abstract O
procedural O
intent O
that O
enable O
training O
and O
evaluation O
of O
joint O
video O
and O
text O
understanding O
models O
. O
We O
then O
introduce O
a O
multimodal O
cascaded O
cross O
- O
attention O
model O
to O
efficiently O
combine O
the O
weaker O
and O
noisier O
video O
signal O
with O
the O
more O
discriminative O
text O
signal O
. O
Our O
experiments O
show O
that O
our O
proposed O
model O
brings O
significant O
gains O
compared O
to O
strong O
baselines O
, O
including O
large O
- O
scale O
pretrained O
multimodal O
models O
. O
Our O
analysis O
further O
identifies O
that O
the O
task O
benefits O
from O
spatial O
as O
well O
as O
motion O
features O
extracted O
from O
videos O
, O
and O
provides O
insight O
on O
how O
the O
video O
signal O
is O
preferentially O
used O
for O
intent O
discovery O
. O
We O
also O
show O
that O
current O
models O
struggle O
to O
comprehend O
the O
nature O
of O
abstract O
intents O
, O
revealing O
important O
gaps O
in O
multimodal O
understanding O
and O
paving O
the O
way O
for O
future O
work O
. O
1 O

Introduction O

Instructional O
videos O
have O
become O
increasingly O
ubiquitous O
as O
users O
generate O
diverse O
" O
how O
- O
to O
" O
, O
DIY O
, O
and O
tutorial O
videos O
. O
A O
Pew O
Research O
Center O
2018 O
survey O
of O
U.S. O
adult O
YouTube O
users O
( O
Smith O
et O
al O
. O
, O
2018 O
) O
found O
that O
over O
half O
of O
surveyed O
users O
use O
video O
content O
to O
learn O
how O
to O
do O
things O
they O
had O
not O
done O
before O
. O
These O
instructional O
videos O
convey O
both O
abstract O
and O
specific O
intent O
for O
physical O
tasks O
such O
as O
cooking O
where O
e.g. O
, O
an O
abstract O
culinary O
intent O
is O
" O
let O
's O
bring O
out O
the O
flavor O
" O
and O
a O
detailed O
intent O
is O
" O
add O
a O
pinch O
of O
nutmeg O
" O
. O
Thus O
, O
a O
key O
task O
in O
instructional O
video O
understanding O
is O
to O
discover O
both O
abstract O
and O
detailed O
intents O
. O
By O
discovering O
these O
intents O
, O
we O
can O
enable O
or O
improve O
important O
tasks O
such O
as O
semantic O
indexing O
of O
videos O
( O
Kofler O
et O
al O
. O
, O
2016 O
) O
, O
knowledge O
graph O
creation O
for O
video O
search O
and O
recommendations O
( O
Pei O
et O
al O
. O
, O
2011 O
; O
Kofler O
et O
al O
. O
, O
2014 O
) O
, O
intent O
highlighting O
, O
and O
video O
summarization O
( O
Nalla O
et O
al O
. O
, O
2020 O
) O
. O

An O
important O
domain O
with O
rich O
and O
complex O
examples O
of O
both O
abstract O
and O
detailed O
intent O
types O
are O
software O
training O
videos O
for O
creative O
tasks O
such O
as O
making O
photo O
or O
video O
effects O
. O
These O
types O
of O
software O
training O
videos O
have O
been O
shown O
to O
be O
effective O
for O
enhanced O
learning O
( O
Van O
der O
Meij O
, O
2017 O
) O
and O
are O
also O
considered O
a O
valuable O
resource O
in O
the O
era O
of O
online O
learning O
( O
Meyer O
, O
2015 O
) O
. O
Existing O
video O
and O
phrase O
datasets O
such O
as O
HowTo100 B-DatasetName
M I-DatasetName
( O
Miech O
et O
al O
. O
, O
2019 O
) O
cover O
a O
wide O
variety O
of O
tutorials O
for O
visual O
tasks O
demonstrated O
by O
humans O
; O
however O
, O
software O
- O
based O
instructional O
videos O
are O
not O
a O
part O
of O
such O
corpora O
. O
Hence O
, O
in O
this O
paper O
, O
we O
present O
a O
new O
corpus O
of O
software O
- O
instructional O
videos O
containing O
instructional O
intents O
, O
which O
are O
derived O
from O
Behance O
Livestreams O
demonstrating O
the O
use O
of O
Adobe O
Photoshop O
software O
. O
2 O
Intent O
detection O
has O
been O
well O
- O
studied O
in O
dialogue O
systems O
, O
but O
is O
less O
explored O
for O
instructional O
video O
content O
, O
especially O
emerging O
livestream O
content O
( O
Fraser O
et O
al O
. O
, O
2019 O
) O
. O
While O
rich O
in O
complex O
procedural O
instruction O
and O
intent O
, O
the O
interactive O
and O
social O
nature O
of O
livestreams O
poses O
unique O
challenges O
. O
Analyzing O
language O
features O
alone O
will O
provide O
only O
limited O
information O
about O
the O
actual O
instructional O
intent O
and O
the O
tools O
and O
commands O
used O
. O
For O
instance O
, O
the O
phrase O
" O
flipping O
the O
canvas O
" O
in O
" O
Are O
you O
flipping O
the O
canvas O
? O
" O
indicates O
a O
tool O
intent O
, O
but O
a O
closer O
look O
at O
the O
video O
clip O
reveals O
that O
it O
is O
in O
fact O
part O
of O
livestream O
chit O
- O
chat O
and O
does O
not O
take O
place O
on O
- O
screen O
. O
Incorporating O
both O
language O
and O
video O
modalities O
can O
enhance O
intent O
extraction O
of O
such O
ambiguous O
intents O
. O
Hence O
, O
in O
this O
paper O
, O
we O
present O
a O
new O
joint O
language O
- O
video O
intent O
discovery O
task O
and O
a O
multimodal O
dataset O
consisting O
of O
: O
Behance O
Intent O
Discovery O
, O
and O
the O
Behance O
Livestream O
video O
and O
transcript O
corpus O
that O
intents O
are O
found O
in O
. O
We O
frame O
intent O
discovery O
as O
a O
sequence O
labelling O
task O
; O
each O
sample O
in O
the O
intent O
discovery O
dataset O
contains O
a O
transcribed O
phrase O
annotated O
with O
token O
- O
level O
tags O
for O
abstract O
and O
detailed O
intents O
, O
and O
an O
associated O
video O
timestamp O
. O
Our O
goal O
is O
to O
predict O
the O
instructional O
intents O
from O
the O
transcript O
in O
each O
video O
. O

To O
perform O
intent O
discovery O
within O
instructional O
videos O
, O
we O
propose O
a O
multimodal O
cascaded O
crossattention O
model O
to O
predict O
both O
the O
abstract O
and O
detailed O
procedural O
intents O
that O
are O
present O
. O
Additionally O
, O
we O
use O
late O
fusion O
of O
multimodal O
embeddings O
to O
prevent O
the O
visual O
modality O
from O
overwhelming O
the O
textual O
signal O
, O
and O
show O
significant O
improvements O
on O
the O
video O
- O
based O
intent O
detection O
task O
using O
unimodal O
and O
multimodal O
pretrained O
models O
like O
HERO B-MethodName
( O
Li O
et O
al O
. O
, O
2020 O
) O
. O
Further O
, O
we O
compare O
the O
performance O
of O
various O
video O
feature O
extractors O
as O
well O
as O
different O
video O
lengths O
, O
and O
present O
benchmark O
results O
on O
the O
proposed O
dataset O
. O
We O
find O
that O
discovery O
of O
tool O
intents O
benefit O
from O
sparsely O
- O
sampled O
spatial O
features O
while O
creative O
intents O
benefit O
from O
densely O
- O
sampled O
motion O
features O
. O
In O
the O
absence O
of O
motion O
features O
, O
most O
models O
struggle O
to O
utilize O
the O
video O
signal O
for O
identification O
of O
creative O
intents O
. O
Further O
, O
visualization O
of O
crossattention O
and O
visual O
gate O
modules O
in O
the O
late O
fusion O
model O
suggests O
strong O
and O
meaningful O
interaction O
between O
the O
two O
modalities O
. O
Our O
contributions O
are O
: O

• O
We O
introduce O
and O
explore O
the O
novel O
task O
of O
video O
- O
based O
multimodal O
intent O
discovery O
, O
and O
present O
an O
annotated O
dataset O
consisting O
of O
nearly O
20 O
K O
sentences O
from O
66 O
livestreams O
for O
extraction O
of O
procedural O
intents O
from O
instructional O
videos O
. O

• O
We O
release O
a O
large O
corpus O
of O
software O
- O
based O
instructional O
videos O
( O
2,049 O
sessions O
, O
3,128 O
hours O
total O
) O
, O
accompanied O
by O
timestamped O
transcripts O
, O
that O
can O
be O
used O
for O
pretraining O
multimodal O
models O
. O

• O
We O
propose O
the O
multimodal O
cascaded O
crossattention O
model O
and O
demonstrate O
the O
effectiveness O
of O
late O
fusion O
of O
multimodal O
embeddings O
in O
this O
task O
. O

• O
We O
present O
empirical O
results O
for O
the O
proposed O
dataset O
using O
unimodal O
and O
multimodal O
approaches O
, O
and O
provide O
insights O
from O
analysis O
of O
modelling O
choices O
for O
future O
research O
. O

Related O
Work O

Intent O
discovery O
has O
been O
widely O
studied O
in O
the O
context O
of O
dialog O
modelling O
and O
generation O
wherein O
it O
has O
been O
framed O
as O
a O
binary O
or O
multi O
- O
class O
classification O
problem O
. O
The O
SNIPS B-DatasetName
( O
Coucke O
et O
al O
. O
, O
2018 O
) O
and O
ATIS B-DatasetName
( O
Dahl O
et O
al O
. O
, O
1994 O
) O
datasets O
consist O
of O
concise O
single O
- O
sentence O
texts O
containing O
intents O
with O
constrained O
vocabulary O
and O
attributes O
. O
Several O
works O
have O
explored O
intent O
classification O
of O
internet O
posts O
in O
the O
context O
of O
racial O
/ O
radicalized O
intent O
( O
Agarwal O
and O
Sureka O
, O
2016 O
) O
, O
purchase O
intents O
( O
Gupta O
et O
al O
. O
, O
2014 O
; O
Wang O
et O
al O
. O
, O
2015 O
) O
, O
discussion O
forums O
( O
Chen O
et O
al O
. O
, O
2013 O
) O
and O
health O
queries O
( O
Cai O
et O
al O
. O
, O
2017 O
) O
. O
Vedula O
et O
al O
. O
( O
2019 O
) O
propose O
open O
intent O
discovery O
with O
unconstrained O
vocabulary O
as O
a O
sequence O
tagging O
task O
. O
Using O
this O
framework O
, O
we O
present O
our O
dataset O
on O
instructional O
intents O
. O

In O
the O
wake O
of O
exploding O
visual O
social O
- O
media O
content O
, O
several O
image O
- O
based O
multi O
- O
modal O
intent O
datasets O
have O
been O
previously O
proposed O
. O
Kiela O
et O
al O
. O
( O
2020 O
) O
; O
Aprosio O
et O
al O
. O
( O
2020 O
) O
study O
abusive O
language O
and O
hateful O
intent O
in O
memes O
and O
photo O
posts O
. O
Jia O
et O
al O
. O
( O
2021 O
) O
explore O
intent O
categories O
derived O
from O
social O
psychology O
and O
use O
object O
localization O
to O
integrate O
visual O
context O
in O
task O
models O
. O
Instagram O
posts O
are O
another O
interesting O
source O
for O
multimodal O
content O
( O
Chen O
and O
Hsieh O
, O
2020 O
; O
Kruk O
et O
al O
. O
, O
2019 O
) O
. O
We O
introduce O
the O
task O
of O
video O
- O
based O
multimodal O
intent O
discovery O
, O
which O
has O
been O
unexplored O
. O

Several O
tasks O
have O
been O
proposed O
in O
the O
recent O
years O
to O
probe O
joint O
video O
and O
text O
understanding O
. O
Lei O
et O
al O
. O
( O
2018 O
) O
; O
; O
Maharaj O
et O
al O
. O
( O
2017 O
) O
; O
Jang O
et O
al O
. O
( O
2017 O
) O
; O
Tapaswi O
et O
al O
. O
( O
2016 O
) O
and O
introduce O
video O
- O
based O
question O
answering O
datasets O
created O
from O
various O
sources O
of O
creative O
visual O
content O
, O
i.e. O
movies O
, O
TV O
shows O
, O
GIFs O
etc O
. O
Lei O
et O
al O
. O
( O
2020b O
) O
and O
Lei O
et O
al O
. O
( O
2020a O
) O
propose O
the O
task O
of O
video O
- O
moment O
retrieval O
and O
next O
frame O
prediction O
respectively O
, O
based O
on O
query O
subtitles O
, O
while O
present O
the O
multimodal O
version O
of O
natural O
language O
inference O
. O
Early O
models O
for O
performing O
these O
tasks O
involve O
combining O
pretrained O
image O
representations O
from O
sparsely O
sampled O
videos O
, O
and O
text O
encodings O
from O
pretrained O
encoders O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
in O
architectures O
for O
modelling O
global O
- O
local O
interactions O
( O
Zhu O
and O
Yang O
, O
2020 O
; O
, O
temporal O
localization O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
graph O
- O
based O
reasoning O
etc O
. O
More O
recent O
attempts O
involve O
pretraining O
models O
on O
large O
video+text O
corpora O
( O
Miech O
et O
al O
. O
, O
2019 O
) O
and O
finetuning O
on O
downstream O
tasks O
( O
Sun O
et O
al O
. O
, O
2019 O
; O
Cho O
et O
al O
. O
, O
2021 O
; O
Tang O
et O
al O
. O
, O
2021 O
; O
. O
We O
explore O
late O
- O
fusion O
of O
video O
and O
text O
embeddings O
for O
intent O
detection O
in O
pretrained O
and O
non O
- O
pretrained O
multimodal O
settings O
. O

Behance B-DatasetName
Datasets I-DatasetName

Dataset O
Collection O
. O
We O
first O
obtain O
2,049 O
videos O
along O
with O
their O
transcripts O
and O
tool O
timelines O
from O
the O
Behance O
platform O
. O
The O
tool O
timeline O
contains O
a O
time O
- O
stamped O
record O
of O
the O
tools O
used O
in O
the O
software O
during O
the O
tutorial O
. O
The O
average O
session O
length O
is O
80 O
minutes O
with O
an O
average O
of O
587 O
transcribed O
phrases O
per O
session O
( O
see O
Table O
1 O
) O
. O
The O
tool O
timelines O
contain O
282 O
distinct O
tools O
with O
varying O
frequencies O
; O
Color O
, O
Select O
Brush O
, O
Select O
Layer O
are O
some O
of O
the O
most O
frequent O
ones O
. O
The O
instructional O
software O
- O
based O
domain O
of O
this O
dataset O
is O
significantly O
different O
from O
existing O
large O
corpora O
drawn O
from O
YouTube O
instructional O
videos O
( O
Miech O
et O
al O
. O
, O
2019 O
) O
and O
TV O
content O
( O
Lei O
et O
al O
. O
, O
2018 O
( O
Lei O
et O
al O
. O
, O
, O
2020b O
) O
, O
but O
it O
is O
an O
important O
learning O
resource O
. O
Hence O
, O
we O
include O
the O
unlabelled O
Behance O
Livestreams O
corpus O
as O
an O
addition O
to O
the O
pool O
of O
video+text O
corpora O
that O
can O
be O
leveraged O
for O
continued O
pretraining O
of O
multimodal O
models O
and O
finetuning O
on O
downstream O
tasks O
relevant O
to O
software O
- O
based O
livestream O
videos O
. O

In O
order O
to O
prepare O
the O
intent O
discovery O
dataset O
, O
we O
extract O
candidate O
intent O
phrases O
from O
the O
transcripts O
of O
the O
Behance B-DatasetName
Livestream I-DatasetName
corpus I-DatasetName
. O
Following O
Vedula O
et O
al O
. O
( O
2019 O
) O
, O
we O
define O
an O
intent O
as O
a O
text O
phrase O
consisting O
of O
: O
( O
i O
) O
an O
action O
word O
or O
phrase O
, O
which O
constitutes O
a O
definite O
task O
, O
goal O
or O
activity O
and O
( O
ii O
) O
an O
object O
, O
which O
represents O
those O
words O
or O
phrases O
that O
the O
action O
is O
going O
to O
act O
or O
operate O
upon O
. O
We O
generate O
the O
dependency O
graph O
of O
sentences O
, O
and O
extract O
the O
VERB O
node O
as O
action O
and O
the O
direct O
object O
of O
the O
VERB O
as O
the O
object O
, O
along O
with O
all O
other O
children O
nodes O
( O
see O
example O
in O
Fig O
. O
2 O
) O
. O
3 O
Through O
manual O
analysis O
, O
we O
identified O
two O
major O
categories O
of O
meaningful O
intent O
: O
tool O
and O
creative O
. O
Tool O
intents O
are O
low O
- O
level O
intents O
that O
can O
be O
typically O
mapped O
to O
a O
single O
tool O
in O
the O
software O
. O
Creative O
intents O
are O
abstract O
intents O
used O
to O
describe O
a O
high O
- O
level O
creative O
goal O
that O
consist O
of O
a O
complex O
set O
of O
actions O
or O
tool O
intents O
. O
For O
instance O
, O
in O
Fig O
. O
3 O
, O
" O
make O
a O
new O
layer O
" O
is O
a O
tool O
intent O
that O
can O
be O
mapped O
to O
the O
tool O
Create O
Layer O
, O
while O
" O
add O
more O
plants O
and O
stuff O
" O
is O
a O
creative O
intent O
. O
All O
other O
intents O
in O
the O
corpus O
, O
predominantly O
from O
chit O
- O
chat O
statements O
, O
are O
irrelevant O
to O
our O
task O
. O
We O
frame O
the O
task O
of O
intent O
discovery O
as O
a O
sequencetagging O
problem O
and O
tag O
the O
intent O
phrases O
within O
each O
sentence O
with O
IOB O
( O
inside O
, O
outside O
, O
beginning O
) O
span O
annotations O
for O
the O
two O
classes O
: O
tool O
and O
creative O
intents O
. O
Each O
sample O
consists O
of O
a O
timestamped O
sentence O
with O
span O
annotations O
and O
the O
video O
session O
it O
is O
extracted O
from O
. O
Based O
on O
the O
above O
defined O
framework O
, O
we O
col- O
Dataset O
Analysis O
. O
We O
analyzed O
tool O
and O
creative O
intents O
to O
find O
the O
most O
frequent O
, O
unique O
verbs O
and O
nouns O
mentioned O
in O
the O
phrases O
. O
While O
there O
are O
action O
verbs O
which O
are O
distinctly O
tool O
- O
specific O
, O
such O
as O
merge O
, O
select O
, O
and O
duplicate O
, O
there O
are O
many O
verbs O
which O
are O
common O
to O
both O
tool O
and O
creative O
intents O
such O
as O
add O
, O
make O
, O
and O
paint O
. O
Hence O
, O
the O
task O
model O
needs O
to O
learn O
the O
difference O
between O
tool O
and O
creative O
intents O
to O
be O
able O
to O
classify O
intents O
with O
similar O
action O
verbs O
into O
the O
correct O
categories O
. O
Further O
, O
we O
examined O
the O
unique O
nouns O
occurring O
in O
the O
intents O
and O
found O
lesser O
overlap O
between O
the O
two O
intent O
classes O
. O
Creative O
intents O
contain O
abstract O
and O
subjective O
visual O
concepts O
which O
pose O
a O
unique O
and O
interesting O
challenge O
to O
multimodal O
models O
. O
See O
Appendix O
for O
probing O
experiments O
. O

Methods O
for O
Intent O
Discovery O

Intuitively O
, O
as O
in O
a O
lot O
of O
instructional O
sources O
like O
text O
books O
, O
the O
text O
or O
audio O
serves O
as O
the O
primary O
mode O
of O
high O
- O
level O
information O
transfer O
, O
while O
the O
video O
/ O
image O
signal O
provides O
detailed O
context O
or O
demonstration O
. O
Thus O
, O
we O
start O
our O
exploration O
using O
text O
models O
, O
which O
are O
built O
on O
two O
pretrained O
models O
: O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
and O
GPT2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
. O
There O
are O
several O
previous O
works O
focusing O
on O
a O
limited O
set O
of O
intents O
( O
Xia O
et O
al O
. O
, O
2018 O
) O
, O
and O
thus O
, O
treat O
the O
problem O
of O
intent O
discovery O
as O
a O
classification O
problem O
. O
In O
our O
case O
, O
given O
the O
vast O
possibilities O
of O
potential O
intents O
in O
our O
sources O
, O
we O
cast O
the O
problem O
as O
a O
span O
detection O
problem O
, O
and O
design O
our O
models O
accordingly O
. O

Unimodal O
Sequence O
Labelling O

Our O
text O
models O
are O
designed O
similar O
to O
Named O
- O
Entity O
- O
Recognition O
models O
with O
a O
pretrained O
embedding O
layer O
and O
a O
sequence O
classification O
layer O
on O
top O
. O
Each O
phrase O
in O
the O
transcript O
is O
annotated O
separately O
in O
the O
intent O
dataset O
, O
leading O
to O
efficient O
processing O
. O
Although O
it O
is O
possible O
to O
process O
longer O
spans O
of O
text O
, O
in O
our O
annotations O
, O
we O
found O
out O
that O
each O
sentence O
usually O
gives O
enough O
information O
to O
extract O
the O
intent O
inside O
it O
, O
and O
extra O
context O
( O
neighboring O
sentences O
) O
does O
not O
significantly O
help O
the O
decision O
. O
We O
denote O
an O
input O
sentence O
as O
X O
= O
[ O
x O
0 O
, O
... O
, O

Multimodal O
Sequence O
Labelling O
: O
Naïve O
Fusion O

Seeking O
to O
leverage O
the O
video O
information O
, O
in O
our O
first O
attempt O
, O
we O
tried O
a O
simple O
feature O
fusion O
between O
the O
text O
signal O
and O
the O
video O
signal O
in O
the O
sequence O
labelling O
framework O
. O
We O
add O
a O
crossattention O
layer O
on O
top O
of O
the O
pretrained O
text O
encoder O
in O
this O
naïve O
joint O
video O
- O
text O
model O
and O
use O
the O
output O
of O
the O
cross O
- O
attention O
layer O
for O
sequence O
label O
classification O
. O
Let O
's O
denote O
the O
video O
features O
as O
V O
. O

Our O
model O
( O
see O
Fig O
. O
4 O
( O
a O
) O
) O
is O
described O
as O
follows O
: O

Z O
= O
sof O
tmax O
( O
W O
c O
* O
f O
self O
( O
f O
cross O
( O
E O
, O
V O
) O
) O
+ O
b O
c O
) O

where O
E O
, O
f O
self O
and O
f O
cross O
are O
text O
encodings O
, O
self O
- O
attention O
and O
cross O
- O
attention O
layers O
respectively O
. O
This O
naïve O
fusion O
model O
, O
however O
, O
does O
not O
provide O
any O
significant O
improvements O
compared O
to O
text O
- O
only O
baselines O
( O
see O
Sec O
. O
7 O
) O
. O
Analysis O
of O
the O
results O
revealed O
that O
the O
textual O
features O
dominate O
the O
final O
decision O
, O
especially O
in O
the O
creative O
intent O
classes O
. O
To O
understand O
this O
behaviour O
, O
we O
performed O
a O
pilot O
task O
in O
which O
a O
human O
annotator O
looks O
through O
the O
video O
segments O
and O
tries O
to O
guess O
the O
intent O
without O
any O
transcript O
or O
audio O
. O
Our O
annotator O
found O
the O
task O
very O
difficult O
, O
and O
only O
possible O
after O
watching O
a O
very O
long O
context O
window O
, O
which O
partially O
explains O
the O
low O
performance O
of O
this O
model O
. O
The O
video O
signal O
is O
much O
more O
ambiguous O
than O
the O
text O
signal O
, O
and O
when O
presented O
with O
two O
sources O
where O
one O
is O
vastly O
less O
informative O
than O
the O
other O
, O
the O
model O
learns O
to O
rely O
only O
on O
the O
text O
, O
leading O
to O
no O
improvement O
compared O
to O
the O
text O
- O
only O
baseline O
. O
Joining O
two O
sources O
of O
features O
with O
different O
predictive O
utility O
is O
difficult O
. O
Given O
the O
fact O
that O
the O
video O
feature O
extractor O
is O
not O
trained O
on O
similar O
data O
, O
the O
video O
feature O
might O
not O
contain O
enough O
information O
for O
a O
direct O
intent O
detection O
task O
. O
Fortunately O
, O
our O
pilot O
task O
also O
reveals O
an O
important O
insight O
, O
i.e. O
, O
the O
video O
signal O
is O
good O
at O
identifying O
whether O
an O
intent O
is O
present O
or O
not O
. O
Many O
intent O
candidates O
identified O
by O
the O
text O
models O
are O
not O
creative O
or O
tool O
intents O
, O
but O
are O
chitchat O
utterances O
from O
the O
instructor O
interacting O
with O
the O
audience O
. O
In O
these O
cases O
, O
we O
posit O
that O
the O
inactivity O
presented O
in O
the O
video O
signal O
is O
a O
strong O
indication O
that O
a O
creative O
/ O
tool O
intent O
does O
not O
occur O
at O
the O
current O
time O
window O
. O
Using O
this O
idea O
, O
we O
propose O
a O
cascaded O
model O
with O
deeper O
interaction O
between O
video O
and O
text O
signal O
. O

Cascaded O
Cross O
- O
Attention O
& O
Late O
Fusion O

Using O
the O
intuition O
that O
the O
text O
signal O
would O
provide O
candidates O
for O
the O
vision O
model O
, O
which O
is O
subsequently O
used O
for O
filtering O
out O
the O
cases O
without O
intent O
, O
we O
design O
the O
cascade O
cross O
- O
attention O
model O
as O
follows O
: O
First O
, O
we O
extract O
the O
set O
of O
contextualized O
embeddings O
E O
from O
the O
text O
encoder O
f O
enc O
and O
transform O
it O
through O
two O
self O
- O
attention O
layers O
to O
create O
a O
two O
- O
stream O
architecture O
( O
see O
Fig O
. O
4 O
) O
. O
In O
the O
first O
stream O
, O
the O
text O
encodings O
are O
processed O
through O
a O
single O
- O
layer O
of O
self O
- O
attention O
to O
produce O
E O
1 O
. O
In O
the O
second O
stream O
, O
the O
output O
from O
selfattention O
i.e. O
E O
2 O
, O
is O
combined O
with O
video O
embeddings O
through O
a O
cascaded O
cross O
- O
attention O
module O
. O

Let O

V O
= O
[ O
v O
1 O
, O
v O
2 O
, O
... O
, O
v O
k O
] O

be O
the O
input O
sequence O
of O
video O
embeddings O
. O
The O
cascaded O
module O
contains O
three O
cross O
- O
attention O
layers O
: O
video O
- O
to O
- O
text O
f O
v2 O
t O
( O
• O
) O
, O
text O
- O
to O
- O
video O
f O
t2v O
( O
• O
) O
and O
text O
- O
to O
- O
text O
cross O
- O
attention O
f O
t2 O
t O
( O
• O
) O
, O
with O
outputs O
computed O
as O
: O

S O
1 O
= O
f O
v2 O
t O
( O
W O
m O
V O
+ O
b O
m O
, O
E O
2 O
) O
S O
2 O
= O
f O
t2 O
t O
( O
E O
2 O
, O
S O
1 O
) O
S O
3 O
= O
f O
t2i O
( O
E O
s2 O
, O
W O
m O
V O
+ O
b O
m O
) O

where O
W O
m O
, O
b O
m O
are O
the O
parameters O
of O
a O
linear O
layer O
for O
transforming O
video O
embeddings O
. O
Next O
, O
the O
outputs O
from O
cross O
- O
attention O
layers O
are O
concatenated O
, O
linearly O
mapped O
and O
transformed O
into O
0 O
- O
1 O
values O
using O
a O
sigmoid O
, O
to O
generate O
the O
visual O
gate O
( O
see O
Fig O
. O
4 O
( O
c O
) O
) O
. O
Finally O
, O
the O
output O
from O
cross O
- O
attention O
layer O
is O
multiplied O
with O
this O
gate O
, O
i.e O
. O

S O
gate O
= O
sigmoid O
( O
W O
g O
[ O
S O
2 O
; O
S O
3 O
] O
+ O
b O
g O
) O
S O
clf O
= O
[ O
S O
gate O
* O
S O
3 O
; O
E O
s1 O
] O

The O
visual O
gate O
is O
dynamically O
computed O
using O
the O
contextualized O
video O
representations O
and O
is O
used O
to O
trim O
the O
video O
signal O
to O
the O
relevant O
bits O
. O
This O
helps O
in O
regulating O
the O
contribution O
of O
the O
two O
modalities O
for O
the O
final O
prediction O
as O
per O
the O
input O
. O
The O
concatenation O
represents O
the O
late O
- O
fusion O
of O
text O
- O
only O
embeddings O
and O
video O
- O
contextualized O
text O
embeddings O
. O
This O
merged O
representation O
is O
then O
sent O
to O
the O
classifier O
layer O
for O
classification O
i.e. O
Z O
= O
sof O
tmax O
( O
W O
c O
* O
S O
clf O
+ O
b O
c O
) O
. O

Sequence O
Labelling O
with O
Joint O
Video O
- O
Text O
Pretraining O

In O
order O
to O
leverage O
joint O
modelling O
of O
video O
and O
text O
modalities O
through O
large O
- O
scale O
pretraining O
, O
we O
adapt O
the O
pretrained O
HERO B-MethodName
( O
Li O
et O
al O
. O
, O
2020 O
) O
and O
ClipBERT B-MethodName
for O
global O
contextualization O
. O
Thus O
, O
the O
output O
is O
: O

S O
temp O
= O
f O
temp O
( O
[ O
V O
cross O
; O
W O
q O
emb O
] O
) O
S O
out O
= O
S O
temp O
[ O
N O
v O
: O
( O
N O
v O
+ O
N O
t O
) O
, O
: O
] O

where O
N O
v O
and O
N O
t O
are O
the O
number O
of O
frames O
and O
tokens O
in O
video O
and O
query O
respectively O
. O
The O
output O
of O
f O
temp O
is O
masked O
to O
select O
the O
representations O
pertaining O
to O
the O
query O
only O
. O
In O
the O
naïve O
fusion O
setting O
, O
S O
out O
is O
then O
sent O
to O
the O
classifier O
layer O
. O

ClipBERT B-MethodName
. O
Similarly O
, O
the O
output O
S O
out O
from O
the O
Cross O
- O
modal O
Transformer O
f O
cross O
in O
ClipBERT B-MethodName
is O
masked O
and O
sent O
to O
the O
classifier O
layer O
for O
prediction O
i.e. O
S O
out O
= O
f O
cross O
( O
[ O
V O
; O
W O
q O
emb O
] O
) O
[ O
: O
N O
t O
, O
: O
] O
. O
Late O
Fusion O
. O
We O
integrate O
the O
late O
fusion O
approach O
into O
HERO B-MethodName
and O
ClipBERT B-MethodName
as O
follows O
: O

S O
gate O
= O
sigmoid O
( O
W O
g O
* O
S O
out O
+ O
b O
g O
) O
S O
clf O
= O
[ O
S O
gate O
* O
S O
out O
; O
W O
q O
emb O
] O

where O
the O
visual O
gate O
is O
computed O
as O
in O
Sec O
. O
5.3 O
( O
see O
Fig O
. O
4 O
) O
and O
S O
clf O
is O
sent O
to O
the O
classifier O
layer O
. O

Experiments O

Evaluation O
. O
Since O
the O
transcribed O
phrases O
in O
Behance O
Livestreams O
are O
the O
result O
of O
an O
automatic O
speech O
recognition O
( O
ASR O
) O
system O
, O
the O
exact O
span O
match O
metrics O
might O
be O
distorted O
by O
ASR O
errors O
. O
Hence O
, O
we O
use O
a O
more O
lenient O
75 O
% O
partial O
matchbased O
Precision O
/ O
Recall O
/ O
F O
- O
score O
metric O
i.e. O
, O
if O
there O
is O
more O
than O
75 O
% O
overlap O
between O
the O
ground O
truth O
and O
predicted O
span O
, O
we O
consider O
it O
as O
a O
match O
. O

Video O
Representations O
. O
We O
experiment O
with O
3D B-MethodName
ResNext-101 I-MethodName
( O
Xie O
et O
al O
. O
, O
2017 O
) O
( O
f O
ps=6 O
) O
, O
SlowFast B-MethodName
( O
Feichtenhofer O
et O
al O
. O
, O
2019 O
) O
( O
clip O
length=2s O
) O
and O
2D B-MethodName
ResNet-152 I-MethodName
( O
He O
et O
al O
. O
, O
2016 O
) O
( O
clip O
length=2s O
) O
following O
preprocessing O
steps O
in O
Li O
et O
al O
. O
( O
2020 O
) O
. O

Models O
. O
We O
use O
the O
RoBERTa B-MethodName
LARGE I-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
models O
for O
the O
unimodal O
experiments O
, O
as O
well O
as O
the O
multimodal O
experiments O
that O
are O
based O
on O
unimodal O
pretrained O
models O
. O
We O
use O
the O
pretrained O
HERO B-MethodName
( O
Li O
et O
al O
. O
, O
2020 O
) O
and O
ClipBERT B-MethodName
in O
the O
remaining O
experiments O
; O
their O
language O
encoders O
are O
initialized O
from O
pretrained O
RoBERTa B-MethodName
BASE I-MethodName
and O
BERT B-MethodName
BASE I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
models O
. O
Each O
model O
is O
trained O
end O
- O
toend O
using O
fully O
- O
supervised O
training O
and O
is O
subjected O
to O
grid O
- O
search O
based O
hyper O
- O
parameter O
optimization O
. O
The O
best O
checkpoints O
are O
selected O
based O
on O
overall O
F O
- O
Score O
. O
See O
Appendix O
for O
bounds O
. O

Results O

In O
this O
section O
, O
we O
discuss O
results O
from O
various O
models O
on O
the O
Behance B-DatasetName
Intent I-DatasetName
Discovery I-DatasetName
dataset O
( O
see O
Table O
4 O
) O
. O

The O
text O
baselines O
. O
Starting O
with O
the O
text O
- O
only O
baselines O
, O
we O
see O
the O
best O
performance O
from O
the O
RoBERTa B-MethodName
models O
, O
i.e. O
, O
58 B-MetricValue
% I-MetricValue
and O
27 B-MetricValue
% I-MetricValue
partial B-MetricName
match I-MetricName
F I-MetricName
- I-MetricName
scores I-MetricName
on O
the O
tool O
and O
creative O
intents O
, O
respectively O
( O
rows O
2 O
and O
3 O
in O
Table O
4 O
) O
. O
Notably O
, O
the O
tool O
intent O
predictor O
is O
biased O
with O
high O
recall B-MetricName
but O
low O
precision B-MetricName
performance O
i.e. O
it O
retains O
too O
many O
candidates O
, O
many O
of O
which O
do O
not O
correspond O
to O
any O
intents O
. O
These O
results O
also O
demonstrate O
that O
large O
pretrained O
language O
models O
like O
RoBERTa B-MethodName
and O
GPT2 B-MethodName
struggle O
to O
comprehend O
the O
abstract O
ideas O
represented O
in O
creative O
intents O
. O

The O
Naïve O
Fusion O
models O
. O
The O
Naïve O
Fusion O
approach O
with O
pretrained O
RoBERTa B-MethodName
yields O
upto O
2 O
% O
improvement O
over O
the O
text O
- O
only O
baselines O
. O
In O
some O
cases O
, O
such O
as O
the O
3D B-MethodName
ResNext I-MethodName
representations O
, O
this O
approach O
degrades O
the O
performance O
, O
especially O
in O
the O
harder O
creative O
intent O
set O
. O
We O
attribute O
this O
to O
the O
difference O
in O
informativeness O
between O
the O
text O
and O
the O
video O
signal O
, O
as O
discussed O
in O
Sec O
. O
5.2 O
. O

The O
Late O
Fusion O
models O
. O
With O
the O
Late O
Fusion O
approach O
, O
we O
see O
significant O
improvements O
in O
almost O
all O
cases O
. O
Compared O
to O
the O
corresponding O
Naïve O
Fusion O
models O
, O
Late O
Fusion O
models O
mainly O
improve O
precision O
for O
tool O
intents O
. O
This O
result O
supports O
our O
hypothesis O
that O
the O
video O
signal O
is O
most O
useful O
as O
a O
gate O
to O
filter O
out O
non O
- O
intent O
candidates O
from O
the O
textual O
signal O
. O
The O
SlowFast B-MethodName
representations O
prove O
especially O
beneficial O
for O
creative O
intents O
, O
as O
seen O
in O
row O
9 O
in O
Table O
4 O
. O
With O
the O
use O
of O
multimodal O
pretrained O
models O
like O
HERO B-MethodName
and O
ClipBERT B-MethodName
, O
we O
observe O
significant O
improvements O
in O
prediction O
of O
tool O
intents O
and O
smaller O
improvements O
for O
creative O
intents O
with O
a O
simple O
adaptation O
of O
the O
prediction O
head O
for O
sequence O
labelling O
( O
see O
Sec O
. O
5.4 O
) O
. O
HERO B-MethodName
uses O
video O
representations O
from O
pretrained O
encoders O
while O
ClipBERT B-MethodName
operates O
on O
raw O
videos O
; O
both O
approaches O
work O
well O
with O
the O
software O
- O
based O
video O
domain O
yielding O
upto O
3 B-MetricValue
% I-MetricValue
and O
1 B-MetricValue
% I-MetricValue
improvement O
on O
tool O
intents O
respectively O
( O
rows O
10 O
, O
12 O
in O
Table O
4 O
) O
over O
the O
unimodal O
RoBERTa B-MethodName
models O
. O
Larger O
improvements O
are O
seen O
from O
further O
augmenting O
these O
models O
with O
late O
fusion O
i.e. O
1 B-MetricValue
% I-MetricValue
improvement O
on O
tool O
intents O
( O
rows O
11 O
, O
12 O
in O
Table O
4 O
) O
. O
The O
late O
fusion O
RoBERTa B-MethodName
model O
using O
SlowFast B-MethodName
features O
( O
row O
9 O
in O
Table O
4 O
) O
performs O
best O
for O
creative O
intents O
, O
with O
3 B-MetricValue
% I-MetricValue
improvement O
over O
the O
text O
- O
only O
baseline O
. O

We O
see O
similar O
trends O
from O
experiments O
on O
the O
validation O
set O
of O
the O
Behance B-DatasetName
Intent I-DatasetName
Discovery I-DatasetName
dataset O
. O
See O
results O
in O
Appendix O
. O

Analysis O
& O
Discussion O

In O
this O
section O
, O
we O
perform O
qualitative O
analysis O
of O
the O
late O
fusion O
approach O
and O
examine O
the O
effect O
of O
video O
clip O
length O
. O
We O
also O
discuss O
a O
semiautomated O
approach O
to O
creating O
annotations O
for O
intent O
extraction O
and O
use O
the O
data O
in O
combination O
with O
manual O
annotations O
for O
improved O
results O
. O
See O
Appendix O
for O
more O
analyses O
. O

Qualitative O
Analysis O

In O
order O
to O
understand O
the O
inner O
workings O
of O
the O
late O
fusion O
architecture O
, O
we O
examine O
the O
cross O
- O
attention O
and O
visual O
gate O
modules O
of O
the O
RoBERTa+Late B-MethodName
Fusion I-MethodName
model O
trained O
with O
2D O
ResNet O
features O
. O
Each O
row O
of O
the O
attention O
score O
matrix O
M O
∈ O
R O
n×f O
( O
for O
n O
tokens O
and O
f O
video O
segments O
) O
in O
text O
- O
to O
- O
video O
cross O
- O
attention O
module O
corresponds O
to O
the O
temporal O
attention O
over O
video O
clips O
( O
represented O
by O
a O
sequence O
of O
ResNet O
feature O
vectors O
) O
for O
a O
given O
token O
. O
We O
plot O
this O
score O
matrix O
for O
the O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
in O
the O
RoBERTa B-MethodName
model O
in O
Figures O
5 O
( O
a O
) O
and O
7 O
( O
a O
) O
. O
The O
attention O
heads O
are O
activated O
in O
the O
intent O
region O
suggesting O
a O
strong O
interaction O
between O
two O
modalities O
in O
important O
segments O
of O
the O
video O
. O

To O
understand O
how O
the O
video O
signal O
helps O
the O
prediction O
, O
we O
first O
plot O
the O
mean O
and O
standard O
deviation O
of O
visual O
gate O
values O
( O
S O
gate O
) O
for O
each O
token O
in O
Figures O
5 O
( O
b O
) O
and O
7 O
( O
b O
) O
. O
Results O
show O
that O
the O
visual O
gate O
preferentially O
relies O
on O
the O
video O
modality O
for O
tokens O
outside O
the O
intent O
span O
. O
Furthermore O
, O
in O
Fig O
. O
6 O
, O
we O
show O
example O
phrases O
where O
the O
text O
only O
model O
classifies O
wrongly O
as O
intent O
while O
the O
joint O
model O
does O
not O
. O
The O
phrases O
themselves O
appear O
to O
be O
intent O
but O
the O
lack O
of O
action O
in O
the O
visual O
frame O
indicates O
that O
these O
are O
chit O
- O
chat O
interactions O
. O
Both O
analyses O
support O
our O
hypothesis O
that O
the O
late O
fusion O
model O
utilizes O
the O
video O
signal O
to O
filter O
intent O
candidates O
and O
improve O
precision O
. O

Video O
Clip O
Length O

As O
we O
discuss O
in O
Sec O
. O
4 O
, O
the O
video O
clip O
durations O
for O
the O
tool O
and O
creative O
intents O
are O
not O
specified O
. O
We O
observe O
that O
the O
intended O
action O
can O
span O
anywhere O
between O
1 O
second O
to O
several O
minutes O
. O
Longer O
clip O
lengths O
are O
relevant O
for O
many O
creative O
intents O
like O
" O
make O
it O
into O
something O
fantasy O
" O
, O
" O
add O
the O
arm O
to O
this O
little O
guy O
" O
, O
etc O
. O
Hence O
, O
we O
experiment O
with O
various O
clip B-HyperparameterName
lengths I-HyperparameterName
( O
10 B-HyperparameterValue
, O
20 B-HyperparameterValue
and O
60 B-HyperparameterValue
secs O
) O
, O
but O
find O
that O
larger O
clip O
lengths O
do O
not O
lead O
to O
further O
improvements O
. O
In O
fact O
, O
with O
60 O
second O
clips O
the O
performance O
of O
RoBERTa+Late B-MethodName
Fusion I-MethodName
model O
drops O
below O
the O
performance O
of O
text O
- O
only O
RoBERTa B-MethodName
. O
This O
issue O
could O
be O
alleviated O
with O
long O
- O
range O
video O
understanding O
models O
( O
Sener O
et O
al O
. O
, O
2020 O
) O
. O

Semi O
- O
automated O
Intent O
Annotations O

Since O
manual O
annotation O
of O
procedural O
intents O
is O
time O
- O
intensive O
and O
expensive O
, O
we O
explore O
a O
semiautomatic O
pipeline O
for O
creation O
of O
intent O
annotations O
. O
The O
Behance B-DatasetName
Livestreams I-DatasetName
corpus I-DatasetName
contains O
tool O
timelines O
for O
each O
livestream O
, O
which O
enumerates O
the O
tools O
used O
within O
the O
software O
at O
different O
points O
in O
the O
livestream O
. O
We O
compute O
the O
tf O
- O
idf O
scores O
for O
co O
- O
occurrence O
of O
896 O
, O
287 O
action O
- O
object O
phrases O
( O
from O
dependency O
parses O
of O
sentences O
) O
and O
corresponding O
tools O
in O
the O
tool O
timelines O
, O
in O
order O
to O
find O
the O
phrases O
that O
are O
frequently O
used O
for O
describing O
particular O
tool O
actions O
, O
such O
as O
" O
grab O
the O
smudge O
tool O
" O
. O
After O
filtering O
the O
phrases O
for O
those O
with O
high O
tf O
- O
idf O
scores O
, O
the O
pool O
of O
intent O
candidates O
was O
further O
cleaned O
manually O
, O
resulting O
in O
a O
final O
set O
of O
3,697 O
tool O
intent O
candidates O
. O
Using O
this O
pool O
of O
candidates O
, O
24,300 O
phrases O
from O
the O
Behance B-DatasetName
Livestreams I-DatasetName
corpus I-DatasetName
were O
identified O
as O
tool O
intent O
samples O
. O
Since O
it O
is O
not O
straightforward O
to O
extract O
creative O
intents O
using O
similar O
methods O
, O
we O
first O
identified O
key O
phrases O
for O
creative O
intents O
from O
the O
set O
of O
action O
- O
object O
phrases O
with O
high O
term O
frequency O
. O
We O
then O
subjected O
it O
to O
manual O
cleaning O
( O
two O
annotators O
per O
sample O
; O
κ=0.986 O
) O
followed O
by O
embedding O
similarity O
to O
select O
creative O
intents O
( O
see O
Appendix O
for O
full O
pipeline O
) O
. O
Using O
this O
method O
, O
we O
recovered O
7,135 O
phrases O
containing O
creative O
intents O
. O

We O
use O
these O
semi O
- O
automatically O
collected O
annotations O
as O
additional O
training O
data O
in O
our O
experiments O
with O
Late B-MethodName
Fusion I-MethodName
RoBERTa I-MethodName
models O
. O
Since O
the O
manually O
annotated O
Behance B-DatasetName
Intent I-DatasetName
Discovery I-DatasetName
dataset O
is O
skewed O
towards O
negative O
samples O
i.e. O
< O
25 B-MetricValue
% I-MetricValue
samples O
contain O
intent O
, O
we O
balance O
the O
training O
data O
by O
adding O
5,000 O
samples O
( O
containing O
tool O
or O
creative O
intents O
) O
from O
the O
aforementioned O
semi O
- O
automatically O
annoated O
dataset O
to O
it O
. O
With O
this O
balanced O
data O
, O
we O
see O
upto O
2 B-MetricValue
% I-MetricValue
improvement O
in O
the O
Late B-MethodName
Fusion I-MethodName
RoBERTa I-MethodName
models O
. O
See O
Appendix O
. O

Conclusion O

In O
this O
paper O
, O
we O
explore O
the O
novel O
task O
of O
videobased O
multimodal O
intent O
discovery O
. O
We O
present O
the O
unlabelled O
Behance B-DatasetName
Livestream I-DatasetName
corpus I-DatasetName
consisting O
of O
instructional O
videos O
for O
software O
tools O
, O
and O
the O
Behance B-DatasetName
Intent I-DatasetName
Discovery I-DatasetName
dataset O
annotated O
with O
tool O
and O
creative O
intents O
. O
We O
propose O
a O
late O
- O
fusion O
approach O
for O
integration O
of O
the O
video O
signal O
with O
the O
text O
signal O
in O
a O
controlled O
manner O
for O
this O
task O
, O
and O
show O
significant O
improvements O
with O
unimodal O
and O
multimodal O
pretrained O
models O
. O

Acknowledgements O

We O
would O
like O
to O
thank O
Tracy O
King O
for O
her O
detailed O
feedback O
and O
Hailin O
Jin O
for O
making O
the O
Behance O
transcript O
available O
. O
We O
would O
also O
like O
to O
thank O
the O
reviewers O
for O
their O
useful O
feedback O
. O
This O
work O
was O
partially O
done O
while O
AM O
was O
interning O
at O
Adobe O
Research O
and O
later O
extended O
at O
UNC O
, O
where O
it O
was O
supported O
by O
ARO O
Award O
W911NF2110220 O
and O
DARPA O
KAIROS O
Grant O
FA8750 O
- O
19 O
- O
2 O
- O
1004 O
. O
The O
views O
contained O
in O
this O
article O
are O
those O
of O
the O
authors O
and O
not O
of O
the O
funding O
agency O
. O

Ethics O
/ O
Broader O
Impacts O

From O
an O
ethics O
standpoint O
, O
we O
provide O
a O
detailed O
overview O
of O
the O
methods O
used O
to O
create O
the O
Behance B-DatasetName
Livestreams I-DatasetName
corpus O
and O
Behance B-DatasetName
Intent I-DatasetName
Discovery I-DatasetName
dataset O
in O
Sec O
. O
4 O
and O
more O
details O
in O
the O
Appendix O
. O
We O
also O
provide O
some O
analyses O
of O
the O
data O
in O
Table O
3 O
. O
All O
of O
the O
language O
data O
consists O
of O
simple O
English O
sentences O
. O
The O
dataset O
comprises O
livestreamed O
video O
tutorials O
by O
users O
of O
the O
Behance O
platform O
. O
Behance O
users O
grant O
full O
usage O
rights O
of O
their O
content O
and O
agree O
to O
not O
hold O
copyright O
claims O
on O
content O
in O
the O
livestreams O
videos O
or O
transcripts O
. O
This O
content O
is O
being O
made O
available O
for O
free O
distribution O
for O
academic O
research O
purposes O
only O
and O
does O
not O
allow O
for O
redistribution O
. O
Aside O
from O
the O
name O
of O
the O
instructor O
in O
each O
video O
( O
which O
is O
public O
information O
) O
, O
real O
names O
of O
livestream O
session O
users O
or O
other O
identifying O
information O
does O
not O
appear O
in O
any O
of O
the O
transcripts O
. O
We O
provide O
full O
descriptions O
of O
the O
models O
used O
in O
this O
paper O
in O
Sec O
. O
5 O
. O
Detailed O
hyperparameters O
and O
bounds O
for O
hyperparameter O
search O
are O
included O
in O
the O
Appendix O
. O

Video O
- O
based O
intent O
discovery O
serves O
to O
enhance O
the O
information O
exploration O
experience O
of O
users O
on O
any O
video O
- O
based O
platform O
. O
Since O
we O
focus O
on O
extracting O
procedural O
intent O
relevant O
to O
the O
goal O
of O
the O
video O
and O
in O
the O
software O
domain O
, O
we O
do O
not O
anticipate O
this O
technology O
to O
cause O
any O
harm O
to O
users O
, O
or O
have O
any O
unintended O
consequences O
. O

B O
Experiments O

For O
HERO B-MethodName
and O
ClipBERT B-MethodName
models O
, O
we O
use O
the O
recommended O
hyperparameters O
for O
finetuning O
in O
their O
Github O
repository O
. O
6,7 O
For O
RoBERTa B-MethodName
- O
based O
models O
, O
see O
the O
hyperparameters O
common O
to O
all O
models O
in O
Table O
8 O
. O
We O
performed O
grid O
- O
search O
based O
optimization O
of O
the O
variable O
hyperparameters O
using O
the O
bounds O
in O
Table O
8 O
. O
The O
best O
performing O
batch O
size O
for O
all O
models O
was O
found O
to O
be O
32 O
. O

6 O
https O
: O
/ O
/ O
github.com O
/ O
linjieli222 O
/ O
HERO O
7 O
https O
: O
/ O
/ O
github.com O
/ O
jayleicn O
/ O
ClipBERT O

C O
Results O

See O
partial O
match O
results O
for O
the O
validation O
split O
of O
Behance B-DatasetName
Intent I-DatasetName
Discovery I-DatasetName
in O
Table O
6 O
. O

D O
Analysis O

D.1 O
Finetuned O
Video O
Representations O

We O
see O
large O
improvements O
with O
sparsely O
- O
sampled O
2D O
ResNet O
video O
embeddings O
( O
see O
Table O
4 O
which O
are O
extracted O
from O
ResNet O
pretrained O
on O
the O
Im O
- O
ageNet O
dataset O
. O
This O
begs O
the O
question O
, O
if O
larger O
improvements O
can O
be O
had O
by O
finetuning O
the O
feature O
extractors O
on O
the O
domain O
of O
Behance B-DatasetName
Livestreams I-DatasetName
. O

To O
facilitate O
this O
, O
we O
create O
a O
dataset O
of O
10,000 O
images O
containing O
snapshots O
of O
video O
livestreams O
and O
classified O
them O
into O
one O
of O
50 O
tool O
categories O
using O
the O
tool O
timeline O
. O
We O
finetune O
ResNet-152 B-MethodName
on O
this O
dataset O
with O
a O
resulting O
classification O
accuracy B-MetricName
of O
47 B-MetricValue
% I-MetricValue
. O
We O
use O
the O
finetuned O
ResNet B-MethodName
to O
extract O
sparsely O
sampled O
video O
embeddings O
and O
re O
- O
run O
the O
late O
fusion O
experiment O
with O
RoBERTa B-MethodName
. O
We O
see O
2 B-MetricValue
% I-MetricValue
improvement O
for O
tool O
intents O
and O
1 B-MetricValue
% I-MetricValue
drop O
in O
performance O
on O
creative O
intents O
. O
This O
suggests O
that O
finetuning O
feature O
extractors O
on O
the O
target O
domain O
can O
be O
beneficial O
for O
low O
- O
level O
intents O
. O

D.2 O
Semi O
- O
automated O
Intent O
Annotations O

As O
discussed O
in O
Sec O
. O
8 O
. O
semi O
- O
automatically O
data O
, O
we O
drastic O
decline O
in O
the O
precision O
of O
the O
model O
for O
both O
tool O
and O
creative O
intents O
( O
see O
row O
3 O
in O
Table O
7 O
) O
. O
With O
the O
use O
of O
better O
methods O
for O
filtering O
out O
the O
useful O
signal O
from O
the O
noisy O
data O
, O
there O
might O
be O
better O
results O
with O
semiautomatically O
created O
annotations O
. O
This O
line O
of O
research O
is O
important O
because O
it O
promotes O
scalable O
annotations O
which O
can O
cover O
a O
diverse O
population O
of O
livestreamers O
from O
many O
livestream O
videos O
. O

A O
Dataset O

For O
the O
semi O
- O
automatically O
created O
annotations O
described O
in O
Sec O
. O
8.3 O
, O
we O
empirically O
select O
a O
window O
of O
10 O
seconds O
for O
computing O
the O
scores O
and O
retain O
intent O
phrases O
with O
a O
term O
frequency O
of O
5 O
or O
higher O
in O
the O
corpus O
and O
tf O
- O
idf O
scores O
of O
0.3 O
or O
higher O
with O
one O
or O
more O
tools O
. O
See O
the O
full O
semi O
- O
automated O
pipeline O
of O
dataset O
creation O
in O
Fig O
. O
8 O
. O

Retrofitting B-MethodName
Multilingual I-MethodName
Sentence I-MethodName
Embeddings I-MethodName
with I-MethodName
Abstract I-MethodName
Meaning I-MethodName
Representation I-MethodName
* O

We O
introduce O
a O
new O
method O
to O
improve O
existing O
multilingual O
sentence O
embeddings O
with O
Abstract O
Meaning O
Representation O
( O
AMR O
) O
. O
Compared O
with O
the O
original O
textual O
input O
, O
AMR O
is O
a O
structured O
semantic O
representation O
that O
presents O
the O
core O
concepts O
and O
relations O
in O
a O
sentence O
explicitly O
and O
unambiguously O
. O
It O
also O
helps O
reduce O
surface O
variations O
across O
different O
expressions O
and O
languages O
. O
Unlike O
most O
prior O
work O
that O
only O
evaluates O
the O
ability O
to O
measure O
semantic O
similarity O
, O
we O
present O
a O
thorough O
evaluation O
of O
existing O
multilingual O
sentence O
embeddings O
and O
our O
improved O
versions O
, O
which O
include O
a O
collection O
of O
five O
transfer O
tasks O
in O
different O
downstream O
applications O
. O
Experiment O
results O
show O
that O
retrofitting B-MethodName
multilingual I-MethodName
sentence I-MethodName
embeddings I-MethodName
with I-MethodName
AMR I-MethodName
leads O
to O
better O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
both O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
and O
transfer O
tasks O
. O
Our O
codebase O
and O
evaluation O
scripts O
can O
be O
found O
at O
https O
: O
/ O
/ O
github.com O
/ O
jcyk O
/ O
MSE O
- O
AMR O
. O

Introduction O

Multilingual O
sentence O
embedding O
( O
MSE O
) O
aims O
to O
provide O
universal O
sentence O
representations O
shared O
across O
different O
languages O
( O
Hermann O
and O
Blunsom O
, O
2014 O
; O
Pham O
et O
al O
. O
, O
2015 O
; O
Schwenk O
and O
Douze O
, O
2017 O
) O
. O
As O
an O
important O
ingredient O
of O
crosslingual O
and O
multilingual O
natural O
language O
processing O
( O
NLP O
) O
, O
MSE O
has O
recently O
attracted O
increasing O
attention O
in O
the O
NLP O
community O
. O
MSE O
has O
been O
widely O
adopted O
to O
bridge O
the O
language O
barrier O
in O
several O
downstream O
applications O
such O
as O
bitext B-TaskName
mining I-TaskName
( O
Guo O
et O
al O
. O
, O
2018 O
; O
Schwenk O
, O
2018 O
) O
, O
document B-TaskName
classification I-TaskName
( O
Eriguchi O
et O
al O
. O
, O
2018 O
; O
Singla O
et O
al O
. O
, O
2018 O
; O
Yu O
et O
al O
. O
, O
2018 O
) O
and O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
Prior O
work O
typically O
borrows O
fixed O
- O
size O
embedding O
vectors O
from O
multilingual O
neural O
machine O
models O
( O
Schwenk O
and O
Douze O
, O
2017 O
; O
Yu O
et O
al O
. O
, O
2018 O
) O
or O
trains O
siamese B-MethodName
neural I-MethodName
networks I-MethodName
to O
align O
the O
semantically O
similar O
sentences O
written O
in O
different O
languages O
( O
Wieting O
et O
al O
. O
, O
2019 O
; O
Feng O
et O
al O
. O
, O
2020 O
) O
. O

Despite O
the O
recent O
progress O
, O
the O
current O
evaluation O
of O
multilingual O
sentence O
embeddings O
has O
focused O
on O
cross B-TaskName
- I-TaskName
lingual I-TaskName
Semantic I-TaskName
Textual I-TaskName
Similarity I-TaskName
( O
STS B-TaskName
) O
( O
Agirre O
et O
al O
. O
, O
2016 O
; O
Cer O
et O
al O
. O
, O
2017 O
) O
or O
bi B-TaskName
- I-TaskName
text I-TaskName
mining I-TaskName
tasks O
( O
Zweigenbaum O
et O
al O
. O
, O
2018 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
Nevertheless O
, O
as O
pointed O
out O
by O
Gao O
et O
al O
. O
( O
2021 O
) O
, O
the O
evaluation O
on O
semantic O
similarity O
may O
not O
be O
sufficient O
because O
better O
performance O
on O
STS B-TaskName
does O
not O
always O
indicate O
better O
embeddings O
for O
downstream O
tasks O
. O
Therefore O
, O
for O
a O
more O
comprehensive O
MSE O
evaluation O
, O
it O
is O
necessary O
to O
additionally O
evaluate O
downstream O
tasks O
, O
which O
is O
largely O
ignored O
in O
recent O
work O
( O
Chidambaram O
et O
al O
. O
, O
2019 O
; O
Reimers O
and O
Gurevych O
, O
2020 O
; O
Feng O
et O
al O
. O
, O
2020 O
) O
. O
In O
this O
paper O
, O
we O
collect O
a O
set O
of O
multilingual O
transfer O
tasks O
and O
test O
various O
existing O
multilingual O
sentence O
embeddings O
. O
We O
find O
that O
different O
methods O
excel O
at O
different O
tasks O
and O
the O
conclusions O
drawn O
from O
the O
STS B-TaskName
evaluation O
do O
not O
always O
hold O
in O
the O
transfer O
tasks O
and O
vice O
versa O
. O
We O
aim O
to O
establish O
a O
standardized O
evaluation O
protocol O
for O
future O
research O
in O
multilingual O
sentence O
embeddings O
. O

To O
improve O
the O
quality O
of O
existing O
MSE O
models O
, O
we O
explore O
Abstract O
Meaning O
Representation O
( O
AMR O
) O
( O
Banarescu O
et O
al O
. O
, O
2013 O
) O
, O
a O
symbolic O
semantic O
representation O
, O
for O
augmenting O
existing O
neural O
semantic O
representations O
. O
Our O
motivation O
is O
twofold O
. O
First O
, O
AMR O
explicitly O
offers O
core O
concepts O
and O
relations O
in O
a O
sentence O
. O
This O
helps O
prevent O
learning O
the O
superficial O
patterns O
or O
spurious O
correlations O
in O
the O
training O
data O
, O
which O
do O
not O
generalize O
well O
to O
new O
domains O
or O
tasks O
( O
Poliak O
et O
al O
. O
, O
2018 O
; O
Clark O
et O
al O
. O
, O
2019 O
) O
. O
Second O
, O
AMR O
reduces O
the O
variances O
in O
surface O
forms O
with O
the O
same O
meaning O
. O
This O
helps O
alleviate O
the O
data O
sparsity O
issue O
as O
there O
are O
rich O
lexical O
variations O
across O
different O
languages O
. O

On O
the O
other O
hand O
, O
despite O
that O
AMR O
is O
advocated O
to O
act O
as O
an O
interlingua O
( O
Xue O
et O
al O
. O
, O
2014 O
; O
Damonte O
and O
Cohen O
, O
2018 O
) O
, O
little O
work O
has O
been O
done O
to O
reflect O
on O
the O
ability O
of O
AMR O
to O
have O
impact O
on O
subsequent O
tasks O
. O
In O
order O
to O
advance O
research O
in O
AMR O
and O
its O
applications O
, O
multilingual O
sentence O
embedding O
can O
be O
seen O
as O
an O
important O
benchmark O
for O
highlighting O
its O
ability O
to O
abstract O
away O
from O
surface O
realizations O
and O
represent O
the O
core O
concepts O
expressed O
in O
the O
sentence O
. O
To O
our O
knowledge O
, O
this O
is O
the O
first O
attempt O
to O
leverage O
the O
AMR O
semantic O
representation O
for O
multilingual O
NLP O
. O

We O
learn O
AMR O
embeddings O
with O
contrastive B-MethodName
siamese I-MethodName
network I-MethodName
( O
Gao O
et O
al O
. O
, O
2021 O
) O
and O
AMR O
graphs O
derived O
from O
different O
languages O
( O
Cai O
et O
al O
. O
, O
2021 O
) O
. O
Experiment O
results O
on O
10 O
STS B-TaskName
tasks O
and O
5 O
transfer O
tasks O
with O
four O
state O
- O
of O
- O
the O
- O
art O
embedding O
methods O
show O
that O
retrofitting B-MethodName
multilingual I-MethodName
sentence I-MethodName
embeddings I-MethodName
with I-MethodName
AMR I-MethodName
improves O
the O
performance O
substantially O
and O
consistently O
. O

Our O
contribution O
is O
three O
- O
fold O
. O
• O
We O
propose O
a O
new O
method O
to O
obtain O
high O
- O
quality O
semantic O
vectors O
for O
multilingual O
sentence O
representation O
, O
which O
takes O
advantage O
of O
languageinvariant O
Abstract O
Meaning O
Representation O
that O
captures O
the O
core O
semantics O
of O
sentences O
. O

• O
We O
present O
a O
thorough O
evaluation O
of O
multilingual O
sentence O
embeddings O
, O
which O
goes O
beyond O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
and O
includes O
various O
transfer O
tasks O
in O
downstream O
applications O
. O

• O
We O
demonstrate O
that O
retrofitting B-MethodName
multilingual I-MethodName
sentence I-MethodName
embeddings I-MethodName
with I-MethodName
Abstract I-MethodName
Meaning I-MethodName
Representation I-MethodName
leads O
to O
better O
performance O
on O
both O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
and O
transfer O
tasks O
. O

Related O
Work O

Universal O
Sentence O
Embeddings O
Our O
work O
aims O
to O
learn O
universal O
sentence O
representations O
, O
which O
should O
be O
useful O
for O
a O
broad O
set O
of O
applications O
. O
There O
are O
two O
lines O
of O
research O
for O
universal O
sentence O
embeddings O
: O
unsupervised O
approaches O
and O
supervised O
approaches O
. O
Early O
unsupervised O
approaches O
( O
Kiros O
et O
al O
. O
, O
2015 O
; O
Hill O
et O
al O
. O
, O
2016 O
; O
Gan O
et O
al O
. O
, O
2017 O
; O
Logeswaran O
and O
Lee O
, O
2018 O
) O
design O
various O
surrounding O
sentence O
reconstruction O
/ O
prediction O
objectives O
for O
sentence O
representation O
learning O
. O
Jernite O
et O
al O
. O
( O
2017 O
) O
exploit O
sentencelevel O
discourse O
relations O
as O
supervision O
signals O
for O
training O
sentence O
embedding O
model O
. O
Instead O
of O
us O
- O
ing O
the O
interactions O
of O
sentences O
within O
a O
document O
, O
Le O
and O
Mikolov O
( O
2014 O
) O
propose O
to O
learn O
the O
embeddings O
for O
texts O
of O
arbitrary O
length O
on O
top O
of O
word O
vectors O
. O
Likewise O
, O
Chen O
( O
2017 O
) O
; O
Pagliardini O
et O
al O
. O
( O
2018 O
) O
; O
Yang O
et O
al O
. O
( O
2019b O
) O
calculate O
sentence O
embeddings O
from O
compositional O
n O
- O
gram O
features O
. O
Recent O
approaches O
often O
adopt O
contrastive O
objectives O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Giorgi O
et O
al O
. O
, O
2021 O
; O
Wu O
et O
al O
. O
, O
2020 O
; O
Meng O
et O
al O
. O
, O
2021 O
; O
Carlsson O
et O
al O
. O
, O
2021 O
; O
Kim O
et O
al O
. O
, O
2021 O
; O
Yan O
et O
al O
. O
, O
2021 O
; O
Gao O
et O
al O
. O
, O
2021 O
) O
by O
taking O
different O
views O
- O
from O
data O
augmentation O
or O
different O
copies O
of O
models O
- O
of O
the O
same O
sentence O
as O
training O
examples O
. O

On O
the O
other O
hand O
, O
supervised O
methods O
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Reimers O
and O
Gurevych O
, O
2019 O
; O
Gao O
et O
al O
. O
, O
2021 O
) O
take O
advantage O
of O
labeled O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
NLI B-TaskName
) O
datasets O
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
, O
where O
a O
sentence O
embedding O
model O
is O
finetuned O
on O
entailment O
or O
contradiction O
sentence O
pairs O
. O
Furthermore O
, O
Wieting O
and O
Gimpel O
( O
2018 O
) O
; O
Wieting O
et O
al O
. O
( O
2020 O
) O
demonstrate O
that O
bilingual O
and O
back O
- O
translation O
corpora O
provide O
useful O
supervision O
for O
learning O
semantic O
similarity O
. O
Another O
line O
of O
work O
focuses O
on O
regularizing O
embeddings O
Su O
et O
al O
. O
, O
2021 O
; O
Huang O
et O
al O
. O
, O
2021 O
) O
to O
alleviate O
the O
representation O
degeneration O
problem O
. O
Very O
recently O
, O
Opitz O
and O
Frank O
( O
2022 O
) O
combine O
the O
strengths O
of O
AMR O
metrics O
and O
embedding O
similarities O
for O
accurate O
and O
explainable O
sentence O
similarity O
rating O
. O

Multilingual O
Sentence O
Embeddings O
Recently O
, O
multilingual O
sentence O
representations O
have O
attracted O
increasing O
attention O
. O
Schwenk O
and O
Douze O
( O
2017 O
) O
; O
Yu O
et O
al O
. O
( O
2018 O
) O
; O
Artetxe O
and O
Schwenk O
( O
2019 O
) O
( O
Bromley O
et O
al O
. O
, O
1993 O
) O
with O
contrastive O
objectives O
using O
parallel O
corpora O
. O
Reimers O
and O
Gurevych O
( O
2020 O
) O
train O
a O
multilingual O
model O
to O
map O
sentences O
to O
the O
same O
embedding O
space O
of O
an O
existing O
English O
model O
. O
Different O
from O
existing O
work O
, O
our O
work O
resorts O
to O
multilingual O
AMR O
, O
a O
languageagnostic O
disambiguated O
semantic O
representation O
, O
for O
performance O
enhancement O
. O

Evaluation O
of O
Sentence O
Embeddings O
Traditionally O
, O
the O
mainstream O
evaluation O
for O
assessing O
the O
quality O
of O
English O
- O
only O
sentence O
embeddings O
is O
based O
on O
the O
Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
( O
STS B-TaskName
) O
tasks O
and O
a O
suite O
of O
downstream O
classification O
tasks O
. O
The O
STS B-TaskName
tasks O
( O
Agirre O
et O
al O
. O
, O
2012 O
( O
Agirre O
et O
al O
. O
, O
, O
2013 O
( O
Agirre O
et O
al O
. O
, O
, O
2014 O
( O
Agirre O
et O
al O
. O
, O
, O
2015 O
( O
Agirre O
et O
al O
. O
, O
, O
2016Marelli O
et O
al O
. O
, O
2014 O
; O
Cer O
et O
al O
. O
, O
2017 O
) O
calculate O
the O
embedding O
distance O
of O
sentence O
pairs O
and O
compare O
them O
with O
the O
human O
- O
annotated O
scores O
for O
semantic O
similarity O
. O
The O
classification O
tasks O
( O
e.g. O
, O
sentiment B-TaskName
analysis I-TaskName
) O
from O
SentEval O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
take O
sentence O
embeddings O
as O
fixed O
input O
features O
to O
a O
logistic O
regression O
classifier O
. O
These O
tasks O
are O
commonly O
used O
to O
benchmark O
the O
transferability O
of O
sentence O
embeddings O
on O
downstream O
tasks O
. O
For O
multilingual O
sentence O
embeddings O
, O
most O
previous O
work O
has O
focused O
on O
crosslingual O
STS B-TaskName
( O
Agirre O
et O
al O
. O
, O
2016 O
; O
Cer O
et O
al O
. O
, O
2017 O
) O
and O
the O
relevant O
bi B-TaskName
- I-TaskName
text I-TaskName
mining I-TaskName
tasks O
( O
Zweigenbaum O
et O
al O
. O
, O
2018 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
The O
evaluation O
on O
downstream O
transfer O
tasks O
has O
been O
largely O
ignored O
( O
Chidambaram O
et O
al O
. O
, O
2019 O
; O
Reimers O
and O
Gurevych O
, O
2020 O
; O
Feng O
et O
al O
. O
, O
2020 O
) O
. O
Nevertheless O
, O
as O
pointed O
out O
in O
Gao O
et O
al O
. O
( O
2021 O
) O
in O
English O
scenarios O
, O
better O
performance O
on O
semantic B-TaskName
similarity I-TaskName
tasks O
does O
not O
always O
indicate O
better O
embeddings O
for O
transfer O
tasks O
. O
For O
a O
more O
comprehensive O
evaluation O
, O
in O
this O
paper O
, O
we O
collect O
a O
set O
of O
multilingual O
transfer O
tasks O
and O
test O
various O
existing O
multilingual O
sentence O
embeddings O
. O
We O
aim O
to O
establish O
a O
standardized O
evaluation O
protocol O
for O
future O
research O
in O
multilingual O
sentence O
embeddings O
. O

Preliminaries O

Contrastive B-MethodName
Siamese I-MethodName
Network I-MethodName

Siamese O
network O
( O
Bromley O
et O
al O
. O
, O
1993 O
) O
has O
attracted O
considerable O
attention O
for O
self O
- O
supervised O
representation O
learning O
. O
It O
has O
been O
extensively O
adopted O
with O
contrastive O
learning O
( O
Hadsell O
et O
al O
. O
, O
2006 O
) O
for O
learning O
dense O
vector O
representations O
of O
images O
and O
sentences O
( O
Reimers O
and O
Gurevych O
, O
2019 O
; O
. O
The O
core O
idea O
of O
contrastive O
learning O
is O
to O
pull O
together O
the O
representations O
of O
semantically O
close O
objects O
( O
images O
or O
sentences O
) O
and O
repulse O
the O
representations O
of O
negative O
pairs O
of O
dissimilar O
ones O
. O
Recent O
work O
in O
computer O
vision O
( O
Caron O
et O
al O
. O
, O
2020 O
; O
Grill O
et O
al O
. O
, O
2020 O
; O
Chen O
and O
He O
, O
2021 O
; O
Zbontar O
et O
al O
. O
, O
2021 O
) O
has O
demonstrated O
that O
negative O
samples O
may O
not O
be O
necessary O
. O
A O
similar O
observation O
was O
made O
in O
NLP O
by O
who O
adopted O
the O
BYOL O
framework O
( O
Grill O
et O
al O
. O
, O
2020 O
) O
for O
sentence O
representation O
learning O
. O
In O
this O
work O
, O
we O
adopt O
the O
framework O
in O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
with O
in O
- O
batch O
negatives O
Henderson O
et O
al O
. O
, O
2017 O
) O
. O

Formally O
, O
we O
assume O
a O
set O
of O
training O
examples O

D O
= O
{ O
( O
x O
i O
, O
x O
+ O
i O
, O
x O
− O
i O
) O
} O
N O
i=1 O

, O
where O
x O
+ O
i O
and O
x O
− O
i O
are O
semantically O
close O
and O
semantically O
irrelevant O
to O
x O
i O
, O
respectively O
. O
The O
training O
is O
done O
with O
stochastic O
mini O
- O
batches O
. O
Each O
mini O
- O
batch O
consists O
of O
M B-HyperparameterName
examples O
and O
the O
training O
objective O
is O
defined O
as O
: O

i O
= O
− O
log O
e O
s O
( O
x O
i O
, O
x O
+ O
i O
) O
/ O
τ O
M O
j=1 O
e O
s O
( O
x O
i O
, O
x O
− O
j O
) O
/ O
τ O
+ O
M O
j=1 O
e O
s O
( O
x O
i O
, O
x O
+ O
j O
) O
/ O
τ O

( O
1 O
) O
where O
s O
( O
• O
, O
• O
) O
measures O
the O
similarity O
of O
two O
objects O
and O
τ B-HyperparameterName
is O
a O
scalar O
controlling O
the O
temperature B-HyperparameterName
of O
training O
. O
As O
seen O
, O
other O
objects O
in O
the O
same O
minibatch O
( O
i.e. O
, O
{ O
x O
− O
j O
} O
j O
= O
i O
and O
{ O
x O
+ O
j O
} O
j O
= O
i O
) O
are O
treated O
as O
negatives O
for O
x O
i O
. O
More O
concretely O
, O
s O
( O
• O
, O
• O
) O
computes O
the O
cosine O
similarity O
between O
the O
representations O
of O
two O
objects O
: O

s O
( O
x O
i O
, O
x O
j O
) O
= O
h O
T O
i O
h O
j O
h O
i O
• O
h O
j O

where O
h O
i O
and O
h O
j O
are O
obtained O
from O
a O
neural O
encoder O
f O
θ O
( O
• O
) O
: O
h O
= O
f O
θ O
( O
x O
) O
. O
The O
model O
parameters O
θ O
are O
then O
optimized O
using O
the O
contrastive O
learning O
objective O
. O

Multilingual O
AMR O
Parsing O

AMR O
( O
Banarescu O
et O
al O
. O
, O
2013 O
) O
is O
a O
broad O
- O
coverage O
semantic O
formalism O
originally O
designed O
for O
English O
. O
The O
accuracy O
of O
AMR O
parsing O
has O
been O
greatly O
improved O
in O
recent O
years O
Lam O
, O
2019 O
, O
2020a O
; O
Bevilacqua O
et O
al O
. O
, O
2021 O
; O
Bai O
et O
al O
. O
, O
2022 O
) O
. O
Because O
AMR O
is O
agnostic O
to O
syntactic O
and O
wording O
variations O
, O
recent O
work O
has O
suggested O
the O
potential O
of O
AMR O
to O
work O
as O
an O
interlingua O
( O
Xue O
et O
al O
. O
, O
2014 O
; O
Damonte O
and O
Cohen O
, O
2018 O
) O
. O
That O
is O
, O
we O
can O
represent O
the O
semantics O
in O
other O
languages O
using O
the O
corresponding O
AMR O
graph O
of O
the O
semantic O
equivalent O
in O
English O
. O
A O
number O
of O
crosslingual O
AMR O
parsers O
( O
Damonte O
and O
Cohen O
, O
2018 O
; O
Blloshmi O
et O
al O
. O
, O
2020 O
; O
Sheth O
et O
al O
. O
, O
2021 O
; O
Procopio O
et O
al O
. O
, O
2021 O
; O
Cai O
et O
al O
. O
, O
2021 O
) O
have O
been O
developed O
to O
transform O
non O
- O
English O
texts O
into O
AMR O
graphs O
. O
Most O
of O
them O
rely O
on O
pre O
- O
trained O
multilingual O
language O
models O
and O
synthetic O
parallel O
data O
. O
In O
particular O
, O
Cai O
et O
al O
. O
( O
2021 O
) O
proposed O
to O
learn O
a O
multilingual O
AMR O
parser O
from O
an O
English O
AMR O
parser O
via O
knowledge O
distillation O
. O
Their O
single O
parser O
is O
trained O
for O
five O
different O
languages O
( O
German O
, O
Spanish O
, O
Italian O
, O
Chinese O
, O
and O
English O
) O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
parsing O
accuracies B-MetricName
. O
In O
addition O
, O
the O
one O
- O
for O
- O
all O
design O
maintains O
parsing O
efficiency O
and O
reduces O
prediction O
inconsistency O
across O
different O
languages O
. O
Thus O
, O
we O
adopt O
the O
multilingual O
AMR O
parser O
of O
Cai O
et O
al O
. O
( O
2021 O
) O
in O
our O
experiments O
. O
1 O
It O
is O
worth O
noting O
that O
the O
multilingual O
parser O
is O
capable O
of O
parsing O
many O
other O
languages O
, O
including O
those O
it O
has O
not O
been O
explicitly O
trained O
for O
, O
thanks O
to O
the O
generalization O
power O
inherited O
from O
pre O
- O
trained O
multilingual O
language O
models O
( O
Tang O
et O
al O
. O
, O
2020 O
; O
. O
In O
Section O
4.2 O
, O
we O
further O
extend O
the O
training O
of O
the O
multilingual O
parser O
to O
French O
, O
another O
major O
language O
, O
for O
improved O
performance O
. O

Proposed O
Method O

We O
first O
introduce O
how O
we O
learn O
AMR O
embeddings O
and O
then O
describe O
the O
whole O
pipeline O
for O
enhancing O
existing O
sentence O
embeddings O
. O

Learning O
AMR O
Embeddings O

Linearization O
& O
Modeling O
Given O
AMR O
is O
graph O
- O
structured O
, O
a O
variety O
of O
graph O
neural O
networks O
( O
Song O
et O
al O
. O
, O
2018 O
; O
Beck O
et O
al O
. O
, O
2018 O
; O
Ribeiro O
et O
al O
. O
, O
2019 O
; O
Guo O
et O
al O
. O
, O
2019 O
; O
Cai O
and O
Lam O
, O
2020b O
; O
Ribeiro O
et O
al O
. O
, O
2019 O
) O
have O
been O
proposed O
for O
the O
representation O
learning O
of O
AMR O
. O
However O
, O
recent O
work O
Mager O
et O
al O
. O
, O
2020 O
; O
Bevilacqua O
et O
al O
. O
, O
2021 O
) O
has O
demonstrated O
that O
the O
power O
of O
existing O
pre O
- O
trained O
language O
models O
based O
on O
the O
Transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
BART O
, O
can O
be O
leveraged O
for O
achieving O
better O
performance O
. O
Following O
them O
, O
we O
also O
take O
BERT B-MethodName
as O
the O
backbone O
model O
. O

Since O
Transformer O
- O
based O
language O
models O
are O
designed O
for O
sequential O
data O
, O
to O
encode O
graphical O
AMR O
, O
we O
resort O
to O
the O
linearization O
techniques O
in O
( O
Bevilacqua O
et O
al O
. O
, O
2021 O
) O
. O
Figure O
1 O
illustrates O
the O
linearization O
of O
AMR O
graphs O
. O
For O
each O
AMR O
graph O
, O
a O
DFS O
traversal O
is O
performed O
starting O
from O
the O
root O
node O
of O
the O
graph O
, O
and O
the O
trajectory O
is O
recorded O
. O
We O
use O
parentheses O
to O
mark O
the O
hierarchy O
of O
node O
depths O
. O
Bevilacqua O
et O
al O
. O
( O
2021 O
) O
also O
proposed O
to O
use O
special O
tokens O
for O
indicating O
variables O
in O
the O
linearized O
graph O
and O
for O
handling O
reentrancies O
( O
i.e. O
, O
a O
node O
plays O
multiple O
roles O
in O
the O
graph O
) O
. O
However O
, O
the O
introduction O
of O
special O
tokens O
significantly O
increases O
the O
length O
of O
the O
output O
sequence O
( O
almost O
50 O
% O
increase O
) O
. O
We O
remove O
this O
feature O
and O
simply O
repeat O
the O
nodes O
when O
revisiting O
happens O
. O
This O
significantly O
reduces O
the O
length O
of O
the O
output O
sequence O
and O
allows O
more O
efficient O
modeling O
with O
Transformer O
- O
based O
language O
models O
. O
The O
downside O
is O
that O
reentrancy O
information O
becomes O
unrecoverable O
. O
However O
, O
we O
empirically O
found O
that O
the O
shortened O
sequences O
lead O
to O
better O
performance O
. O
The O
linearizations O
of O
AMR O
graphs O
are O
then O
treated O
as O
plain O
token O
sequences O
when O
being O
fed O
into O
Transformer O
- O
based O
language O
models O
. O
Note O
that O
AMR O
linearization O
introduces O
additional O
tokens O
that O
are O
rarely O
shown O
in O
English O
( O
e.g. O
, O
" O
ARG2 O
" O
and O
" O
belong-01 O
" O
) O
. O
These O
tokens O
may O
not O
be O
included O
in O
the O
original O
vocabulary O
of O
existing O
language O
models O
and O
could O
be O
segmented O
into O
sub O
- O
tokens O
( O
e.g. O
, O
" O
belong-01 O
" O
⇒ O
" O
belong O
" O
, O
" O
- O
" O
, O
" O
01 O
" O
) O
, O
which O
are O
less O
meaningful O
and O
increase O
the O
sequence O
length O
. O
To O
deal O
with O
this O
problem O
, O
we O
extend O
the O
original O
vocabulary O
of O
existing O
language O
models O
to O
include O
all O
the O
relation O
and O
frame O
names O
occurring O
at O
least O
5 B-HyperparameterValue
times O
in O
the O
AMR O
sembank O
( O
LDC2017T10 O
) O
. O
( O
Wu O
et O
al O
. O
, O
2020 O
; O
Meng O
et O
al O
. O
, O
2021 O
) O
or O
introducing O
some O
random O
noise O
( O
e.g. O
, O
dropout O
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
) O
to O
the O
modeling O
function O
f O
θ O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
. O
On O
the O
other O
hand O
, O
negative O
examples O
x O
− O
i O
are O
usually O
sampled O
from O
other O
sentences O
. O
However O
, O
prior O
work O
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Gao O
et O
al O
. O
, O
2021 O
) O
has O
demonstrated O
that O
entailment O
/ O
contradiction O
sentence O
pairs O
in O
supervised O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
NLI B-TaskName
) O
datasets O
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
are O
better O
positive O
/ O
negative O
pairs O
for O
learning O
sentence O
embeddings O
. O
Following O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
, O
we O
borrow O
the O
supervisions O
from O
two O
NLI B-TaskName
datasets O
, O
namely O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
MNLI B-DatasetName
. O
In O
the O
NLI B-TaskName
datasets O
, O
given O
one O
premise O
, O
there O
are O
one O
entailment O
hypothesis O
and O
another O
contradiction O
hypothesis O
accompanying O
. O
Therefore O
, O
in O
each O
training O
example O

( O
x O
i O
, O
x O
+ O
i O
, O
x O
− O
i O
) O

, O
x O
i O
is O
the O
premise O
, O
x O
+ O
i O
is O
the O
entailment O
hypothesis O
, O
and O
x O
− O
i O
is O
the O
contradiction O
hypothesis O
. O
Specifically O
, O
we O
use O
the O
multilingual O
AMR O
parser O
described O
in O
Section O
3.2 O
to O
parse O
sentences O
into O
AMR O
graphs O
. O
Because O
the O
sentences O
in O
the O
NLI B-TaskName
datasets O
are O
in O
English O
, O
the O
resultant O
AMR O
graphs O
are O
all O
derived O
from O
English O
. O
This O
is O
in O
contrast O
to O
downstream O
applications O
where O
an O
AMR O
graph O
may O
be O
derived O
from O
a O
foreign O
language O
. O
To O
reduce O
the O
discrepancy O
between O
training O
and O
testing O
, O
we O
use O
OPUS O
- O
MT O
( O
Tiedemann O
and O
Thottingal O
, O
2020 O
) O
2 O
, O
an O
off O
- O
the O
- O
shelf O
translation O
system O
, O
to O
translate O
English O
sentences O
in O
the O
NLI B-TaskName
datasets O
to O
other O
languages O
. O
The O
translations O
in O
other O
languages O
are O
then O
parsed O
by O
our O
multilingual O
AMR O
parser O
. O
In O
this O
way O
, O
we O
extend O
the O
training O
of O
AMR O
embeddings O
to O
multilingual O
scenarios O
as O
well O
. O

Mixed O
Training O
To O
better O
cover O
both O
the O
monolingual O
and O
cross O
- O
lingual O
settings O
in O
downstream O
applications O
, O
the O
training O
aims O
to O
capture O
the O
interactions O
between O
AMR O
graphs O
derived O
from O
the O
same O
language O
as O
well O
as O
those O
derived O
from O
different O
languages O
. O
To O
this O
end O
, O
we O
mix O
up O
AMR O
graphs O
from O
different O
languages O
during O
training O
. O
Moreover O
, O
to O
alleviate O
the O
drawback O
of O
imperfect O
parsing O
and O
avoid O
catastrophic O
forgetting O
of O
pre O
- O
trained O
language O
models O
, O
we O
also O
mix O
up O
AMR O
graphs O
and O
original O
English O
sentences O
during O
training O
. O
The O
details O
are O
shown O
in O
Algorithm O
1 O
. O

We O
hypothesize O
that O
the O
noise O
introduced O
by O
automatic O
translation O
could O
negatively O
affect O
the O
performance O
but O
a O
suitable O
amount O
of O
noise O
might O
also O
serve O
as O
a O
helpful O
regularizer O
. O
Unfortunately O
, O
due O
to O
the O
lack O
of O
gold O
translations O
, O
we O
could O
not O
perform O
a O
rigorous O
quantitative O
comparison O
. O
In O
our O
preliminary O
experiments O
, O
we O
also O
tried O
another O
automatic O
translation O
system O
, O
mBART O
- O
mmt O
( O
Tang O
et O
al O
. O
, O
2020 O
) O
3 O
, O
other O
than O
OPUS O
- O
MT O
. O
We O
found O
that O
mBARTmmt O
leads O
to O
worse O
performance O
in O
general O
, O
likely O
2 O
https O
: O
/ O
/ O
huggingface.co O
/ O
docs O
/ O
transformers O
/ O
model_doc O
/ O
marian O
3 O
https O
: O
/ O
/ O
huggingface.co O
/ O
facebook O
/ O
mbart O
- O
large-50 O
- O
many O
- O
to O
- O
many O
- O
mmt O
Algorithm O
1 O
: O
Learning O
AMR O
Embeddings O
. O

Input O
: O
Dataset O
: O
D O
= O
{ O
( O
xi O
, O
x O
+ O
i O
, O
x O
− O
i O
) O
} O
N O
i=1 O

, O
Systems O
: O
AMR O
parser O
parse O
( O
• O
) O
and O
English O
- O
to O
- O
l O
translator O
translate O
( O
• O
, O
l O
) O
, O
Maximum B-HyperparameterName
training I-HyperparameterName
steps I-HyperparameterName
: O
T B-HyperparameterName
, O
Batch B-HyperparameterName
size I-HyperparameterName
M B-HyperparameterName
, O
Language O
set O
: O
L O
. O

1 O
for O
t O
← O
1 O
to O
T O
do O
2 O
Draw O
a O
mini O
- O
batch O
B O
= O
{ O
( O
xi O
, O
x O
+ O
i O
, O
x O
− O
i O
) O
} O
M O
i=1 O
from O
D O
3 O
foreach O
sentence O
x O
in O
B O
do O

Incorporating O
AMR O
Embeddings O

The O
learned O
AMR O
embeddings O
can O
be O
used O
to O
augment O
any O
existing O
sentence O
embedding O
model O
. O
For O
any O
input O
sentence O
x O
, O
it O
is O
processed O
through O
two O
channels O
: O
( O
1 O
) O
the O
sentence O
is O
first O
parsed O
into O
an O
AMR O
graph O
y O
= O
parse O
( O
x O
) O
. O
The O
graph O
is O
then O
fed O
into O
our O
AMR O
encoder O
: O
h O
= O
f O
θ O
( O
y O
) O
. O
( O
2 O
) O
the O
sentence O
is O
directly O
encoded O
by O
an O
off O
- O
the O
- O
shelf O
sentence O
embedding O
model O
g O
( O
• O
) O
: O
s O
= O
g O
( O
x O
) O
. O
Lastly O
, O
we O
combine O
the O
text O
and O
graph O
embeddings O
( O
s O
and O
h O
) O
to O
produce O
the O
final O
sentence O
representation O
. O

Parsing O
Theoretically O
, O
the O
multilingual O
AMR O
parser O
introduced O
in O
Cai O
et O
al O
. O
( O
2021 O
) O
can O
parse O
50 O
different O
languages O
as O
it O
inherits O
the O
multilingual O
encoder O
pre O
- O
trained O
on O
these O
languages O
from O
Tang O
et O
al O
. O
( O
2020 O
) O
. O
However O
, O
the O
original O
parser O
has O
only O
been O
explicitly O
trained O
for O
German O
( O
de O
) O
, O
Spanish O
( O
es O
) O
, O
Italian O
( O
it O
) O
, O
Chinese O
( O
zh O
) O
, O
and O
English O
( O
en O
) O
. O
We O
hypothesize O
that O
including O
more O
languages O
in O
training O
can O
help O
improve O
the O
overall O
parsing O
accuracy B-MetricName
. O
Therefore O
, O
we O
add O
French O
( O
fr O
) O
, O
another O
major O
language O
, O
to O
the O
training O
of O
the O
parser O
. O
4 O
Integration O
We O
explore O
four O
different O
choices O
for O
the O
integration O
of O
the O
text O
embedding O
s O
and O
the O
AMR O
embedding O
h O
: O
s O
⊕ O
h O
, O
s O
+ O
h O
, O
, O
s O
s O
⊕ O
h O
h O
, O
s O
s O
+ O
h O
h O
, O
where O
⊕ O
denotes O
the O
concatenation O
of O
two O
vectors O
. O
Empirically O
, O
we O
find O
that O
s O
s O
⊕ O
h O
h O
generally O
works O
best O
. O

Evaluation O
Benchmark O

To O
provide O
a O
more O
comprehensive O
evaluation O
of O
multilingual O
sentence O
representations O
, O
In O
addition O
to O
traditional O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
tasks O
, O
we O
also O
introduce O
a O
set O
of O
downstream O
transfer O
tasks O
. O

Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName

Multilingual B-TaskName
STS I-TaskName
The O
goal O
of O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
( O
STS B-TaskName
) O
is O
to O
assign O
for O
a O
pair O
of O
sentences O
a O
score O
indicating O
their O
semantic O
similarity O
. O
For O
example O
, O
a O
score O
of O
0 O
indicates O
not O
related O
and O
5 O
indicates O
semantically O
equivalent O
. O
We O
use O
the O
datasets O
in O
Reimers O
and O
Gurevych O
( O
2020 O
) O
, O
which O
is O
an O
extended O
version O
of O
the O
multilingual B-DatasetName
STS I-DatasetName
2017 I-DatasetName
dataset O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O
The O
evaluation O
is O
done O
by O
comparing O
the O
distance O
in O
the O
embedding O
space O
and O
the O
human O
- O
annotated O
scores O
in O
the O
dataset O
. O

Transfer O
Tasks O

We O
evaluate O
the O
quality O
of O
the O
multilingual O
sentence O
embeddings O
on O
the O
following O
cross O
- O
lingual O
sentence O
/ O
sentence O
- O
pair O
classification O
benchmarks O
: O

XNLI B-DatasetName
The O
Cross O
- O
lingual O
Natural O
Language O
Inference O
benchmark O
is O
used O
to O
estimate O
the O
capability O
of O
cross O
- O
lingual O
/ O
multilingual O
models O
in O
recognizing O
textual O
entailment O
. O
The O
evaluation O
sets O
of O
XNLI B-DatasetName
are O
created O
by O
manually O
translating O
the O
development O
corpus O
and O
the O
testing O
corpus O
of O
MultiNLI B-DatasetName
to O
15 O
other O
languages O
. O

PAWS B-DatasetName
- I-DatasetName
X I-DatasetName
The O
Cross O
- O
lingual O
Paraphrase B-DatasetName
Adversaries I-DatasetName
from I-DatasetName
Word I-DatasetName
Scrambling I-DatasetName
benchmark O
( O
Yang O
et O
al O
. O
, O
2019a O
) O
consists O
of O
golden O
English O
paraphrase O
identification O
pairs O
from O
PAWS B-DatasetName
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
and O
around O
24k B-HyperparameterValue
human O
translations O
of O
PAWS B-DatasetName
evaluation O
sets O
( O
i.e. O
, O
development O
set O
and O
testing O
set O
) O
in O
English O
, O
French O
, O
Spanish O
, O
German O
, O
Chinese O
, O
Japanese O
( O
ja O
) O
, O
and O
Korean O
( O
ko O
) O
. O

QAM B-DatasetName
The O
Question B-TaskName
- I-TaskName
Answer I-TaskName
Matching I-TaskName
task O
aims O
to O
predict O
if O
the O
given O
( O
question O
, O
passage O
) O
pair O
is O
a O
QA O
pair O
. O
We O
use O
the O
multilingual B-DatasetName
QAM I-DatasetName
dataset O
from O
XGLUE B-DatasetName
( O
Liang O
et O
al O
. O
, O
2020 O
) O
, O
which O
provides O
the O
labeled O
instance O
( O
question O
, O
passage O
, O
label O
) O
in O
English O
, O
French O
, O
and O
German O
, O
to O
evaluate O
the O
effectiveness O
of O
multilingual O
sentence O
embeddings O
. O
MLDoc B-DatasetName
The O
Multilingual B-DatasetName
Document I-DatasetName
Classification I-DatasetName
benchmark I-DatasetName
( O
Schwenk O
and O
Li O
, O
2018 O
) O
is O
a O
multilingual O
corpus O
with O
a O
collection O
of O
news O
documents O
written O
in O
English O
, O
German O
, O
Spanish O
, O
French O
, O
Italian O
, O
Chinese O
, O
Japanese O
, O
and O
Russian O
( O
ru O
) O
. O
The O
entire O
corpus O
is O
manually O
classified O
into O
four O
groups O
according O
to O
the O
topic O
of O
the O
document O
. O

MARC B-DatasetName
The O
Multilingual B-DatasetName
Amazon I-DatasetName
Review I-DatasetName
Corpus I-DatasetName
( O
Keung O
et O
al O
. O
, O
2020 O
) O
is O
a O
large O
- O
scale O
collection O
of O
Amazon O
user O
reviews O
for O
multilingual O
rating O
classification O
. O
The O
corpus O
covers O
6 O
languages O
, O
including O
English O
, O
German O
, O
French O
, O
Spanish O
, O
and O
Chinese O
, O
Japanese O
. O

6 O
Experiments O

Experimental O
Setup O

For O
STS B-TaskName
tasks O
, O
following O
previous O
work O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
, O
we O
define O
the O
similarity O
score O
as O
the O
cosine O
similarity O
of O
sentence O
embeddings O
and O
compute O
the O
Spearman O
's O
rank O
correlation O
between O
the O
computed O
score O
and O
the O
gold O
score O
. O

For O
downstream O
transfer O
tasks O
, O
we O
follow O
the O
conventional O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
setting O
( O
Liang O
et O
al O
. O
, O
2020 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
, O
where O
annotated O
training O
data O
is O
provided O
in O
English O
but O
none O
is O
provided O
in O
other O
languages O
. O
We O
fit O
a O
logistic O
regression O
classifier O
on O
top O
of O
fixed O
sentence O
representations O
and O
follow O
default O
configurations O
in O
Conneau O
and O
Kiela O
( O
2018 O
) O
; O
Gao O
et O
al O
. O
( O
2021 O
) O
. O
To O
faithfully O
reflect O
the O
multilinguality O
of O
multilingual O
sentence O
embeddings O
, O
we O
train O
exactly O
one O
model O
for O
each O
task O
. O
The O
union O
of O
the O
development O
sets O
in O
different O
languages O
is O
adopted O
for O
model O
selection O
. O

Implementation O
Details O

We O
initialize O
our O
AMR O
encoder O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
( O
uncased O
) O
and O
take O
the O
[ O
CLS O
] O
representation O
as O
the O
sentence O
embedding O
. O
By O
default O
, O
the O
AMR O
encoder O
is O
trained O
on O
English O
, O
German O
, O
Spanish O
, O
Italian O
, O
Chinese O
, O
French O
, O
and O
Arabic O
( O
ar O
) O
. O
Each O
model O
is O
trained O
for O
a O
maximum O
of O
9 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
. O
The O
temperature B-HyperparameterName
in O
Eq O
. O
( O
1 O
) O
is O
set O
to O
be O
0.05 B-HyperparameterValue
. O
For O
model O
selection O
, O
we O
use O
the O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
development O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O
We O
train O
a O
multilingual O
AMR O
parser O
on O
English O
, O
German O
, O
Spanish O
, O
Italian O
, O
Chinese O
, O
and O
French O
using O
the O
same O
recipe O
in O
Cai O
et O
al O
. O
( O
2021 O
) O
. O
We O
release O
our O
code O
at O
https O
: O
/ O
/ O
github.com O
/ O
jcyk O
/ O
MSE-AMR O
. O

Baseline O
Systems O

We O
evaluate O
the O
following O
systems O
: O
mBERT B-MethodName
/ O
XLM B-MethodName
- I-MethodName
R I-MethodName
We O
use O
the O
mean O
pooling O
of O
the O
outputs O
from O
the O
pre O
- O
trained O
mBERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
which O
are O
pre O
- O
trained O
on O
multilingual O
data O
. O
However O
, O
no O
parallel O
or O
labeled O
data O
was O
used O
. O

mUSE B-MethodName
Multilingual B-MethodName
Universal I-MethodName
Sentence I-MethodName
Encoder I-MethodName
( O
Chidambaram O
et O
al O
. O
, O
2019 O
) O
uses O
a O
dual O
- O
encoder O
transformer O
architecture O
and O
adopts O
contrastive O
objectives O
. O
It O
was O
trained O
on O
mined O
question O
- O
answer O
pairs O
, O
SNLI B-DatasetName
data O
, O
translated O
SNLI B-DatasetName
data O
, O
and O
parallel O
corpora O
over O
16 O
languages O
. O

LASER B-MethodName
LASER B-MethodName
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
trains O
a O
sequence O
- O
to O
- O
sequence O
encoder O
- O
decoder O
architecture O
on O
parallel O
corpora O
for O
machine O
translation O
. O
The O
sentence O
representation O
is O
obtained O
via O
max O
- O
pooling O
over O
the O
output O
of O
the O
encoder O
. O
LASER B-MethodName
was O
trained O
over O
93 O
languages O
. O

LaBSE B-MethodName
Language B-MethodName
- I-MethodName
agnostic I-MethodName
BERT I-MethodName
Sentence I-MethodName
Embedding I-MethodName
( O
LaBSE B-MethodName
) O
( O
Feng O
et O
al O
. O
, O
2020 O
) O
was O
trained O
similar O
to O
mUSE B-MethodName
with O
a O
translation O
ranking O
loss O
. O
It O
fine O
- O
tunes O
a O
dual O
- O
BERT B-MethodName
architecture O
with O
6 B-HyperparameterValue
Billion I-HyperparameterValue
translation O
pairs O
for O
109 O
languages O
. O

Xpara O
Reimers O
and O
Gurevych O
( O
2020 O
) O
fine O
- O
tunes O
XLM B-MethodName
- I-MethodName
R I-MethodName
to O
imitate O
SBERT B-MethodName
- O
paraphrases O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
, O
a O
RoBERTa B-MethodName
model O
trained O
on O
more O
than O
50 B-HyperparameterValue
Million I-HyperparameterValue
English O
paraphrase O
pairs O
, O
with O
massive O
bilingual O
sentence O
pairs O
over O
50 O
languages O
. O

Model O
Variants O

To O
study O
the O
effect O
of O
each O
modeling O
choice O
, O
we O
implement O
a O
series O
of O
model O
variants O
. O

• O
# O
1 O
: O
To O
show O
if O
learning O
from O
English O
data O
suffices O
, O
we O
train O
the O
AMR O
encoder O
with O
only O
English O
sentences O
and O
the O
AMR O
graphs O
derived O
from O
them O
. O

• O
# O
2 O
: O
To O
study O
the O
effect O
of O
extending O
the O
training O
of O
the O
multilingual O
AMR O
parser O
to O
French O
, O
we O
use O
the O
original O
parser O
in O
Cai O
et O
al O
. O
( O
2021 O
) O
, O
which O
does O
not O
include O
French O
. O

• O
# O
3 O
: O
To O
measure O
the O
help O
of O
involving O
more O
languages O
when O
training O
the O
AMR O
encoder O
, O
we O
train O
the O
AMR O
encoder O
without O
the O
AMR O
graphs O
derived O
from O
French O
and O
Arabic O
. O

• O
# O
4 O
: O
To O
validate O
the O
usefulness O
of O
adding O
the O
English O
sentences O
to O
the O
training O
of O
the O
AMR O
encoder O
, O
we O
train O
the O
AMR O
encoder O
without O
English O
sentences O
. O

• O
# O
5 O
: O
The O
standard O
model O
as O
described O
in O
Section O
6.2 O
. O

For O
each O
model O
variant O
, O
we O
report O
the O
average O
performance O
over O
five O
different O
runs O
( O
different O
random O
seeds O
) O
throughout O
this O
paper O
. O

Results O

Multilingual B-TaskName
STS I-TaskName
Table O
2 O
and O
Table O
3 O
show O
the O
evaluation O
results O
on O
3 O
monolingual B-TaskName
STS I-TaskName
tasks O
and O
7 O
cross B-TaskName
- I-TaskName
lingual I-TaskName
STS I-TaskName
tasks O
respectively O
. O
As O
seen O
, O
the O
best O
- O
performing O
models O
in O
the O
literature O
are O
mUSE B-MethodName
and O
Xpara B-MethodName
. O
Thus O
, O
we O
present O
the O
results O
of O
augmenting O
mUSE B-MethodName
and O
Xpara B-MethodName
with O
our O
AMR O
embeddings O
, O
denoted O
by O
mUSE++ B-MethodName
and O
Xpara++ B-MethodName
respectively O
. O
Using O
AMR O
embeddings O
substantially O
improves O
both O
two O
models O
across O
the O
monolingual O
( O
up O
to O
+2.31 B-MetricValue
on O
avg O
. O
) O
and O
cross O
- O
lingual O
settings O
( O
up O
to O
+2.22 B-MetricValue
on O
avg O
. O
) O
, O
greatly O
advancing O
the O
state O
- O
of O
- O
theart O
performance O
. O
with O
our O
AMR O
encoders O
. O
We O
hypothesize O
that O
it O
is O
because O
Xpara B-MethodName
is O
trained O
on O
paraphrase O
corpus O
, O
which O
diminishes O
the O
ability O
of O
AMR O
to O
group O
different O
expressions O
of O
the O
same O
semantics O
. O

One O
interesting O
finding O
is O
that O
model O
variant O
# O
2 O
performs O
best O
on O
monolingual O
settings O
while O
model O
variant O
# O
5 O
attains O
the O
best O
results O
on O
cross O
- O
lingual O
settings O
. O
We O
believe O
that O
adding O
more O
languages O
to O
the O
training O
of O
the O
AMR O
parser O
helps O
the O
generalization O
to O
other O
languages O
and O
reduces O
the O
parsing O
inconsistency O
across O
different O
languages O
. O
Thus O
, O
the O
AMR O
graphs O
from O
different O
languages O
are O
better O
aligned O
, O
leading O
to O
a O
better O
- O
aligned O
vector O
space O
. O
On O
the O
other O
hand O
, O
adding O
more O
language O
may O
decrease O
the O
parsing O
accuracies B-MetricName
on O
individual O
languages O
due O
to O
the O
fixed O
model O
capacity O
. O
Note O
that O
all O
other O
model O
variants O
except O
# O
2 O
underperform O
# O
5 O
, O
confirming O
the O
effectiveness O
of O
the O
proposed O
mixed O
training O
strategy O
. O

Transfer O
Tasks O
Table O
4 O
shows O
the O
evaluation O
results O
on O
transfer O
tasks O
. O
For O
each O
task O
, O
we O
report O
the O
macro O
- O
average O
scores O
across O
different O
languages O
. O
The O
results O
for O
each O
language O
can O
be O
found O
in O
Appendix O
. O
Different O
to O
previous O
work O
, O
our O
AMR O
encoders O
are O
only O
trained O
with O
a O
few O
languages O
( O
en O
, O
de O
, O
es O
, O
it O
, O
zh O
, O
fr O
, O
and O
ar O
) O
at O
most O
. O
To O
isolate O
the O
effect O
on O
unseen O
languages O
, O
we O
separate O
the O
results O
on O
those O
seen O
languages O
from O
all O
languages O
( O
seen O
/ O
all O
) O
. O
First O
of O
all O
, O
we O
find O
that O
the O
rankings O
of O
existing O
models O
are O
quite O
different O
to O
the O
results O
on O
STS B-TaskName
tasks O
. O
LASER B-MethodName
and O
LaBSE B-MethodName
achieve O
the O
best O
results O
on O
most O
transfer O
tasks O
except O
for O
QAM B-TaskName
, O
and O
outperforms O
mUSE B-MethodName
and O
Xpara B-MethodName
by O
large O
margins O
in O
most O
cases O
. O
The O
results O
demonstrate O
the O
limitation O
of O
solely O
testing O
on O
sentence O
similarity O
measurement O
. O

Next O
, O
we O
augment O
the O
best O
- O
performing O
models O
, O
LASER B-MethodName
and O
LaBSE B-MethodName
, O
with O
our O
AMR O
embeddings O
( O
LASER++ B-MethodName
and O
LaBSE++ B-MethodName
) O
. O
For O
seen O
languages O
, O
our O
methods O
substantially O
boost O
the O
performance O
of O
these O
two O
models O
across O
different O
tasks O
( O
up O
to O
+2.02 B-MetricValue
on O
avg O
. O
) O
. O
The O
performance O
gains O
over O
LASER B-MethodName
are O
greater O
than O
those O
over O
LaBSE B-MethodName
. O
Note O
that O
LASER B-MethodName
is O
trained O
with O
an O
encoder O
- O
decoder O
architecture O
and O
both O
LaBSE B-MethodName
and O
our O
AMR O
encoders O
are O
trained O
with O
a O
Siamese O
network O
. O
Therefore O
, O
we O
believe O
the O
AMR O
embeddings O
are O
more O
complementary O
to O
LASER B-MethodName
. O

When O
considering O
all O
languages O
, O
the O
improvements O
over O
LASER B-MethodName
are O
also O
considerable O
( O
up O
to O
+1.11 B-MetricName
on O
avg O
. O
) O
. O
However O
, O
according O
to O
the O
average O
scores O
over O
different O
tasks O
, O
the O
AMR O
embeddings O
seem O
to O
fail O
to O
improve O
LaBSE B-MethodName
; O
We O
even O
observe O
a O
performance O
drop O
for O
model O
variants O
# O
1- O
# O
3 O
. O
Nevertheless O
, O
the O
performance O
drop O
largely O
comes O
from O
XNLI B-DatasetName
while O
the O
scores O
on O
other O
tasks O
are O
instead O
boosted O
. O
This O
is O
because O
the O
test O
sets O
of O
XNLI B-TaskName
include O
some O
distant O
languages O
( O
e.g. O
, O
Swahili O
and O
Urdu O
) O
that O
our O
multilingual O
AMR O
parser O
can O
not O
handle O
well O
( O
see O
the O
results O
on O
individual O
languages O
in O
Table O
6 O
in O
Appendix O
) O
. O
We O
conjecture O
that O
further O
extending O
the O
multilingual O
AMR O
parser O
to O
more O
languages O
can O
alleviate O
this O
problem O
. O
The O
comparison O
among O
different O
model O
variants O
provides O
a O
basis O
for O
the O
above O
speculation O
. O
As O
we O
can O
see O
, O
model O
variant O
# O
2 O
( O
exclude O
French O
from O
the O
training O
of O
the O
multilingual O
AMR O
parser O
) O
performs O
worst O
. O
Also O
, O
model O
variants O
# O
1 O
( O
drop O
all O
non O
- O
English O
AMR O
graphs O
for O
training O
) O
and O
# O
2 O
( O
drop O
the O
AMR O
graphs O
derived O
from O
French O
and O
Arabic O
) O
are O
the O
other O
two O
variants O
that O
negatively O
impact O
the O
average O
performance O
. O
Another O
interesting O
observation O
is O
that O
model O
variant O
# O
4 O
performs O
best O
on O
MLDoc B-DatasetName
and O
QAM B-DatasetName
, O
suggesting O
English O
sentences O
might O
not O
be O
necessary O
. O

Conclusion O

This O
paper O
presented O
a O
thorough O
evaluation O
of O
existing O
multilingual O
sentence O
embeddings O
, O
ranging O
from O
traditional O
text O
similarity O
measurement O
to O
a O
new O
variety O
of O
transfer O
tasks O
. O
We O
found O
that O
different O
methods O
excel O
at O
different O
tasks O
. O
We O
then O
proposed O
to O
improve O
existing O
methods O
with O
universal O
AMR O
embeddings O
, O
which O
leads O
to O
better O
performance O
on O
all O
tasks O
. O

Limitations O

Although O
our O
work O
provides O
an O
effective O
solution O
for O
improving O
multilingual O
sentence O
embeddings O
with O
AMR O
, O
we O
acknowledge O
some O
limitations O
of O
this O
study O
and O
further O
discuss O
them O
in O
the O
following O
: O

( O
1 O
) O
Our O
framework O
treats O
the O
text O
encoder O
as O
a O
black O
box O
and O
does O
not O
care O
too O
much O
about O
its O
implementation O
. O
Although O
it O
is O
flexible O
and O
straightforward O
to O
apply O
our O
framework O
to O
any O
multilingual O
sentence O
embedding O
model O
, O
designing O
more O
specific O
interaction O
mechanisms O
for O
different O
text O
encoders O
is O
supposed O
to O
be O
better O
and O
we O
leave O
it O
as O
future O
work O
. O

( O
2 O
) O
The O
improvement O
from O
our O
framework O
is O
higher O
in O
seen O
languages O
than O
unseen O
languages O
. O
Further O
extending O
the O
language O
coverage O
in O
the O
training O
phases O
of O
both O
the O
multilingual O
AMR O
parser O
and O
the O
AMR O
encoder O
is O
presumably O
beneficial O
to O
the O
cross O
- O
lingual O
generalization O
capability O
of O
the O
AMR O
embeddings O
. O
However O
, O
due O
to O
the O
limit O
of O
computational O
resources O
, O
we O
only O
consider O
a O
few O
languages O
in O
the O
experiments O
. O

A O
Transfer O
task O

We O
provide O
the O
detailed O
results O
on O
each O
language O
for O
each O
transfer O
task O
in O

Training B-MethodName
Language I-MethodName
Models I-MethodName
with I-MethodName
Memory I-MethodName
Augmentation I-MethodName

Recent O
work O
has O
improved O
language O
models O
( O
LMs O
) O
remarkably O
by O
equipping O
them O
with O
a O
non O
- O
parametric O
memory O
component O
. O
However O
, O
most O
existing O
approaches O
only O
introduce O
memories O
at O
testing O
time O
or O
represent O
them O
using O
a O
separately O
trained O
encoder O
, O
resulting O
in O
suboptimal O
training O
of O
the O
language O
model O
. O
In O
this O
work O
, O
we O
present O
TRIME B-MethodName
, O
a O
novel O
yet O
simple O
training O
approach O
designed O
for O
training O
LMs O
with O
memory O
augmentation O
. O
Our O
approach O
uses O
a O
training O
objective O
that O
directly O
takes O
inbatch O
examples O
as O
accessible O
memory O
. O
We O
also O
present O
new O
methods O
for O
memory O
construction O
and O
data O
batching O
, O
which O
are O
used O
for O
adapting O
to O
different O
sets O
of O
memories O
- O
local O
, O
longterm O
, O
and O
external O
memory O
- O
at O
testing O
time O
. O
We O
evaluate O
TRIME B-MethodName
on O
multiple O
language B-TaskName
modeling I-TaskName
and O
machine B-TaskName
translation I-TaskName
benchmarks O
and O
show O
that O
it O
is O
able O
to O
achieve O
significant O
improvements O
across O
all O
the O
settings O
. O
Concretely O
, O
TRIME B-MethodName
reduces O
the O
perplexity B-MetricName
from O
18.70 B-MetricValue
to O
15.37 B-MetricValue
on O
WIKITEXT-103 B-DatasetName
, O
by O
effectively O
leveraging O
a O
large O
memory O
set O
from O
the O
training O
corpus O
. O
Compared O
to O
standard O
LM O
training O
, O
TRIME B-MethodName
adds O
negligible O
computational O
overhead O
and O
is O
compatible O
with O
different O
neural O
architectures O
, O
making O
it O
a O
versatile O
solution O
for O
training O
memory O
- O
augmented O
LMs O
. O
1 O

Introduction O

Memory O
augmentation O
has O
become O
a O
remarkable O
approach O
to O
enhance O
language O
modeling O
performance O
without O
significantly O
increasing O
the O
amount O
of O
parameters O
and O
computation O
. O
By O
accessing O
memory O
units O
such O
as O
a O
neural O
cache O
of O
recent O
inputs O
( O
Merity O
et O
al O
. O
, O
2017 O
; O
Grave O
et O
al O
. O
, O
2017b O
) O
and O
an O
external O
look O
- O
up O
table O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
, O
a O
memory O
- O
augmented O
language O
model O
( O
LM O
) O
enjoys O
increased O
memorization O
capacity O
and O
sets O
* O
TL O
currently O
works O
at O
Google O
Research O
. O
The O
collaboration O
was O
initialized O
before O
TL O
joined O
Google O
. O

1 O
Our O
code O
and O
pre O
- O
trained O
models O
are O
publicly O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
princeton-nlp O
/ O
TRIME O
. O
new O
state O
- O
of O
- O
the O
- O
art O
records O
in O
various O
language O
modeling O
benchmarks O
. O

A O
major O
limitation O
of O
existing O
approaches O
, O
however O
, O
is O
that O
the O
memory O
units O
are O
either O
introduced O
at O
testing O
time O
( O
Grave O
et O
al O
. O
, O
2017b O
, O
a O
; O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
or O
taken O
from O
a O
separately O
trained O
model O
. O
As O
a O
consequence O
, O
they O
are O
not O
directly O
optimized O
during O
the O
training O
process O
, O
resulting O
in O
a O
missed O
opportunity O
to O
achieve O
even O
stronger O
results O
. O
In O
this O
paper O
, O
we O
pioneer O
and O
present O
a O
novel O
yet O
simple O
training O
approach O
TRIME B-MethodName
( O
Training O
with O
In O
- O
batch O
Memories O
) O
2 O
, O
that O
is O
well O
- O
suited O
for O
memory O
augmentation O
in O
language O
modeling O
. O
Our O
approach O
makes O
two O
major O
departures O
compared O
to O
standard O
language O
model O
training O
: O

Training O
objective O
Inspired O
by O
contrastive O
representation O
learning O
, O
we O
propose O
a O
training O
objective O
that O
directly O
leverages O
in O
- O
batch O
examples O
as O
accessible O
memory O
( O
Figure O
1 O
) O
. O
Our O
training O
ob O
- O
jective O
is O
closely O
connected O
to O
neural B-MethodName
cache I-MethodName
models I-MethodName
( O
Grave O
et O
al O
. O
, O
2017b O
; O
Merity O
et O
al O
. O
, O
2017 O
) O
and O
nearest B-MethodName
- I-MethodName
neighbor I-MethodName
language I-MethodName
models I-MethodName
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
, O
where O
the O
next O
- O
token O
probabilities O
are O
calculated O
by O
comparing O
encoder O
outputs O
against O
static O
token O
embeddings O
and O
memory O
representations O
. O
However O
, O
previous O
work O
only O
considers O
incorporating O
memories O
at O
testing O
time O
, O
while O
we O
do O
for O
both O
training O
and O
testing O
. O

In O
- O
batch O
memory O
construction O
With O
this O
training O
objective O
in O
mind O
, O
the O
key O
challenge O
is O
how O
to O
construct O
memories O
effectively O
during O
training O
while O
keeping O
it O
efficient O
. O
We O
identify O
three O
types O
of O
memories O
that O
can O
be O
leveraged O
at O
testing O
time O
and O
have O
been O
explored O
in O
the O
literature O
: O
( O
a O
) O
local O
memory O
denotes O
the O
words O
that O
appear O
in O
the O
recent O
past O
and O
are O
modeled O
using O
attention O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
; O
( O
b O
) O
long O
- O
term O
memory O
3 O
denotes O
longrange O
context O
from O
the O
same O
document O
but O
can O
not O
be O
directly O
accessed O
due O
to O
the O
limit O
of O
input O
length O
; O
( O
c O
) O
external O
memory O
is O
used O
to O
store O
the O
entire O
training O
set O
or O
any O
additional O
corpus O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
; O
Borgeaud O
et O
al O
. O
, O
2021 O
) O
. O

To O
better O
leverage O
these O
memories O
at O
testing O
time O
, O
we O
devise O
new O
data O
batching O
strategies O
to O
improve O
the O
construction O
of O
training O
memories O
( O
§ O
4 O
) O
. O
By O
packing O
consecutive O
segments O
from O
the O
same O
document O
in O
one O
training O
batch O
, O
our O
model O
can O
access O
long O
- O
term O
memories O
beyond O
the O
attention O
context O
. O
We O
pack O
segments O
from O
other O
documents O
that O
have O
high O
lexical O
overlap O
as O
a O
proxy O
to O
all O
external O
memory O
units O
. O
Importantly O
, O
these O
working O
memories O
are O
generated O
on O
the O
fly O
during O
training O
, O
allowing O
us O
to O
back O
- O
propagate O
to O
all O
memory O
representations O
. O

We O
instantiate O
TRIME B-MethodName
in O
three O
models O
by O
considering O
different O
sets O
of O
training O
and O
testing O
memories O
( O
Table O
1 O
) O
and O
evaluate O
them O
on O
multiple O
language B-TaskName
modeling I-TaskName
and O
machine B-TaskName
translation I-TaskName
benchmarks O
. O
We O
highlight O
our O
results O
as O
follows O
: O

• O
We O
first O
show O
that O
we O
can O
simply O
optimize O
a O
language O
model O
using O
our O
training O
objective O
without O
long O
- O
term O
and O
external O
memory O
. O
Without O
any O
other O
modifications O
, O
we O
demonstrate O
that O
a O
247 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-HyperparameterName
- I-HyperparameterName
based I-HyperparameterName
model I-HyperparameterName
can O
achieve O
an O
improved O
perplexity B-MetricName
from O
18.70 B-MetricValue
to O
17.76 B-MetricValue
on O
WIKITEXT-103 B-DatasetName
( O
Merity O
et O
al O
. O
, O
2017 O
) O
for O
vanilla O
language O
models O
. O

• O
By O
training O
with O
consecutive O
segments O
in O
the O
same O
batch O
, O
our O
approach O
is O
capable O
of O
leveraging O
very O
long O
context O
at O
testing O
time O
- O
up O
to O
15k-25k B-HyperparameterValue
tokens O
on O
WIKITEXT-103 B-DatasetName
and O
ENWIK8 B-DatasetName
( O
Mahoney O
, O
2009 O
) O
. O
Our O
approach O
achieves O
at O
least O
competitive O
performance O
as O
previous O
works O
( O
Dai O
et O
al O
. O
, O
2019 O
; O
Martins O
et O
al O
. O
, O
2022 O
; O
Ji O
et O
al O
. O
, O
2022 O
) O
that O
modify O
the O
Transformer B-MethodName
architecture O
to O
incorporate O
memories O
from O
previous O
segments O
, O
yet O
our O
solution O
is O
conceptually O
simpler O
and O
computationally O
cheaper O
. O

• O
Finally O
, O
we O
train O
language O
models O
by O
incorporating O
all O
other O
segments O
in O
the O
same O
batch O
as O
memories O
. O
Our O
model O
works O
better O
with O
a O
large O
datastore O
at O
testing O
time O
and O
improves O
over O
the O
kNN B-MethodName
- I-MethodName
LM I-MethodName
model O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
by O
reducing O
the O
test O
perplexity B-MetricName
from O
16.23 B-MetricValue
to O
15.41 B-MetricValue
on O
WIKITEXT-103 B-DatasetName
. O
We O
also O
demonstrate O
significant O
improvements O
over O
the O
kNN B-MethodName
- I-MethodName
MT I-MethodName
baseline O
( O
Khandelwal O
et O
al O
. O
, O
2021 O
) O
on O
an O
IWSLT'14 B-DatasetName
De O
- O
En O
machine B-TaskName
translation I-TaskName
task O
. O

In O
summary O
, O
we O
propose O
a O
simple O
approach O
TRIME B-MethodName
for O
optimizing O
language O
models O
with O
memory O
augmentation O
and O
demonstrate O
consistent O
and O
significant O
gains O
in O
multiple O
experimental O
settings O
. O
Our O
approach O
only O
uses O
memories O
at O
the O
final O
prediction O
step O
, O
and O
hence O
adds O
little O
computational O
overhead O
and O
can O
be O
combined O
with O
different O
model O
architectures O
such O
as O
recurrent O
networks O
and O
other O
attention O
variants O
( O
Lei O
, O
2021 O
; O
Dai O
et O
al O
. O
, O
2019 O
; O
Rae O
et O
al O
. O
, O
2020 O
) O
. O
We O
hope O
that O
our O
work O
can O
encourage O
the O
research O
community O
to O
think O
about O
better O
training O
objectives O
for O
language O
models O
, O
given O
their O
significant O
societal O
impacts O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Chowdhery O
et O
al O
. O
, O
2022 O
; O
. O

Preliminaries O

Language B-TaskName
Modeling I-TaskName

In O
this O
paper O
, O
we O
mainly O
focus O
on O
improving O
language O
models O
, O
although O
our O
solutions O
may O
extend O
to O
most O
text O
generation O
tasks O
( O
see O
one O
example O
of O
machine B-TaskName
translation I-TaskName
in O
§ O
5.4 O
) O
. O
Neural O
language O
models O
take O
a O
sequence O
of O
tokens O
as O
context O
c O
t O
= O
x O
1 O
, O
. O
. O
. O
, O
x O
t−1 O
and O
map O
it O
to O
a O
vector O
representation O
f O
θ O
( O
c O
t O
) O
∈ O
R O
d O
, O
where O
f O
θ O
( O
⋅ O
) O
is O
parameterized O
by O
a O
neural O
network O
. O
The O
next O
- O
token O
probability O
is O
: O

P O
( O
w O
c O
t O
) O
∝ O
exp O
( O
E O
⊺ O
w O
f O
θ O
( O
c O
t O
) O
) O
, O
( O
1 O
) O

where O
E O
w O
∈ O
R O
d O
denotes O
the O
output O
embedding O
of O
token O
w O
∈ O
V. O
The O
parameters O
are O
optimized O
to O
minimize O
the O
negative O
log O
- O
likelihood O
of O
ground O
truth O
x O
t O
during O
training O
. O

Memory O
Augmentation O

We O
consider O
memory O
as O
a O
set O
of O
context O
- O
target O
pairs O
{ O
( O
c O
i O
, O
x O
i O
) O
} O
following O
Grave O
et O
al O
. O
( O
2017b O
) O
; O
Khandelwal O
et O
al O
. O
( O
2020 O
) O
. O
These O
context O
- O
target O
pairs O
can O
be O
aggregated O
to O
obtain O
the O
next O
- O
token O
probability O
weighted O
by O
the O
similarity O
between O
hidden O
representations O
. O
4 O
We O
formalize O
three O
types O
of O
contexttarget O
memories O
as O
follows O
: O

Local O
memory O
The O
local O
memory O
is O
simply O
the O
preceding O
tokens O
in O
the O
same O
input O
. O
Specifically O
, O
for O
c O
t O
= O
x O
1 O
, O
. O
. O
. O
, O
x O
t−1 O
, O
it O
is O
defined O
as O
: O

M O
local O
( O
c O
t O
) O
= O
{ O
( O
c O
j O
, O
x O
j O
) O
} O
1≤j≤t−1 O
. O

( O
2 O
) O
Grave O
et O
al O
. O
( O
2017b O
) O
use O
the O
local O
memory O
at O
testing O
time O
, O
denoted O
by O
the O
" O
continuous O
cache O
" O
model O
. O
However O
, O
it O
has O
been O
argued O
less O
effective O
for O
Transformer B-MethodName
- O
based O
models O
because O
they O
can O
already O
learn O
to O
leverage O
recent O
tokens O
in O
the O
selfattention O
layers O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
. O
Interestingly O
, O
we O
show O
that O
using O
local O
memory O
is O
still O
beneficial O
if O
we O
consider O
it O
during O
training O
. O

Long O
- O
term O
memory O
Long O
- O
term O
memory O
denotes O
long O
- O
range O
context O
from O
the O
same O
document O
, O
but O
they O
can O
not O
be O
directly O
accessed O
by O
attention O
. O
For O
example O
, O
if O
a O
document O
contains O
10 B-HyperparameterValue
K I-HyperparameterValue
tokens O
, O
only O
a O
short O
segment O
of O
text O
( O
e.g. O
, O
100 O
- O
3 B-HyperparameterValue
K I-HyperparameterValue
tokens O
) O
can O
be O
fed O
into O
a O
Transformer B-MethodName
model O
because O
the O
complexity O
scales O
quadratically O
with O
the O
input O
length O
. O
Formally O
, O
we O
divide O
a O
document O
into O
consecutive O
segments O
s O
( O
1 O
) O
, O
. O
. O
. O
, O
s O
( O
T O
) O
, O
where O
a O
segment O

s O
( O
i O
) O
contains O
L O
contexts O
s O
( O
i O
) O
= O
{ O
c O
( O
i O
) O
1 O
, O
. O
. O
. O
, O
c O
( O
i O
) O
L O
} O
. O
The O
long O
- O
term O
memory O
for O
c O
( O
i O
) O
t O
is O
: O
M O
long O
( O
c O
( O
i O
) O
t O
) O
= O
{ O
( O
c O
( O
k O
) O
j O
, O
x O
( O
k O
) O
j O
) O
} O
1≤k O
< O
i,1≤j≤L O
. O
( O
3 O
) O

Previous O
works O
( O
Dai O
et O
al O
. O
, O
2019 O
; O
Rae O
et O
al O
. O
, O
2020 O
; O
Martins O
et O
al O
. O
, O
2022 O
; O
Ji O
et O
al O
. O
, O
2022 O
; O
Lei O
, O
2021 O
) O
leverage O
hidden O
representations O
from O
previous O
segments O
with O
modified O
Transformer B-MethodName
architectures O
to O
learn O
long O
- O
range O
dependency O
. O
Our O
approach O
does O
not O
modify O
the O
model O
architecture O
and O
is O
compatible O
with O
these O
neural O
architectures O
. O
5 O
External O
memory O
Finally O
, O
external O
memory O
assumes O
a O
large O
corpus O
D O
and O
the O
external O
memory O
set O
can O
be O
defined O
as O
: O

M O
ext O
= O
{ O
( O
c O
j O
, O
x O
j O
) O
∈ O
D O
} O
. O
( O
4 O
) O

D O
can O
be O
simply O
the O
training O
corpus O
, O
or O
a O
domainspecific O
corpus O
when O
the O
testing O
domain O
shifts O
( O
§ O
5.3 O
) O
. O
Note O
that O
M O
ext O
is O
usually O
several O
orders O
of O
magnitude O
larger O
than O
previous O
two O
types O
( O
e.g. O
, O
10 O
8 O
) O
; O
accessing O
all O
the O
memories O
is O
computationally O
expensive O
and O
requires O
approximate O
nearest O
neighbor O
search O
( O
Johnson O
et O
al O
. O
, O
2019 O
) O
. O
defines O
the O
next O
- O
token O
probability O
distribution O
as O
: O

P O
( O
w O
c O
) O
∝ O
exp O
( O
E O
⊺ O
w O
f O
θ O
( O
c O
) O
) O
+ O
( O
c O
j O
, O
x O
j O
) O
∈M O
train O
∶x O
j O
= O
w O
exp O
( O
sim O
( O
g O
θ O
( O
c O
) O
, O
g O
θ O
( O
c O
j O
) O
) O
) O
. O
( O
5 O
) O

Here O
, O
f O
θ O
( O
c O
) O
is O
the O
output O
representation O
of O
a O
Transformer B-MethodName
model O
and O
E O
w O
is O
the O
token O
embedding O
. O
g O
θ O
( O
⋅ O
) O
denotes O
the O
representations O
that O
can O
be O
used O
to O
compute O
similarity O
between O
c O
and O
all O
the O
contexts O
c O
j O
in O
the O
memory O
M O
train O
. O
It O
is O
possible O
to O
simply O
take O
g O
θ O
= O
f O
θ O
; O
however O
, O
we O
find O
that O
taking O
g O
θ O
to O
be O
the O
input O
of O
the O
final O
feed O
- O
forward O
layer O
in O
Transformer B-MethodName
works O
better O
, O
which O
is O
consistent O
with O
the O
observation O
in O
Khandelwal O
et O
al O
. O
( O
2020 O
) O
. O
In O
addition O
, O
sim O
( O
⋅ O
, O
⋅ O
) O
is O
a O
similarity O
function O
and O
we O
found O
using O
the O
scaled O
dot O
- O
product O
sim O
( O
q O
, O
k O
) O
= O
q⋅k O
√ O
d O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
leads O
to O
stable O
training O
and O
better O
performance O
in O
our O
preliminary O
experiments O
. O

This O
training O
objective O
can O
be O
viewed O
as O
a O
contrastive O
loss O
( O
Hadsell O
et O
al O
. O
, O
2006 O
) O
: O
for O
a O
contexttarget O
pair O
( O
c O
, O
w O
* O
) O
, O
the O
goal O
is O
to O
align O
the O
query O
representation O
f O
θ O
( O
c O
) O
( O
and O
g O
θ O
( O
c O
) O
) O
with O
the O
static O
token O
representation O
E O
w O
* O
, O
and O
contextualized O
representations O
that O
share O
the O
same O
next O
token O
i.e. O
, O
g O
θ O
( O
c O
j O
) O
for O
x O
j O
= O
w O
* O
. O
Our O
objective O
handles O
rare O
words O
nicely O
- O
if O
w O
* O
does O
not O
appear O
in O
the O
training O
memory O
, O
the O
objective O
will O
fall O
back O
to O
aligning O
f O
θ O
( O
c O
) O
with O
only O
the O
word O
embedding O
E O
w O
* O
. O
Similar O
to O
the O
vanilla O
training O
loss O
( O
Eq O
. O
1 O
) O
, O
our O
TRIME B-MethodName
loss O
is O
optimized O
to O
minimize O
the O
negative O
log O
- O
likelihood O
of O
next O
token O
w O
* O
and O
all O
the O
parameters O
θ O
and O
E O
w O
are O
updated O
during O
training O
. O

Our O
training O
objective O
is O
also O
inspired O
by O
the O
success O
of O
contrastive O
learning O
in O
dense O
retrieval O
. O
As O
we O
will O
show O
in O
§ O
6 O
, O
it O
can O
help O
improve O
retrieving O
contexts O
that O
share O
the O
same O
next O
token O
effectively O
when O
the O
set O
of O
testing O
memories O
is O
large O
. O
Our O
objective O
is O
also O
closely O
connected O
to O
the O
objective O
used O
in O
Grave O
et O
al O
. O
( O
2017b O
) O
; O
Khandelwal O
et O
al O
. O
( O
2020 O
) O
, O
which O
linearly O
interpolates O
the O
distribution O
of O
standard O
language O
modeling O
, O
and O
a O
distribution O
defined O
by O
cache O
/ O
external O
datastore O
, O
e.g. O
, O
P O
( O
w O
c O
) O
= O
( O
1−λ B-HyperparameterName
) O
P O
lm O
( O
w O
c O
) O
+λP B-HyperparameterName
kNN O
( O
w O
c O
) O
. O
Our O
work O
differs O
from O
previous O
works O
that O
we O
use O
this O
objective O
during O
training O
( O
and O
testing O
) O
, O
while O
they O
only O
used O
it O
at O
testing O
time O
- O
the O
key O
is O
how O
to O
construct O
training O
memories O
that O
we O
will O
elaborate O
next O
. O
6 O

Adaption O
to O
Different O
Memories O

Inference O
We O
are O
interested O
in O
incorporating O
the O
three O
types O
of O
memories O
defined O
in O
§ O
2.2 O
and O
their O
combinations O
at O
testing O
time O
. O
The O
testing O
objective O
is O
basically O
the O
same O
as O
the O
training O
objective O
( O
Eq O
. O
5 O
) O
except O
that O
we O
take O
testing O
memories O
as O
a O
combination O
of O
M O
local O
, O
M O
long O
and O
M O
ext O
. O
As O
M O
ext O
can O
be O
very O
large O
, O
we O
approximate O
it O
by O
retrieving O
the O
top O
- O
K B-HyperparameterName
closest O
terms O
to O
g O
θ O
( O
c O
) O
. O
We O
tune O
a O
temperature B-HyperparameterName
term O
τ B-HyperparameterName
to O
adjust O
the O
weight O
of O
the O
memory O
component O
( O
see O
Appendix O
A O
for O
details O
) O
. O

Notation O
Throughout O
this O
section O
, O
we O
use O
L B-HyperparameterName
to O
denote O
segment O
length O
, O
B B-HyperparameterName
to O
denote O
the O
total O
number O
of O
segments O
used O
in O
the O
one O
training O
batch O
, O
and O
m O
to O
denote O
the O
number O
of O
consecutive O
segments O
from O
each O
document O
in O
the O
batch O
. O
Correspondingly O
, O
each O
batch O
will O
contain O
b B-HyperparameterName
≈ O
B B-HyperparameterName
m B-HyperparameterName
different O
documents O
. O
L B-HyperparameterName
, O
B B-HyperparameterName
and O
m B-HyperparameterName
are O
hyper O
- O
parameters O
that O
we O
will O
choose O
for O
training O
, O
and O
will O
vary O
as O
we O
consider O
different O
memories O
during O
inference O
. O

A O
key O
challenge O
is O
that O
the O
testing O
memories O
can O
be O
very O
large O
( O
e.g. O
, O
M O
long O
∼ O
10 O
4 O
and O
M O
ext O
∼ O
10 O
8 O
in O
our O
experiments O
) O
and O
it O
is O
computationally O
infeasible O
to O
keep O
training O
memories O
the O
same O
as O
testing O
memories O
. O
In O
the O
following O
, O
we O
will O
discuss O
three O
ways O
of O
constructing O
training O
memories O
and O
data O
batching O
, O
aiming O
to O
reduce O
the O
discrepancy O
between O
training O
and O
testing O
. O
Along O
the O
way O
, O
we O
will O
also O
present O
three O
major O
model O
instantiations O
: O
TRIMELM B-MethodName
, O
TRIMELM B-MethodName
long I-MethodName
, O
TRIMELM B-MethodName
ext O
( O
Table O
1 O
) O
, O
which O
combine O
the O
training O
strategies O
and O
different O
sets O
of O
testing O
memories O
. O

Local O
Memory O

M O
local O
only O
considers O
all O
the O
previous O
tokens O
in O
the O
same O
segment O
. O
It O
is O
straightforward O
that O
we O
can O
simply O
use O
M O
train O
= O
M O
local O
. O
As O
shown O
in O
Fig O
. O
2 O
( O
a O
) O
, O
we O
basically O
do O
not O
need O
to O
make O
any O
modifications O
compared O
to O
standard O
language O
model O
training O
. O
All O
we O
need O
is O
to O
replace O
the O
training O
objective O
of O
Eq O
. O
1 O
by O
our O
objective O
in O
Eq O
. O
5 O
, O
by O
incorporating O
( O
c O
j O
, O
x O
j O
) O
, O
∀j O
< O
t O
in O
the O
memory O
during O
both O
training O
and O
testing O
. O
The O
computational O
overhead O
is O
also O
negligible O
compared O
to O
running O
neural O
encoders O
on O
the O
segment O
x O
1 O
, O
. O
. O
. O
, O
x O
L O
itself O
. O
We O
denote O
this O
model O
as O
TRIMELM B-MethodName
, O
which O
can O
be O
viewed O
as O
a O
lightweight O
replacement O
for O
vanilla O
language O
models O
. O
As O
we O
will O
show O
in O
the O
experiments O
, O
simply O
incorporating O
local O
memory O
provides O
a O
notable O
gain O
on O
multi O
- O
ple O
LM O
benchmarks O
, O
showing O
the O
effectiveness O
of O
training O
with O
memories O
explicitly O
. O

Long O
- O
term O
Memory O

In O
order O
to O
enable O
long O
- O
term O
memory O
augmentation O
, O
we O
pack O
multiple O
consecutive O
segments O
from O
the O
same O
document O
in O
a O
training O
batch O
( O
i.e. O
, O
m B-HyperparameterName
> O
1 B-HyperparameterValue
) O
. O
For O
a O
context O
- O
target O
pair O
( O
c O
, O
w O
) O
in O
the O
training O
batch O
, O
its O
accessible O
memory O
M O
train O
includes O
tokens O
from O
previous O
segments O
as O
well O
as O
the O
preceding O
tokens O
in O
the O
same O
segment O
. O
We O
denote O
this O
model O
as O
TRIMELM B-MethodName
long I-MethodName
. O
It O
shares O
a O
similar O
motivation O
with O
many O
previous O
works O
which O
aim O
to O
leverage O
memory O
from O
previous O
segments O
through O
attention O
recurrence O
( O
Dai O
et O
al O
. O
, O
2019 O
; O
Ji O
et O
al O
. O
, O
2022 O
) O
, O
or O
memory O
compression O
( O
Rae O
et O
al O
. O
, O
2020 O
; O
Martins O
et O
al O
. O
, O
2022 O
; O
. O
However O
, O
our O
solution O
deviates O
significantly O
from O
previous O
approaches O
. O
First O
, O
previous O
works O
need O
to O
store O
the O
hidden O
representations O
( O
of O
every O
layer O
) O
from O
previous O
segments O
and O
modify O
the O
self O
- O
attention O
layers O
to O
incorporate O
them O
. O
Our O
approach O
does O
not O
modify O
the O
architecture O
and O
only O
uses O
the O
outputs O
from O
the O
last O
layer O
. O
Additionally O
, O
previous O
works O
use O
stale O
memory O
representations O
and O
do O
not O
back O
- O
propagate O
gradients O
to O
the O
rep O
- O
resentations O
of O
previous O
segments O
, O
whereas O
our O
batching O
method O
enables O
gradient O
propagation O
to O
the O
memory O
and O
previous O
segments O
. O
7 O
As O
we O
will O
show O
in O
the O
experiments O
, O
our O
approach O
is O
competitive O
with O
previous O
works O
while O
being O
conceptually O
simpler O
and O
computationally O
cheaper O
. O

External O
Memory O

Finally O
, O
we O
consider O
external O
memory O
M O
ext O
. O
Since O
M O
ext O
contains O
the O
context O
- O
target O
pairs O
in O
a O
large O
corpus O
such O
as O
the O
entire O
training O
set O
, O
we O
need O
to O
retrieve O
top O
- O
K B-HyperparameterName
pairs O
from O
M O
ext O
measured O
by O
sim O
( O
g O
θ O
( O
c O
) O
, O
g O
θ O
( O
c O
j O
) O
) O
through O
( O
approximate O
) O
similarity O
search O
( O
more O
details O
are O
given O
in O
§ O
5.2 O
) O
. O

Since O
the O
retrieved O
contexts O
at O
testing O
time O
are O
expected O
to O
be O
similar O
to O
the O
query O
context O
, O
we O
propose O
a O
simple O
heuristic O
for O
constructing O
training O
memories O
M O
train O
by O
packing O
segments O
that O
have O
large O
lexical O
overlap O
into O
the O
same O
batch O
using O
BM25 B-MetricName
scores I-MetricName
( O
Robertson O
and O
Zaragoza O
, O
2009 O
) O
. O
Specifically O
, O
we O
start O
with O
a O
single O
segment O
and O
repeatedly O
add O
segments O
with O
highest O
BM25 B-MetricName
scores O
into O
the O
same O
batch O
( O
Appendix O
B O
) O
. O
A O
high O
BM25 B-MetricName
score I-MetricName
indicates O
that O
two O
segments O
have O
high O
lexical O
overlap O
and O
can O
serve O
as O
a O
good O
proxy O
to O
nearest O
neighbors O
in O
the O
external O
memory O
, O
which O
improves O
our O
model O
predictions O
at O
testing O
time O
. O
M O
train O
contains O
all O
tokens O
from O
other O
segments O
as O
well O
as O
the O
previous O
tokens O
in O
the O
same O
segment O
( O
Figure O
2 O
( O
c O
) O
) O
. O
We O
set O
m B-HyperparameterName
= O
1 B-HyperparameterValue
during O
training O
as O
many O
segments O
from O
the O
same O
document O
tend O
to O
have O
high O
lexical O
overlap O
and O
denote O
this O
model O
by O
TRIMELM B-MethodName
ext I-MethodName
. O

In O
practice O
, O
when O
considering O
tokens O
from O
both O
the O
current O
segment O
and O
other O
segments O
in O
the O
batch O
, O
we O
observe O
that O
the O
model O
tends O
to O
leverage O
local O
memory O
more O
and O
ignore O
other O
segments O
. O

To O
encourage O
the O
use O
of O
information O
from O
other O
segments O
, O
we O
exclude O
the O
local O
memory O
from O
M O
train O
with O
a O
probability B-HyperparameterName
of O
p B-HyperparameterName
during O
training O
( O
we O
find O
that O
p B-HyperparameterName
= O
90 B-HyperparameterValue
% I-HyperparameterValue
works O
the O
best O
, O
see O
Appendix O
H O
) O
. O
This O
significantly O
improves O
performance O
when O
the O
model O
is O
evaluated O
with O
a O
large O
set O
of O
external O
memory O
. O

Experiments O

Datasets O
and O
Tasks O

We O
evaluate O
our O
approach O
on O
two O
popular O
language B-TaskName
modeling I-TaskName
benchmarks O
: O
WIKITEXT-103 B-DatasetName
( O
Merity O
et O
al O
. O
, O
2017 O
) O
, O
ENWIK8 B-DatasetName
( O
Mahoney O
, O
2009 O
) O
, O
and O
a O
machine B-TaskName
translation I-TaskName
benchmark O
: O
IWSLT'14 B-DatasetName
De I-DatasetName
- I-DatasetName
En I-DatasetName
. O
We O
also O
evaluate O
domain O
- O
adaptation O
performance O
on O
the O
BOOKSCORPUS B-DatasetName
dataset O
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
. O

WIKITEXT-103 B-DatasetName
is O
a O
word B-TaskName
- I-TaskName
level I-TaskName
language I-TaskName
modeling I-TaskName
dataset O
consisting O
of O
103 B-HyperparameterValue
M I-HyperparameterValue
training O
tokens O
. O
We O
evaluate O
on O
two O
model O
configurations O
: O
one O
uses O
a O
247 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-MethodName
model O
and O
a O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
3 B-HyperparameterValue
, I-HyperparameterValue
072 I-HyperparameterValue
and O
another O
one O
uses O
a O
150 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-MethodName
model O
with O
a O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
150 B-HyperparameterValue
. O

ENWIK8 B-DatasetName
is O
a O
character B-TaskName
- I-TaskName
level I-TaskName
language I-TaskName
modeling I-TaskName
dataset O
that O
contains O
a O
total O
of O
100 B-HyperparameterValue
M I-HyperparameterValue
characters O
. O
We O
use O
a O
12 B-HyperparameterValue
- O
layer B-HyperparameterName
Transformer B-HyperparameterName
model O
with O
a O
hidden B-HyperparameterName
dimension I-HyperparameterName
512 B-HyperparameterValue
and O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
512 B-HyperparameterValue
. O

BOOKSCORPUS B-DatasetName
is O
a O
word B-TaskName
- I-TaskName
level I-TaskName
language I-TaskName
modeling I-TaskName
dataset O
. O
We O
build O
our O
own O
train B-HyperparameterName
/ O
dev B-HyperparameterName
/ O
test B-HyperparameterName
splits B-HyperparameterName
which O
consist O
of O
100M B-HyperparameterValue
/ O
250K B-HyperparameterValue
/ O
250 B-HyperparameterValue
K I-HyperparameterValue
tokens O
. O
On O
this O
dataset O
, O
we O
evaluate O
the O
models O
trained O
on O
WIKITEXT-103 B-DatasetName
to O
study O
how O
our O
approach O
can O
adapt O
to O
new O
domain O
without O
re O
- O
training O
. O

IWSLT'14 B-DatasetName
De O
- O
En O
is O
a O
machine B-TaskName
translation I-TaskName
task O
, O
which O
consists O
of O
170 B-HyperparameterValue
K I-HyperparameterValue
translation O
pairs O
. O
We O
use O
a O
Transformer B-MethodName
encoder O
- O
decoder O
model O
. O
See O
Appendix O
C O
for O
how O
we O
adapt O
our O
approach O
to O
the O
machine B-TaskName
translation I-TaskName
task O
. O

See O
Appendix O
C O
for O
data O
statistics O
and O
task O
setups O
and O
Appendix O
D O
for O
model O
configurations O
. O

Training O
and O
Inference O
Details O

We O
implement O
our O
approach O
using O
the O
Fairseq O
library O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O
For O
TRIMELM B-MethodName
long I-MethodName
and O
TRIMELM B-MethodName
ext O
, O
we O
tune O
the O
number O
of O
segments O
used O
in O
M O
long O
on O
the O
development O
set O
during O
evaluation O
. O
Our O
TRIMELM B-MethodName
ext O
model O
requires O
building O
a O
large O
datastore O
at O
testing O
time O
and O
we O
use O
the O
FAISS O
library O
( O
Johnson O
et O
al O
. O
, O
2019 O
) O
for O
approximate O
nearest O
neighbor O
search O
( O
details O
in O
Appendix O
D O
) O
. O

We O
first O
train O
our O
model O
with O
the O
standard O
LM O
objective O
( O
Eq O
. O
1 O
) O
for O
the O
first O
5 B-HyperparameterValue
% I-HyperparameterValue
updates O
. O
Without O
this O
warmup O
stage O
, O
we O
observe O
the O
training O
process O
to O
be O
unstable O
probably O
due O
to O
a O
large O
variance O
in O
the O
estimated O
distributions O
. O
We O
use O
different O
memories O
when O
evaluating O
different O
instantiations O
of O
TRIME B-MethodName
, O
as O
shown O
in O
Table O
1 O
. O
We O
find O
that O
when O
a O
large O
set O
of O
external B-HyperparameterName
memory I-HyperparameterName
M B-HyperparameterName
ext O
is O
considered O
during O
inference O
, O
the O
performance O
can O
be O
improved O
by O
linearly O
interpolating O
the O
output O
distribution O
and O
a O
distribution O
over O
the O
memory O
, O
similarly O
to O
kNN B-MethodName
- I-MethodName
LM I-MethodName
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
. O
Thus O
, O
we O
apply O
an O
additional O
linear O
interpolation O
to O
our O
output O
probability O
distribution O
when O
considering O
external O
mem- O
cache O
to O
the O
vanilla O
Transformer B-MethodName
model O
and O
find O
it O
to O
underperform O
our O
model O
, O
demonstrating O
the O
importance O
of O
joint O
training O
using O
our O
approach O
. O
Compared O
to O
previous O
methods O
which O
explicitly O
leverage O
hidden O
representations O
from O
previous O
segments O
( O
Dai O
et O
al O
. O
, O
2019 O
; O
Rae O
et O
al O
. O
, O
2020 O
; O
Martins O
et O
al O
. O
, O
2022 O
; O
Ji O
et O
al O
. O
, O
2022 O
; O
Lei O
, O
2021 O
) O
, O
our O
approach O
achieves O
better O
or O
at O
least O
competitive O
performance O
. O
Different O
from O
these O
approaches O
which O
need O
to O
store O
all O
the O
hidden O
representations O
of O
every O
layer O
and O
modify O
the O
model O
architecture O
, O
we O
only O
incorporate O
the O
outputs O
from O
the O
last O
layerrequiring O
less O
computations O
and O
GPU O
memory O
. O
Our O
approach O
is O
orthogonal O
and O
can O
be O
applied O
on O
top O
of O
these O
models O
. O
To O
verify O
this O
, O
we O
adapt O
our O
approach O
to O
SRU++ O
( O
Lei O
, O
2021 O
) O
though O
the O
memory O
representations O
are O
optimized O
on O
one O
domain O
, O
our O
approach O
does O
not O
overfit O
, O
and O
building O
an O
external O
memory O
using O
the O
target O
domain O
dataset O
enables O
the O
model O
to O
perform O
well O
with O
domain O
shifts O
. O

Results O
: O
Machine B-TaskName
Translation I-TaskName

To O
showcase O
the O
generality O
of O
our O
training O
approach O
TRIME B-MethodName
to O
other O
generation O
tasks O
, O
we O
evaluate O
our O
approach O
on O
the O
IWSLT'14 B-DatasetName
de B-TaskName
- I-TaskName
en I-TaskName
translation I-TaskName
task O
. O

Since O
it O
is O
a O
sentence O
- O
level O
task O
, O
we O
do O
not O
use O
any O
local O
or O
long O
- O
term O
memory O
( O
M O
local O
, O
M O
long O
) O
, O
as O
there O
are O
few O
repetitive O
tokens O
. O
We O
denote O
our O
model O
as O
TRIMEMT B-MethodName
ext I-MethodName
. O

As O
shown O
in O
Table O
6 O
, O
our O
approach O
improves O
the O
vanilla O
Transformer B-MethodName
by O
1.15 B-MetricValue
BLEU B-MetricName
score O
and O
outperforms O
kNN B-MethodName
- I-MethodName
MT I-MethodName
( O
Khandelwal O
et O
al O
. O
, O
2021 O
demonstrates O
that O
our O
approach O
is O
able O
to O
improve O
the O
performance O
on O
other O
language O
generation O
tasks O
with O
different O
memory O
access O
. O

Analysis O

We O
conduct O
ablation O
studies O
and O
analysis O
to O
further O
understand O
individual O
components O
of O
our O
approach O
. O
Due O
to O
the O
limited O
computation O
budget O
, O
some O
experiments O
on O
WIKITEXT-103 B-DatasetName
are O
conducted O
with O
a O
small O
7 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-MethodName
model O
( O
8 B-HyperparameterValue
layers B-HyperparameterName
, O
hidden B-HyperparameterName
dimension I-HyperparameterName
128 B-HyperparameterValue
) O
in O
this O
section O
and O
the O
trends O
are O
generally O
similar O
for O
smaller O
models O
( O
see O
Appendix O
D O
and O
Appendix O
F O
for O
details O
) O
. O

Memory O
construction O
We O
first O
study O
how O
different O
data O
batching O
and O
memory O
construction O
strategies O
affect O
the O
performance O
when O
different O
testing O
memories O
are O
used O
. O
We O
compare O
our O
three O
models O
( O
TRIMELM B-MethodName
, O
TRIMELM B-MethodName
long I-MethodName
, O
TRIMELM B-MethodName
ext I-MethodName
) O
in O
Table O
7 O
. O
This O
ablation O
study O
clearly O
shows O
that O
packing O
consecutive O
segments O
and O
segments O
with O
high O
BM25 B-MetricName
scores O
in O
the O
same O
training O
batch O
and O
constructing O
memories O
properly O
can O
improve O
the O
performance O
when O
the O
long O
- O
range O
and O
external O
memories O
are O
used O
. O
This O
demonstrates O
the O
importance O
of O
closing O
the O
gap O
between O
training O
and O
inference O
. O

Leveraging O
long O
- O
range O
contexts O
We O
study O
if O
our O
model O
is O
able O
to O
handle O
large O
long O
- O
term O
memory O
. O
As O
Figure O
3 O
shows O
, O
our O
model O
is O
able O
to O
effectively O
handle O
long O
- O
range O
context O
( O
more O
than O
10k O
tokens O
) O
, O
which O
goes O
beyond O
typical O
attention O
context O
. O
Compared O
to O
continuous O
cache O
( O
Grave O
et O
al O
. O
, O
2017b O
, O
a O
) O
, O
the O
improvement O
of O
our O
approach O
becomes O
larger O
when O
more O
long O
- O
term O
memory O
is O
incorporated O
. O
This O
suggests O
that O
our O
model O
is O
able O
to O
leverage O
long O
- O
range O
context O
much O
more O
effectively O
. O
Additional O
analysis O
We O
conduct O
more O
ablation O
studies O
and O
analysis O
in O
Appendix O
G. O
We O
summarize O
them O
as O
follows O
. O
( O
1 O
) O
Our O
ablation O
studies O
show O
using O
BM25 O
batching O
method O
and O
enabling O
back O
- O
propagation O
to O
update O
memory O
representations O
are O
important O
for O
our O
approach O
( O
Table O
11 O
) O
. O

( O
2 O
) O
TRIMELM B-MethodName
is O
able O
to O
leverage O
local O
memory O
effectively O
to O
improve O
performance O
with O
different O
segment B-HyperparameterName
lengths I-HyperparameterName
L B-HyperparameterName
( O
Table O
12 O
) O
. O
( O
3 O
) O
TRIMELM B-MethodName
ext I-MethodName
outperforms O
kNN B-MethodName
- I-MethodName
LM I-MethodName
in O
terms O
of O
top O
- O
K O
retrieval O
accuracy B-MetricName
given O
the O
external O
memory O
set O
( O
Table O
13 O
) O
. O

( O
4 O
) O
We O
study O
the O
perplexity B-MetricName
of O
tokens O
in O
different O
frequency O
groups O
and O
find O
that O
TRIMELM B-MethodName
and O
TRIMELM B-MethodName
long I-MethodName
achieve O
larger O
improvements O
on O
rare O
words O
while O
TRIMELM B-MethodName
ext I-MethodName
improves O
results O
across O
the O
board O
( O
Table O
14 O
) O
. O

Related O
Work O

Memory O
- O
augmented O
language O
models O
We O
have O
discussed O
continuous O
cache O
, O
kNN B-MethodName
- I-MethodName
LM I-MethodName
and O
models O
that O
leverage O
representations O
from O
long O
- O
range O
context O
in O
the O
previous O
sections O
. O
also O
aim O
to O
combine O
several O
types O
of O
memories O
by O
learning O
an O
adaptive O
gating O
function O
; O
however O
, O
their O
external O
memory O
uses O
a O
pre O
- O
trained O
vanilla O
language O
model O
. O
Borgeaud O
et O
al O
. O
( O
2021 O
) O
demonstrate O
a O
remarkable O
performance O
by O
augmenting O
LMs O
with O
an O
external O
datastore O
of O
trillion O
of O
tokens O
and O
their O
datastore O
is O
built O
based O
on O
chunks O
of O
text O
using O
off O
- O
the O
- O
shelf O
BERT B-MethodName
embeddings O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
Our O
approach O
differs O
from O
prior O
works O
in O
the O
following O
aspects O
, O
which O
help O
our O
model O
achieve O
superior O
performance O
with O
little O
overhead O
: O
( O
1 O
) O
we O
update O
the O
memory O
representations O
through O
back O
- O
propagation O
from O
the O
end O
loss O
; O

( O
2 O
) O
our O
model O
does O
not O
modify O
the O
base O
architecture O
; O

( O
3 O
) O
we O
consider O
different O
types O
of O
memories O
in O
a O
unified O
framework O
. O
GNN B-MethodName
- I-MethodName
LM I-MethodName
( O
Meng O
et O
al O
. O
, O
2022 O
) O
augments O
LMs O
with O
a O
graph O
neural O
network O
to O
aggregate O
information O
of O
retrieved O
items O
from O
external O
memory O
, O
which O
makes O
an O
orthogonal O
contribution O
to O
our O
paper O
. O

Transformers O
for O
long O
inputs O
A O
large O
body O
of O
research O
has O
investigated O
how O
to O
scale O
self O
- O
attention O
mechanism O
to O
long O
contexts O
, O
either O
through O
sparse O
attention O
( O
Liu O
et O
al O
. O
, O
2018 O
; O
Child O
et O
al O
. O
, O
2019 O
; O
Beltagy O
et O
al O
. O
, O
2020 O
; O
Zaheer O
et O
al O
. O
, O
2020 O
) O
or O
subquadratic O
- O
time O
attention O
Choromanski O
et O
al O
. O
, O
2020 O
; O
Peng O
et O
al O
. O
, O
2021 O
; O
Katharopoulos O
et O
al O
. O
, O
2020 O
) O
. O
See O
Tay O
et O
al O
. O
( O
2020 O
) O
for O
a O
comprehensive O
survey O
of O
efficient O
Transformers O
. O
Our O
approach O
is O
orthogonal O
, O
as O
we O
only O
change O
the O
training O
objective O
and O
data O
batching O
to O
enable O
models O
to O
use O
large O
contexts O
during O
inference O
. O

Memory O
- O
augmented O
models O
for O
downstream O
tasks O
While O
our O
paper O
focuses O
on O
improving O
language O
models O
with O
memory O
augmentation O
, O
other O
works O
improve O
models O
for O
downstream O
tasks O
with O
a O
retrieval O
component O
, O
such O
as O
question O
answering O
( O
Kumar O
et O
al O
. O
, O
2016 O
; O
de O
Masson O
D'Autume O
et O
al O
. O
, O
2019 O
; O
Guu O
et O
al O
. O
, O
2020 O
; O
Zemlyanskiy O
et O
al O
. O
, O
2021 O
; O
Chen O
et O
al O
. O
, O
2022 O
; O
Izacard O
and O
Grave O
, O
2021 O
; O
Singh O
et O
al O
. O
, O
2021 O
) O
, O
dialogue O
, O
and O
other O
knowledge O
- O
intensive O
NLP O
tasks O
Petroni O
et O
al O
. O
, O
2021 O
) O
. O

Conclusion O

In O
this O
work O
, O
we O
propose O
TRIME B-MethodName
, O
a O
training O
approach O
for O
language O
modeling O
. O
We O
present O
three O
model O
instantiations O
TRIMELM B-MethodName
, O
TRIMELM B-MethodName
long I-MethodName
, O
TRIMELM B-MethodName
ext I-MethodName
: O
Through O
carefully O
- O
designed O
data O
batching O
and O
memory O
construction O
during O
training O
, O
we O
show O
that O
our O
models O
can O
leverage O
long O
- O
range O
contexts O
and O
external O
memory O
effectively O
at O
testing O
time O
. O
Our O
approach O
adds O
little O
computational O
overhead O
and O
does O
not O
modify O
model O
architectures O
, O
making O
it O
compatible O
with O
other O
neural O
models O
and O
techniques O
. O
For O
future O
work O
, O
we O
are O
interested O
in O
training O
TRIME B-MethodName
with O
large O
language O
models O
and O
other O
text O
generation O
tasks O
. O

Limitations O

We O
discuss O
limitations O
of O
our O
research O
as O
follows O
. O

• O
Despite O
the O
strong O
performance O
achieved O
by O
our O
approach O
when O
incorporating O
a O
large O
set O
of O
external O
memory O
, O
it O
results O
in O
a O
reduced O
inference O
efficiency O
at O
the O
same O
time O
due O
to O
the O
nearest O
neighbor O
search O
. O
For O
example O
, O
the O
model O
is O
10× O
slower O
when O
incorporating O
external O
memory O
. O
This O
issue O
can O
be O
more O
crucial O
when O
the O
external O
memory O
is O
even O
larger O
. O
Potential O
solutions O
to O
this O
issue O
include O
( O
1 O
) O
constructing O
the O
memory O
using O
a O
coarser O
granularity O
( O
e.g. O
, O
text O
blocks O
) O
( O
Borgeaud O
et O
al O
. O
, O
2021 O
) O
; O

( O
2 O
) O
compressing O
the O
external O
memory O
set O
and O
reducing O
the O
dimension O
of O
memory O
representations O
( O
He O
et O
al O
. O
, O
2021 O
) O
. O

• O
We O
mainly O
experiment O
with O
Transformerbased B-MethodName
models O
and O
additionally O
adapt O
our O
approach O
to O
SRU++ B-MethodName
( O
Lei O
, O
2021 O
) O
. O
We O
believe O
our O
approach O
is O
compatible O
with O
other O
architectures O
or O
techniques O
such O
as O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
( O
Dai O
et O
al O
. O
, O
2019 O
) O
and O
Compressive B-MethodName
Transformer I-MethodName
( O
Rae O
et O
al O
. O
, O
2020 O
) O
. O
We O
plan O
to O
explore O
them O
as O
future O
work O
. O

• O
We O
evaluate O
our O
approach O
on O
machine B-TaskName
translation I-TaskName
to O
test O
the O
generality O
of O
TRIME B-MethodName
to O
other O
generation O
tasks O
. O
However O
, O
due O
to O
compute O
limitation O
, O
we O
only O
evaluate O
it O
on O
a O
small O
dataset O
( O
i.e. O
, O
IWSLT'14 B-DatasetName
) O
, O
which O
consists O
of O
4 B-HyperparameterValue
M I-HyperparameterValue
tokens O
in O
the O
external O
memory O
. O
We O
leave O
the O
evaluation O
on O
larger O
machine B-TaskName
translation I-TaskName
datasets O
as O
future O
work O
. O

• O
Our O
paper O
mainly O
studies O
language O
modeling O
tasks O
and O
machine O
translation O
tasks O
. O
Although O
we O
believe O
our O
approach O
is O
compatible O
with O
all O
language O
generation O
tasks O
, O
how O
to O
adapt O
TRIME B-MethodName
to O
natural O
language O
understanding O
tasks O
such O
as O
text O
classification O
still O
remains O
an O
open O
question O
. O

• O
The O
biggest O
model O
we O
experimented O
with O
consists O
of O
247 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
due O
to O
our O
compute O
limit O
. O
The O
state O
- O
of O
- O
the O
- O
art O
auto O
- O
regressive O
LMs O
contain O
hundreds O
of O
billions O
of O
parameters O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O
We O
hope O
to O
see O
future O
efforts O
in O
scaling O
up O
our O
approach O
and O
evaluating O
the O
effectiveness O
on O
large O
LMs O
. O

Ethical O
Considerations O

Our O
proposed O
approach O
leverages O
external O
memory O
to O
achieve O
strong O
results O
on O
multiple O
language O
modeling O
benchmarks O
. O
In O
our O
experiments O
, O
we O
construct O
the O
external O
memory O
using O
the O
corpus O
on O
which O
the O
model O
is O
trained O
, O
while O
it O
can O
be O
constructed O
using O
any O
corpus O
. O
In O
general O
, O
we O
suggest O
practitioners O
constructing O
external O
memory O
using O
a O
public O
corpus O
, O
as O
retrieving O
from O
the O
external O
datastore O
can O
cause O
information O
leakage O
from O
the O
corpus O
. O
We O
acknowledge O
this O
ethical O
consideration O
and O
caution O
those O
who O
apply O
our O
approach O
to O
privacy O
- O
sensitive O
domains O
. O
domain O
adaptation O
( O
Appendix O
5.3 O
) O
. O
Table O
8 O
shows O
the O
statistics O
. O
WIKITEXT-103 B-DatasetName
( O
Merity O
et O
al O
. O
, O
2017 O
) O
is O
a O
wordlevel B-TaskName
language I-TaskName
modeling I-TaskName
dataset O
consisting O
of O
103 B-HyperparameterValue
M I-HyperparameterValue
training O
tokens O
. O
Following O
standard O
practice O
, O
we O
use O
adaptive O
softmax O
and O
adaptive O
token O
embeddings O
in O
our O
model O
and O
report O
perplexity B-MetricName
. O
In O
order O
to O
better O
compare O
with O
previous O
work O
, O
we O
evaluate O
on O
two O
model O
configurations O
- O
one O
uses O
a O
247 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-MethodName
model O
and O
a O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
3 B-HyperparameterValue
, I-HyperparameterValue
072 I-HyperparameterValue
following O
; O
Khandelwal O
et O
al O
. O
( O
2020 O
) O
and O
another O
one O
uses O
a O
150 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-MethodName
model O
with O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
150 B-HyperparameterValue
following O
Dai O
et O
al O
. O
( O
2019 O
) O
. O
More O
details O
are O
provided O
in O
Appendix O
D O
. O

ENWIK8 B-DatasetName
( O
Mahoney O
, O
2009 O
) O
is O
a O
character B-TaskName
- I-TaskName
level I-TaskName
language I-TaskName
modeling I-TaskName
dataset O
that O
contains O
a O
total O
of O
100 B-HyperparameterValue
M I-HyperparameterValue
characters O
. O
Following O
previous O
work O
, O
we O
report O
bit O
- O
per O
- O
character O
( O
bpc O
) O
on O
this O
dataset O
. O
We O
use O
a O
12 B-HyperparameterValue
- O
layer B-HyperparameterName
Transformer O
model O
with O
a O
hidden B-HyperparameterName
dimension I-HyperparameterName
512 B-HyperparameterValue
and O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
512 B-HyperparameterValue
. O

We O
also O
evaluate O
the O
IWSLT'14 B-DatasetName
DE→EN I-DatasetName
machine B-TaskName
translation I-TaskName
task O
, O
which O
consists O
of O
170 B-HyperparameterValue
K I-HyperparameterValue
translation O
pairs O
. O
Following O
Khandelwal O
et O
al O
. O
( O
2021 O
) O
, O
we O
build O
an O
external O
memory O
by O
taking O
all O
the O
translation O
contexts O
and O
the O
corresponding O
target O
token O
( O
( O
x O
, O
y O
< O
t O
) O
, O
y O
t O
) O
on O
the O
training O
set O
. O
We O
use O
the O
output O
representation O
as O
f O
( O
( O
x O
, O
y O
< O
t O
) O
) O
and O
the O
input O
representation O
of O
last O
FFN O
layer O
as O
g O
( O
( O
x O
, O
y O
< O
t O
) O
) O
to O
compute O
the O
loss O
. O
Similarly O
, O
we O
use O
BM25 O
to O
batch O
training O
data O
-we O
encourage O
two O
target O
sentences O
with O
a O
high O
BM25 B-MetricName
score I-MetricName
to O
be O
in O
the O
same O
training O
batch O
( O
see O
Algorithm O
1 O
) O
. O
We O
use O
the O
default O
model O
configuration O
in O
the O
Fairseq O
library O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
, O
and O
sacrebleu B-MetricName
( O
Post O
, O
2018 O
) O
to O
compute O
BLEU B-MetricName
scores O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
. O

We O
evaluate O
our O
approach O
for O
domain O
adaptation O
on O
the O
BOOKSCORPUS B-DatasetName
dataset O
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
, O
which O
is O
a O
word B-TaskName
- I-TaskName
level I-TaskName
language I-TaskName
modeling I-TaskName
dataset O
. O
The O
complete O
BOOKSCORPUS B-DatasetName
dataset O
consists O
of O
0.7B B-HyperparameterValue
tokens O
. O
We O
build O
our O
own O
train B-HyperparameterName
/ O
dev B-HyperparameterName
/ O
test B-HyperparameterName
splits B-HyperparameterName
which O
consist O
of O
100M B-HyperparameterValue
/ O
250K B-HyperparameterValue
/ O
250 B-HyperparameterValue
K I-HyperparameterValue
tokens O
respectively O
. O
The O
train O
set O
is O
only O
used O
to O
build O
external O
memory O
. O
On O
this O
dataset O
, O
we O
evaluate O
the O
models O
trained O
on O
WIKITEXT-103 B-DatasetName
to O
study O
how O
our O
approach O
can O
adapt O
to O
new O
domain O
without O
re O
- O
training O
or O
fine O
- O
tuning O
. O
The O
model O
we O
used O
on O
this O
dataset O
is O
the O
247 B-HyperparameterValue
M I-HyperparameterValue
Transformer B-MethodName
model O
with O
a O
segment B-HyperparameterName
length I-HyperparameterName
L B-HyperparameterName
= O
3,072 B-HyperparameterValue
. O

D O
Model O
Configurations O
and O
Hyperparameters O

Table O
9 O
shows O
the O
model O
configurations O
and O
hyperparameters O
that O
we O
used O
in O
our O
experiments O
. O
Following O
, O
during O
training O
, O
we O
train O
the O
model O
with O
fixed O
- O
length O
segments O
; O
during O
evaluation O
, O
we O
evaluate O
on O
the O
tokens O
at O
the O
end O
of O
the O
segment O
( O
i.e. O
, O
an O
evaluation O
segment O
can O
overlap O
with O
others O
) O
. O
When O
evaluating O
with O
large O
external O
memory O
, O
we O
always O
retrieve O
top O
- O
K B-HyperparameterName
( O
K B-HyperparameterName
= O
1,024 B-HyperparameterValue
) O
context O
- O
target O
pairs O
for O
language O
modeling O
. O
For O
machine B-TaskName
translation I-TaskName
, O
we O
tune O
K B-HyperparameterName
= O
{ O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
8 B-HyperparameterValue
, O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue
, O
64 B-HyperparameterValue
} O
following O
Zheng O
et O
al O
. O
( O
2021 O
) O
. O

E O
Applying O
TRIMELM B-MethodName
long I-MethodName
to O
SRU++ B-MethodName

We O
apply O
our O
approach O
to O
SRU++ B-MethodName
( O
Lei O
, O
2021 O
) O
and O
we O
believe O
our O
approach O
is O
also O
compatible O
with O
other O
architectures O
such O
as O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
( O
Dai O
et O
al O
. O
, O
2019 O
) O
. O
SRU++ B-MethodName
is O
a O
language O
model O
which O
combines O
recurrent O
units O
and O
the O
attention O
mechanism O
. O
SRU++ B-MethodName
use O
hidden O
representations O
from O
the O
previous O
segment O
at O
attention O
layers O
to O
incorporate O
long O
- O
range O
contexts O
, O
similarly O
to O
Dai O
et O
al O
. O
( O
2019 O
) O
. O

To O
apply O
our O
approach O
to O
SRU++ B-MethodName
, O
we O
follow O
their O
data O
- O
batching O
method O
as O
it O
is O
required O
due O
to O
the O
recurrence O
of O
the O
model O
architecture O
. O
We O
construct O
the O
training O
memory O
using O
all O
the O
contexts O
in O
the O
current O
segment O
( O
i.e. O
, O
local O
memory O
) O
and O
all O
contexts O
in O
the O
previous O
segment O
( O
i.e. O
, O
long O
memory O
) O
. O
Note O
that O
the O
memory O
representations O
from O
the O
previous O
segment O
will O
be O
stale O
, O
thus O
we O
do O
not O
back O
- O
propagate O
to O
that O
part O
. O
16 O
. O
For O
other O
hyper O
- O
parameters O
and O
the O
optimizer O
, O
we O
follow O
the O
default O
ones O
in O
their O
implementation O
. O

During O
inference O
, O
we O
can O
use O
more O
contexts O
to O
construct O
memory O
. O
We O
train O
with O
different O
segment B-HyperparameterName
lengths I-HyperparameterName
, O
i.e. O
, O
L B-HyperparameterName
= O
512 B-HyperparameterValue
or O
L B-HyperparameterName
= O
2048 B-HyperparameterValue
. O
For O
the O
model O
trained O
with O
L B-HyperparameterName
= O
512 B-HyperparameterValue
, O
it O
can O
leverage O
a O
long O
- O
term O
memory O
of O
a O
size O
6,144 B-HyperparameterValue
during O
inference O
; O
for O
the O
model O
trained O
with O
L B-HyperparameterName
= O
2048 B-HyperparameterValue
, O
it O
can O
leverage O
a O
long B-HyperparameterName
- I-HyperparameterName
term I-HyperparameterName
memory I-HyperparameterName
of I-HyperparameterName
a I-HyperparameterName
size I-HyperparameterName
12,228 B-HyperparameterValue
. O

G O
Additional O
Analysis O

Ablation O
study O
on O
TRIMELM B-MethodName
ext O
We O
study O
the O
importance O
of O
packing O
segments O
with O
high O
BM25 B-MetricName
scores I-MetricName
in O
the O
same O
training O
batch O
, O
as O
well O
as O
the O
effectiveness O
of O
enabling O
back O
- O
propagation O
to O
memory O
representations O
during O
training O
. O
As O
shown O
in O
Table O
11 O
, O
when O
we O
random O
batch O
training O
segments O
( O
instead O
of O
using O
BM25 B-MetricName
scores O
) O
, O
the O
perplexity B-MetricName
increases O
to O
45.71 B-MetricValue
( O
+4.21 B-MetricValue
) O
. O
Also O
, O
enabling O
backpropagation O
to O
memory O
is O
crucial O
for O
our O
approach O
-the O
performance O
is O
much O
worse O
if O
we O
disable O
it O
. O

Effectiveness O
of O
using O
local O
memory O
We O
study O
the O
effectiveness O
of O
our O
model O
TRIMELM B-MethodName
that O
uses O
only O
local O
memory O
with O
different O
segment B-HyperparameterName
lengths I-HyperparameterName
L. B-HyperparameterName
As O
shown O
in O
Table O
12 O
, O
our O
model O
significantly O
outperforms O
the O
baselines O
in O
all O
the O
settings O
. O
This O
suggests O
that O
our O
model O
can O
leverage O
local O
memory O
very O
effectively O
to O
improve O
performance O
. O

Retrieval O
performance O
on O
external O
memory O

When O
external O
memory O
is O
used O
in O
our O
experiments O
, O
we O
perform O
nearest O
- O
neighbor O
search O
over O
the O
entire O
memory O
set O
M O
ext O
to O
retrieve O
the O
top O
K B-HyperparameterName
keys O
( O
we O
use O
K B-HyperparameterName
= O
1024 B-HyperparameterValue
) O
. O
Table O
13 O
compares O
the O
retrieval O
accuracy O
of O
our O
approach O
and O
kNN B-MethodName
- I-MethodName
LM I-MethodName
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
for O
different O
K. B-HyperparameterName
Our O
approach O
outperforms O
kNN B-MethodName
- I-MethodName
LM I-MethodName
in O
terms O
of O
retrieval O
results O
; O
this O
explains O
how O
our O
final O
perplexity B-MetricName
surpasses O
kNN B-MethodName
- I-MethodName
LM I-MethodName
when O
incorporating O
external O
memory O
. O

Perplexity B-MetricName
breakdown O
for O
different O
frequencies O

We O
aim O
to O
understand O
which O
type O
of O
memories O
improves O
perplexity O
of O
tokens O
in O
different O
frequency O
groups O
. O
We O
group O
tokens O
into O
5 O
buckets O
according O
to O
their O
frequency B-HyperparameterName
on O
the O
development O
set O
. O
Table O
14 O
shows O
the O
results O
for O
different O
models O
. O
TRIMELM B-MethodName
and O
TRIMELM B-MethodName
long I-MethodName
improve O
the O
perplexity B-MetricName
of O
rare O
words O
( O
i.e. O
, O
frequency B-HyperparameterName
≤ O
1k B-HyperparameterValue
) O
while O
achieving O
similar O
or O
slightly O
worse O
results O
for O
frequent O
words O
compared O
to O
the O
Transformer B-MethodName
baseline O
. O
TRIMELM B-MethodName
ext I-MethodName
improves O
perplexity B-MetricName
in O
all O
the O
buckets O
. O
Interestingly O
, O
kNN B-MethodName
- I-MethodName
LM I-MethodName
with O
continuous O
cache O
does O
not O
perform O
significantly O
better O
compared O
to O
TRIMELM B-MethodName
and O
TRIMELM B-MethodName
long O
although O
these O
two O
models O
do O
not O
use O
external O
memory O
. O
This O
suggests O
that O
jointly O
training O
memory O
representations O
and O
the O
language O
model O
particularly O
help O
improve O
the O
performance O
of O
rare O
words O
. O

H O
Tuning O
p O
for O
training O
with O
external O
memory O

When O
training O
the O
model O
with O
local O
and O
external O
memory O
, O
to O
avoid O
the O
model O
to O
only O
relies O
on O
highquality O
local O
memory O
, O
we O
disable O
the O
local O
memory O
with O
a O
probability B-HyperparameterName
of O
p. B-HyperparameterName
Here O
we O
study O
how O
p B-HyperparameterName
will O
affect O
the O
final O
performance O
of O
our O
model O
. O
The O
results O
of O
using O
different O
p O
are O
shown O
in O
Table O
15 O
. O
We O
find O
that O
when O
p B-HyperparameterName
= O
0 B-HyperparameterValue
, O
the O
model O
performs O
poorly O
with O
external O
memory O
as O
the O
model O
learns O
to O
only O
leverage O
local O
memory O
and O
ignores O
external O
memory O
during O
training O
. O
By O
increasing O
p B-HyperparameterName
, O
this O
issue O
is O
mitigated O
. O
We O
set O
p B-HyperparameterName
= O
0.9 B-HyperparameterValue
in O
our O
main O
experiments O
. O

Acknowledgments O

We O
thank O
Jane O
Pan O
, O
Howard O
Chen O
, O
Alexander O
Wettig O
, O
Tianyu O
Gao O
, O
Kaiyu O
Yang O
, O
Mengzhou O
Xia O
, O
Jinhyuk O
Lee O
, O
and O
the O
members O
of O
Princeton O
NLP O
group O
for O
helping O
with O
proofreading O
and O
providing O
valuable O
feedback O
. O
This O
research O
is O
partially O
supported O
by O
the O
James O
Mi O
* O
91 O
Research O
Innovation O
Fund O
for O
Data O
Science O
and O
a O
gift O
from O
Apple O
. O
ZZ O
is O
also O
supported O
by O
a O
JP O
Morgan O
PhD O
fellowship O
. O

A O
Inference O
Method O

Testing O
objective O
Formally O
speaking O
, O
our O
testing O
objective O
is O
basically O
the O
same O
as O
the O
training O
objective O
( O
Eq O
. O
5 O
) O
: O
P O
( O
w O
c O
) O
∝ O
exp O
( O
E O
⊺ O
w O
f O
θ O
( O
c O
) O
) O
+ O
( O
c O
j O
, O
x O
j O
) O
∈M O
eval O
∶x O
j O
= O
w O
exp O
( O
sim O
( O
g O
θ O
( O
c O
) O
, O
g O
θ O
( O
c O
j O
) O
) O
τ O
) O
, O

except O
that O
we O
take O
M O
eval O
as O
a O
combination O
of O
M O
local O
, O
M O
long O
and O
M O
ext O
. O
As O
M O
ext O
can O
be O
very O
large O
, O
we O
approximate O
it O
by O
retrieving O
the O
top O
- O
K B-HyperparameterName
closest O
terms O
to O
g O
θ O
( O
c O
) O
. O
Formally O
, O
M O
eval O
of O
three O
instantiations O
of O
TRIME B-MethodName
is O
constructed O
as O
follows O
, O
Linear O
interpolation O
when O
using O
M O
ext O
We O
find O
that O
when O
a O
large O
set O
of O
external O
memory O
M O
ext O
is O
considered O
during O
inference O
, O
the O
performance O
can O
be O
improved O
by O
calibrating O
a O
separated O
distribution O
over O
the O
memory O
and O
interpolating O
the O
output O
distribution O
and O
the O
memory O
distribution O
, O
similarly O
to O
kNN B-MethodName
- I-MethodName
LM I-MethodName
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
. O
We O
think O
this O
is O
because O
the O
distribution O
of O
the O
similarity O
values O
has O
been O
significantly O
shifted O
during O
inference O
, O
while O
the O
relative O
ranking O
preserves O
. O
As O
a O
result O
, O
having O
values O
from O
two O
different O
distributions O
in O
one O
softmax O
normalization O
is O
sub O
- O
optimal O
compared O
to O
computing O
two O
separated O
probabilities O
and O
interpolating O
them O
. O

Thus O
, O
we O
apply O
an O
additional O
linear O
interpolation O
to O
our O
output O
probability O
distribution O
. O
Specifically O
, O
we O
first O
use O
Eq O
. O
6 O
to O
compute O
the O
distribution O
P O
( O
w O
c O
) O
. O
Then O
, O
we O
compute O
a O
probability O
distribution O
over O
the O
tokens O
in O
memory O
P O
′ O
( O
w O
c O
) O
as O
follow O
, O

We O
linearly O
interpolate O
these O
two O
probability O
distributions O
with O
a O
coefficient O
λ B-HyperparameterName
and O
get O
the O
final O
output O
P O
final O
( O
w O
c O
) O
: O

We O
tune O
the O
temperature B-HyperparameterName
terms O
and O
λ B-HyperparameterName
on O
the O
development O
set O
. O

B O
Packing O
Segments O
Using O
BM25 B-MetricName
Scores I-MetricName

In O
§ O
4.3 O
, O
we O
construct O
training O
memories O
M O
train O
by O
packing O
segments O
that O
have O
large O
lexical O
overlap O
into O
the O
same O
batch O
using O
BM25 B-MetricName
( O
Robertson O
and O
Zaragoza O
, O
2009 O
) O
. O
Algorithm O
1 O
shows O
the O
process O
to O
pack O
segments O
into O
training O
batches O
. O
We O
start O
with O
a O
single O
segment O
and O
repeatedly O
add O
segments O
with O
highest O
BM25 B-MetricName
scores O
into O
the O
same O
batch O
. O

C O
Dataset O
Statistics O
and O
Tasks O

We O
evaluate O
our O
approach O
on O
three O
benchmarks O
: O
WIKITEXT-103 B-DatasetName
, O
ENWIK8 B-DatasetName
, O
and O
IWSLT'14 B-DatasetName
. O
We O
also O
evaluate O
our O
approach O
on O
BOOKSCORPUS B-DatasetName
for O

EmpHi B-MethodName
: O
Generating O
Empathetic B-MethodName
Responses I-MethodName
with I-MethodName
Human I-MethodName
- I-MethodName
like I-MethodName
Intents I-MethodName

In O
empathetic O
conversations O
, O
humans O
express O
their O
empathy O
to O
others O
with O
empathetic O
intents O
. O
However O
, O
most O
existing O
empathetic O
conversational O
methods O
suffer O
from O
a O
lack O
of O
empathetic O
intents O
, O
which O
leads O
to O
monotonous O
empathy O
. O
To O
address O
the O
bias O
of O
the O
empathetic O
intents O
distribution O
between O
empathetic O
dialogue O
models O
and O
humans O
, O
we O
propose O
a O
novel O
model O
to O
generate O
empathetic O
responses O
with O
humanconsistent O
empathetic O
intents O
, O
EmpHi B-MethodName
for O
short O
. O
Precisely O
, O
EmpHi B-MethodName
learns O
the O
distribution O
of O
potential O
empathetic O
intents O
with O
a O
discrete O
latent O
variable O
, O
then O
combines O
both O
implicit O
and O
explicit O
intent O
representation O
to O
generate O
responses O
with O
various O
empathetic O
intents O
. O
Experiments O
show O
that O
EmpHi B-MethodName
outperforms O
state O
- O
ofthe O
- O
art O
models O
in O
terms O
of O
empathy O
, O
relevance O
, O
and O
diversity O
on O
both O
automatic O
and O
human O
evaluation O
. O
Moreover O
, O
the O
case O
studies O
demonstrate O
the O
high O
interpretability O
and O
outstanding O
performance O
of O
our O
model O
. O
Our O
code O
are O
avaliable O
at O
https O
: O
/ O
/ O
github.com O
/ O
mattc95 O
/ O
EmpHi O
. O

Introduction O

Empathy O
is O
a O
basic O
yet O
essential O
human O
ability O
in O
our O
daily O
life O
. O
It O
is O
a O
capacity O
to O
show O
one O
's O
caring O
and O
understanding O
to O
others O
. O
Many O
types O
of O
research O
have O
been O
conducted O
on O
empathetic O
expression O
to O
enhance O
the O
empathy O
ability O
of O
Artificial O
Intelligence O
, O
e.g. O
, O
computational O
approach O
for O
empathy O
measurement O
( O
Sharma O
et O
al O
. O
, O
2020 O
) O
, O
empathetic O
expression O
understanding O
in O
newswire O
( O
Buechel O
et O
al O
. O
, O
2018 O
) O
, O
online O
mental O
health O
support O
( O
Sharma O
et O
al O
. O
, O
2021 O
) O
, O
etc O
. O
In O
this O
work O
, O
we O
focus O
on O
the O
task O
of O
generating O
empathetic O
responses O
( O
Rashkin O
et O
al O
. O
, O
2019 O
; O
Lin O
et O
al O
. O
, O
2019 O
; O
Majumder O
et O
al O
. O
, O
2020 O
) O
in O
open O
- O
domain O
conversation O
. O

Existing O
empathetic O
dialogue O
models O
pay O
more O
attention O
to O
the O
emotion O
- O
dependent O
response O
generation O
( O
Lin O
et O
al O
. O
, O
2019 O
; O
Majumder O
et O
al O
. O
, O
2020 O
) O
. O

I O
just O
started O
college O
again O
, O
and O
while O
I O
'm O
doing O
great O
in O
school O
, O
it O
has O
lead O
me O
to O
feel O
very O
lonely O
with O
a O
lack O
of O
social O
life O
. O

I O
am O
sorry O
to O
hear O
that O
! O
I O
know O
I O
broke O
up O
with O
my O
ex O
, O
but O
I O
ca O
n't O
help O
but O
feel O
irritated O
when O
he O
talks O
about O
going O
on O
dates O
. O

A O
while O
back O
my O
cat O
knocked O
over O
and O
broke O
my O
mother O
's O
urn O
. O

Oh O
my O
goodness O
, O
I O
have O
a O
cat O
, O
I O
know O
how O
you O
feel O
. O

I O
am O
sorry O
to O
hear O
that O
. O
I O
hope O
you O
find O
someone O
to O
help O
you O
. O

That O
is O
so O
annoying O
. O
I O
would O
be O
upset O
too O
. O

Acknowledging O

Sympathizing O
Agreeing O

EmpHi B-MethodName
Output O

Figure O
1 O
: O
EmpHi B-MethodName
generates O
empathetic B-MethodName
responses I-MethodName
with I-MethodName
human I-MethodName
- I-MethodName
like I-MethodName
empathetic I-MethodName
intents I-MethodName
( O
text O
in O
blue O
box O
) O
, O
while O
existing O
empathetic O
dialogue O
models O
generate O
responses O
with O
contextually O
irrelevant O
and O
monotonous O
empathy O
( O
text O
in O
orange O
box O
) O
. O

However O
, O
using O
emotion O
alone O
to O
generate O
responses O
is O
coarse O
- O
grained O
, O
and O
the O
model O
needs O
to O
incorporate O
empathetic O
intent O
information O
. O
On O
the O
one O
hand O
, O
the O
speaker O
often O
talks O
with O
a O
particular O
emotion O
while O
the O
listener O
shows O
their O
empathy O
with O
specific O
empathetic O
intents O
, O
e.g. O
, O
Acknowledging O
, O
Agreeing O
, O
Consoling O
and O
Questioning O
etc O
( O
Welivita O
and O
Pu O
, O
2020 O
) O
. O
On O
the O
other O
hand O
, O
see O
in O
Figure O
1 O
, O
when O
the O
user O
expresses O
sadness O
, O
existing O
models O
tend O
to O
generate O
sympathetic O
responses O
like O
" O
I O
'm O
sorry O
to O
hear O
that O
. O
" O
However O
, O
empathy O
is O
not O
the O
same O
as O
sympathy O
, O
so O
the O
models O
should O
not O
only O
generate O
responses O
with O
Sympathizing O
intent O
. O
We O
demonstrate O
this O
phenomenon O
elaborately O
with O
a O
quantitative O
evaluation O
in O
Section O
2 O
. O
In O
real O
life O
situation O
, O
humans O
could O
reply O
with O
various O
empathetic O
intents O
to O
the O
same O
context O
which O
depends O
on O
personal O
preference O
. O
For O
example O
, O
given O
a O
context O
, O
" O
I O
just O
failed O
my O
exam O
" O
, O
an O
individual O
may O
respond O
" O
Oh O
no O
, O
what O
happened O
? O
" O
with O
Questioning O
intent O
to O
explore O
the O
experience O
of O
the O
user O
, O
or O
" O
I O
understand O
this O
feeling O
, O
know O
how O
you O
feel O
" O
with O
Agreeing O
intent O
. O
These O
types O
of O
empathy O
are O
more O
relevant O
, O
interactive O
, O
and O
diverse O
. O
To O
address O
the O
above O
issues O
, O
we O
propose O
a O
new O
framework O
to O
generate O
empathetic B-MethodName
responses I-MethodName
with I-MethodName
human I-MethodName
- I-MethodName
like I-MethodName
empathetic I-MethodName
intents I-MethodName
( O
EmpHi B-MethodName
) O
which O
could O
generate O
responses O
with O
various O
empathetic O
intents O
, O
see O
examples O
in O
Figure O
1 O
. O
Specifically O
, O
Em B-MethodName
- I-MethodName
pHi I-MethodName
learns O
the O
empathetic O
intent O
distribution O
with O
a O
discrete O
latent O
variable O
and O
adopts O
intent O
representation O
learning O
in O
the O
training O
stage O
. O
During O
the O
generation O
process O
, O
EmpHi B-MethodName
first O
predicts O
a O
potential O
empathetic O
intent O
and O
then O
combines O
both O
implicit O
and O
explicit O
intent O
representation O
to O
generate O
a O
response O
corresponding O
to O
the O
predicted O
intent O
. O
Our O
main O
contributions O
are O
: O

• O
We O
discover O
and O
quantify O
the O
severe O
bias O
of O
empathetic O
intents O
between O
existing O
empathetic O
dialogue O
models O
and O
humans O
. O
This O
finding O
will O
lead O
subsequent O
researchers O
to O
pay O
more O
attention O
to O
fine O
- O
grained O
empathetic O
intents O
. O

• O
To O
address O
the O
above O
problem O
, O
we O
propose O
EmpHi B-MethodName
, O
which O
generates O
responses O
with O
human O
- O
like O
empathetic O
intents O
. O
Experiments O
have O
proved O
the O
effectiveness O
of O
our O
model O
through O
the O
significant O
improvement O
in O
both O
automatic O
and O
human O
evaluation O
. O

• O
According O
to O
the O
quantitative O
evaluation O
and O
analysis O
, O
EmpHi B-MethodName
successfully O
captures O
humans O
' O
empathetic O
intent O
distribution O
, O
and O
shows O
high O
interpretability O
in O
generation O
process O
. O

Related O
Work O

Empathetic B-TaskName
Response I-TaskName
Generation I-TaskName
. O
Providing O
dialogue O
agents O
the O
ability O
to O
recognize O
speaker O
feelings O
and O
reply O
according O
to O
the O
context O
is O
challenging O
and O
meaningful O
. O
Rashkin O
et O
al O
. O
( O
2019 O
) O
propose O
the O
EmpatheticDialogues B-DatasetName
for O
empathetic O
response O
generation O
research O
. O
Most O
subsequent O
empathetic O
conversation O
researches O
are O
evaluated O
on O
this O
dataset O
, O
including O
ours O
. O
They O
also O
propose O
Multitask B-MethodName
- I-MethodName
Transformer I-MethodName
, O
which O
is O
jointly O
trained O
with O
context O
emotion O
classification O
and O
response O
generation O
. O
To O
further O
capture O
the O
fine O
- O
grained O
emotion O
information O
, O
Lin O
et O
al O
. O
( O
2019 O
) O
introduce O
MoEL B-MethodName
, O
a O
transformer O
with O
a O
multi O
- O
decoder O
. O
Each O
of O
them O
is O
responsible O
for O
the O
response O
generation O
of O
one O
specific O
emotion O
. O
Majumder O
et O
al O
. O
( O
2020 O
) O
propose O
MIME B-MethodName
, O
which O
mimics O
the O
speaker O
emotion O
to O
a O
varying O
degree O
. O
All O
these O
models O
focus O
on O
emotion O
- O
dependent O
empathetic O
response O
generation O
, O
whereas O
we O
pay O
more O
attention O
to O
the O
empathetic O
intents O
and O
propose O
to O
generate O
a O
response O
that O
is O
not O
only O
emotionally O
appropriate O
but O
also O
empathetically O
humanlike O
. O

One O
- O
to O
- O
many O
Response O
Generation O
. O
Given O
dialogue O
history O
, O
there O
could O
be O
various O
responses O
depends O
on O
personal O
preference O
. O
Zhao O
et O
al O
. O
( O
2017 O
) O
propose O
to O
learn O
the O
potential O
responses O
with O
continuous O
latent O
variable O
and O
maximize O
the O
loglikelihood O
using O
Stochastic O
Gradient O
Variational O
Bayes O
( O
SGVB O
) O
( O
Kingma O
and O
Welling O
, O
2014 O
) O
. O
To O
further O
improve O
the O
interpretability O
of O
response O
generation O
, O
Zhao O
et O
al O
. O
( O
2018 O
) O
propose O
to O
capture O
potential O
sentence O
- O
level O
representations O
with O
discrete O
latent O
variables O
. O
MIME B-MethodName
( O
Majumder O
et O
al O
. O
, O
2020 O
) O
introduces O
stochasticity O
into O
the O
emotion O
mixture O
for O
various O
empathetic O
responses O
generation O
. O

Different O
from O
the O
previous O
works O
, O
we O
propose O
a O
discrete O
latent O
variable O
to O
control O
the O
empathetic O
intent O
of O
response O
and O
achieve O
intent O
- O
level O
diversity O
. O

Empathetic O
Expression O
Bias O

Although O
existing O
empathetic O
conversational O
methods O
have O
shown O
promising O
progress O
, O
we O
reveal O
there O
is O
a O
severe O
bias O
of O
empathetic O
expression O
between O
them O
and O
humans O
according O
to O
quantitative O
evaluation O
. O

Empathy O
plays O
a O
vital O
role O
in O
human O
conversation O
, O
Welivita O
and O
Pu O
( O
2020 O
) O
their O
empathy O
naturally O
by O
Questioning O
, O
Acknowledging O
, O
and O
Agreeing O
intents O
etc O
. O
However O
, O
there O
are O
no O
empirical O
experiments O
have O
shown O
how O
empathetic O
dialogue O
models O
express O
their O
empathy O
? O
To O
further O
study O
, O
we O
finetune O
a O
BERT B-MethodName
classifier O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
on O
the O
released O
EmpatheticIntents B-DatasetName
1 I-DatasetName
dataset O
( O
Welivita O
and O
Pu O
, O
2020 O
) O
. O
Our O
classifier O
achieves O
87.75 B-MetricValue
% I-MetricValue
accuracy B-MetricName
in O
intent O
classification O
and O
we O
apply O
it O
to O
identify O
the O
empathetic O
intents O
of O
responses O
generated O
by O
the O
state O
- O
of O
- O
the O
- O
art O
empathetic O
dialogue O
model O
MIME B-MethodName
( O
Majumder O
et O
al O
. O
, O
2020 O
) O
. O
As O
shown O
in O
Figure O
4 O
, O
the O
severe O
empathetic O
intent O
distribution O
bias O
emerges O
while O
comparing O
humans O
to O
MIME B-MethodName
. O
Given O
context O
with O
sad O
emotion O
, O
existing O
models O
usually O
generate O
" O
I O
am O
sorry O
to O
hear O
that O
" O
with O
Sympathiz O
- O
ing O
intent O
, O
which O
is O
not O
human O
- O
like O
and O
contextually O
relevant O
. O
In O
addition O
, O
we O
can O
tell O
that O
the O
empathetic O
expression O
of O
MIME B-MethodName
is O
monotonous O
. O
We O
also O
quantify O
the O
intent O
distribution O
of O
other O
empathetic O
dialogue O
models O
in O
the O
Appendix O
A. O
The O
results O
are O
similar O
to O
Figure O
4 O
. O

We O
believe O
this O
phenomenon O
is O
caused O
by O
that O
existing O
models O
only O
generate O
responses O
according O
to O
context O
emotion O
and O
lack O
fine O
- O
grained O
empathetic O
intent O
modeling O
. O
Therefore O
, O
we O
propose O
EmpHi B-MethodName
, O
which O
generates O
empathetic O
responses O
with O
humanlike O
empathetic O
intents O
. O

EmpHi B-MethodName
Method O

Task O
Definition O
and O
Overview O

Given O
the O
context O
, O

C O
= O
[ O
c O
1 O
, O
c O
2 O
, O
• O
• O
• O
, O
c O
m O
] O

, O
which O
consists O
of O
m O
words O
for O
single O
or O
multiple O
utterances O
. O
We O
aim O
to O
generate O
empathetic O
response O
, O

X O
= O
[ O
x O
1 O
, O
x O
2 O
, O
• O
• O
• O
, O
x O
n O
] O

, O
with O
human O
- O
like O
empathetic O
intent O
. O
The O
whole O
model O
architecture O
is O
shown O
in O
Figure O
3 O
. O

EmpHi B-MethodName
learns O
the O
potential O
empathetic O
intent O
distribution O
with O
a O
latent O
variable O
z O
, O
which O
could O
be O
seen O
in O
Figure O
5 O
. O
Conditional B-MethodName
Variational I-MethodName
AutoEncoder I-MethodName
( O
CVAE B-MethodName
) O
( O
Yan O
et O
al O
. O
, O
2016 O
; O
Zhao O
et O
al O
. O
, O
2017 O
; O
Gu O
et O
al O
. O
, O
2019 O
) O
is O
trained O
to O
maximize O
the O
conditional O
log O
likelihood O
, O
log O
p O
( O
X|C O
) O
, O
which O
involves O
an O
intractable O
marginalization O
over O
z. O
We O
train O
the O
CVAE B-MethodName
efficiently O
with O
Stochastic O
Gradient O
Variational O
Bayes O
( O
SGVB O
) O
( O
Kingma O
and O
Welling O
, O
2014 O
) O
by O
maximizing O
the O
variational O
lower O
bound O
of O
the O
log O
likelihood O
: O
p O
( O
X|C O
, O
z O
) O
denotes O
response O
reconstruction O
probability O
, O
q O
( O
z|X O
, O
C O
) O
is O
recognition O
probability O
and O
p O
( O
z|C O
) O
is O
prior O
probability O
. O
Our O
method O
mainly O
consists O
of O
three O
aspects O
: O

log O
p O
( O
X|C O
) O
≥E O
q O
( O
z|X O
, O
C O
) O
[ O
log O
p O
( O
X|C O
, O
z O
) O
] O
− O
KL O
( O
q O
( O
z|X O
, O
C O
) O
||p O
( O
z|C O
) O
) O
, O
( O
1 O
) O

Z O
X O
C O
C O
X O
p O
( O
| O
) O
q O
( O
| O
, O
) O
p O
( O
| O
, O
) O
p O
( O
| O
) O
( O
b O
) O
( O
a O
) O

• O
To O
capture O
the O
explicit O
relationship O
between O
the O
latent O
variable O
and O
the O
intent O
, O
we O
propose O
an O
intent O
representation O
learning O
approach O
to O
learn O
the O
intent O
embeddings O
. O

• O
We O
construct O
an O
intent O
predictor O
to O
predict O
potential O
response O
intent O
using O
contextual O
information O
and O
then O
use O
this O
intent O
for O
guiding O
the O
response O
generation O
. O

• O
During O
the O
generation O
process O
, O
EmpHi B-MethodName
combines O
both O
implicit O
intent O
embedding O
and O
explicit O
intent O
keywords O
to O
generate O
responses O
corresponding O
to O
the O
given O
intents O
. O

Learning O
Intent O
Representation O

To O
achieve O
more O
interpretability O
, O
we O
choose O
a O
discrete O
latent O
variable O
that O
obeys O
categorical O
distribution O
with O
nine O
categories O
, O
each O
corresponding O
to O
one O
empathetic O
intent O
. O
Directly O
maximizing O
Eq.1 O
would O
cause O
two O
serious O
problems O
: O
the O
relation O
between O
the O
latent O
variable O
and O
intent O
is O
intractable O
; O
the O
vanishing O
latent O
problem O
results O
in O
insufficient O
information O
provided O
by O
the O
latent O
variable O
during O
generation O
. O
( O
Bowman O
et O
al O
. O
, O
2016 O
; O
Zhao O
et O
al O
. O
, O
2017 O
; O
Gu O
et O
al O
. O
, O
2019 O
) O
. O
To O
solve O
the O
above O
issues O
, O
we O
separately O
train O
a O
recognition O
network O
q O
r O
( O
z|X O
) O
to O
encourage O
intent O
variable O
z O
to O
capture O
context O
- O
independent O
semantics O
, O
which O
is O
essential O
for O
z O
to O
be O
interpretable O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
. O
The O
task O
of O
the O
recognition O
network O
is O
to O
provide O
the O
accurate O
intent O
label O
of O
the O
response O
, O
which O
corresponds O
to O
an O
intent O
embedding O
. O
Then O
, O
by O
maximizing O
likelihood O
p O
( O
X|C O
, O
z O
) O
, O
the O
embedding O
captures O
corresponding O
intent O
representation O
automatically O
. O
The O
recognition O
network O
q O
r O
( O
z|X O
) O
does O
not O
need O
additional O
training O
. O
We O
utilize O
the O
BERT B-MethodName
intent O
classifier O
mentioned O
above O
, O
which O
achieves O
87.75 B-MetricValue
% I-MetricValue
accuracy B-MetricName
in O
intent O
classification O
. O
In O
addition O
, O
as O
the O
sample O
operation O
easily O
brings O
noise O
for O
the O
intent O
representation O
learning O
when O
sampling O
a O
wrong O
intent O
, O
we O
use O
argmax O
operation O
to O
avoid O
the O
noise O
, O
the O
response O
reconstruction O
loss O
is O
: O

L O
1 O
= O
− O
log O
p O
( O
X|C O
, O
z O
k O
) O
( O
2 O
) O
z O
k O
= O
arg O
max O
z O
k O
q O
r O
( O
z O
k O
|X O
) O
( O
3 O
) O
k O
∈ O
{ O
0 O
, O
1 O
, O
2 O
, O
• O
• O
• O
, O
8 O
} O

, O
each O
integer O
corresponds O
to O
a O
specific O
empathetic O
intent O
as O
in O
Figure O
2 O
. O

Intent O
Predictor O
and O
Emotion O
Classifier O

The O
intent O
predictor O
is O
based O
on O
the O
prior O
network O
p O
i O
( O
z|C O
) O
, O
which O
predicts O
the O
distribution O
of O
response O
intent O
by O
the O
given O
context O
. O
During O
inference O
, O
we O
sample O
potential O
intents O
from O
this O
distribution O
in O
order O
to O
generate O
human O
- O
like O
empathetic O
responses O
. O

Specifically O
, O
the O
context O
is O
encoded O
with O
gated O
recurrent O
units O
( O
GRU O
) O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
: O

h O
t O
= O
GRU O
( O
h O
t−1 O
, O
E O
( O
c O
t O
) O
) O
, O
( O
4 O
) O

where O
h O
t O
is O
the O
hidden O
state O
of O
GRU O
encoder O
, O
E O
( O
c O
t O
) O
denotes O
the O
word O
embedding O
of O
the O
t O
- O
th O
word O
in O
context O
, O
we O
use O
h O
m O
as O
context O
embedding O
, O
then O
the O
prior O
network O
is O
: O

p O
i O
( O
z|C O
) O
= O
Softmax O
( O
FFN O
z O
( O
h O
m O
) O
) O
, O
( O
5 O
) O

where O
FFN O
represents O
Feed O
- O
Forward O
Network O
with O
two O
layers O
. O
The O
prior O
intent O
distribution O
is O
supervised O
by O
recognition O
distribution O
with O
KLdivergence O
in O
Eq.1 O
: O

L O
2 O
= O
KL O
( O
q O
r O
( O
z|X O
) O
||p O
i O
( O
z|C O
) O
) O
= O
K O
k=1 O
q O
r O
( O
z O
k O
|X O
) O
log O
q O
r O
( O
z O
k O
|X O
) O
p O
i O
( O
z O
k O
|C O
) O
. O
( O
6 O
) O

Since O
the O
context O
emotion O
is O
proved O
to O
be O
beneficial O
to O
empathetic O
dialogue O
generation O
( O
Rashkin O
et O
al O
. O
, O
2019 O
; O
Lin O
et O
al O
. O
, O
2019 O
; O
Majumder O
et O
al O
. O
, O
2020 O
) O
, O
we O
also O
employ O
an O
emotion O
classifier O
to O
classify O
the O
emotion O
of O
context O
: O

P O
= O
Softmax O
( O
FFN O
e O
( O
h O
m O
) O
) O
] O

p O
e O
i O
= O
P O
[ O
i O
] O
( O
7 O
) O

Given O
the O
ground O
truth O
emotion O
label O
e O
t O
, O
the O
emotion O
classifier O
is O
trained O
with O
cross O
- O
entropy O
loss O
: O

L O
3 O
= O
− O
log O
p O
e O
t O
. O
( O
8 O
) O

Response O
Generator O

As O
for O
the O
response O
generation O
p O
( O
X|C O
, O
z O
) O
, O
we O
consider O
implicit O
intent O
embedding O
for O
the O
high O
- O
level O
abstraction O
of O
an O
intent O
. O
In O
addition O
, O
we O
also O
introduce O
intent O
keywords O
for O
explicitly O
utilizing O
intent O
knowledge O
during O
the O
generation O
process O
. O
Implicit O
. O
To O
generate O
response O
with O
an O
empathetic O
intent O
, O
the O
most O
intuitive O
approach O
is O
taking O
the O
intent O
embedding O
as O
additional O
input O
to O
decoder O
during O
the O
generation O
process O
. O
We O
also O
consider O
emotion O
embedding O
as O
traditional O
empathetic O
dialogue O
models O
: O

s O
t O
= O
GRU O
( O
s O
t−1 O
, O
[ O
E O
( O
x O
t−1 O
) O
; O
v O
( O
z O
) O
; O
v O
( O
e O
) O
; O
c O
att O
] O
) O
, O
( O
9 O
) O

where O
s O
t O
is O
the O
state O
of O
GRU O
decoder O
, O
c O
att O
denotes O
the O
context O
attention O
value O
which O
contains O
key O
information O
of O
context O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
v O
( O
z O
) O
is O
intent O
embedding O
and O
v O
( O
e O
) O
is O
emotion O
embedding O
, O
both O
will O
not O
change O
during O
the O
generation O
process O
. O
However O
, O
this O
may O
sacrifice O
grammatical O
correctness O
( O
Zhou O
et O
al O
. O
, O
2018 O
; O
Ghosh O
et O
al O
. O
, O
2017 O
) O
. O
Therefore O
we O
add O
a O
gate O
operation O
to O
capture O
intent O
and O
emotion O
dynamically O
: O

Input O
= O
FFN O
i O
( O
[ O
E O
( O
x O
t O
) O
; O
c O
att O
; O
s O
t O
] O
) O
, O
Gate O
= O
Sigmoid O
( O
Input O
) O
, O
v O
( O
z O
) O
= O
Gate O
⊙ O
v O
( O
z O
) O
, O
( O
10 O
) O

where O
⊙ O
represents O
element O
- O
wise O
product O
. O
Each O
time O
step O
, O
the O
intent O
representation O
is O
used O
appropriately O
according O
to O
current O
word O
, O
state O
, O
and O
context O
value O
. O
The O
gate O
operation O
for O
emotion O
is O
the O
same O
as O
above O
. O

Explicit O
. O
The O
empathetic O
expression O
is O
quite O
distinct O
over O
vocabularies O
, O
e.g. O
, O
' O
know O
' O
, O
' O
understand O
' O
, O
' O
agree O
' O
, O
are O
indicative O
of O
the O
empathetic O
intent O
Agreeing O
. O
Therefore O
, O
we O
employ O
the O
copy O
mechanism O
to O
explicitly O
utilize O
intent O
keywords O
for O
intent O
conditional O
generation O
. O
See O
in O
Appendix O
B O
for O
more O
details O
about O
intent O
keywords O
. O

α O
t O
= O
Sigmoid O
( O
v O
⊤ O
s O
s O
t O
) O
, O
p O
( O
x O
t O
= O
w O
g O
) O
= O
Softmax O
( O
W O
g O
s O
t O
) O
, O
p O
( O
x O
t O
= O
w O
i O
) O
= O
Softmax O
( O
W O
i O
s O
t O
) O
, O
p O
( O
x O
t O
) O
= O
( O
1 O
− O
α O
t O
) O
• O
p O
( O
w O
g O
) O
+ O
α O
t O
• O
p O
( O
w O
i O
) O
, O
( O
11 O
) O

where O
{ O
s O
t O
, O
v O
s O
} O
∈ O
R O
d×1 O
, O
{ O
W O
g O
, O
W O
i O
} O
∈ O
R O
V O
×d O
, O
d O
is O
hidden O
size O
and O
V O
denotes O
the O
vocabulary O
size O
. O
The O
copy O
rate O
α O
t O
is O
used O
to O
balance O
the O
choice O
between O
intent O
keywords O
and O
generic O
words O
, O
it O
is O
trained O
with O
binary O
cross O
entropy O
loss O
: O

L O
4 O
= O
n O
t=1 O
q O
t O
• O
log O
α O
t O
+ O
( O
1 O
− O
q O
t O
) O
• O
log O
( O
1 O
− O
α O
t O
) O
, O
( O
12 O

n O
is O
the O
amount O
of O
words O
in O
response O
, O
q O
t O
∈ O
{ O
0 O
, O
1 O
} O
indicates O
that O
whether O
x O
t O
is O
a O
intent O
keyword O
. O

Loss O
Function O

To O
summarize O
, O
the O
total O
loss O
is O
: O

L O
= O
λ O
1 O
L O
1 O
+ O
λ O
2 O
L O
2 O
+ O
λ O
3 O
L O
3 O
+ O
λ O
4 O
L O
4 O
, O
( O
13 O
) O

In O
order O
to O
join O
all O
losses O
with O
weighting O
method O
, O
we O
add O
4 O
hyperparameters O
in O
total O
loss O
, O
λ O
i O
, O
where O
each O
λ O
i O
is O
corresponding O
to O
L O
i O
. O
L O
1 O
, O
L O
2 O
, O
L O
3 O
, O
L O
4 O
denote O
the O
losses O
of O
response O
reconstruction O
, O
intent O
prediction O
, O
emotion O
classification O
and O
copy O
rate O
prediction O
respectively O
. O

Experiments O

Dataset O

We O
evaluate O
our O
method O
and O
compare O
with O
others O
on O
EmpatheticDialogues B-DatasetName
2 I-DatasetName
( O
Rashkin O
et O
al O
. O
, O
2019 O
) O
which O
contains O
25k O
open O
domain O
dialogues O
. O
Follow O
the O
same O
setting O
as O
the O
authors O
of O
this O
dataset O
, O
the O
proportion O
of O
train O
/ O
validation O
/ O
test O
data O
is O
8 O
: O
1 O
: O
1 O
. O
Each O
dialogue O
consists O
of O
at O
least O
two O
utterances O
between O
a O
speaker O
and O
listener O
. O
There O
are O
32 O
emotion O
situations O
in O
total O
, O
which O
are O
uniformly O
distributed O
. O

Baselines O

We O
compare O
our O
model O
with O
the O
three O
latest O
empathetic O
conversational O
models O
: O

• O
Multitask B-MethodName
Transformer I-MethodName
( O
Multi B-MethodName
- I-MethodName
TRS I-MethodName
) O
. O
A O
transformer O
model O
trained O
by O
the O
response O
generation O
task O
and O
the O
context O
emotion O
classification O
task O
( O
Rashkin O
et O
al O
. O
, O
2019 O
) O
. O

• O
Mixture B-MethodName
of I-MethodName
Empathetic I-MethodName
Listeners I-MethodName
( O
MoEL B-MethodName
) O
. O

An O
enhanced O
transformer O
model O
with O
32 O
emotion O
- O
specific O
decoders O
to O
respond O
appropriately O
for O
each O
emotion O
( O
Lin O
et O
al O
. O
, O
2019 O
) O
. O

• O
MIMicking B-MethodName
Emotions I-MethodName
for I-MethodName
Empathetic I-MethodName
Response I-MethodName
Generation I-MethodName
( O
MIME B-MethodName
) O
. O
The O
state O
- O
ofthe O
- O
art O
empathetic O
dialogue O
model O
allows O
the O
generator O
to O
mimic O
the O
context O
emotion O
to O
a O
varying O
degree O
based O
on O
its O
positivity O
, O
negativity O
, O
and O
content O
. O
Furthermore O
, O
they O
introduce O
stochasticity O
into O
the O
emotion O
mixture O
and O
achieves O
one O
- O
to O
- O
many O
generation O
( O
Majumder O
et O
al O
. O
, O
2020 O
) O
. O

Evaluation O

Automatic O
Metrics O

• O
BLEU B-MetricName
. O
We O
choose O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
for O
relevance O
evaluation O
which O
measures O
the O
n O
- O
gram O
overlaps O
with O
reference O
and O
compute O
BLEU B-MetricName
scores O
for O
n O
≤ O
4 O
using O
smoothing O
techniques O
( O
Chen O
and O
Cherry O
, O
2014 O
) O
. O
Since O
the O
state O
- O
of O
- O
art O
model O
MIME B-MethodName
and O
ours O
are O
both O
one O
- O
to O
- O
many O
generators O
, O
we O
calculate O
BLEU B-MetricName
recall O
and O
BLEU B-MetricName
precision O
( O
Zhao O
et O
al O
. O
, O
2017 O
; O
Gu O
et O
al O
. O
, O
2019 O
) O
. O
For O
each O
test O
case O
, O
we O
sample O
5 O
responses O
from O
latent O
space O
and O
use O
greedy O
search O
for O
MIME B-MethodName
and O
EmpHi B-MethodName
, O
use O
beam O
search O
for O
MoEL B-MethodName
and O
Multitask B-MethodName
- I-MethodName
Transformer I-MethodName
. O

• O
Distinct O
. O
Distinct O
( O
Li O
et O
al O
. O
, O
2016 O
) O
is O
a O
widely O
used O
metric O
for O
diversity O
evaluation O
. O
Specifically O
, O
we O
compute O
the O
number O
of O
distinct O
unigrams O
( O
Distinct-1 O
) O
and O
bigrams O
( O
Distinct-2 O
) O
, O
then O
scale O
them O
by O
the O
total O
number O
of O
unigrams O
and O
bigrams O
. O

Human O
Ratings O

First O
, O
we O
randomly O
sample O
100 O
dialogues O
and O
their O
corresponding O
generations O
from O
the O
three O
baseline O
models O
and O
EmpHi B-MethodName
. O
Then O
, O
we O
invite O
five O
volunteers O
with O
master O
degrees O
to O
do O
the O
human O
evaluation O
. O

The O
annotators O
mark O
each O
response O
from O
1 O
to O
5 O
for O
empathy O
, O
relevance O
, O
and O
fluency O
. O

To O
clarify O
the O
marking O
criteria O
, O
we O
provide O
an O
explanation O
for O
each O
metric O
: O

• O
Empathy O
. O
Whether O
the O
response O
shows O
that O
the O
listener O
understands O
and O
shares O
the O
speaker O
's O
feeling O
. O
Can O
the O
listener O
imagine O
what O
it O
would O
be O
like O
in O
the O
speaker O
's O
situation O
? O

• O
Relevance O
. O
Whether O
the O
response O
is O
relevant O
to O
the O
context O
. O

• O
Fluency O
. O
Whether O
the O
response O
is O
easy O
to O
read O
and O
grammatically O
correct O
. O

Human O
A O
/ O
B O
Test O

Following O
( O
Lin O
et O
al O
. O
, O
2019 O
; O
Majumder O
et O
al O
. O
, O
2020 O
) O
, O
we O
construct O
this O
evaluation O
task O
to O
directly O
compare O
our O
model O
with O
each O
baseline O
. O
We O
randomly O
sample O
100 O
dialogue O
responses O
from O
EmpHi B-MethodName
vs O
{ O
Multitask B-MethodName
- I-MethodName
Trans I-MethodName
, O
MoEL B-MethodName
, O
MIME B-MethodName
} O
. O
Given O
randomly O
ordered O
responses O
from O
above O
models O
, O
four O
annotators O
select O
the O
better O
response O
, O
or O
tie O
if O
they O
think O
the O
two O
responses O
have O
the O
same O
quality O
. O
The O
average O
score O
of O
four O
results O
is O
calculated O
, O
and O
shown O
in O
Table O
6 O
. O

Implement O
Detail O

For O
MIME B-MethodName
3 I-MethodName
( O
Majumder O
et O
al O
. O
, O
2020 O
) O
and O
MoEL B-MethodName
4 I-MethodName
( O
Lin O
et O
al O
. O
, O
2019 O
) O
, O
we O
reproduce O
their O
results O
using O
their O
open O
- O
source O
codes O
and O
their O
default O
hyperparameters O
. O
According O
to O
the O
log O
- O
likelihood O
in O
the O
validation O
dataset O
for O
Multitask B-MethodName
- I-MethodName
Transformer I-MethodName
, O
we O
use O
grid O
search O
for O
the O
best O
head O
number O
, O
layer O
number O
, O
and O
feed O
- O
forward O
neural O
network O
size O
. O
The O
best O
set O
is O
2 O
, O
10 O
, O
and O
256 O
, O
respectively O
. O
EmpHi B-MethodName
uses O
a O
two O
- O
layer O
Bi O
- O
GRU O
as O
the O
encoder O
and O
a O
two O
- O
layer O
GRU O
as O
the O
decoder O
, O
λ B-HyperparameterName
is O
set O
as O
[ O
1 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
1 B-HyperparameterValue
] O
respectively O
. O
All O
the O
feed O
- O
forward O
neural O
networks O
in O
EmpHi B-MethodName
have O
two O
layers O
, O
300 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
and O
ReLU B-HyperparameterValue
activations B-HyperparameterName
. O
For O
the O
sake O
of O
fairness O
, O
we O
use O
pretrained O
Glove O
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
with O
300 O
dimensions O
as O
the O
word O
embedding O
for O
all O
models O
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
1e B-HyperparameterValue
−4 I-HyperparameterValue
. O

6 O
Results O
and O
Discussions O

Results O
Analysis O

In O
this O
section O
, O
we O
mainly O
testify O
: O

• O
human O
- O
like O
empathetic O
intent O
boost O
EmpHi B-MethodName
's O
performance O
in O
terms O
of O
empathy O
, O
relevance O
, O
and O
diversity O
. O

• O
EmpHi B-MethodName
successfully O
captures O
the O
empathetic O
intent O
distribution O
of O
humans O
. O

Human O
Evaluation O

As O
shown O
in O
metric O
in O
empathetic O
dialogue O
generation O
. O
EmpHi B-MethodName
outperforms O
the O
previous O
SOTA O
on O
empathy O
by O
9.43 B-MetricValue
% I-MetricValue
, O
which O
directly O
indicates O
that O
human O
- O
like O
empathetic O
intents O
are O
beneficial O
to O
the O
empathy O
ability O
of O
the O
dialogue O
model O
. O
Last O
but O
not O
least O
, O
a O
decent O
fluency O
score O
proves O
that O
our O
generated O
response O
could O
be O
understood O
by O
humans O
easily O
, O
where O
our O
model O
has O
an O
improvement O
of O
9.87 B-MetricValue
% I-MetricValue
from O
MoEL B-MethodName
. O
In O
addition O
, O
the O
human O
A O
/ O
B O
test O
results O
in O
Table O
2 O
also O
confirm O
that O
the O
responses O
from O
our O
model O
are O
preferable O
to O
baselines O
. O
Overall O
, O
EmpHi B-MethodName
successfully O
generates O
empathetic O
, O
relevant O
, O
and O
fluent O
responses O
. O

Automatic O
Evaluation O

As O
seen O
in O
Table O
1 O
, O
the O
automatic O
evaluation O
is O
consistent O
with O
human O
evaluation O
. O
The O
BLEU B-MetricName
recall I-MetricName
and O
F1 B-MetricName
score I-MetricName
are O
improved O
by O
14.2 B-MetricValue
% I-MetricValue
and O
8.34 B-MetricValue
% I-MetricValue
, O
respectively O
. O
However O
, O
we O
only O
have O
a O
slight O
im- O
provement O
on O
BLEU B-MetricName
precision I-MetricName
, O
which O
is O
similar O
to O
( O
Zhao O
et O
al O
. O
, O
2017 O
; O
Gu O
et O
al O
. O
, O
2019 O
) O
because O
the O
precision O
is O
penalized O
when O
the O
model O
generates O
diverse O
responses O
. O
Also O
, O
the O
distinct O
value O
of O
unigrams O
and O
bigrams O
are O
32.04 O
% O
and O
19.32 O
% O
higher O
than O
the O
previous O
SOTA O
, O
respectively O
. O
As O
shown O
in O
Figure O
4 O
and O
Figure O
6 O
, O
the O
empathy O
intents O
of O
EmpHi B-MethodName
's O
responses O
are O
more O
diverse O
than O
existing O
models O
, O
so O
the O
distinct O
scores O
improve O
significantly O
. O

Our O
method O
enhances O
the O
relevance O
and O
diversity O
simultaneously O
, O
which O
proves O
the O
effectiveness O
of O
human O
- O
like O
intent O
in O
empathetic O
response O
generation O
. O

Empathetic O
Intent O
Distribution O

We O
apply O
the O
same O
approach O
in O
Section O
3 O
and O
quantify O
the O
empathetic O
intent O
distribution O
of O
EmpHi O
's O
responses O
to O
prove O
that O
EmpHi O
accurately O
captures O
humans O
' O
empathetic O
intent O
distribution O
. O
Comparing O
Figure O
4 O
and O
Figure O
6 O
, O
the O
difference O
between O
them O
illustrates O
that O
our O
model O
successfully O
reduces O
the O
bias O
of O
empathetic O
expression O
. O
The O
KL O
- O
divergence O
of O
intent O
distributions O
between O
models O
and O
humans O
are O
0.025 O
for O
EmpHi O
, O
1.949 O
for O
MIME O
, O
1.545 O
for O
MoEL O
, O
and O
4.570 O
for O
Multitask O
- O
Transformer O
( O
See O
in O
Appendix O
A O
) O
. O

Ablation O
Study O

We O
evaluate O
each O
component O
of O
EmpHi B-MethodName
using O
BLEU B-MetricName
and O
ACC B-MetricName
, O
where O
ACC B-MetricName
indicates O
the O
accuracy B-MetricName
of I-MetricName
predicted I-MetricName
empathethetic I-MetricName
intent I-MetricName
of I-MetricName
generated I-MetricName
response I-MetricName
. O
Since O
each O
conversation O
could O
have O
multiple O
empathetic O
responses O
, O
the O
ACC B-MetricName
of O
26.8 B-MetricValue
% I-MetricValue
is O
pretty O
ideal O
. O
As O
seen O
in O
Table O
3 O
, O
there O
is O
a O
dra- O

Emotion O

Nostalgic O

Context O

When O
my O
wife O
and O
i O
started O
dating O
, O
our O
second O
date O
ended O
up O
lasting O
like O
12 O
hours O
. O

Reference O

That O
is O
a O
pretty O
long O
date O
! O
Where O
did O
you O
guys O
go O
? O
Multitask O
- O
Trans O
That O
is O
so O
sad O
, O
I O
'm O
sorry O
to O
hear O
that O
. O

MoEL B-MethodName

That O
is O
great O
! O
I O
am O
sure O
she O
will O
be O
fine O
. O

MIME B-MethodName

That O
is O
a O
good O
thing O
to O
do O
. O
I O
am O
sure O
you O
will O
do O
great O
. O

EmpHi B-MethodName

That O
's O
awesome O
! O
How O
long O
have O
you O
been O
together O
? O
Emotion O
Angry O
Turn-1 O
I O
just O
moved O
to O
this O
neighborhood O
and O
some O
dumb O
criminals O
shot O
one O
of O
my O
neighbors O
and O
ran O
into O
the O
woods O
! O
Turn-2 O
That O
's O
not O
good O
. O
Do O
you O
own O
a O
gun O
? O
Turn-3 O
I O
do O
! O
I O
want O
to O
be O
able O
to O
protect O
my O
son O
. O

Reference O

That O
is O
always O
number O
one O
goal O
. O
matic O
drop O
in O
the O
performance O
of O
EmpHi B-MethodName
without O
any O
intent O
information O
( O
both O
implicit O
embedding O
and O
explicit O
keywords O
) O
. O
Therefore O
, O
this O
proves O
the O
effectiveness O
of O
empathetic O
intents O
and O
the O
intent O
representation O
learning O
approach O
. O
As O
for O
implicit O
gate O
control O
, O
it O
improves O
both O
response O
quality O
and O
intent O
accuracy O
since O
it O
helps O
EmpHi B-MethodName
dynamically O
capture O
intent O
information O
during O
generation O
. O
Same O
conclusion O
has O
been O
made O
in O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
. O

Multitask B-MethodName
- I-MethodName
Trans I-MethodName

The O
copy O
mechanism O
provides O
EmpHi O
the O
ability O
to O
explicitly O
use O
intent O
keywords O
and O
thus O
contributes O
to O
the O
intent O
accuracy O
. O

Case O
Study O

Intent O
- O
level O
diverse O
generation O
. O
Through O
sampling O
intents O
in O
the O
discrete O
latent O
space O
, O
EmpHi B-MethodName
generates O
different O
responses O
with O
empathetic O
intents O
. O
As O
in O
Figure O
7 O
, O
the O
speaker O
shows O
an O
exciting O
emotion O
for O
getting O
a O
better O
job O
. O
EmpHi B-MethodName

generates O
empathetic O
yet O
contextually O
relevant O
responses O
as O
humans O
. O
Besides O
, O
EmpHi B-MethodName
predicts O
the O
potential O
intent O
distribution O
and O
shows O
successful O
conditional O
generation O
based O
on O
the O
corresponding O
intents O
, O
which O
improves O
the O
interpretability O
and O
controllability O
of O
empathetic O
response O
generation O
. O
See O
Appendix O
C O
for O
error O
analysis O
. O

Compare O
with O
existing O
models O
. O
For O
the O
first O
instance O
in O
Table O
4 O
, O
even O
though O
baseline O
models O
show O
naive O
empathy O
in O
their O
response O
, O
it O
is O
hard O
for O
the O
speaker O
to O
feel O
empathy O
because O
the O
response O
is O
not O
relevant O
to O
the O
topic O
. O
In O
contrast O
, O
EmpHi B-MethodName
shows O
its O
understanding O
of O
the O
speaker O
's O
feelings O
and O
asks O
a O
relevant O
question O
to O
explore O
the O
speaker O
's O
experience O
. O
For O
second O
case O
, O
all O
baselines O
express O
contextually O
irrelevant O
empathy O
, O
while O
EmpHi B-MethodName
truly O
understands O
the O
dialogue O
history O
and O
put O
itself O
into O
speaker O
's O
situation O
, O
then O
further O
reply O
: O
" O
Maybe O
you O
should O
go O
to O
the O
police O
" O
with O
the O
Suggesting O
intent O
. O

Conclusion O

Overall O
, O
we O
reveal O
the O
severe O
bias O
of O
empathetic O
expression O
between O
existing O
dialogue O
models O
and O
humans O
. O
To O
address O
this O
issue O
, O
this O
paper O
proposes O
EmpHi B-MethodName
to O
generate O
empathetic O
responses O
with O
human O
- O
like O
empathetic O
intents O
. O
As O
a O
result O
, O
both O
automatic O
and O
human O
evaluation O
prove O
that O
EmpHi B-MethodName
has O
a O
huge O
improvement O
on O
empathetic O
conversation O
. O
According O
to O
the O
anlaysis O
and O
case O
studies O
, O
EmpHi B-MethodName
successfully O
learns O
the O
emapthetic O
intent O
distribution O
of O
human O
and O
shows O
high O
interpretability O
and O
controllability O
during O
the O
generation O
process O
. O
We O
will O
try O
large O
pretrained O
language O
models O
with O
empathetic O
intent O
in O
our O
future O
work O
. O

Ethical O
Statement O

Since O
this O
paper O
involves O
subjects O
related O
to O
human O
conversation O
, O
we O
have O
ensured O
that O
all O
the O
experiments O
will O
cause O
no O
harm O
to O
humans O
. O
The O
dataset O
EmpatheticDialogues B-DatasetName
is O
collected O
by O
( O
Rashkin O
et O
al O
. O
, O
2019 O
) O
, O
all O
the O
participants O
join O
the O
data O
collection O
voluntarily O
. O
Also O
, O
the O
dataset O
provider O
filters O
all O
personal O
information O
and O
obscene O
languages O
. O
Therefore O
, O
we O
believe O
that O
the O
dataset O
Empathetic B-DatasetName
- I-DatasetName
Dialogues I-DatasetName
used O
in O
our O
experiments O
are O
harmless O
to O
users O
, O
and O
the O
model O
trained O
on O
this O
dataset O
is O
not O
dangerous O
to O
humans O
. O

A O
Empathetic O
Expression O
Gap O

For O
more O
comprehensive O
recognization O
of O
the O
severe O
emathy O
expression O
bias O
between O
existing O
empathetic O
dialogue O
models O
and O
humans O
, O
we O
further O
quantify O
the O
bias O
of O
Multitask B-MethodName
- I-MethodName
Transformer I-MethodName
( O
Rashkin O
et O
al O
. O
, O
2019 O
) O
in O
Figure O
8 O
and O
MoEL B-MethodName
( O
Lin O
et O
al O
. O
, O
2019 O
) O
in O
Figure O
9 O
, O
the O
intent O
index O
is O
consistent O
with O
Figure O
2 O
. O
The O
results O
are O
similar O
with O
MIME B-MethodName
( O
Majumder O
et O
al O
. O
, O
2020 O
) O
, O
we O
can O
see O
the O
large O
intent O
distribution O
bias O
and O
the O
monotony O
of O
empathetic O
expression O
of O
existing O
models O
. O

B O
Intent O
Keywords O
Collection O

The O
keywords O
are O
retrieved O
from O
the O
training O
set O
of O
Empathetic B-DatasetName
Intents I-DatasetName
dataset O
( O
Welivita O
and O
Pu O
, O
2020 O
) O
by O
using O
TF O
- O
IDF O
method O
. O
Empathetic B-DatasetName
Intents I-DatasetName
has O
a O
training O
set O
of O
5490 O
responses O
, O
where O
each O
intent O
group O
has O
610 O
responses O
. O
Based O
on O
the O
labeled O
intent O
for O
each O
response O
in O
the O
training O
set O
, O
we O
concatenate O
all O
the O
responses O
which O
are O
in O
the O
same O
group O
and O
remove O
all O
the O
stop O
words O
. O
Finally O
, O
we O
apply O
TF O
- O
IDF O
to O
obtain O
the O
top O
k O
keywords O
for O
each O
intent O
group O
, O
we O
set O
k O
to O
30 O
in O
our O
experiments O
. O
See O
Table O
5 O
for O
top O
ten O
keywords O
for O
each O
intent O
. O

C O
Error O
Analysis O

Although O
EmpHi B-MethodName
achieves O
huge O
improvement O
in O
terms O
of O
empathy O
, O
relevance O
, O
and O
diversity O
in O
empathetic O
dialogue O
generation O
, O
there O
is O
still O
some O
flaws O
. O
At O
first O
, O
the O
generation O
task O
of O
EmpHi B-MethodName
is O
far O
difficult O
than O
existing O
models O
, O
because O
it O
needs O
to O
generate O
response O
condition O
on O
both O
context O
and O
the O
predicted O
intent O
, O
while O
other O
models O
generate O
response O
only O
condition O
on O
the O
context O
, O
therefor O
the O
exposure O
bias O
of O
EmpHi B-MethodName
is O
more O
severe O
. O
See O
in O
Table O
6 O
, O
although O
the O
predicted O
intent O
of O
EmpHi B-MethodName
is O
the O
same O
as O
reference O
and O
its O
corresponding O
response O
is O
great O
, O
EmpHi B-MethodName
also O
gives O
high O
probability O
for O
Questioning O
intent O
and O
the O
corresponding O
response O
is O
not O
very O
contextually O
relevant O
, O
EmpHi B-MethodName
knows O
it O
is O
suitable O
for O
asking O
more O
details O
to O
show O
its O
caring O
, O
but O
it O
does O
not O
know O
how O
to O
ask O
under O
this O
context O
, O
thus O
EmpHi B-MethodName
needs O
better O
understanding O
for O
context O
information O
. O
We O
believe O
this O
issue O
could O
be O
mitigated O
when O
using O
more O
dialogue O
data O
for O
pretraining O
. O

Acknowledgements O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
and O
suggestions O
. O
This O
research O
was O
supported O
in O
part O
by O
the O
National O
Key O
Research O
and O
Development O
Program O
of O
China O
( O
No O
. O
2020YFB1708200 O
) O
, O
the O
Guangdong O
Basic O
and O
Applied O
Basic O
Research O
Foundation O
( O
No O
. O
2019A1515011387 O
) O
and O
the O
Shenzhen O
Key O
Laboratory O
of O
Marine O
IntelliSense O
and O
Computation O
under O
Contract O
ZDSYS20200811142605016 O
. O

Analyzing B-TaskName
Encoded I-TaskName
Concepts I-TaskName
in I-TaskName
Transformer I-TaskName
Language I-TaskName
Models I-TaskName

We O
propose O
a O
novel O
framework O
ConceptX B-MethodName
, O
to O
analyze O
how O
latent O
concepts O
are O
encoded O
in O
representations O
learned O
within O
pre O
- O
trained O
language O
models O
. O
It O
uses O
clustering O
to O
discover O
the O
encoded O
concepts O
and O
explains O
them O
by O
aligning O
with O
a O
large O
set O
of O
human O
- O
defined O
concepts O
. O
Our O
analysis O
on O
seven O
transformer O
language O
models O
reveal O
interesting O
insights O
: O
i O
) O
the O
latent O
space O
within O
the O
learned O
representations O
overlap O
with O
different O
linguistic O
concepts O
to O
a O
varying O
degree O
, O
ii O
) O
the O
lower O
layers O
in O
the O
model O
are O
dominated O
by O
lexical O
concepts O
( O
e.g. O
, O
affixation O
) O
, O
whereas O
the O
core O
- O
linguistic O
concepts O
( O
e.g. O
, O
morphological O
or O
syntactic O
relations O
) O
are O
better O
represented O
in O
the O
middle O
and O
higher O
layers O
, O
iii O
) O
some O
encoded O
concepts O
are O
multi O
- O
faceted O
and O
can O
not O
be O
adequately O
explained O
using O
the O
existing O
human O
- O
defined O
concepts O
. O
1 O

Introduction O

Contextualized O
word O
representations O
learned O
in O
deep O
neural O
network O
models O
( O
DDNs O
) O
capture O
rich O
concepts O
making O
them O
ubiquitous O
for O
transfer O
learning O
towards O
downstream O
NLP O
. O
Despite O
their O
revolution O
, O
the O
blackbox O
nature O
of O
the O
deep O
NLP O
models O
is O
a O
major O
bottle O
- O
neck O
for O
their O
large O
scale O
adaptability O
. O
Understanding O
the O
inner O
dynamics O
of O
these O
models O
is O
important O
to O
ensure O
fairness O
, O
robustness O
, O
reliability O
and O
control O
. O

A O
plethora O
of O
research O
has O
been O
carried O
out O
to O
probe O
DNNs O
for O
the O
linguistic O
knowledge O
( O
e.g. O
morphology O
, O
syntactic O
and O
semantic O
roles O
) O
captured O
within O
the O
learned O
representations O
. O
A O
commonly O
used O
framework O
to O
gauge O
how O
well O
linguistic O
information O
can O
be O
extracted O
from O
these O
models O
is O
the O
Probing O
Framework O
( O
Hupkes O
et O
al O
. O
, O
2018 O
) O
, O
where O
they O
train O
an O
auxiliary O
classifier O
using O
representations O
as O
features O
to O
predict O
the O
property O
of O
interest O
. O
The O
performance O
of O
the O
classifier O
reflects O
the O
amount O
of O
knowledge O
learned O
within O
representations O
. O
To O
this O
end O
, O
the O
researchers O
have O
analyzed O
what O
knowledge O
is O
learned O
within O
the O
representations O
through O
relevant O
extrinsic O
phenomenon O
varying O
from O
word O
morphology O
( O
Vylomova O
et O
al O
. O
, O
2016 O
; O
Belinkov O
et O
al O
. O
, O
2017a O
) O
to O
high O
level O
concepts O
such O
as O
syntactic O
structure O
( O
Blevins O
et O
al O
. O
, O
2018 O
; O
Marvin O
and O
Linzen O
, O
2018 O
) O
and O
semantics O
( O
Qian O
et O
al O
. O
, O
2016 O
; O
Reif O
et O
al O
. O
, O
2019 O
; O
Belinkov O
et O
al O
. O
, O
2017b O
) O
or O
more O
generic O
properties O
( O
Adi O
et O
al O
. O
, O
2016 O
; O
Rogers O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
work O
, O
we O
approach O
the O
representation O
analysis O
from O
a O
different O
angle O
and O
present O
a O
novel O
framework O
ConceptX. B-MethodName
In O
contrast O
to O
relying O
on O
the O
prediction O
capacity O
of O
the O
representations O
, O
we O
analyze O
the O
latent O
concepts O
learned O
within O
these O
representations O
and O
how O
knowledge O
is O
structured O
, O
using O
an O
unsupervised O
method O
. O
More O
specifically O
, O
we O
question O
: O
i O
) O
do O
the O
representations O
encode O
knowledge O
inline O
with O
linguistic O
properties O
such O
as O
word O
morphology O
and O
semantics O
? O
ii O
) O
which O
properties O
dominate O
the O
overall O
structure O
in O
these O
representations O
? O
iii O
) O
does O
the O
model O
learn O
any O
novel O
concepts O
beyond O
linguistic O
properties O
? O
Answers O
to O
these O
questions O
reveal O
how O
deep O
neural O
network O
models O
structure O
language O
information O
to O
learn O
a O
task O
. O

Our O
inspiration O
to O
use O
the O
term O
concept O
comes O
from O
" O
concept O
based O
explanation O
" O
in O
computer O
vision O
( O
Kim O
et O
al O
. O
, O
2018 O
; O
Ghorbani O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2020 O
) O
. O
Stock O
( O
2010 O
) O
defined O
a O
concept O
as O
" O
a O
class O
containing O
certain O
objects O
as O
elements O
, O
where O
the O
objects O
have O
certain O
properties O
" O
. O
We O
define O
an O
encoded O
concept O
as O
a O
cluster O
of O
contextaware O
latent O
representations O
of O
words O
, O
where O
the O
representations O
are O
encoder O
layer O
outputs O
. O

Our O
framework O
clusters O
contextualized O
representations O
using O
agglomerative O
hierarchical O
clustering O
( O
Gowda O
and O
Krishna O
, O
1978 O
) O
. O
The O
resulting O
clusters O
represent O
encoded O
concepts O
, O
captured O
within O
the O
learned O
representations O
( O
Please O
see O
Figure O
1 O
for O
illustration O
) O
. O
We O
then O
use O
a O
novel O
align O
- O
Figure O
1 O
: O
ConceptX B-MethodName
: O
i O
) O
Extract O
representations O
from O
trained O
model O
, O
ii O
) O
Cluster O
the O
representations O
to O
obtain O
encoded O
concepts O
, O
iii O
) O
Align O
the O
concepts O
to O
human O
- O
defined O
concepts O
ment O
function O
that O
measures O
the O
amount O
of O
overlap O
between O
encoded O
concepts O
and O
a O
range O
of O
predefined O
categories O
( O
that O
we O
call O
as O
human O
- O
defined O
concepts O
in O
this O
paper O
) O
. O
We O
experimented O
with O
affixes O
, O
casing O
, O
morphological O
, O
syntactic O
, O
semantic O
, O
WordNet O
( O
Miller O
, O
1995 O
) O
, O
and O
psycholinguistic O
concepts O
( O
LIWC O
Pennebaker O
et O
al O
. O
( O
2001 O
) O
) O
. O
The O
use O
of O
such O
a O
diverse O
set O
of O
human O
- O
defined O
concepts O
enables O
us O
to O
cover O
various O
abstractions O
of O
language O
. O
In O
Figure O
3 O
we O
present O
a O
few O
examples O
of O
human O
- O
defined O
concepts O
that O
were O
aligned O
with O
the O
encoded O
concepts O
. O

We O
carry O
out O
our O
study O
on O
seven O
pre O
- O
trained O
transformer O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
XLM B-MethodName
- I-MethodName
RoBERTa I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
with O
varying O
optimization O
functions O
, O
architectural O
details O
and O
training O
data O
. O
Some O
notable O
findings O
emerging O
from O
our O
analysis O
are O
as O
follows O
: O

• O
Shallow O
concepts O
such O
as O
lexical O
ngrams O
or O
suffixes O
are O
predominantly O
captured O
in O
the O
lower O
layers O
of O
the O
network O
. O

• O
WordNet O
and O
psycholinguistic O
- O
based O
concepts O
( O
LIWC O
) O
are O
also O
learned O
in O
the O
lower O
layers O
. O

• O
Middle O
and O
higher O
layers O
encode O
concepts O
that O
capture O
core O
linguistic O
properties O
such O
as O
morphology O
, O
semantics O
and O
syntax O
. O

• O
Roughly O
50 O
% O
of O
the O
encoded O
concepts O
adhere O
to O
our O
suite O
of O
human O
- O
defined O
linguistic O
concepts O
. O

• O
The O
models O
learn O
novel O
concepts O
that O
are O
multi O
- O
faceted O
and O
can O
not O
be O
adequately O
explained O
using O
the O
existing O
human O
- O
defined O
concepts O
. O

Our O
contributions O
in O
this O
paper O
are O
as O
follow O
: O
i O
) O
We O
present O
ConceptX B-MethodName
, O
a O
framework O
that O
interprets O
encoded O
concepts O
in O
the O
learned O
representation O
by O
measuring O
their O
alignment O
to O
the O
human O
- O
defined O
concepts O
. O
ii O
) O
We O
provide O
a O
qualitative O
and O
quantitative O
evidence O
of O
how O
knowledge O
is O
structured O
within O
deep O
NLP O
models O
with O
respect O
to O
a O
large O
suite O
of O
human O
- O
defined O
concepts O
. O

Related O
Work O

Most O
of O
the O
work O
done O
on O
interpretability O
in O
deep O
NLP O
addresses O
two O
questions O
in O
particular O
: O
( O
i O
) O
what O
linguistic O
( O
and O
non O
- O
linguistic O
) O
knowledge O
is O
learned O
within O
contextualized O
representations O
, O
Concept O
Analysis O
and O
( O
ii O
) O
how O
this O
information O
is O
utilized O
in O
the O
decision O
making O
process O
, O
Attribution O
Analysis O
. O
The O
former O
thrives O
on O
post O
- O
hoc O
decomposability O
, O
where O
we O
analyze O
representations O
to O
uncover O
linguistic O
phenomenon O
that O
are O
captured O
as O
the O
network O
is O
trained O
towards O
any O
NLP O
task O
( O
Adi O
et O
al O
. O
, O
2016 O
; O
Conneau O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2019a O
; O
Tenney O
et O
al O
. O
, O
2019 O
; O
and O
the O
latter O
characterize O
the O
role O
of O
model O
components O
and O
input O
features O
towards O
a O
specific O
prediction O
( O
Linzen O
et O
al O
. O
, O
2016 O
; O
Gulordava O
et O
al O
. O
, O
2018 O
; O
Marvin O
and O
Linzen O
, O
2018 O
) O
. O
Our O
work O
falls O
into O
the O
former O
category O
. O
Previous O
studies O
have O
explored O
visualization O
methods O
to O
analyze O
the O
learned O
representations O
( O
Karpathy O
et O
al O
. O
, O
2015 O
; O
Kádár O
et O
al O
. O
, O
2017 O
) O
, O
attention O
heads O
( O
Clark O
et O
al O
. O
, O
2019 O
; O
Vig O
, O
2019 O
) O
, O
language O
compositionality O
( O
Li O
et O
al O
. O
, O
2016 O
) O
etc O
. O
A O
more O
commonly O
used O
framework O
analyzes O
representations O
by O
correlating O
parts O
of O
the O
neural O
network O
with O
linguistic O
properties O
, O
by O
training O
a O
classifier O
to O
predict O
a O
feature O
of O
interest O
( O
Adi O
et O
al O
. O
, O
2016 O
; O
Belinkov O
et O
al O
. O
, O
2017a O
; O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O
Several O
researchers O
used O
probing O
classifiers O
for O
investigating O
the O
contextualized O
representations O
learned O
from O
a O
variety O
of O
neural O
language O
models O
on O
a O
variety O
of O
character- O
, O
word- O
( O
Liu O
et O
al O
. O
, O
2019a O
) O
or O
sub O
- O
sentence O
level O
( O
Tenney O
et O
al O
. O
, O
2019 O
) O
linguistic O
tasks O
. O
Rather O
than O
analyzing O
the O
representations O
as O
a O
whole O
, O
several O
researchers O
also O
explored O
identifying O
salient O
neurons O
within O
the O
model O
that O
capture O
different O
properties O
( O
Dalvi O
et O
al O
. O
, O
2019a O
; O
Suau O
et O
al O
. O
, O
2020 O
; O
Mu O
and O
Andreas O
, O
2020 O
) O
or O
are O
salient O
for O
the O
model O
irrespective O
of O
the O
property O
( O
Bau O
et O
al O
. O
, O
2019 O
; O
Wu O
et O
al O
. O
, O
2020 O
) O
. O

Our O
work O
is O
inline O
with O
( O
Michael O
et O
al O
. O
, O
2020 O
; O
Dalvi O
et O
al O
. O
, O
2022 O
) O
, O
who O
analyzed O
latent O
concepts O
learned O
in O
pre O
- O
trained O
models O
. O
Michael O
et O
al O
. O
( O
2020 O
) O
used O
a O
binary O
classification O
task O
to O
induce O
latent O
concepts O
relevant O
to O
a O
task O
and O
showed O
the O
presence O
of O
linguistically O
motivated O
and O
novel O
concepts O
in O
the O
representation O
. O
However O
, O
different O
from O
them O
, O
we O
analyze O
representations O
in O
an O
unsupervised O
fashion O
. O
Dalvi O
et O
al O
. O
( O
2022 O
) O
used O
human O
- O
in O
- O
the O
- O
loop O
to O
analyze O
latent O
spaces O
in O
BERT O
. O
Our O
framework O
uses O
human O
- O
defined O
concepts O
to O
automatically O
generate O
explanations O
for O
the O
latent O
concepts O
. O
This O
enabled O
us O
to O
scale O
our O
study O
to O
many O
transformer O
models O
. O

In O
a O
similar O
work O
, O
Mamou O
et O
al O
. O
( O
2020 O
) O
applied O
manifold O
analysis O
technique O
to O
understand O
the O
amount O
of O
information O
stored O
about O
object O
categories O
per O
unit O
. O
Our O
approach O
does O
away O
from O
the O
methodological O
limitations O
of O
probing O
framework O
such O
as O
complexity O
of O
the O
probes O
, O
effect O
of O
randomness O
etc O
( O
Belinkov O
, O
2021 O
) O
. O
However O
, O
it O
is O
important O
to O
mention O
that O
the O
two O
frameworks O
are O
orthogonal O
and O
complement O
each O
other O
. O

Methodology O

A O
vector O
representation O
in O
the O
neural O
network O
model O
is O
composed O
of O
feature O
attributes O
of O
the O
input O
words O
. O
We O
group O
the O
encoded O
vector O
representations O
using O
a O
clustering O
approach O
discussed O
below O
. O
The O
underlying O
clusters O
, O
that O
we O
term O
as O
the O
encoded O
concepts O
, O
are O
then O
matched O
with O
the O
human O
- O
defined O
concepts O
using O
an O
alignment O
function O
. O
Formally O
, O
consider O
a O
Neural O
Network O
( O
NN O
) O
model O
M O
with O
L O
encoder O
layers O
{ O
l O
1 O
, O
l O
2 O
, O
... O
l O
l O
, O
... O
, O
l O
L O
} O
, O
with O
H O
hidden O
nodes O
per O
layer O
. O
An O
input O
sentence O
consisting O
of O
M O
words O
w O
1 O
, O
w O
2 O
, O
... O
w O
i O
, O
... O
, O
w O
M O
is O
fed O
into O
a O
NN O
. O
For O
each O
input O
word O
i O
, O
we O
compute O
the O
node O
output O
( O
after O
applying O
the O
activation O
func O
- O
tions O
) O
y O
l O
h O
( O
w O
i O
) O
of O
every O
hidden O
node O
h O
∈ O
{ O
1 O
, O
... O
, O
H O
} O
in O
each O
layer O
l O
, O
where O
− O
→ O
y O
l O
( O
w O
i O
) O
is O
the O
vector O
representation O
composing O
the O
outputs O
of O
all O
hidden O
nodes O
in O
layer O
l O
for O
w O
i O
. O
Our O
goal O
is O
to O
cluster O
representations O
− O
→ O
y O
l O
, O
from O
a O
large O
training O
data O
to O
obtain O
encoded O
concepts O
. O
We O
then O
align O
these O
with O
various O
human O
- O
defined O
concepts O
to O
obtain O
an O
explanation O
of O
them O
to O
build O
an O
understanding O
of O
how O
these O
concepts O
are O
represented O
across O
the O
network O
. O

Clustering O

We O
use O
agglomerative O
hierarchical O
clustering O
( O
Gowda O
and O
Krishna O
, O
1978 O
) O
, O
which O
we O
found O
to O
be O
effective O
for O
this O
task O
. O
It O
assigns O
each O
word O
to O
a O
separate O
cluster O
and O
then O
iteratively O
combines O
them O
based O
on O
Ward O
's O
minimum O
variance O
criterion O
that O
minimizes O
intra O
- O
cluster O
variance O
. O
Distance O
between O
two O
representations O
is O
calculated O
with O
the O
squared O
Euclidean O
distance O
. O
The O
algorithm O
terminates O
when O
the O
required O
K O
clusters O
( O
aka O
encoded O
concepts O
) O
are O
formed O
, O
where O
K O
is O
a O
hyperparameter O
. O
Each O
encoded O
concept O
represents O
a O
latent O
relationship O
between O
the O
words O
present O
in O
the O
cluster O
. O
Appendix O
C O
presents O
the O
algorithm O
. O

Alignment O

Now O
we O
define O
the O
alignment O
function O
between O
the O
encoded O
and O
human O
- O
defined O
concepts O
. O
Consider O
a O
human O
- O
defined O
concept O
as O
z O
, O
where O
a O
function O
z O
( O
w O
) O
= O
z O
denotes O
that O
z O
is O
the O
human O
- O
defined O
concept O
of O
word O
w. O
For O
example O
, O
parts O
- O
of O
- O
speech O
is O
a O
human O
- O
defined O
concept O
and O
each O
tag O
such O
as O
noun O
, O
verb O
etc O
. O
represents O
a O
class O
/ O
label O
within O
the O
concept O
, O
e.g. O
z O
( O
sea O
) O
= O
noun O
. O
Similarly O
, O
suffix O
is O
a O
human O
- O
defined O
concept O
with O
various O
suffixes O
representing O
a O
class O
, O
e.g. O
z O
( O
bigger O
) O
= O
er O
. O
A O
reverse O
function O
of O
z O
is O
a O
one O
- O
to O
- O
many O
function O
that O
outputs O
a O
set O
of O
unique O
words O
with O
the O
given O
humandefined O
concept O
, O
i.e. O
, O
z O
−1 O
( O
z O
) O
= O
{ O
w O
1 O
, O
w O
2 O
, O
. O
. O
. O
, O
w O
J O
} O
, O
like O
z O
−1 O
( O
noun O
) O
= O
{ O
sea O
, O
tree O
, O
. O
. O
. O
} O
, O
where O
J O
is O
the O
total O
number O
of O
words O
with O
the O
human O
- O
defined O
concept O
of O
z. O
Following O
this O
notation O
, O
an O
encoded O
concept O
is O
indicated O
as O
c O
, O
where O
c O
( O
w O
) O
= O
c O
is O
a O
function O
of O
applying O
encoded O
concept O
on O
w O
, O
and O
its O
reverse O
function O
outputs O
a O
set O
of O
unique O
words O
with O
the O
encoded O
concept O
of O
c O
, O
i.e. O
, O
c O
−1 O
( O
c O
) O
= O
{ O
w O
1 O
, O
w O
2 O
, O
. O
. O
. O
, O
w O
I O
} O
, O
where O
I O
is O
the O
set O
size O
. O

To O
align O
the O
encoded O
concepts O
with O
the O
humandefined O
concepts O
, O
we O
auto O
- O
annotate O
the O
input O
data O
that O
we O
used O
to O
get O
the O
clusters O
, O
with O
the O
humandefined O
concepts O
. O
We O
call O
our O
encoded O
concept O
( O
c O
) O
to O
be O
θ O
- O
aligned O
( O
Λ O
θ O
) O
with O
a O
human O
- O
defined O
concept O
( O
z O
) O
as O
follows O
: O

Λ O
θ O
( O
z O
, O
c O
) O
= O
1 O
, O
if O
w O
′ O
∈z O
−1 O
w∈c O
−1 O
δ O
( O
w O
, O
w O
′ O
) O
J O
≥ O
θ O
0 O
, O
otherwise O
, O

where O
Kronecker O
function O
δ O
( O
w O
, O
w O
′ O
) O
is O
defined O
as O

δ O
( O
w O
, O
w O
′ O
) O
= O
1 O
, O
if O
w O
= O
w O
′ O
0 O
, O
otherwise O

We O
compute O
c O
and O
Λ O
θ O
( O
z O
, O
c O
) O
for O
the O
encoder O
output O
from O
each O
layer O
l O
of O
a O
neural O
network O
. O
To O
compute O
a O
network O
- O
wise O
alignment O
, O
we O
simply O
average O
θagreement O
over O
layers O
. O

4 O
Experimental O
Setup O

Dataset O

We O
used O
a O
subset O
of O
WMT B-DatasetName
News I-DatasetName
2018 I-DatasetName
2 O
( O
359 O
M O
tokens O
) O
dataset O
. O
We O
randomly O
selected O
250k O
sentences O
from O
the O
dataset O
( O
≈5 O
M O
tokens O
) O
to O
train O
our O
clustering O
model O
. O
We O
discarded O
words O
with O
a O
frequency O
of O
less O
than O
10 O
and O
selected O
maximum O
10 O
occurrences O
of O
a O
word O
type O
. O
3 O
The O
final O
dataset O
consists O
of O
25k O
word O
types O
with O
10 O
contexts O
per O
word O
. O

Pre O
- O
trained O
Models O

We O
carried O
out O
our O
analysis O
on O
various O
12 O
- O
layered O
transformer O
models O
such O
as O
BERT B-MethodName
- I-MethodName
cased I-MethodName
( O
BERTc B-MethodName
, O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
- I-MethodName
uncased I-MethodName
( O
BERT B-MethodName
- I-MethodName
uc I-MethodName
) O
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
, O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
. O
We O
also O
analyzed O
multilingual O
models O
such O
as O
multilingualbert O
- O
cased O
( O
mBERT B-MethodName
) O
and O
XLM B-MethodName
- I-MethodName
RoBERTa I-MethodName
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
Conneau O
et O
al O
. O
, O
2020 O
) O
where O
the O
embedding O
space O
is O
shared O
across O
many O
languages O
. O
This O
choice O
of O
models O
is O
motivated O
from O
interesting O
differences O
in O
their O
architectural O
designs O
, O
training O
data O
settings O
( O
cased O
vs. O
un O
- O
cased O
) O
and O
multilinguality O
. O

Clustering O
and O
Alignment O

We O
extract O
contextualized O
representation O
of O
words O
by O
performing O
a O
forward O
pass O
over O
the O
network O
using O
the O
NeuroX O
toolkit O
( O
Dalvi O
et O
al O
. O
, O
2019b O
) O
. O
We O
cluster O
representations O
in O
every O
layer O
into O
K O
groups O
. O

To O
find O
an O
optimum O
value O
of O
K B-HyperparameterName
, O
we O
experimented O
with O
the O
ELbow O
( O
Thorndike O
, O
1953 O
) O
and O
Silhouette O
( O
Rousseeuw O
, O
1987 O
) O
methods O
. O
However O
, O
we O
did O
not O
observe O
reliable O
results O
( O
see O
Appendix O
C O
) O
. O
Therefore O
, O
we O
empirically O
selected O
K B-HyperparameterName
= O
1000 B-HyperparameterValue
based O
on O
finding O
a O
decent O
balance O
between O
many O
small O
clusters O
( O
over O
- O
clustering O
) O
and O
a O
few O
large O
clusters O
( O
under O
- O
clustering O
) O
. O
We O
found O
that O
our O
results O
are O
not O
sensitive O
to O
this O
parameter O
and O
generalize O
for O
different O
cluster O
settings O
( O
See O
Section O
5.4 O
) O
. O
For O
the O
alignment O
between O
encoded O
and O
human O
- O
defined O
concepts O
, O
we O
use O
θ B-HyperparameterName
= O
90 B-HyperparameterValue
% I-HyperparameterValue
i.e. O
, O
we O
consider O
an O
encoded O
concept O
and O
a O
human O
- O
defined O
concept O
to O
be O
aligned O
, O
if O
they O
have O
at O
least O
90 O
% O
match O
. O

Human O
- O
defined O
concepts O

We O
experiment O
with O
the O
various O
Human O
- O
defined O
concepts O
, O
which O
we O
categorize O
into O
four O
groups O
: O

• O
Lexical O
Concepts O
: O
Ngrams O
, O
Affixes O
, O
Casing O
, O
First O
and O
the O
Last O
Word O
( O
in O
a O
sentence O
) O
• O
Morphology O
and O
Semantics O
: O
POS O
tags O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
and O
SEM O
tags O
( O
Abzianidze O
et O
al O
. O
, O
2017 O
) O
• O
Syntactic O
: O
Chunking O
tags O
( O
Tjong O
Kim O
Sang O
and O
Buchholz O
, O
2000 O
) O
and O
CCG O
super O
- O
tags O
( O
Hockenmaier O
, O
2006 O
) O
• O
Linguistic O
Ontologies O
: O
WordNet O
( O
Miller O
, O
1995 O
) O
and O
LIWC O
( O
Pennebaker O
et O
al O
. O
, O
2001 O
) O
At O
various O
places O
in O
this O
paper O
, O
we O
also O
refer O
to O
Morphology O
, O
Semantics O
and O
Syntactic O
concepts O
as O
core O
- O
linguistic O
concepts O
. O
We O
trained O
BERT O
- O
based O
classifiers O
using O
gold O
- O
annotated O
training O
data O
and O
standard O
splits O
for O
each O
core O
- O
linguistic O
concepts O
and O
auto O
- O
labelled O
the O
selected O
news O
dataset O
using O
these O
. O
4 O

Analysis O

In O
this O
section O
, O
we O
analyze O
the O
encoded O
concepts O
by O
aligning O
them O
with O
the O
human O
- O
defined O
concepts O
. O

Overall O
Alignment O

First O
we O
present O
to O
what O
extent O
the O
encoded O
concepts O
in O
the O
entire O
network O
align O
with O
the O
humandefined O
concepts O
. O
We O
compute O
the O
overall O
score O
as O
the O
percentage O
of O
the O
aligned O
encoded O
concepts O
to O
the O
human O
- O
defined O
concepts O
across O
layers O
using O
the O
function O
described O
in O
Section O
3.2 O
. O
We O
BERT B-MethodName
- I-MethodName
c I-MethodName
BERT B-MethodName
- I-MethodName
uc I-MethodName
mBERT B-MethodName
XLM B-MethodName
- I-MethodName
R I-MethodName
RoBERTa B-MethodName
ALBERT B-MethodName
XLNet B-MethodName
Figure O
2 O
presents O
the O
results O
. O

Lexical O
Concepts O
Pre O
- O
trained O
models O
encode O
varying O
amount O
of O
lexical O
concepts O
such O
as O
casing O
, O
ngrams O
and O
suffixes O
. O
We O
found O
between O
7 O
- O
11 O
% O
encoded O
concepts O
that O
align O
with O
the O
casing O
concept O
( O
title O
case O
or O
upper O
case O
) O
. O
We O
observed O
that O
most O
of O
these O
encoded O
concepts O
consist O
of O
named O
entities O
, O
which O
were O
grouped O
together O
based O
on O
semantics O
. O

Comparing O
suffixes O
and O
ngrams O
While O
affixes O
often O
have O
linguistic O
connotation O
( O
e.g. O
, O
the O
prefix O
anti O
negates O
the O
meaning O
of O
the O
stem O
and O
the O
suffix O
ies O
is O
used O
for O
pluralization O
) O
, O
the O
ngram O
units O
that O
become O
part O
of O
the O
vocabulary O
as O
an O
artifact O
of O
statistical O
segmentation O
( O
e.g. O
, O
using O
BPE O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
or O
Word O
- O
piece O
( O
Schuster O
and O
Nakajima O
, O
2012 O
) O
) O
often O
lack O
any O
linguistic O
meaning O
. O
However O
, O
models O
learn O
to O
encode O
such O
information O
. O
We O
found O
a O
match O
ranging O
from O
1 O
% O
( O
BERT B-MethodName
- I-MethodName
cased I-MethodName
) O
up O
to O
25 O
% O
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
when O
comparing O
encoded O
concepts O
with O
the O
suffix O
concept O
. O
A O
similar O
pattern O
is O
observed O
in O
the O
case O
of O
the O
ngram O
concept O
( O
which O
is O
a O
superset O
of O
the O
suffix O
concept O
) O
where O
a O
staggering O
48 O
% O
matches O
were O
found O
. O
Figure O
6a O
shows O
an O
ngram O
cluster O
found O
in O
layer O
2 O
of O
Morphology O
and O
Semantics O
We O
found O
that O
the O
encoded O
concepts O
based O
on O
word O
morphology O
( O
POS O
) O
consistently O
showed O
a O
higher O
match O
across O
all O
models O
in O
comparison O
to O
the O
other O
abstract O
concepts O
, O
aligning O
a O
quarter O
of O
the O
encoded O
concepts O
in O
the O
case O
of O
mBERT B-MethodName
. O
The O
alignment O
with O
semantic O
concepts O
is O
relatively O
lower O
, O
with O
at O
most O
16 O
% O
match O
across O
models O
. O
This O
reflects O
that O
while O
the O
models O
learn O
both O
linguistic O
properties O
, O
morphological O
ontology O
is O
relatively O
preferred O
compared O
to O
the O
semantic O
hierarchy O
. O

Syntactic O
These O
concepts O
capture O
grammatical O
orientation O
of O
a O
word O
, O
for O
example O
Chunking O
: O
B O
- O
NP O
is O
a O
syntactic O
concept O
describing O
words O
in O
the O
beginning O
of O
a O
noun O
phrase O
. O
CCG O
: O
PP O
/ O
NP O
is O
a O
concept O
in O
CCG O
super O
tagging O
, O
describing O
words O
that O
takes O
a O
noun O
phrase O
on O
the O
right O
and O
outputs O
a O
preposition O
phrase O
for O
example O
" O
[ O
in O
[ O
the O
US O
] O
] O
" O
. O
We O
found O
relatively O
fewer O
matches O
, O
a O
maximum O
of O
7 O
% O
and O
14 O
% O
matching O
encoded O
concepts O
for O
Chunking O
and O
CCG O
concepts O
respectively O
. O
The O
low O
matches O
for O
syntactic O
concepts O
suggest O
that O
the O
models O
do O
not O
encode O
the O
same O
syntactic O
hierarchy O
suggested O
by O
these O
human O
- O
defined O
syntactic O
tasks O
. O

Linguistic O
Ontologies O
Comparing O
the O
encoded O
concepts O
with O
static O
linguistic O
ontologies O
, O
we O
found O
WordNet O
concepts O
to O
be O
the O
second O
most O
aligned O
concept O
( O
11 O
- O
21 O
% O
) O
with O
the O
human O
- O
defined O
concepts O
. O
LIWC O
also O
shows O
a O
relatively O
higher O
alignment O
compared O
to O
the O
other O
human O
- O
defined O
concepts O
in O
a O
few O
models O
( O
e.g. O
, O
BERT B-MethodName
- I-MethodName
c I-MethodName
) O
. O
However O
, O
this O
observation O
is O
not O
consistent O
across O
models O
and O
we O
found O
a O
range O
between O
5 O
- O
16 O
% O
matches O
. O
These O
results O
present O
an O
interesting O
case O
where O
several O
models O
prefer O
the O
distinction O
of O
lexical O
ontology O
over O
abstract O
linguistic O
concepts O
such O
as O
morphology O
. O
Figure O
3 O
shows O
examples O
of O
encoded O
concepts O
aligned O
with O
WordNet O
and O
LIWC O
. O
We O
see O
that O
these O
concepts O
are O
built O
based O
on O
a O
semantic O
relationship O
e.g. O
, O
the O
clusters O
in O
Figure O
3b O
, O
3c O
and O
3d O
group O
words O
based O
on O
religious O
, O
facial O
anatomy O
, O
and O
specific O
motion O
- O
related O
vocabulary O
respectively O
. O

Comparing O
Models O

The O
results O
of O
multilingual O
models O
( O
mBERT B-MethodName
, O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
are O
intriguing O
given O
that O
their O
encoded O
concepts O
are O
dominated O
by O
ngrambased O
concepts O
and O
POS O
concepts O
, O
and O
their O
relatively O
lesser O
alignment O
with O
the O
linguistic O
ontologies O
. O
On O
the O
contrary O
, O
several O
monolingual O
models O
( O
BERT B-MethodName
- I-MethodName
c I-MethodName
, O
ALBERT B-MethodName
) O
showed O
a O
better O
match O
with O
linguistic O
ontologies O
specially O
WordNet O
. O
The O
higher O
number O
of O
matches O
to O
the O
ngram O
( O
and O
suffix O
) O
concepts O
in O
the O
multilingual O
models O
is O
due O
to O
the O
difference O
in O
subword O
segmentation O
. O
The O
subword O
models O
in O
XLM B-MethodName
- I-MethodName
R I-MethodName
and O
mBERT B-MethodName
are O
optimized O
for O
multiple O
languages O
, O
resulting O
in O
a O
vocabulary O
consisting O
of O
a O
large O
number O
of O
small O
ngram O
units O
. O
This O
causes O
the O
multilingual O
models O
to O
aggressively O
segment O
the O
input O
sequence O
, O
compared O
to O
the O
monolingual O
models O
7 O
and O
resulted O
in O
highly O
dominated O
ngram O
- O
based O
encoded O
concepts O
, O
especially O
in O
the O
lower O
layers O
. O
This O
may O
also O
explain O
the O
relatively O
lower O
match O
that O
multilingual O
models O
exhibit O
to O
the O
linguistic O
ontologies O
. O
We O
discuss O
this O
further O
in O
the O
context O
of O
layer O
- O
wise O
analysis O
in O
Section O
5.2 O
. O

Comparing O
BERT B-MethodName
cased I-MethodName
vs. O
uncased O
, O
interestingly O
BERT O
- O
uc O
consistently O
showed O
higher O
matches O
for O
the O
core O
- O
linguistic O
concepts O
( O
See O
Figure O
2 O
) O
. O
We O
speculate O
that O
in O
the O
absence O
of O
casing O
information O
, O
BERT B-MethodName
- I-MethodName
uc I-MethodName
is O
forced O
to O
learn O
more O
linguistic O
concepts O
, O
whereas O
BERT B-MethodName
- I-MethodName
c I-MethodName
leverages O
the O
explicit O
casing O
information O
to O
capture O
more O
semantically O
motivated O
concepts O
based O
on O
linguistic O
ontologies O
. O

The O
higher O
matches O
in O
multilingual O
models O
in O
comparison O
to O
the O
monolingual O
models O
, O
and O
BERTuncased B-MethodName
in O
comparison O
to O
BERT B-MethodName
- I-MethodName
cased I-MethodName
suggest O
that O
the O
training O
complexity O
is O
one O
factor O
that O
plays O
a O
role O
in O
a O
model O
's O
ability O
to O
learn O
linguistic O
nuances O
. O
For O
example O
, O
multilingual O
models O
need O
to O
optimize O
many O
languages O
, O
which O
is O
a O
harder O
task O
compared O
to O
learning O
one O
language O
. O
Similarly O
, O
the O
absence O
of O
capitalization O
in O
training O
data O
makes O
the O
learning O
task O
relatively O
harder O
for O
BERT B-MethodName
- I-MethodName
uc I-MethodName
compared O
to O
BERT B-MethodName
- I-MethodName
c I-MethodName
models O
, O
thus O
resulting O
in O
higher O
matches O
for O
BERT B-MethodName
- I-MethodName
uc I-MethodName
. O
We O
speculate O
that O
the O
harder O
the O
training O
task O
, O
the O
more O
language O
nuances O
are O
learned O
by O
a O
model O
. O
made O
a O
similar O
observation O
, O
where O
they O
showed O
that O
the O
linguistic O
knowledge O
learned O
within O
the O
encoder O
- O
decoder O
representations O
in O
NMT O
models O
correlates O
with O
complexity O
of O
a O
language O
- O
pair O
involved O
in O
the O
task O
. O

Layer O
- O
wise O
Alignment O

Now O
we O
study O
the O
alignment O
of O
human O
- O
defined O
concepts O
across O
layers O
to O
understand O
how O
concepts O
Figure O
4 O
: O
Layer O
- O
wise O
concept O
alignment O
. O
Y O
- O
axis O
is O
the O
normalized O
number O
of O
aligned O
concepts O
. O
The O
number O
within O
brackets O
of O
each O
human O
- O
defined O
concept O
, O
e.g. O
Casing O
( O
166 O
) O
, O
shows O
the O
maximum O
layer O
- O
wise O
match O
evolve O
in O
the O
network O
. O
Figure O
4 O
shows O
results O
for O
selected O
models O
. O
8 O
The O
y O
- O
axis O
is O
the O
normalized O
number O
of O
aligned O
concepts O
across O
layers O
. O

Overall O
Trend O

We O
observed O
mostly O
consistent O
patterns O
across O
models O
except O
for O
ALBERT B-MethodName
, O
which O
we O
will O
discuss O
later O
in O
this O
section O
. O
We O
found O
that O
the O
shallow O
concepts O
( O
such O
as O
ngram O
and O
suffixes O
) O
and O
the O
linguistic O
ontologies O
( O
LIWC O
and O
WORD O
- O
NET O
) O
are O
better O
represented O
in O
the O
initial O
layers O
and O
exhibit O
a O
downward O
trend O
in O
the O
higher O
layers O
of O
the O
network O
. O
On O
the O
contrary O
the O
core O
linguistic O
concepts O
( O
POS O
, O
Chunking O
, O
etc O
. O
) O
are O
better O
repre-8 O
See O
Figure O
10 O
in O
the O
Appendix O
for O
complete O
results O
. O
sented O
in O
the O
higher O
layers O
( O
layer O
8 O
- O
10 O
) O
. O
The O
last O
layers O
do O
not O
show O
any O
consistently O
dominating O
human O
- O
defined O
concepts O
considered O
in O
this O
work O
. O
We O
can O
generalize O
on O
these O
trends O
and O
hypothesize O
on O
how O
encoded O
concepts O
evolve O
in O
the O
network O
: O
the O
initial O
layers O
of O
the O
pretrained O
models O
, O
group O
words O
based O
on O
their O
lexical O
and O
semantic O
similarities O
where O
the O
former O
is O
an O
artifact O
of O
subword O
segmentation O
. O
With O
the O
inclusion O
of O
context O
and O
abstraction O
in O
the O
higher O
layers O
, O
these O
groups O
evolve O
into O
linguistic O
manifolds O
. O
The O
encoded O
concepts O
in O
the O
last O
layers O
are O
influenced O
by O
the O
objective O
function O
and O
learn O
concepts O
relevant O
to O
the O
task O
. O
also O
made O
similar O
observation O
when O
analyzing O
linguistic O
concepts O
in O
pre O
- O
trained O
models O
that O
are O
fine O
- O
tuned O
towards O
different O
GLUE O
tasks O
. O

Concept O
- O
wise O
Trend O

In O
the O
following O
, O
we O
discuss O
different O
concepts O
in O
detail O
. O
As O
we O
mentioned O
earlier O
, O
the O
high O
presence O
of O
ngram O
and O
suffix O
concepts O
in O
the O
lower O
layers O
is O
due O
to O
subword O
segmentation O
. O
At O
the O
higher O
layers O
, O
the O
models O
start O
encoding O
abstract O
concepts O
, O
therefore O
get O
better O
alignment O
with O
the O
core O
linguistic O
concepts O
. O
Casing O
shows O
an O
exception O
to O
other O
lexical O
concepts O
and O
has O
similar O
trend O
to O
POS O
and O
SEM O
. O
Upon O
investigating O
we O
observed O
that O
the O
words O
appearing O
in O
these O
clusters O
have O
a O
hybrid O
connotation O
. O
For O
example O
, O
more O
than O
98 O
% O
of O
the O
encoded O
concepts O
that O
match O
with O
Casing O
are O
named O
entities O
, O
which O
explains O
the O
trend O
. O
The O
syntactic O
concepts O
observe O
peak O
in O
the O
higher O
- O
middle O
layers O
and O
a O
downward O
trend O
towards O
the O
end O
. O
These O
findings O
resonate O
with O
the O
earlier O
work O
on O
interpreting O
neural O
network O
representations O
for O
BERT B-MethodName
. O
For O
example O
Liu O
et O
al O
. O
( O
2019a O
) O
also O
showed O
that O
probes O
trained O
with O
layers O
7 O
- O
8 O
give O
the O
highest O
accuracy O
when O
trained O
towards O
predicting O
the O
tasks O
of O
Chunking O
and O
CCG O
tagging O
. O
Although O
here O
, O
we O
are O
targeting O
a O
slightly O
different O
question O
i.e. O
how O
the O
latent O
concepts O
are O
encoded O
within O
the O
representations O
and O
how O
they O
evolve O
from O
input O
to O
output O
layers O
of O
the O
network O
. O

We O
observed O
a O
downward O
trend O
in O
linguistic O
ontologies O
( O
WordNet O
, O
LIWC O
) O
as O
we O
go O
from O
lower O
layers O
to O
higher O
layers O
as O
opposed O
to O
the O
core O
linguistic O
concepts O
( O
POS O
, O
CCG O
, O
etc O
. O
) O
. O
This O
is O
because O
of O
the O
context O
independent O
nature O
of O
these O
concepts O
as O
opposed O
to O
the O
core O
- O
linguistic O
concepts O
which O
are O
annotated O
based O
on O
the O
context O
. O
The O
embedding O
layer O
is O
non O
- O
contextualized O
, O
thus O
shows O
a O
high O
match O
with O
linguistic O
ontologies O
. O
With O
the O
availability O
of O
context O
in O
contextualized O
layers O
, O
the O
encoded O
concepts O
evolve O
into O
context O
- O
aware O
groups O
, O
resulting O
in O
higher O
matches O
with O
core O
- O
linguistic O
concepts O
. O

Comparing O
Models O
While O
the O
overall O
trend O
is O
consistent O
among O
BERT B-MethodName
- I-MethodName
uc I-MethodName
, O
mBERT B-MethodName
and O
XLNet B-MethodName
( O
and O
other O
studied O
models O
-Figure O
10 O
in O
Appendix O
) O
, O
the O
models O
somewhat O
differ O
in O
the O
last O
layers O
: O
see O
the O
large O
drop O
in O
core O
- O
linguistic O
concepts O
such O
as O
POS O
and O
Chunking O
for O
XLNet B-MethodName
and O
mBERT B-MethodName
in O
comparison O
to O
BERT B-MethodName
. O
This O
suggests O
that O
BERT B-MethodName
retains O
much O
of O
the O
core O
- O
linguistic O
information O
at O
the O
last O
layers O
. O
observed O
a O
similar O
pattern O
in O
their O
study O
, O
where O
they O
showed O
BERT B-MethodName
to O
retain O
linguistic O
information O
deeper O
in O
the O
model O
as O
opposed O
to O
XLNet B-MethodName
where O
it O
was O
more O
localized O
and O
predominantly O
preserved O
earlier O
in O
the O
network O
. O

While O
the O
overall O
layer O
- O
wise O
trends O
of O
multilingual O
models O
look O
similar O
to O
some O
monolingual O
models O
( O
mBERT B-MethodName
vs. O
XLNet B-MethodName
in O
Fig O
4b O
, O
c O
) O
, O
the O
former O
's O
absolute O
layer O
- O
wise O
matches O
( O
numbers O
inside O
the O
brackets O
in O
Figure O
4 O
e.g. O
Casing O
( O
166 O
) O
) O
are O
generally O
substantially O
higher O
than O
the O
monolingual O
counterparts O
. O
For O
example O
, O
the O
POS O
and O
SEM O
matches O
of O
mBERT B-MethodName
are O
38.9 O
% O
and O
30 O
% O
respectively O
which O
are O
18 O
% O
and O
15 O
% O
higher O
than O
BERT B-MethodName
- I-MethodName
uc I-MethodName
. O
On O
the O
contrary O
, O
the O
number O
of O
matches O
with O
linguistic O
ontologies O
is O
often O
lower O
for O
multilingual O
models O
( O
mBERT B-MethodName
LIWC O
alignment O
of O
65 O
vs. O
BERT B-MethodName
- I-MethodName
uc I-MethodName
alignment O
of O
186 O
) O
. O
We O
hypothesize O
that O
the O
variety O
of O
training O
languages O
in O
terms O
of O
their O
morphological O
and O
syntactic O
structure O
has O
caused O
the O
multilingual O
models O
to O
learn O
more O
core O
- O
linguistic O
concepts O
in O
order O
to O
optimize O
the O
training O
task O
. O
Although O
, O
the O
knowledge O
captured O
within O
linguistic O
ontologies O
is O
essential O
, O
it O
may O
not O
be O
as O
critical O
to O
the O
training O
of O
the O
model O
as O
the O
linguistic O
concepts O
. O

ALBERT B-MethodName
showed O
a O
very O
different O
trend O
from O
the O
other O
models O
. O
Note O
that O
ALBERT B-MethodName
shares O
parameters O
across O
layers O
while O
the O
other O
models O
have O
separate O
parameters O
for O
every O
layer O
. O
This O
explains O
the O
ALBERT B-MethodName
results O
where O
we O
see O
relatively O
less O
variation O
across O
layers O
. O
More O
interestingly O
, O
the O
encoded O
concepts O
in O
the O
last O
layers O
of O
ALBERT B-MethodName
showed O
presence O
of O
all O
human O
- O
defined O
concepts O
considered O
here O
( O
see O
the O
relatively O
smaller O
drop O
of O
ALBERT B-MethodName
alignment O
curves O
in O
Figure O
4 O
) O
. O

Unaligned O
Concepts O

In O
Table O
1 O
we O
observed O
that O
at O
least O
27.6 O
% O
( O
in O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
and O
up O
to O
56.4 O
% O
( O
in O
XLNet B-MethodName
) O
encoded O
concepts O
did O
not O
align O
with O
the O
human O
- O
defined O
concepts O
. O
What O
concepts O
do O
these O
unaligned O
clusters O
contain O
? O
In O
an O
effort O
to O
answer O
this O
question O
, O
we O
analyzed O
these O
clusters O
and O
observed O
that O
many O
of O
them O
were O
compositional O
concepts O
that O
involves O
more O
than O
one O
fine O
- O
grained O
categories O
of O
the O
human O
defined O
concepts O
. O
Figure O
5a O
shows O
an O
example O
of O
the O
unaligned O
concept O
which O
partly O
aligns O
with O
a O
semantic O
category O
( O
SEM O
: O
geopolitical O
entity O
) O
and O
a O
morphological O
category O
( O
POS O
: O
adjective O
) O
. O
Similarly O
, O
Figure O
5b O
is O
a O
verbs O
related O
to O
cognitive O
processes O
and O
Figure O
5c O
shows O
an O
unaligned O
cluster O
that O
is O
composed O
of O
different O
verb O
forms O
( O
past O
, O
present O
and O
gerunds O
) O
. O
The O
alignment O
with O
multiple O
human- O
defined O
concepts O
can O
be O
used O
to O
generate O
explanations O
for O
these O
unaligned O
concepts O
. O
For O
example O
, O
Figure O
5a O
can O
be O
aligned O
as O
a O
mix O
of O
geopolitical O
entities O
and O
adjectives O
. O
We O
also O
quantitatively O
verified O
the O
number O
of O
unaligned O
encoded O
concepts O
that O
can O
be O
explained O
using O
composition O
of O
different O
concepts O
( O
See O
Appendix O
E O
: O
Table O
9 O
) O
and O
found O
that O
a O
majority O
of O
the O
clusters O
can O
be O
explained O
using O
a O
combination O
of O
three O
pre O
- O
defined O
concepts O
.. O
Moreover O
, O
note O
that O
encoded O
concepts O
are O
often O
multifacet O
i.e. O
, O
they O
represent O
more O
than O
one O
relationship O
. O
For O
example O
, O
the O
encoded O
concept O
in O
Figure O
5c O
consists O
of O
different O
forms O
of O
verbs O
but O
at O
the O
same O
time O
, O
these O
verbs O
are O
semantically O
similar O
. O
The O
semantic O
relationship O
present O
here O
is O
not O
adequately O
captured O
using O
the O
human O
- O
defined O
concepts O
used O
in O
this O
work O
. O
These O
are O
the O
novel O
concepts O
that O
require O
richer O
annotations O
or O
human O
- O
in O
- O
the O
- O
loop O
setup O
to O
generate O
adequate O
explanations O
. O

Generalization O
of O
Results O

Do O
the O
results O
generalize O
over O
different O
dataset O
selection O
and O
using O
different O
number O
of O
clusters O
? O
We O
ran O
experiments O
using O
different O
split O
of O
the O
news O
dataset O
for O
several O
models O
, O
and O
also O
performed O
alignment O
using O
different O
values O
of O
K O
, O
the O
number O
of O
clusters O
. O
The O
results O
are O
consistent O
across O
the O
board O
. O
Please O
see O
Appendix O
F O
for O
details O
. O

Conclusion O

We O
presented O
ConceptX B-MethodName
, O
a O
novel O
framework O
for O
analyzing O
the O
encoded O
concepts O
within O
deep O
NLP O
models O
. O
Our O
method O
uses O
unsupervised O
clustering O
to O
discover O
latent O
concepts O
within O
the O
contextualized O
representations O
and O
then O
aligned O
these O
concepts O
with O
a O
suite O
of O
human O
- O
defined O
concepts O
to O
generate O
explanations O
for O
them O
. O
Our O
results O
illuminate O
how O
DNNs O
structure O
language O
information O
. O
A O
few O
notable O
findings O
are O
: O
i O
) O
lower O
layers O
capture O
shallow O
linguistic O
concepts O
, O
ii O
) O
whereas O
the O
abstract O
linguistic O
concepts O
such O
as O
morphology O
and O
semantics O
are O
preserved O
higher O
in O
the O
network O
, O
iii O
) O
the O
extent O
of O
alignment O
varies O
across O
different O
models O
and O
different O
human O
- O
defined O
concepts O
, O
iv O
) O
we O
found O
that O
novel O
explanations O
and O
an O
improved O
coverage O
of O
concepts O
can O
be O
achieved O
via O
compositionality O
. O

Appendix O

A O
Human O
- O
defined O
concept O
labels O
A.1 O
Lexical O
Concepts O
: O
Ngrams O
, O
Affixes O
, O
Casing O
, O
First O
and O
the O
Last O
Word O
. O

A.2 O
Morphology O
and O
Semantics O
: O

POS O
tags O
: O
We O
used O
the O
Penn O
Treebank O
POS O
tags O
discussed O
in O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
, O
which O
consists O
of O
36 O
POS O
tags O
and O
12 O
other O
tags O
( O
i.e. O
, O
punctuation O
and O
currency O
symbols O
) O
. O
In O
Table O
2 O
, O
we O
provide O
POS O
tags O
and O
their O
description O
. O

SEM O
tags O
: O
( O
Abzianidze O
et O
al O
. O
, O
2017 O
) O
consists O
of O
73 O
sem O
- O
tags O
grouped O
into O
13 O
meta O
- O
tags O
. O
In O
Table O
3 O
, O
we O
provide O
a O
detailed O
information O
of O
the O
tagset O
, O
and O
in O
Table O
5 O
, O
we O
provide O
fine O
and O
coarse O
tags O
mapping O
. O

A.3 O
Syntactic O
: O

Chunking O
tags O
: O
For O
Chunking O
we O
used O
the O
tagset O
discussed O
in O
( O
Tjong O
Kim O
Sang O
and O
Buchholz O
, O
2000 O
) O
, O
which O
consists O
of O
11 O
tags O
as O
follows O
: O
NP O
( O
Noun O
phrase O
) O
, O
VP O
( O
Verb O
phrase O
) O
, O
PP O
( O
Prepositional O
phrase O
) O
, O
ADVP O
( O
Adverb O
phrase O
) O
, O
SBAR O
( O
Subordinate O
phrase O
) O
, O
ADJP O
( O
Adjective O
phrase O
) O
, O
PRT O
( O
Particles O
) O
, O
CONJP O
( O
Conjunction O
) O
, O
INTJ O
( O
Interjection O
) O
, O
LST O
( O
List O
marker O
) O
, O
UCP O
( O
Unlike O
coordinate O
phrase O
) O
. O
For O
the O
annotation O
, O
chunks O
are O
represented O
using O
IOB O
format O
, O
which O
results O
in O
22 O
tags O
in O
the O
dataset O
as O
reported O
in O
Table O
4 O
. O

CCG O
super O
- O
tags O
Hockenmaier O
( O
2006 O
) O
developed O
, O
CCGbank O
, O
a O
dataset O
with O
Combinatory O
Categorial O
Grammar O
( O
CCG O
) O
derivations O
and O
dependency O
structures O
from O
the O
Penn O
Treebank O
. O
CCG O
is O
a O
lexicalized O
grammar O
formalism O
, O
which O
is O
expressive O
and O
efficiently O
parseable O
. O
It O
consists O
of O
1272 O
tags O
. O

A.4 O
Linguistic O
Ontologies O
: O

WordNet O
: O
( O
Miller O
, O
1995 O
) O
consists O
of O
26 O
lexicographic O
senses O
for O
nouns O
, O
2 O
for O
adjectives O
, O
and O
1 O
for O
adverbs O
. O
Each O
of O
them O
represent O
a O
supersense O
and O
a O
hierarchy O
can O
be O
formed O
from O
hypernym O
to O
hyponym O
. O

LIWC O
: O
Over O
the O
past O
few O
decades O
, O
Pennebaker O
et O
al O
. O
( O
Pennebaker O
et O
al O
. O
, O
2001 O
) O
have O
designed O
psycholinguistic O
concepts O
using O
high O
frequency O
words O
. O
These O
word O
categories O
are O
mostly O
used O
to O
study O
gender O
, O
age O
, O
personality O
, O
and O
health O
to O
estimate O
the O

C O
Clustering O
details O

Algorithm O
1 O
assigns O
each O
word O
to O
a O
separate O
cluster O
and O
then O
iteratively O
combines O
them O
based O
on O
Ward O
's O
minimum O
variance O
criterion O
that O
minimizes O
intra O
- O
cluster O
variance O
. O
Distance O
between O
two O
vector O
representations O
is O
calculated O
with O
the O
squared O
Euclidean O
distance O
. O

Algorithm O
1 O
Clustering O
Procedure O
Input O
: O
− O
→ O
y O
l O
: O
word O
representation O
of O
words O
Parameter O
: O
K O
: O
the O
total O
number O
of O
clusters O
1 O
: O
for O
each O
word O
w O
i O
do O
The O
Elbow O
curve O
did O
not O
show O
any O
optimum O
clustering O
point O
, O
with O
the O
increase O
in O
number O
of O
clusters O
the O
distortion O
score O
kept O
decreasing O
, O
resulting O
in O
over O
- O
clustering O
( O
a O
large O
number O
of O
clusters O
consisted O
of O
less O
than O
5 O
words O
) O
. O
The O
over O
- O
clustering O
resulted O
in O
high O
but O
wrong O
alignment O
scores O
e.g. O
consider O
a O
two O
word O
cluster O
having O
words O
" O
good O
" O
and O
" O
great O
" O
. O
The O
cluster O
will O
have O
a O
successful O
match O
with O
" O
adjective O
" O
since O
more O
than O
90 O
% O
of O
the O
words O
in O
the O
cluster O
are O
adjectives O
. O
In O
this O
way O
, O
a O
lot O
of O
small O
clusters O
will O
have O
a O
successful O
match O
with O
many O
human O
- O
defined O
concepts O
and O
the O
resulting O
alignment O
scores O
will O
be O
high O
. O
On O
the O
other O
hand O
, O
Silhouette O
resulted O
in O
under O
- O
clustering O
, O
giving O
the O
best O
score O
at O
number O
of O
clusters O
= O
10 O
. O
We O
handled O
this O
empirically O
by O
trying O
several O
values O
for O
the O
number O
of O
clusters O
i.e. O
, O
200 O
to O
1600 O
with O
step O
size O
200 O
. O
We O
selected O
1000 O
to O
find O
a O
good O
balance O
with O
over O
and O
under O
clustering O
. O
We O
understand O
that O
this O
may O
not O
be O
the O
best O
optimal O
point O
. O
We O
presented O
the O
results O
of O
600 O
and O
1000 O
clusters O
to O
show O
that O
our O
findings O
are O
not O
sensitive O
to O
the O
number O
of O
clusters O
parameter O
. O

D O
Coarse O
vs. O
Fine O
- O
grained O
Categories O

D.1 O
Coarse O
vs. O
Fine O
- O
grained O
Categories O

Our O
analysis O
of O
compositional O
concepts O
showed O
that O
several O
fine O
- O
grained O
concepts O
could O
be O
combined O
to O
explain O
an O
unaligned O
concept O
. O
For O
example O
, O
by O
combining O
verb O
categories O
of O
POS O
to O
one O
coarse O
verb O
category O
, O
we O
can O
align O
the O
encoded O
concept O
present O
in O
Figure O
5c O
. O
To O
probe O
this O
more O
formally O
, O
we O
collapsed O
POS O
and O
SEM O
fine O
- O
grained O
concepts O
into O
coarser O
categories O
( O
27 O
POS O
tags O
and O
15 O
SEM O
tags O
) O
. O
We O
then O
recomputed O
the O
alignment O
with O
the O
encoded O
concepts O
. O
For O
most O
of O
the O
models O
, O
the O
alignment O
doubled O
compared O
to O
the O
fine O
- O
grained O
categorizes O
with O
at O
least O
39 O
% O
and O
at O
most O
53 O
% O
percent O
match O
for O
POS O
. O
This O
reflects O
that O
in O
several O
cases O
, O
models O
learn O
the O
coarse O
language O
hierarchy O
. O
We O
further O
questioned O
how O
many O
encoded O
concepts O
can O
be O
explained O
using O
coarse O
human O
- O
defined O
concepts O
. O
Compared O
to O
Table O
1 O
, O
the O
matches O
increased O
by O
at O
most O
17 O
points O
in O
the O
case O
of O
BERT B-MethodName
- I-MethodName
uc I-MethodName
. O
The O
XLM B-MethodName
- I-MethodName
R I-MethodName
showed O
the O
highest O
matching O
percentage O
of O
81 O
% O
. O
The O
higher O
alignment O
suggests O
that O
most O
of O
the O
encoded O
concepts O
learned O
by O
pre O
- O
trained O
models O
can O
be O
explained O
using O
human O
- O
defined O
concepts O
. O
( O
See O
Appendix O
D O
for O
detailed O
results O
) O
. O

D.2 O
Corase O
POS O
and O
SEM O
labels O

Tables O
5 O
and O
6 O
the O
alignment O
doubles O
in O
most O
of O
the O
cases O
which O
reflects O
that O
in O
several O
cases O
, O
models O
learn O
the O
coarse O
language O
hierarchy O
. O
However O
, O
they O
do O
not O
strictly O
adhere O
to O
fine O
- O
grained O
categories O
existed O
in O
humandefined O
concepts O
. O
We O
further O
extend O
the O
alignment O
of O
coarse O
POS O
and O
SEM O
categories O
to O
the O
overall O
alignment O
with O
the O
human O
- O
defined O
concepts O
. O
Table O
8 O
presents O
the O
results O
. O
We O
see O
a O
match O
of O
up O
to O
81 B-MetricValue
% I-MetricValue
in O
the O
case O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName
The O
high O
alignment O
suggests O
that O
many O
of O
the O
encoded O
concepts O
can O
be O
explained O
using O
coarse O
human O
- O
defined O
concepts O
. O

E O
Compositional O
Coverage O

Table O
9 O
shows O
the O
amount O
of O
coverage O
we O
obtain O
when O
aligning O
with O
the O
morphological O
concepts O
when O
allowing O
90 O
% O
of O
the O
words O
in O
the O
cluster O
to O
be O
from O
N O
concepts O
. O

F O
Robustness O
of O
Methodology O
across O
Datasets O
and O
Settings O

Figure O
8 O
shows O
the O
layer O
- O
wise O
patterns O
using O
600 O
clusters O
instead O
of O
1000 O
as O
used O
in O
the O
main O
paper O
. O
We O
observe O
that O
the O
overall O
trends O
largely O
remain O
the O
same O
. O

To O
further O
demonstrate O
the O
robustness O
of O
our O
method O
with O
respect O
to O
dataset O
, O
we O
sub O
- O
sampled O
another O
dataset O
from O
the O
News O
corpus O
with O
a O
different O
vocabulary O
by O
selecting O
words O
that O
appear O
between O
2 O
to O
10 O
times O
in O
the O
corpus O
. O
Note O
that O
the O
selection O
of O
vocabulary O
is O
due O
to O
the O
memory O
and O
computation O
limitations O
. O
Figure O
9 O
shows O
the O
results O
using O
this O
selection O
of O
data O
. O
Compared O
to O
Figure O
4 O
, O
we O
can O
see O
that O
the O
overall O
patterns O
are O
largely O
similar O
and O
confirms O
the O
robustness O
of O
our O
findings O
. O
The O
slight O
difference O
in O
the O
patterns O
of O
WordNet O
and O
LIWC O
are O
due O
to O
the O
large O
selection O
of O
proper O
nouns O
in O
the O
second O
set O
of O
the O
data O
. O

G O
Layer O
- O
wise O
results O

Figure O
10 O
present O
layer O
- O
wise O
results O
for O
all O
the O
understudied O
models O
. O

Introduction O

Approach O

D O
= O
{ O
U O
1 O
, O
U O
2 O
, O
... O
, O
U O
T O
} O
= O
{ O
x O
1 O
, O
. O
. O
. O
, O
x O
n O
} O
( O
1 O
) O

Datasets O

Experimental O
Setup O

Conclusion O

A O
Related O
Work O

B O
Implementation O
Details O

Acknowledgement O

This O
research O
is O
partially O
supported O
by O
NSFC O
Grant O
No O
. O
91646205 O
, O
and O
SJTU O
- O
CMBCC O
Joint O
Research O
Scheme O
. O

