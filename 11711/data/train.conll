Efficiently B-MethodName
Tuned I-MethodName
Parameters I-MethodName
are O
Task O
Embeddings O

Intermediate O
- O
task O
transfer O
can O
benefit O
a O
wide O
range O
of O
NLP O
tasks O
with O
properly O
selected O
source O
datasets O
. O
However O
, O
it O
is O
computationally O
infeasible O
to O
experiment O
with O
all O
intermediate O
transfer O
combinations O
, O
making O
choosing O
a O
useful O
source O
task O
a O
challenging O
problem O
. O
In O
this O
paper O
, O
we O
anticipate O
that O
task O
- O
specific O
parameters O
updated O
in O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
methods O
are O
likely O
to O
encode O
task O
- O
specific O
information O
. O
Therefore O
, O
such O
parameters O
can O
be O
predictive O
for O
inter O
- O
task O
transferability O
. O
Thus O
, O
we O
propose O
to O
exploit O
these O
efficiently O
tuned O
parameters O
as O
off O
- O
the O
- O
shelf O
task O
embeddings O
for O
the O
efficient O
selection O
of O
source O
datasets O
for O
intermediate O
- O
task O
transfer O
. O
We O
experiment O
with O
11 O
text B-TaskName
classification I-TaskName
tasks O
and O
11 O
question B-TaskName
answering I-TaskName
tasks O
. O
Experimental O
results O
show O
that O
our O
approach O
can O
consistently O
outperform O
existing O
inter O
- O
task O
transferability O
prediction O
methods O
while O
being O
conceptually O
simple O
and O
computationally O
efficient O
. O
Our O
analysis O
also O
reveals O
that O
the O
ability O
of O
efficiently O
tuned O
parameters O
on O
transferability O
prediction O
is O
disentangled O
with O
their O
in O
- O
task O
performance O
. O
This O
allows O
us O
to O
use O
parameters O
from O
early O
checkpoints O
as O
task O
embeddings O
to O
further O
improve O
efficiency O
. O
1 O

Introduction O

The O
pretraining B-MethodName
then I-MethodName
fine I-MethodName
- I-MethodName
tuning I-MethodName
paradigm O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2018Radford O
et O
al O
. O
, O
, O
2019Brown O
et O
al O
. O
, O
2020 O
; O
Lewis O
et O
al O
. O
, O
2020 O
; O
Raffel O
et O
al O
. O
, O
2019 O
) O
has O
substantially O
improved O
the O
state O
- O
of O
- O
the O
- O
art O
on O
a O
wide O
range O
of O
natural O
language O
processing O
( O
NLP O
) O
tasks O
. O
In O
this O
paradigm O
, O
we O
first O
pretrain O
a O
large O
language O
model O
on O
large O
- O
scale O
corpora O
in O
a O
general O
domain O
, O
and O
then O
fine O
- O
tune O
the O
pretrained O
model O
to O
be O
a O
task O
- O
specific O
model O
on O
the O
target O
dataset O
. O
In O
addition O
to O
directly O
transferring O
from O
a O
general O
pretrained O
language O
model O
, O
prior O
work O
( O
Phang O
et O
al O
. O
, O
2018 O
) O
also O
shows O
that O
intermediate O
- O
task O
transfer O
, O
i.e. O
, O
fine O
- O
tuning O
on O
intermediate O
source O
tasks O
before O
the O
target O
task O
, O
can O
further O
improve O
target O
task O
performance O
. O
However O
, O
the O
success O
of O
intermediate O
- O
task O
transfer O
heavily O
relies O
on O
the O
selection O
of O
a O
proper O
source O
dataset O
while O
an O
inappropriate O
source O
dataset O
often O
leads O
to O
performance O
degradation O
compared O
to O
plain O
finetuning O
. O
Therefore O
, O
some O
recent O
works O
( O
Vu O
et O
al O
. O
, O
2020 O
; O
Poth O
et O
al O
. O
, O
2021 O
) O
investigate O
methods O
to O
efficiently O
predict O
inter O
- O
task O
transferability O
without O
actually O
trying O
out O
all O
intermediate O
- O
task O
combinations O
. O

The O
current O
state O
of O
the O
art O
( O
Vu O
et O
al O
. O
, O
2020 O
) O
on O
predicting O
inter O
- O
task O
transferability O
is O
built O
on O
Task2Vec B-MethodName
( O
Achille O
et O
al O
. O
, O
2019 O
) O
, O
which O
considers O
the O
Fisher O
information O
matrix O
of O
a O
model O
finetuned O
on O
a O
task O
as O
the O
" O
task O
embedding O
" O
, O
and O
predicts O
inter O
- O
task O
transferability O
by O
computing O
the O
cosine O
similarity O
between O
the O
task O
embedding O
of O
the O
source O
and O
target O
tasks O
. O
Despite O
empirically O
performing O
well O
, O
this O
approach O
requires O
fine O
- O
tuning O
the O
full O
model O
and O
( O
inefficiently O
) O
computing O
the O
Fisher O
matrix O
of O
the O
model O
. O
Moreover O
, O
the O
resulting O
task O
embeddings O
generally O
have O
a O
high O
dimensionality O
similar O
to O
the O
size O
of O
the O
underlying O
model O
. O
Therefore O
, O
intermediate O
task O
selection O
, O
which O
requires O
storing O
task O
embeddings O
for O
each O
source O
/ O
target O
task O
, O
can O
be O
space O
- O
consuming O
, O
especially O
when O
experi O
- O
menting O
with O
large O
language O
models O
. O

In O
this O
work O
, O
we O
opt O
for O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
approaches O
( O
Houlsby O
et O
al O
. O
, O
2019 O
; O
Li O
and O
Liang O
, O
2021 O
; O
Guo O
et O
al O
. O
, O
2021 O
; O
Hu O
et O
al O
. O
, O
2022 O
; O
Zaken O
et O
al O
. O
, O
2022 O
) O
for O
the O
efficient O
and O
accurate O
prediction B-TaskName
of I-TaskName
inter I-TaskName
- I-TaskName
task I-TaskName
transferability I-TaskName
. O
Our O
key O
insight O
is O
that O
task O
- O
specific O
parameters O
updated O
in O
parameter O
- O
efficient O
tuning O
methods O
are O
likely O
to O
encode O
high O
density O
task O
- O
specific O
information O
since O
they O
are O
used O
as O
a O
query O
for O
retrieving O
task O
- O
related O
knowledge O
in O
a O
frozen O
pretrained O
language O
model O
. O
Therefore O
, O
we O
propose O
to O
directly O
use O
task O
- O
specific O
parameters O
learned O
via O
parameter O
- O
efficient O
tuning O
on O
source O
/ O
target O
datasets O
as O
task O
embeddings O
, O
as O
shown O
in O
Figure O
1 O
. O
Compared O
to O
task O
embeddings O
obtained O
by O
calculating O
the O
Fisher O
matrix O
of O
the O
fine O
- O
tuned O
model O
( O
Achille O
et O
al O
. O
, O
2019 O
; O
Vu O
et O
al O
. O
, O
2020 O
) O
, O
efficiently O
tuned O
parameters O
are O
of O
much O
lower O
dimensionality O
and O
do O
not O
suffer O
from O
noise O
from O
uninformative O
weights O
in O
the O
model O
parameters O
, O
thus O
leading O
to O
more O
accurate O
transferability O
prediction O
. O
Also O
, O
our O
method O
only O
requires O
parameter O
- O
efficient O
tuning O
on O
the O
tasks O
and O
stores O
task O
- O
specific O
parameters O
, O
making O
both O
computing O
and O
storing O
task O
embeddings O
more O
efficient O
. O
Moreover O
, O
with O
the O
development O
of O
open O
- O
source O
parameter O
- O
efficient O
tuning O
platforms O
like O
Adapter O
- O
Hub O
( O
Pfeiffer O
et O
al O
. O
, O
2020 O
) O
, O
we O
can O
easily O
access O
off O
- O
the O
- O
shelf O
parameters O
of O
the O
source O
and O
target O
datasets O
downloaded O
from O
the O
model O
zoo O
and O
then O
compute O
the O
similarity O
between O
the O
downloaded O
parameters O
. O

We O
empirically O
verify O
the O
effectiveness O
of O
our O
approach O
by O
experimenting O
with O
11 O
text B-TaskName
classification I-TaskName
tasks O
and O
11 O
question B-TaskName
answering I-TaskName
tasks O
, O
following O
Vu O
et O
al O
. O
( O
2020 O
) O
. O
Our O
results O
show O
that O
our O
approach O
consistently O
outperforms O
existing O
intertask O
transferability O
prediction O
methods O
while O
being O
simpler O
and O
more O
efficient O
. O
In O
addition O
, O
we O
find O
that O
the O
ability O
of O
efficiently O
tuned O
parameters O
on O
transferability O
prediction O
is O
not O
strongly O
correlated O
with O
their O
in O
- O
task O
performance O
. O
Therefore O
, O
task O
- O
specific O
parameters O
tuned O
with O
a O
relatively O
small O
number O
of O
steps O
are O
already O
highly O
predictive O
for O
inter B-TaskName
- I-TaskName
task I-TaskName
transferability I-TaskName
, O
allowing O
us O
to O
further O
improve O
the O
efficiency O
of O
intermediate B-TaskName
task I-TaskName
selection I-TaskName
. O

Related O
Work O

Prior O
work O
( O
Phang O
et O
al O
. O
, O
2018 O
) O
shows O
that O
positive O
transfer O
can O
be O
elicited O
by O
training O
a O
model O
on O
intermediate O
source O
tasks O
before O
fine O
- O
tuning O
on O
the O
target O
task O
. O
However O
, O
the O
choice O
of O
an O
appropriate O
source O
task O
is O
crucial O
for O
effective O
transfer O
. O
Phang O
et O
al O
. O
( O
2018 O
) O
show O
that O
the O
size O
of O
the O
source O
dataset O
is O
an O
good O
prior O
for O
source O
task O
selection O
. O
Pruksachatkun O
et O
al O
. O
( O
2020 O
) O
propose O
to O
use O
task O
requiring O
complex O
reasoning O
and O
inference O
as O
source O
tasks O
. O
Besides O
these O
heuristics O
, O
a O
number O
of O
work O
also O
focuses O
on O
systematic O
prediction O
of O
intermediate O
task O
transferability O
. O
Vu O
et O
al O
. O
( O
2020 O
) O
propose O
to O
used O
TASK2VEC B-MethodName
to O
construct O
task O
embeddings O
based O
on O
the O
input O
text O
or O
Fisher O
information O
matrix O
of O
a O
fine O
- O
tuned O
model O
. O
Poth O
et O
al O
. O
( O
2021 O
) O
further O
extend O
similar O
ideas O
for O
adapter O
- O
based O
transfer O
learning O
. O
More O
recently O
, O
Vu O
et O
al O
. O
( O
2021 O
) O
explore O
prompt O
- O
based O
transfer O
and O
propose O
to O
use O
prompt O
similarity O
as O
a O
predictor O
for O
prompt O
transferability O
to O
select O
proper O
soft O
prompts O
for O
initialization O
. O
This O
can O
be O
viewed O
as O
a O
special O
case O
of O
our O
proposed O
method O
where O
the O
parameter O
- O
efficient O
tuning O
method O
is O
restricted O
to O
vanilla O
prompt O
tuning O
and O
the O
transfer O
method O
is O
restricted O
to O
prompt O
transfer O
instead O
of O
general O
intermediate O
- O
task O
transfer O
. O

Methodology O

Parameter B-MethodName
- I-MethodName
Efficient I-MethodName
Tuning I-MethodName

Parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
only O
updates O
a O
small O
portion O
of O
parameters O
in O
a O
large O
pretrained O
model O
. O
In O
this O
paper O
, O
we O
experiment O
with O
three O
types O
of O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
: O
Prompt B-MethodName
Tuning I-MethodName
( O
Liu O
et O
al O
. O
, O
2021 O
) O
, O
Bias B-MethodName
Tuning I-MethodName
( O
Zaken O
et O
al O
. O
, O
2022 O
) O
, O
and O
Low B-MethodName
- I-MethodName
Rank I-MethodName
Tuning I-MethodName
( O
Hu O
et O
al O
. O
, O
2022 O
. O
Prompt B-MethodName
Tuning I-MethodName
We O
experiment O
with O
P B-MethodName
- I-MethodName
Tuning I-MethodName
v2 I-MethodName
( O
Liu O
et O
al O
. O
, O
2021 O
) O
. O
Specifically O
, O
P B-MethodName
- I-MethodName
Tuning I-MethodName
v2 I-MethodName
implements O
a O
prompt O
tuning O
method O
by O
introducing O
additional O
attention O
prefix O
matrices O
K O
t O
= O
{ O
k O
1 O
. O
. O
. O
k O
n O
} O
and O
V O
t O
= O
{ O
v O
1 O
. O
. O
. O
v O
n O
} O
for O
each O
Transformer O
layer O
, O
where O
n B-HyperparameterName
is O
a O
hyperparameter O
controlling O
the O
added O
prefix O
length O
; O
k O
* O
and O
v O
* O
are O
vectors O
with O
dimension O
d B-HyperparameterName
h I-HyperparameterName
; O
d B-HyperparameterName
h I-HyperparameterName
is O
the O
hidden O
size O
of O
the O
Transformer O
model O
. O

For O
each O
Transformer O
layer O
, O
the O
added O
vectors O
are O
concatenated O
with O
the O
original O
key O
and O
value O
matrices O
to O
be O
K O
′ O
= O
K O
t O
⊕ O
K O
and O
V O
′ O
= O
V O
t O
⊕ O
V O
, O
where O
K O
and O
V O
are O
the O
original O
key O
and O
value O
in O
each O
layer O
's O
attention O
block O
. O
Then O
, O
the O
new O
scaled O
dot O
- O
product O
attention O
is O
calculated O
by O
replacing O
the O
original O
K O
and O
V O
with O
the O
new O
K O
′ O
and O
V O
′ O
, O
respectively O
. O

Bias B-MethodName
Tuning I-MethodName
BitFit I-MethodName
( O
Zaken O
et O
al O
. O
, O
2022 O
) O
simply O
updates O
all O
bias O
terms O
b O
in O
all O
linear O
layers O
h O
= O
W O
x O
+ O
b O
in O
each O
Transformer O
layer O
. O

Low B-MethodName
- I-MethodName
Rank I-MethodName
Tuning I-MethodName
LoRA I-MethodName
( O
Hu O
et O
al O
. O
, O
2022 O
) O
injects O
trainable O
rank O
decomposition O
matrices O
into O
each O
layer O
of O
the O
Transformer O
model O
. O
For O
each O
linear O
layer O
h O
= O
W O
x O
where O
W O
∈ O
R O
d×k O
, O
the O
forward O
pass O
is O
modified O
to O
h O
= O
W O
x O
+ O
BAx O
, O
where O
B O
∈ O
R O
d×r O
, O
A O
∈ O
R O
r×k O
, O
and O
the O
rank O
r O
≪ O
min O
( O
d O
, O
k O
) O
. O

Tuned O
Parameters O
as O
Task O
Embeddings O

After O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
, O
we O
concatenate O
all O
tuned O
parameters O
in O
each O
Transformer O
layer O
and O
average O
them O
across O
all O
layers O
to O
obtain O
a O
vector O
as O
a O
representation O
for O
a O
task O
, O
namely O
Tuned B-MethodName
Parameters I-MethodName
as I-MethodName
Task I-MethodName
Embedding I-MethodName
( O
TuPaTE B-MethodName
) O
. O
Following O
Vu O
et O
al O
. O
( O
2020 O
) O
, O
we O
calculate O
the O
cosine O
similarity O
between O
the O
embeddings O
of O
a O
given O
targeted O
task O
and O
the O
candidate O
source O
tasks O
. O
Then O
, O
we O
rank O
the O
candidate O
source O
tasks O
in O
descending O
order O
by O
the O
similarity O
scores O
. O

Experiments O

Datasets O

Following O
Vu O
et O
al O
. O
( O
2020 O
) O
, O
we O
conduct O
experiments O
with O
11 O
tasks O
of O
text B-TaskName
classification I-TaskName
or O
regression B-TaskName
( O
CR O
) O
and O
11 O
tasks O
of O
question B-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
. O
Note O
that O
Vu O
et O
al O
. O
( O
2020 O
) O
also O
includes O
11 O
tasks O
of O
sequence O
labeling O
. O
We O
do O
not O
include O
those O
datasets O
since O
most O
of O
them O
are O
not O
publicly O
available O
. O
The O
list O
of O
datasets O
can O
be O
found O
in O
Appendix O
A. O
To O
be O
consistent O
with O
Vu O
et O
al O
. O
( O
2020 O
) O
, O
we O
use O
two O
metrics O
to O
evaluate O
the O
performance O
of O
the O
task O
embeddings O
: O
( O
1 O
) O
the O
average B-MetricName
rank I-MetricName
ρ I-MetricName
of I-MetricName
the I-MetricName
source I-MetricName
task I-MetricName
with I-MetricName
the I-MetricName
highest I-MetricName
absolute I-MetricName
transfer I-MetricName
gain I-MetricName
; O
( O
2 O
) O
Normalized B-MetricName
Discounted I-MetricName
Cumulative I-MetricName
Gain I-MetricName
( O
NDCG B-MetricName
) O
, O
which O
is O
a O
widely O
used O
metric O
for O
evaluating O
the O
quality O
of O
the O
entire O
ranking O
, O
instead O
of O
focusing O
on O
the O
highest O
rank O
as O
ρ B-MetricName
does O
. O

Baselines O

We O
use O
the O
following O
methods O
as O
baselines O
: O
( O
1 O
) O
DATASIZE B-MethodName
( O
Vu O
et O
al O
. O
, O
2020 O
) O
is O
a O
simple O
baseline O
that O
ranks O
all O
source O
tasks O
by O
the O
number O
of O
training O
examples O
. O
( O
2 O
) O
CURVEGRAD B-MethodName
( O
Bingel O
and O
Søgaard O
, O
2017 O
; O
Vu O
et O
al O
. O
, O
2020 O
) O
is O
a O
baseline O
that O
uses O
the O
gradients O
of O
the O
loss O
curve O
of O
BERT O
for O
each O
task O
. O
It O
is O
originally O
proposed O
in O
Bingel O
and O
Søgaard O
( O
2017 O
) O
for O
predicting O
gains O
from O
multi O
- O
task O
learning O
and O
adapted O
by O
Vu O
et O
al O
. O
( O
2020 O
) O

Training O
Details O

We O
apply O
P B-MethodName
- I-MethodName
Tuning I-MethodName
v2 I-MethodName
, O
BitFit B-MethodName
, O
and O
LoRA B-MethodName
on I-MethodName
BERTbase I-MethodName
for O
fine O
- O
tuning O
on O
the O
aforementioned O
datasets O
. O

For O
each O
method O
, O
we O
adopt O
the O
default O
hyperparameters O
from O
their O
corresponding O
papers O
. O
Specifically O
, O
for O
P B-MethodName
- I-MethodName
Tuning I-MethodName
v2 I-MethodName
, O
we O
use O
a O
prefix B-HyperparameterName
length I-HyperparameterName
of O
20 B-HyperparameterValue
and O
search O
the O
learning B-HyperparameterName
rate I-HyperparameterName
from O
{ O
1e-2 B-HyperparameterValue
, O
1e-3 B-HyperparameterValue
} O
; O
For O
LoRA B-MethodName
, O
we O
set O
LoRA B-MethodName
's I-MethodName
r B-HyperparameterName
to O
8 B-HyperparameterValue
and O
α B-HyperparameterName
to O
8 B-HyperparameterValue
, O
and O
search O
a O
learning B-HyperparameterName
rate I-HyperparameterName
from O
{ O
5e-4 B-HyperparameterValue
, O
2e-4 B-HyperparameterValue
} O
; O
For O
BitFit B-MethodName
, O
we O
search O
a O
learning B-HyperparameterName
rate I-HyperparameterName
from O
{ O
1e-4 B-HyperparameterValue
, O
4e-4 B-HyperparameterValue
} O
. O
We O
train O
all O
models O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
on O
all O
datasets O
. O
We O
use O
the O
parameters O
tuned O
for O
2 B-HyperparameterValue
epochs B-HyperparameterName
as O
" O
early O
" O
task O
embeddings O
and O
those O
corresponding O
to O
the O
best O
validation O
set O
performance O
as O
" O
late O
" O
task O
embeddings O
. O
We O
compare O
the O
number O
of O
tunable O
parameters O
and O
the O
final O
task O
embedding O
dimensions O
in O
Table O
1 O
. O
We O
can O
see O
that O
TuPaTE B-MethodName
has O
a O
significantly O
lower O
dimensionality O
compare O
to O
the O
TASKEMB B-MethodName
baseline O
. O
We O
also O
include O
an O
ensemble O
of O
the O
three O
efficient O
tuning O
methods O
( O
denoted O
as O
" O
3 B-MethodName
ENSEMBLE I-MethodName
" O
) O
, O
by O
averaging O
the O
inter O
- O
task O
similarity O
scores O
of O
each O
model O
. O

Experimental O
Results O

We O
present O
the O
main O
results O
in O
Table O
2 O
. O
We O
find O
that O
TuPaTE B-MethodName
with O
different O
parameter O
- O
efficient O
tuning O
methods O
consistently O
outperforms O
prior O
works O
including O
TEXTEMB B-MethodName
and O
TASKEMB B-MethodName
. O
Interestingly O
, O
the O
performance O
improvement O
is O
larger O
in O
FULL O
→ O
LIMITED O
and O
LIMITED O
→ O
LIMITED O
settings O
. O
We O
conjecture O
that O
this O
is O
because O
in O
limited O
resource O
Table O
2 O
: O
To O
evaluate O
TuPaTE B-MethodName
, O
we O
measure O
the O
average B-MetricName
rank I-MetricName
( O
ρ B-MetricName
) O
assigned O
to O
the O
best O
source O
task O
( O
i.e. O
, O
the O
one O
that O
results O
in O
the O
largest O
transfer O
gain O
) O
across O
target O
tasks O
, O
as O
well O
as O
the O
average O
NDCG B-MetricName
measure O
of O
the O
overall O
ranking O
's O
quality O
. O
Parentheses O
denote O
the O
number O
of O
source O
tasks O
in O
each O
setting O
. O
Some O
results O
of O
CURVEGRAD B-MethodName
are O
missing O
( O
marked O
with O
" O
- O
" O
) O
since O
its O
code O
is O
not O
available O
. O
The O
other O
results O
of O
CURVEGRAD B-MethodName
are O
taken O
from O
Vu O
et O
al O
. O
( O
2020 O
) O
. O
Table O
3 O
: O
Analysis O
on O
the O
correlation B-MetricName
between O
taskspecific O
performance O
( O
e.g. O
, O
accuracy B-MetricName
) O
and O
transferability O
prediction O
results O
( O
i.e. O
, O
ρ B-MetricName
and O
NDCG B-MetricName
) O
for O
different O
parameter O
- O
efficient O
tuning O
methods O
. O
∆ρ B-MetricName
and O
∆NDCG B-MetricName
denote O
the O
difference O
of O
ρ B-MetricName
and O
NDCG B-MetricName
between O
the O
parameters O
with O
the O
highest O
and O
lowest O
task O
- O
specific O
performance O
. O

settings O
, O
parameter O
- O
efficient O
tuning O
methods O
generally O
perform O
much O
better O
than O
full O
fine O
- O
tuning O
, O
which O
is O
used O
in O
the O
TASKEMB B-MethodName
method O
. O
Moreover O
, O
we O
find O
that O
PTUNING B-MethodName
and O
BITFIT B-MethodName
outperform O
LORA B-MethodName
in O
all O
settings O
. O
We O
suspect O
this O
is O
because O
the O
amount O
of O
tunable O
parameters O
in O
LORA B-MethodName
is O
much O
larger O
than O
PTUNING B-MethodName
and O
BITFIT B-MethodName
. O
Also O
, O
the O
ensemble O
of O
three O
methods O
achieve O
even O
better O
performance O
than O
only O
using O
one O
approach O
. O

Analysis O

We O
conduct O
additional O
experiments O
in O
the O
in O
- O
class O
setting O
on O
classification B-TaskName
/ O
regression B-TaskName
tasks O
to O
better O
understand O
how O
TuPaTE B-MethodName
works O
. O
We O
first O
analyze O
the O
correlation B-MetricName
between O
the O
in O
- O
task O
perfor- O
mance O
( O
e.g. O
, O
accuracy B-MetricName
) O
and O
transferability O
prediction O
ability O
of O
efficiently O
tuned O
parameters O
. O
We O
train O
TuPaTE B-MethodName
with O
5 O
random O
combinations O
between O
searchable O
hyperparameters O
and O
random O
seeds O
, O
and O
present O
the O
correlation B-MetricName
in O
Table O
3 O
. O
We O
observe O
that O
there O
is O
only O
a O
weak O
correlation B-MetricName
between O
in O
- O
task O
performance O
and O
transferability O
prediction O
results O
, O
indicating O
that O
the O
ability O
of O
efficiently O
tuned O
parameters O
to O
encode O
task O
- O
related O
information O
is O
disentangled O
with O
their O
final O
in O
- O
task O
performance O
. O
This O
also O
shows O
the O
robustness O
of O
TuPaTE B-MethodName
with O
respect O
to O
hyperparameters O
. O
The O
fact O
that O
in O
- O
task O
performance O
only O
correlates O
weakly O
with O
transferability O
prediction O
motivates O
us O
to O
explore O
whether O
early O
checkpoints O
of O
efficiently O
tuned O
parameters O
can O
be O
used O
for O
transferability O
prediction O
. O
From O
Table O
4 O
, O
we O
find O
that O
early O
checkpoints O
are O
also O
effective O
task O
embeddings O
. O
This O
allows O
us O
to O
reduce O
the O
computation O
cost O
by O
around O
90 O
% O
while O
substantially O
outperforming O
the O
TASKEMB B-MethodName
baseline O
. O

Conclusion O

In O
this O
paper O
, O
we O
show O
that O
efficiently O
tuned O
parameters O
are O
highly O
predictive O
for O
inter O
- O
task O
transferability O
and O
thus O
can O
be O
used O
as O
off O
- O
the O
- O
shelf O
task O
embeddings O
for O
source O
task O
selection O
in O
intermediatetask O
transfer O
learning O
. O
Our O
empirical O
investigation O
with O
three O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
methods O
on O
22 O
NLP O
tasks O
demonstrates O
that O
our O
approach O
outperforms O
prior O
works O
on O
inter O
- O
task O
transferability O
prediction O
despite O
being O
more O
efficient O
. O

Limitations O

We O
select O
three O
representative O
works O
for O
three O
types O
of O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
. O
However O
, O
there O
are O
other O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
tuning I-MethodName
methods O
that O
we O
have O
not O
investigated O
. O
Although O
we O
believe O
our O
conclusion O
can O
generalize O
to O
other O
methods O
, O
we O
will O
conduct O
more O
experiments O
to O
confirm O
for O
future O
work O
. O

Ethics O
Statement O

We O
propose O
to O
use O
efficiently O
tuned O
parameters O
as O
task O
embedding O
, O
only O
for O
predicting O
the O
performance O
of O
intermediate O
transfer O
learning O
. O
Thus O
, O
we O
do O
not O
anticipate O
any O
major O
ethical O
concern O
. O

Acknowledgements O

We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O
This O
project O
is O
partly O
supported O
by O
NSF O
Award O
# O
1750063 O
. O

( O
Williams O
et O
al O
. O
, O
2018 O
) O

393k O
QQP B-DatasetName
( O
Iyer O
et O
al O
. O
, O
2017 O
) O
364k O
QNLI B-DatasetName
105k O
SST-2 B-DatasetName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
67k O
SciTail B-DatasetName
( O
Khot O
et O
al O
. O
, O
2018 O
) O
27k O
CoLA B-DatasetName
( O
Warstadt O
et O
al O
. O
, O
2019 O
) O
8.5k O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
( O
Cer O
et O
al O
. O
, O
2017 O
) O
7k O
MRPC B-DatasetName
( O
Dolan O
and O
Brockett O
, O
2005 O
) O
3.7k O
RTE B-DatasetName
( O
Dagan O
et O
al O
. O
, O
2005 O
) O
2.5k O
WNLI B-DatasetName
( O
Levesque O
, O
2011 O
) O
634 O

Question B-TaskName
Answering I-TaskName
( O
QA B-TaskName
) O

SQuAD-2 B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2018 O
) O
162k O
NewsQA B-DatasetName
( O
Trischler O
et O
al O
. O
, O
2017 O
) O
120k O
HotpotQA B-DatasetName
( O
Yang O
et O
al O
. O
, O
2018 O
) O
113k O
SQuAD-1 B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
108k O
DuoRC B-DatasetName
- I-DatasetName
p I-DatasetName
( O
Saha O
et O
al O
. O
, O
2018 O
) O
100k O
DuoRC B-DatasetName
- I-DatasetName
s I-DatasetName
( O
Saha O
et O
al O
. O
, O
2018 O
) O
86k O
DROP B-DatasetName
( O
Dua O
et O
al O
. O
, O
2019 O
) O
77k O
WikiHop B-DatasetName
( O
Welbl O
et O
al O
. O
, O
2018 O
) O
51k O
BoolQ B-DatasetName
( O
Clark O
et O
al O
. O
, O
2019 O
) O
16k O
ComQA B-DatasetName
( O
Abujabal O
et O
al O
. O
, O
2019 O
) O
11k O
CQ B-DatasetName
( O
Bao O
et O
al O
. O
, O
2016 O
) O
2k O
( O
Vu O
et O
al O
. O
, O
2020 O
) O
. O

ZS4IE B-MethodName
: O
A O
toolkit O
for O
Zero B-TaskName
- I-TaskName
Shot I-TaskName
Information I-TaskName
Extraction I-TaskName
with O
simple O
Verbalizations O

The O
current O
workflow O
for O
Information B-TaskName
Extraction I-TaskName
( O
IE B-TaskName
) O
analysts O
involves O
the O
definition O
of O
the O
entities O
/ O
relations O
of O
interest O
and O
a O
training O
corpus O
with O
annotated O
examples O
. O
In O
this O
demonstration O
we O
introduce O
a O
new O
workflow O
where O
the O
analyst O
directly O
verbalizes O
the O
entities O
/ O
relations O
, O
which O
are O
then O
used O
by O
a O
Textual O
Entailment O
model O
to O
perform O
zero B-TaskName
- I-TaskName
shot I-TaskName
IE I-TaskName
. O
We O
present O
the O
design O
and O
implementation O
of O
a O
toolkit O
with O
a O
user O
interface O
, O
as O
well O
as O
experiments O
on O
four O
IE B-TaskName
tasks O
that O
show O
that O
the O
system O
achieves O
very O
good O
performance O
at O
zero O
- O
shot O
learning O
using O
only O
5 O
- O
15 O
minutes O
per O
type O
of O
a O
user O
's O
effort O
. O
Our O
demonstration O
system O
is O
open O
- O
sourced O
at O
https O
: O

Introduction O

Information B-TaskName
Extraction I-TaskName
( O
IE B-TaskName
) O
systems O
are O
very O
costly O
to O
build O
. O
The O
current O
define O
- O
then O
- O
annotate O
- O
andtrain O
workflow O
uses O
supervised O
machine O
learning O
, O
where O
the O
analyst O
first O
defines O
the O
schema O
with O
the O
entities O
and O
relations O
of O
interest O
and O
then O
builds O
a O
training O
corpus O
with O
annotated O
examples O
. O
Unfortunately O
, O
each O
new O
domain O
and O
schema O
requires O
starting O
from O
scratch O
, O
as O
there O
is O
very O
little O
transfer O
between O
domains O
. O

We O
present O
an O
alternative O
verbalize O
- O
whiledefining O
workflow O
where O
the O
analyst O
defines O
the O
schema O
interactively O
in O
a O
user O
interface O
using O
natural O
language O
verbalizations O
of O
the O
target O
entity O
and O
relation O
types O
. O
Figure O
1 O
shows O
sample O
verbalization O
templates O
for O
a O
simple O
schema O
involving O
an O
employee O
relation O
and O
a O
passing O
away O
event O
, O
as O
well O
as O
a O
sample O
output O
annotated O
with O
the O
schema O
. O
The O
annotation O
of O
the O
EMPLOYEEOF O
relation O
requires O
performing O
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
and O
Relation O
* O
Denotes O
equal O
contribution O
. O
Extraction B-TaskName
( O
RE B-TaskName
) O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
, O
while O
annotating O
the O
LIFE.DIE O
event O
involves O
NER B-TaskName
, O
Event B-TaskName
Extraction I-TaskName
( O
EE B-TaskName
) O
, O
and O
Event B-TaskName
Argument I-TaskName
Extraction I-TaskName
( O
EAE B-TaskName
) O
( O
Walker O
et O
al O
. O
, O
2006 O
) O
. O
Our O
toolkit O
is O
able O
to O
perform O
those O
four O
IE B-TaskName
tasks O
using O
a O
single O
user O
interface O
, O
allowing O
the O
analyst O
to O
easily O
model O
and O
test O
the O
schema O
without O
the O
need O
to O
annotate O
examples O
. O

Our O
toolkit O
leans O
on O
recent O
work O
which O
has O
successfully O
recast O
several O
IE B-TaskName
tasks O
as O
Textual B-TaskName
Entailment I-TaskName
( O
TE B-TaskName
) O
tasks O
( O
White O
et O
al O
. O
, O
2017 O
; O
Poliak O
et O
al O
. O
, O
2018 O
; O
Levy O
et O
al O
. O
, O
2017 O
; O
. O
For O
instance O
, O
model O
relation O
types O
between O
entity O
pairs O
using O
type O
- O
specific O
verbalization O
templates O
that O
describe O
the O
relation O
, O
generates O
a O
verbalization O
( O
hypothesis O
) O
automatically O
using O
those O
templates O
and O
then O
uses O
a O
pre B-MethodName
- I-MethodName
trained I-MethodName
TE I-MethodName
model O
to O
predict O
if O
the O
premise O
( O
the O
sentence O
where O
the O
pair O
appears O
) O
entails O
the O
hypothesis O
, O
therefore O
leading O
to O
a O
prediction O
of O
the O
relation O
or O
" O
no O
relation O
" O
. O

In O
this O
paper O
we O
thus O
present O
ZS4IE B-MethodName
, O
a O
toolkit O
for O
zero B-TaskName
- I-TaskName
shot I-TaskName
IE I-TaskName
. O
We O
show O
that O
the O
four O
mainstream O
IE B-TaskName
tasks O
mentioned O
above O
can O
be O
reformulated O
as O
TE B-TaskName
problems O
, O
and O
that O
it O
is O
possible O
to O
achieve O
strong O
zero O
- O
shot O
performances O
leveraging O
pre O
- O
trained O
TE B-TaskName
models O
and O
a O
small O
amount O
of O
templates O
curated O
by O
the O
user O
. O
Our O
toolkit O
allows O
a O
novice O
user O
to O
curate O
templates O
for O
each O
new O
types O
of O
entities O
, O
relations O
, O
events O
, O
and O
event O
argument O
roles O
, O
and O
validate O
their O
effectiveness O
online O
over O
any O
example O
. O
We O
also O
present O
strong O
results O
on O
widely O
used O
datasets O
with O
only O
5 O
- O
15 O
minutes O
per O
type O
of O
a O
user O
's O
effort O
. O

Related O
Work O

Textual B-TaskName
Entailment I-TaskName
has O
been O
shown O
to O
be O
a O
reasonable O
proxy O
for O
classification O
tasks O
like O
topic O
or O
sentiment O
analysis O
( O
Yin O
et O
al O
. O
, O
2019 O
; O
Sainz O
and O
Rigau O
, O
2021 O
; O
Zhong O
et O
al O
. O
, O
2021 O
) O
. O
To O
reformulate O
a O
classification O
problem O
as O
TE B-TaskName
, O
it O
often O
starts O
with O
defining O
templates O
to O
describe O
each O
class O
label O
, O
leading O
to O
a O
natural O
language O
text O
( O
a O
" O
verbalization O
" O
of O
a O
hypothesis O
) O
for O
each O
possible O
label O
. O
Inference O
is O
performed O
by O
selecting O
the O
most O
probable O
candidate O
hypothesis O
entailing O
the O
premise O
. O
TE B-TaskName
is O
usually O
implemented O
with O
pre O
- O
trained O
language O
model O
finetuned O
on O
TE B-TaskName
datasets O
, O
such O
as O
MNLI O
, O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
FEVER O
( O
Thorne O
et O
al O
. O
, O
2018 O
) O
, O
ANLI O
( O
Nie O
et O
al O
. O
, O
2020 O
) O
or O
XNLI O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O
The O
results O
on O
classification O
have O
been O
particularly O
strong O
for O
zero O
- O
shot O
and O
few O
- O
shot O
learning O
, O
with O
Wang O
et O
al O
. O
( O
2021b O
) O
hypothesizing O
that O
entailment O
is O
a O
true O
language O
understanding O
task O
, O
where O
a O
model O
that O
performs O
entailment O
well O
is O
likely O
to O
succeed O
on O
similarlyframed O
tasks O
. O
reformulated O
relation O
extraction O
as O
a O
TE B-TaskName
task O
surpassing O
the O
state O
- O
of O
- O
the O
- O
art O
in O
zero O
- O
and O
few O
- O
short O
learning O
. O
A O
similar O
approach O
was O
previously O
explored O
by O
Obamuyide O
and O
Vlachos O
( O
2018 O
) O
, O
using O
TE O
models O
that O
are O
not O
based O
on O
pre O
- O
trained O
language O
models O
. O
Similar O
to O
TE O
, O
( O
Clark O
et O
al O
. O
, O
2019 O
) O
performs O
yes O
/ O
no O
Question O
Answering O
, O
in O
which O
a O
model O
is O
asked O
about O
the O
veracity O
of O
some O
fact O
given O
a O
passage O
. O
Lyu O
et O
al O
. O
( O
2021 O
) O
recast O
the O
zero O
- O
shot O
event O
extraction O
as O
a O
TE B-TaskName
task O
, O
using O
TE O
model O
to O
check O
whether O
a O
piece O
of O
text O
is O
about O
a O
type O
of O
event O
. O
Lastly O
, O
Sainz O
et O
al O
. O
( O
2022 O
) O
showed O
that O
TE B-TaskName
allows O
to O
leverage O
the O
knowledge O
from O
other O
tasks O
and O
schemas O
. O

IE B-TaskName
via O
Textual B-TaskName
Entailment I-TaskName

We O
first O
describe O
how O
to O
recast O
each O
of O
the O
IE B-TaskName
tasks O
( O
NER B-TaskName
, O
RE B-TaskName
, O
EE B-TaskName
, O
EAE B-TaskName
) O
as O
TE B-TaskName
independently O
, O
and O
leave O
the O
workflow O
between O
the O
tasks O
for O
the O
next O
section O
. O
At O
a O
high O
level O
, O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
TE I-TaskName
reformulation O
consists O
of O
three O
steps O
: O
candidate O
generation O
, O
label O
verbalization O
and O
TE B-TaskName
inference O
( O
Figure O
2 O
illustrates O
the O
steps O
for O
NER B-TaskName
) O
. O
The O
first O
step O
, O
candidate O
generation O
, O
identifies O
text O
spans O
( O
e.g. O
, O
proper O
nouns O
for O
NER B-TaskName
) O
or O
span O
pairs O
( O
a O
pair O
of O
entity O
mentions O
for O
relation B-TaskName
extraction I-TaskName
) O
in O
the O
input O
sentence O
as O
the O
focus O
of O
the O
prediction O
. O
Taking O
a O
text O
span O
( O
or O
span O
pair O
) O
as O
input O
, O
the O
label O
verbalization O
step O
applies O
a O
verbalization O
template O
to O
generate O
a O
hypothesis O
, O
which O
is O
a O
natural O
language O
sentence O
describing O
the O
span O
( O
or O
span O
pair O
) O
being O
an O
instance O
of O
a O
type O
of O
entity O
, O
relation O
, O
event O
, O
or O
event O
argument O
. O
The O
verbalization O
generates O
hypothesis O
for O
each O
of O
the O
target O
types O
. O
Finally O
, O
the O
TE B-TaskName
inference O
step O
takes O
the O
original O
sentence O
( O
the O
premise O
) O
and O
each O
hypothesis O
as O
input O
, O
and O
uses O
a O
pre B-MethodName
- I-MethodName
trained I-MethodName
TE I-MethodName
model O
to O
predict O
if O
the O
premise O
entails O
, O
contradicts O
, O
or O
is O
neutral O
to O
the O
hypothesis O
. O
The O
type O
with O
the O
verbalization O
having O
the O
highest O
entailment O
probability O
is O
selected O
. O
We O
next O
describe O
each O
step O
in O
detail O
. O

Candidate O
Generation O

We O
describe O
the O
candidate O
generation O
for O
each O
of O
the O
task O
below O
. O

Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
: O
Candidates O
are O
extracted O
using O
specific O
patterns O
of O
PoS O
tags O
as O
returned O
by O
Stanza O
( O
Qi O
et O
al O
. O
, O
2020 O
) O
. O
For O
instance O
, O
for O
the O
simple O
example O
in O
Figure O
1 O
it O
suffices O
to O
select O
proper O
nouns O
( O
shown O
in O
Figure O
2 O
) O
, O
which O
are O
easily O
extended O
with O
other O
PoS O
patterns O
if O
needed O
. O
The O
toolkit O
also O
allows O
the O
usage O
of O
a O
constituency O
parser O
( O
Kitaev O
and O
Klein O
, O
2018 O
) O
. O

Relation B-TaskName
Extraction I-TaskName
( O
RE B-TaskName
) O
: O
Each O
relation O
requires O
a O
pair O
of O
entities O
that O
satisfy O
specific O
type O
constraints O
, O
e.g. O
the O
EMPLOYEEOF O
relation O
requires O
a O
PERSON O
and O
an O
ORGANIZATION O
. O
A O
NER B-TaskName
module O
is O
used O
to O
extract O
all O
candidate O
entities O
that O
follow O
the O
required O
entity O
types O
according O
to O
the O
target O
schema O
. O
The O
toolkit B-MethodName
uses O
the O
TE B-TaskName
based O
NER B-TaskName
module O
, O
although O
it O
also O
allows O
usage O
of O
a O
supervised O
NER B-TaskName
system O
( O
Qi O
et O
al O
. O
, O
2020 O
) O
. O

Event B-TaskName
( I-TaskName
Trigger I-TaskName
) I-TaskName
Extraction I-TaskName
( O
EE B-TaskName
) O
: O

The O
main O
goal O
of O
this O
task O
is O
to O
detect O
whether O
the O
input O
sentence O
contains O
a O
mention O
of O
any O
of O
the O
target O
event O
types O
in O
the O
schema O
, O
e.g. O
LIFE.DIE O
. O
This O
task O
can O
be O
formulated O
as O
a O
multi O
- O
label O
text O
classification O
task O
, O
and O
in O
this O
case O
the O
full O
sentence O
is O
the O
candidate O
. O
Alternatively O
, O
the O
textual O
span O
that O
most O
likely O
expresses O
the O
event O
( O
the O
so O
- O
called O
trigger O
) O
can O
be O
extracted O
. O
In O
this O
case O
, O
the O
candidates O
are O
generated O
using O
specific O
PoS O
tags O
, O
e.g. O
verbs O
like O
died O
( O
cf O
. O
Figure O
1 O
) O
. O
Our O
toolkit B-MethodName
allows O
both O
options O
. O

Event B-TaskName
Argument I-TaskName
Extraction I-TaskName
: O
Given O
a O
sentence O
containing O
an O
event O
type O
( O
as O
detected O
by O
EE O
above O
) O
, O
the O
goal O
is O
to O
extract O
entity O
mentions O
that O
are O
fillers O
of O
the O
target O
arguments O
in O
the O
schema O
. O
For O
example O
, O
the O
schema O
in O
Figure O
1 O
involves O
three O
target O
arguments O
. O
Each O
of O
the O
arguments O
requires O
specific O
entity O
types O
, O
e.g. O
PERSON O
for O
the O
VICTIM O
argument O
. O
The O
candidates O
of O
the O
required O
types O
are O
extracted O
using O
the O
same O
NER B-TaskName
module O
as O
for O
RE B-TaskName
. O

Label O
Verbalization O

For O
each O
of O
the O
IE B-TaskName
tasks O
, O
the O
label O
verbalization O
process O
takes O
a O
sentence O
, O
a O
set O
of O
candidates O
and O
the O
set O
of O
target O
types O
( O
e.g. O
NER B-TaskName
types O
) O
, O
and O
generates O
a O
natural O
language O
text O
( O
the O
hypothesis O
) O
describing O
the O
existence O
of O
the O
type O
in O
the O
sentence O
( O
the O
premise O
) O
using O
verbalization O
templates O
. O
Each O
candidate O
is O
a O
span O
( O
or O
pair O
of O
spans O
) O
that O
can O
belong O
to O
a O
specific O
type O
( O
e.g. O
being O
a O
PERSON O
in O
NER B-TaskName
) O
. O
Therefore O
, O
the O
textual O
verbalization O
is O
generated O
to O
express O
each O
potential O
type O
for O
the O
span O
or O
the O
pair O
of O
spans O
. O
For O
the O
NER B-TaskName
and O
event O
extraction O
tasks O
, O
each O
verbalization O
expresses O
one O
potential O
entity O
( O
or O
event O
type O
) O
for O
the O
target O
candidate O
. O
For O
the O
relation O
and O
event O
argument O
extraction O
tasks O
, O
the O
verbalization O
template O
combines O
the O
informa O
- O
tion O
from O
the O
text O
spans O
of O
the O
candidate O
pair O
and O
produces O
a O
text O
that O
expresses O
a O
relation O
( O
or O
event O
argument O
role O
) O
. O
The O
analyst O
just O
needs O
to O
write O
the O
verbalization O
templates O
for O
each O
target O
type O
, O
and O
they O
are O
applied O
to O
the O
candidates O
to O
generate O
the O
hypothesis O
, O
as O
shown O
in O
the O
second O
step O
in O
Figure O
2 O
for O
NER B-TaskName
. O

Figure O
1 O
shows O
sample O
TE B-TaskName
verbalization O
templates O
for O
entity O
, O
relation O
, O
event O
, O
and O
event O
argument O
types O
corresponding O
to O
the O
4 O
IE B-TaskName
tasks O
, O
as O
well O
as O
sample O
example O
as O
output O
. O
The O
templates O
for O
NER B-TaskName
and O
event B-TaskName
extraction I-TaskName
( O
leftmost O
part O
of O
the O
figure O
) O
are O
applied O
over O
a O
single O
candidate O
as O
extracted O
in O
the O
previous O
step O
( O
the O
candidate O
entity O
or O
event O
trigger O
, O
respectively O
) O
. O
Note O
that O
for O
event B-TaskName
extraction I-TaskName
it O
is O
also O
possible O
to O
produce O
hypothesis O
using O
templates O
with O
no O
slots O
, O
e.g. O
" O
A O
person O
died O
" O
for O
LIFE.DIE O
. O
In O
the O
case O
of O
relation B-TaskName
extraction I-TaskName
, O
the O
verbalization O
templates O
contain O
two O
slots O
for O
the O
two O
entity O
spans O
potentially O
holding O
the O
relation O
. O
Finally O
, O
templates O
for O
event B-TaskName
argument I-TaskName
extraction I-TaskName
can O
be O
more O
varied O
. O
The O
figure O
shows O
two O
examples O
: O
a O
template O
using O
a O
single O
slot O
for O
the O
candidate O
filler O
, O
and O
a O
template O
which O
, O
in O
addition O
to O
the O
filler O
slot O
, O
uses O
the O
trigger O
( O
" O
died O
" O
in O
this O
case O
, O
for O
PLACE O
) O
. O

Inference O

Given O
a O
premise O
( O
the O
original O
sentence O
) O
and O
a O
hypothesis O
( O
an O
verbalization O
generated O
by O
label O
verbalization O
templates O
) O
, O
we O
use O
a O
pre B-MethodName
- I-MethodName
trained I-MethodName
TE I-MethodName
model O
to O
decide O
whether O
the O
hypothesis O
is O
entailed O
by O
, O
contradicted O
with O
, O
or O
is O
neutral O
to O
the O
premise O
. O
In O
principle O
, O
any O
model O
trained O
on O
an O
entailment O
dataset O
can O
be O
used O
. O
The O
inference O
is O
mainly O
determined O
by O
three O
key O
factors O
: O
the O
TE O
probabilities O
for O
the O
verbalizations O
of O
all O
templates O
for O
all O
labels O
, O
the O
type O
- O
specific O
input O
span O
constraints O
, O
and O
a O
threshold O
that O
decides O
if O
the O
probability O
is O
high O
enough O
to O
consider O
the O
candidate O
a O
positive O
instance O
. O
The O
type O
- O
specific O
input O
span O
constraints O
are O
enforced O
to O
make O
sure O
we O
do O
n't O
have O
candidates O
that O
violates O
the O
constraints O
. O
We O
return O
the O
class O
label O
of O
the O
hypothesis O
with O
highest O
entailment O
probability O
. O
If O
none O
of O
the O
hypothesis O
is O
higher O
than O
the O
threshold O
, O
we O
return O
the O
negative O
class O
, O
that O
is O
the O
class O
that O
represents O
that O
there O
is O
not O
a O
valid O
entity O
, O
relation O
, O
event O
, O
or O
event O
argument O
role O
type O
for O
the O
input O
candidate O
. O
The O
threshold O
for O
minimal O
entailment O
probability O
is O
set O
by O
default O
to O
0.5 O
. O

ZS4IE B-MethodName
toolkit I-MethodName

ZS4IE B-MethodName
comprises O
a O
pipeline O
and O
a O
user O
interface O
. O

The O
ZS4IE B-MethodName
Pipeline O

As O
described O
in O
Section O
3.1 O
and O
illustrated O
in O
Figure O
3 O
, O
there O
are O
inter O
- O
task O
dependencies O
between O
the O
four O
IE B-TaskName
tasks O
( O
e.g. O
, O
relation B-TaskName
extraction I-TaskName
requires O
that O
entity O
mentions O
have O
already O
been O
tagged O
in O
the O
input O
sentence O
) O
. O
Some O
task O
also O
require O
external O
NLP O
tools O
for O
generating O
candidates O
. O
To O
address O
these O
issues O
and O
to O
allow O
maximal O
flexibility O
for O
the O
users O
, O
we O
support O
the O
following O
two O
workflows O
. O

The O
End O
- O
to O
- O
End O
( O
E2E O
) O
Mode O
: O
This O
mode O
will O
run O
the O
ZS4IE B-MethodName
modules O
in O
a O
pipeline O
: O
we O
allow O
the O
users O
to O
start O
from O
raw O
text O
, O
and O
perform O
customization O
( O
e.g. O
, O
develop O
templates O
for O
new O
types O
of O
interest O
) O
for O
all O
four O
IE B-TaskName
tasks O
. O
The O
user O
has O
to O
follow O
the O
inter O
- O
task O
dependencies O
as O
illustrated O
in O
Figure O
3 O
: O
the O
user O
must O
finish O
NER B-TaskName
customization O
before O
moving O
on O
to O
relation O
extraction O
or O
the O
event O
argument O
extraction O
task O
, O
because O
the O
later O
two O
tasks O
needs O
NER B-TaskName
to O
generate O
their O
input O
candidates O
. O
Similarly O
, O
the O
user O
must O
finish O
customization O
for O
the O
event B-TaskName
trigger I-TaskName
classification I-TaskName
task O
, O
before O
working O
on O
the O
event B-TaskName
argument I-TaskName
extraction I-TaskName
task O
. O

The O
end O
- O
to O
- O
end O
pipeline O
also O
runs O
a O
customizable O
pre O
- O
processing O
step O
including O
a O
POS O
tagger O
and O
a O
constituency O
parser O
, O
before O
any O
of O
the O
later O
modules O
. O

The O
Task O
Mode O
: O
In O
this O
mode O
, O
the O
user O
can O
choose O
to O
work O
on O
each O
of O
the O
four O
IE B-TaskName
tasks O
independently O
. O
In O
order O
to O
address O
the O
inter O
- O
dependencies O
, O
the O
user O
can O
choose O
to O
run O
an O
independent O
NER B-TaskName
module O
instead O
, O
as O
part O
of O
the O
pre O
- O
processing O
step O
. O
The O
user O
interface O
allows O
the O
user O
to O
tag O
any O
spans O
for O
entity O
or O
event O
trigger O
types O
, O
before O
running O
customization O
for O
the O
more O
complex O
tasks O
such O
as O
relation O
extraction O
or O
event O
argument O
extraction O
. O
This O
option O
allows O
to O
explore O
additional O
entity O
and O
event O
trigger O
types O
before O
actually O
implementing O
them O

User O
Interface O
( O
UI O
) O

Figure O
4 O
shows O
the O
User O
Interface O
. O
It O
allows O
the O
user O
to O
add O
new O
types O
of O
entities O
, O
relations O
, O
events O
and O
event O
argument O
roles O
, O
and O
then O
develop O
templates O
( O
along O
with O
input O
type O
constraints O
for O
each O
type O
) O
. O
Figure O
5 O
shows O
the O
NER B-TaskName
extraction I-TaskName
results O
on O
an O
user O
- O
input O
sentence O
. O
It O
also O
displays O
the O
likelihood O
scores O
produced O
by O
the O
TE O
model O
of O
those O
templates O
that O
are O
above O
the O
threshold O
, O
to O
allow O
the O
user O
to O
validate O
templates O
. O

To O
show O
why O
it O
extracts O
each O
entity O
, O
it O
displays O
a O
ranked O
list O
of O
likely O
entity O
types O
, O
the O
template O
that O
led O
to O
that O
type O
, O
along O
with O
the O
entailment O
probability O
produced O
by O
the O
pre O
- O
trained O
TE O
model O
. O
The O
user O
can O
click O
on O
" O
+ O
" O
and O
" O
- O
" O
sign O
next O
to O
each O
extraction O
to O
label O
its O
correctness O
. O
Our O
system O
will O
track O
the O
total B-MetricName
number I-MetricName
of I-MetricName
extractions I-MetricName
and O
and O
accuracy B-MetricName
for O
each O
task O
, O
each O
type O
and O
each O
template O
, O
to O
allow O
the O
user O
to O
quickly O
validate O
the O
effectiveness O
of O
the O
templates O
and O
to O
spot O
any O
low O
- O
precision O
template O
. O

Supplying O
Input O
Text O
: O

The O
user O
can O
supply O
a O
text O
snippet O
, O
one O
at O
a O
time O
, O
to O
test O
writing O
templates O
. O
As O
described O
in O
Section O
4.1 O
, O
when O
using O
the O
task O
mode O
, O
the O
user O
can O
label O
spans O
in O
the O
input O
text O
for O
the O
more O
complex O
relation O
extraction O
and O
event O
argument O
extraction O
tasks O
, O
so O
that O
the O
text O
already O
has O
the O
right O
entity O
or O
event O
trigger O
spans O
and O
types O
to O
begin O
with O
. O

Develop O
Templates O
for O
New O
Types O
: O
The O
user O
can O
add O
new O
types O
of O
entities O
, O
relations O
, O
events O
, O
and O
event O
argument O
role O
. O
For O
each O
type O
, O
the O
user O
can O
create O
templates O
along O
with O
the O
input O
span O
type O
constraints O
, O
and O
then O
run O
inference O
interactively O
on O
the O
input O
text O
, O
to O
see O
whether O
these O
templates O
Figure O
4 O
: O
The O
UI O
for O
curating O
templates O
for O
types O
of O
interests O
for O
NER B-TaskName
, O
relation B-TaskName
extraction I-TaskName
, O
event B-TaskName
extraction I-TaskName
and O
event B-TaskName
argument I-TaskName
extraction I-TaskName
tasks O
. O
The O
NER O
tab O
is O
partially O
shown O
with O
two O
types O
. O

Figure O
5 O
: O
The O
UI O
for O
displaying O
NER B-TaskName
extraction I-TaskName
results O
on O
an O
user O
- O
input O
sentence O
. O
We O
show O
the O
extractions O
and O
the O
likelihood O
scores O
of O
the O
templates O
above O
the O
threshold O
( O
e.g. O
T O
= O
0.5 O
) O
. O
can O
be O
used O
for O
extract O
the O
instances O
. O
The O
user O
can O
label O
the O
correctness O
of O
the O
extracted O
instances O
, O
resulting O
a O
small O
development O
dataset O
( O
the O
dev O
set O
) O
to O
help O
measuring O
the O
precision O
and O
relative O
recall O
for O
each O
template O
, O
and O
to O
tune O
the O
threshold O
for O
the O
TE B-TaskName
inference I-TaskName
. O

Display O
Metrics O
: O

The O
UI O
displays O
the O
accuracy B-MetricName
and O
yield B-MetricName
for O
each O
template O
and O
each O
type O
in O
realtime O
, O
to O
allow O
the O
user O
to O
monitor O
the O
progress O
and O
make O
adjustments O
on O
the O
fly O
. O

More O
screenshots O
and O
details O
of O
our O
UI O
are O
describe O
in O
Appendix O
A O
. O

Experiments O

We O
evaluated O
our O
system O
using O
publicly O
available O
datasets O
. O
We O
use O
CoNLL B-DatasetName
2003 I-DatasetName
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
for O
NER B-TaskName
evaluation O
, O
TA B-DatasetName
- I-DatasetName
CRED I-DatasetName
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
for O
RE B-TaskName
, O
and O
ACE B-DatasetName
for O
EE B-TaskName
and O
EAE B-TaskName
( O
Walker O
et O
al O
. O
, O
2006 O
) O
. O
We O
evaluate O
each O
task O
independently O
( O
not O
as O
a O
pipeline O
) O
to O
make O
as O
comparable O
as O
possible O
to O
existing O
zero O
- O
shot O
systems O
. O
In O
order O
to O
apply O
our O
toolkit B-MethodName
we O
made O
some O
adaptations O
as O
follows O
: O
We O
consider O
only O
proper O
nouns O
as O
candidates O
for O
NER B-TaskName
, O
and O
we O
ignore O
the O
MISC O
label O
because O
it O
is O
not O
properly O
defined O
in O
the O
task O
1 O
. O
We O
evaluate O
EE B-TaskName
as O
event B-TaskName
classification I-TaskName
, O
where O
the O
task O
is O
to O
output O
the O
events O
mentioned O
in O
the O
sentence O
without O
extracting O
the O
trigger O
words O
, O
as O
we O
found O
that O
deciding O
which O
is O
the O
trigger O
word O
is O
in O
many O
cases O
an O
arbitrary O
decision O
2 O
. O
In O
the O
case O
of O
RE B-TaskName
we O
used O
the O
templates O
from O
, O
which O
are O
publicly O
available O
. O
We O
will O
release O
the O
templates O
used O
on O
the O
experiments O
as O
additional O
material O
along O
with O
the O
paper O
. O
The O
analysts O
spent O
between O
5 O
- O
15 O
minutes O
per O
type O
, O
depending O
on O
the O
task O
, O
with O
NER B-TaskName
and O
EE B-TaskName
being O
the O
fastest O
. O

Table O
1 O
shows O
the O
zero O
- O
shot O
results O
for O
NER B-TaskName
, O
RE B-TaskName
, O
EE B-TaskName
, O
and O
EAE B-TaskName
tasks O
. O
We O
report O
the O
results O
of O
three O
entailment O
models O
: O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
trained O
on O
MNLI B-DatasetName
, O
RoBERTa O
* O
trained O
on O
MNLI B-DatasetName
, O
SNLI B-DatasetName
, O
FEVER B-DatasetName
and O
ANLI B-DatasetName
; O
and O
DeBERTa O
( O
He O
et O
al O
. O
, O
2021 O
) O
trained O
on O
MNLI B-DatasetName
. O
The O
main O
results O
( O
top O
three O
rows O
) O
use O
the O
default O
threshold O
( O
T B-HyperparameterName
= O
0.5 B-HyperparameterValue
) O
, O
we O
selected O
the O
T O
blindly O
, O
without O
checking O
any O
development O
result O
. O

The O
results O
show O
strong O
zero O
- O
shot O
performance O
. O
Note O
that O
there O
is O
no O
best O
entailment O
model O
, O
suggesting O
that O
there O
still O
exists O
margin O
for O
improvement O
. O
However O
, O
we O
see O
that O
RoBERTa O
* O
performs O
relatively O
well O
in O
all O
scenarios O
except O
EE B-TaskName
( O
see O
Section O
6 O
for O
further O
discussion O
) O
. O

The O
table O
also O
shows O
in O
the O
middle O
three O
rows O
the O
results O
where O
we O
optimize O
the O
threshold O
on O
development O
. O
The O
results O
improve O
in O
most O
of O
the O
cases O
, O
and O
allow O
comparison O
to O
other O
zero O
- O
shot O
systems O
which O
sometimes O
optimize O
a O
threshold O
in O
development O
data O
. O

Furthermore O
, O
we O
compare O
our O
system O
with O
zeroshot O
task O
specific O
approaches O
from O
other O
authors O
when O
available O
. O
For O
RE B-TaskName
, O
Wang O
et O
al O
. O
( O
2021a O
) O
propose O
a O
text B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
triple I-MethodName
translation I-MethodName
method O
that O
given O
a O
text O
and O
a O
set O
of O
entities O
returns O
the O
existing O
relations O
. O
For O
EE B-TaskName
, O
Lyu O
et O
al O
. O
( O
2021 O
) O
to O
us O
, O
the O
use O
of O
an O
entailment B-MethodName
model I-MethodName
, O
but O
in O
their O
case O
the O
input O
sentence O
is O
split O
in O
clauses O
according O
to O
the O
output O
of O
a O
Semantic O
Role O
Labelling O
system O
. O
In O
order O
to O
compare O
their O
results O
with O
ours O
, O
we O
only O
use O
the O
event O
types O
, O
not O
the O
trigger O
information O
3 O
. O
The O
results O
from O
our O
system O
can O
be O
seen O
as O
an O
ablation O
where O
we O
do O
not O
make O
use O
of O
any O
SRL O
preprocessing O
. O
For O
EAE B-TaskName
, O
Liu O
et O
al O
. O
( O
2020 O
) O
perform O
zero B-TaskName
- I-TaskName
shot I-TaskName
EAE I-TaskName
by O
recasting O
the O
task O
as O
QA B-TaskName
. O
Some O
of O
these O
approaches O
also O
optimize O
a O
threshold O
on O
development O
data O
, O
although O
it O
is O
not O
always O
clear O
. O

We O
show O
that O
our O
toolkit O
with O
default O
threshold O
obtains O
excellent O
results O
despite O
being O
an O
all O
- O
in O
- O
one O
method O
. O

Discussion O

Towards O
post O
- O
editing O
on O
IE O
. O
Our O
internal O
evaluation O
suggest O
that O
verbalizing O
- O
while O
- O
defining O
workflow O
can O
have O
similar O
impact O
as O
post O
- O
editing O
machine O
translated O
text O
, O
where O
human O
translators O
obtain O
quality O
translations O
with O
less O
effort O
( O
Toral O
et O
al O
. O
, O
2018 O
) O
. O
The O
idea O
of O
this O
new O
framework O
will O
bring O
down O
the O
effort O
required O
to O
create O
larger O
and O
higher O
quality O
datasets O
. O
Current O
IE O
system O
are O
subject O
to O
a O
predefined O
schema O
and O
are O
useless O
to O
classify O
new O
types O
of O
entities O
, O
relations O
and O
events O
. O
The O
use O
interface O
of O
ZS4IE B-MethodName
brings O
to O
the O
annotators O
the O
opportunity O
of O
defining O
the O
schema O
interactively O
and O
manually O
annotating O
the O
dataset O
with O
the O
help O
of O
the O
entailment O
model O
. O
In O
the O
future O
we O
would O
like O
to O
use O
the O
manual O
annotations O
to O
fine O
- O
tune O
the O
TE O
model O
, O
which O
would O
further O
improve O
the O
performance O
, O
as O
shown O
by O
the O
excellent O
few O
- O
shot O
results O
of O
. O

Implicit O
events O
extraction O
. O
During O
the O
development O
of O
the O
EE B-TaskName
verbalizations O
we O
found O
out O
that O
the O
3 O
Output O
kindly O
provided O
by O
the O
authors O
. O

entailment O
model O
is O
prone O
to O
predict O
implicit O
events O
that O
are O
implied O
by O
other O
events O
. O
For O
example O
, O
an O
event O
type O
of O
JUSTICE O
: O
JAIL O
implies O
an O
event O
of O
JUSTICE O
: O
CONVICT O
where O
as O
the O
same O
time O
it O
implies O
event O
type O
of O
JUSTICE O
: O
TRIAL O
- O
HEARING O
. O
As O
the O
entailment O
models O
are O
not O
specifically O
trained O
for O
a O
particular O
IE B-TaskName
task O
( O
e.g. O
EE B-TaskName
) O
they O
are O
not O
limited O
to O
the O
extraction O
of O
explicit O
mentions O
of O
types O
( O
e.g. O
event O
types O
) O
annotated O
in O
the O
dataset O
. O
We O
think O
that O
this O
phenomenon O
might O
have O
penalized O
the O
RoBERTa O
* O
model O
on O
the O
EE B-TaskName
task O
, O
as O
ACE B-DatasetName
dataset O
only O
contains O
annotations O
of O
explicit O
events O
. O
On O
the O
contrary O
, O
rather O
than O
a O
limitation O
of O
our O
approach O
, O
we O
believe O
that O
this O
is O
a O
positive O
feature O
that O
can O
be O
exploited O
by O
the O
users O
. O

Conclusions O

The O
ZS4IE B-MethodName
toolkit I-MethodName
allows O
a O
novice O
user O
to O
model O
complex O
IE O
schemas O
, O
curating O
simple O
yet O
effective O
templates O
for O
a O
target O
schema O
with O
new O
types O
of O
entities O
, O
relations O
, O
events O
, O
and O
event O
arguments O
. O
Empirical O
validation O
showed O
that O
reformulating O
the O
IE B-MethodName
tasks O
as O
an O
entailment O
problem O
is O
easy O
and O
effective O
, O
as O
spending O
only O
5 O
- O
15 O
minutes O
per O
type O
allows O
to O
achieve O
very O
strong O
zero O
- O
shot O
performance O
. O
ZS4IE B-MethodName
brings O
to O
the O
users O
the O
opportunity O
of O
defining O
the O
desired O
schema O
on O
the O
fly O
. O
In O
addition O
it O
allows O
to O
annotate O
examples O
, O
similar O
to O
post O
editing O
MT O
output O
. O
Rather O
than O
being O
a O
finalized O
toolkit O
, O
we O
envision O
several O
exciting O
directions O
, O
such O
as O
including O
further O
NLP O
tasks O
, O
allowing O
the O
user O
to O
select O
custom O
pre O
- O
processing O
steps O
for O
candidate O
generation O
and O
allowing O
the O
user O
to O
interactively O
improve O
the O
system O
annotating O
examples O
that O
are O
used O
to O
fine O
- O
tune O
the O
TE O
model O
. O
More O
generally O
, O
we O
would O
like O
to O
extend O
the O
inference O
capability O
of O
our O
models O
, O
perhaps O
acquired O
from O
other O
tasks O
or O
schemas O
( O
Sainz O
et O
al O
. O
, O
2022 O
) O
, O
in O
a O
research O
avenue O
where O
entailment O
and O
task O
performance O
improve O
in O
tandem O
. O
entity O
pairs O
each O
relation O
is O
defined O
over O
. O
For O
example O
, O
the O
" O
per O
: O
date_of_death O
" O
relation O
is O
only O
valid O
between O
a O
pair O
of O
PERSON O
and O
DATE O
mentions O
. O
Our O
UI O
allows O
the O
user O
to O
specify O
the O
" O
LeftEntity O
- O
Type O
" O
( O
left O
entity O
type O
) O
and O
the O
" O
RightEntityType O
" O
( O
right O
entity O
type O
) O
for O
each O
relation O
type O
under O
" O
allowed O
type O
" O
. O
These O
type O
constraints O
are O
show O
on O
the O
top O
box O
for O
each O
relation O
card O
on O
the O
left O
figure O
in O
Figure O
6b O
( O
e.g. O
, O
" O
PERSON- O
> O
DATE O
" O
under O
" O
per O
: O
date_of_death O
" O
) O
. O
Second O
, O
a O
relation O
involves O
a O
pair O
of O
entity O
mentions O
. O
Therefore O
, O
each O
pattern O
has O
two O
placeholders O
, O
" O
X O
" O
and O
" O
Y O
" O
, O
which O
can O
be O
replaced O
with O
two O
entity O
candidates O
that O
are O
likely O
to O
participate O
in O
the O
relationship O
. O

Template O
development O
for O
the O
event B-TaskName
extraction I-TaskName
task O
( O
Figure O
6c O
) O
is O
also O
similar O
to O
NER B-TaskName
, O
except O
that O
the O
template O
may O
not O
contain O
any O
trigger O
. O
For O
example O
, O
" O
Someone O
died O
" O
is O
a O
template O
for O
the O
" O
Death O
" O
event O
( O
Figure O
6c O
) O
. O
This O
template O
would O
allow O
the O
TE B-TaskName
approach O
to O
classify O
whether O
an O
extent O
( O
e.g. O
, O
a O
sentence O
) O
expresses O
a O
type O
of O
event O
. O

Template O
development O
for O
the O
event O
argument O
extraction O
task O
( O
Figure O
6d O
) O
is O
similar O
to O
relation O
extraction O
, O
except O
that O
the O
template O
can O
include O
either O
two O
placeholders O
" O
X O
" O
and O
" O
Y O
" O
in O
which O
" O
X O
" O
is O
an O
event O
trigger O
and O
" O
Y O
" O
is O
an O
event O
argument O
candidate O
filler O
( O
an O
entity O
) O
, O
or O
only O
one O
placeholder O
" O
Y O
" O
which O
is O
the O
event O
argument O
candidate O
filler O
. O
The O
later O
would O
require O
the O
template O
to O
implicitly O
describes O
the O
event O
type O
as O
well O
( O
for O
example O
, O
" O
Someone O
died O
in O
Y O
" O
for O
the O
LOCATION O
event O
argument O
role O
in O
Figure O
6d O
) O
. O

Template O
validation O
. O
We O
developed O
an O
interactive O
workflow O
to O
allow O
the O
user O
to O
quickly O
develop O
templates O
and O
validate O
their O
effectiveness O
in O
our O
TEbased O
framework O
. O
To O
support O
this O
workflow O
, O
our O
UI O
allows O
the O
user O
to O
run O
inference O
over O
any O
free O
text O
supplied O
by O
the O
user O
herself O
/ O
himself O
. O
For O
simplicity O
, O
we O
omit O
the O
UI O
where O
we O
allow O
the O
user O
type O
in O
free O
text O
. O
We O
show O
the O
UI O
that O
displays O
the O
extraction O
output O
on O
the O
free O
text O
, O
that O
also O
allows O
the O
user O
to O
label O
the O
correcness O
of O
the O
extractions O
. O
Based O
on O
those O
labeled O
examples O
, O
the O
UI O
also O
automatically O
calculate O
a O
few O
metrics O
to O
help O
the O
user O
to O
find O
the O
effectiveness O
of O
the O
templates O
curated O
so O
far O
. O

Figure O
7 O
shows O
the O
UI O
for O
displaying O
NER B-TaskName
extraction I-TaskName
outputs O
( O
left O
) O
and O
automatically O
calculated O
metrics O
( O
right O
) O
. O
Taken O
the O
user O
- O
supplied O
sentence O
" O
John O
Smith O
, O
an O
executive O
at O
XYZ O
Corp. O
, O
died O
in O
Florida O
on O
Sunday O
" O
as O
input O
, O
the O
UI O
on O
the O
left O
- O
hand O
side O
shows O
the O
extracted O
named O
entities O
. O
It O
shows O
extractions O
such O
as O
" O
John O
Smith O
is O
a O
/ O
an O
PERSON O
" O
, O
" O
Sunday O
is O
a O
/ O
an O
DATE O
" O
, O
and O
so O
on O
. O
To O
provide O
rationale O
for O
each O
extraction O
, O
it O
displays O
a O
rank O
list O
of O
possible O
entity O
types O
, O
the O
template O
led O
to O
that O
type O
, O
along O
with O
the O
entailment O
probability O
produced O
by O
the O
pre O
- O
trained O
TE O
model O
. O
The O
user O
can O
click O
on O
" O
+ O
" O
and O
" O
- O
" O
sign O
next O
to O
each O
extraction O
to O
label O
its O
correctness O
. O
In O
Figure O
7 O
, O
all O
extractions O
are O
green O
( O
labeled O
by O
the O
user O
as O
correct O
) O
except O
that O
" O
Florida O
is O
a O
/ O
an O
CITY O
" O
is O
in O
red O
( O
labeled O
as O
incorrect O
by O
the O
user O
) O
. O
Based O
on O
these O
user O
- O
labeled O
extractions O
, O
the O
system O
calculated O
a O
number O
of O
metrics O
to O
facilitate O
template O
validation O
: O
the O
total O
number O
of O
extracted O
named O
entities O
( O
shown O
under O
" O
total O
" O
) O
, O
the O
number O
of O
correct O
and O
incorrect O
extractions O
under O
" O
correct O
" O
and O
" O
incorrect O
" O
, O
respectively O
( O
the O
accuracy O
number O
is O
also O
shown O
in O
the O
parenthesis O
next O
to O
" O
correct O
" O
) O
for O
the O
overall O
task O
, O
each O
type O
, O
and O
each O
pattern O
. O
The O
right O
- O
hand O
side O
UI O
in O
Figure O
7 O
displays O
these O
metrics O
, O
and O
allows O
the O
user O
to O
sort O
patterns O
/ O
types O
by O
each O
of O
the O
metric O
. O
The O
user O
can O
quickly O
identify O
some O
templates O
are O
low O
- O
precision O
( O
e.g. O
, O
" O
X O
is O
a O
location O
" O
for O
the O
entity O
type O
CITY O
) O
, O
and O
can O
revise O
them O
to O
improve O
precision O
. O

Figure O
8a O
, O
8b O
, O
and O
8c O
shows O
the O
UI O
for O
displaying O
extraction O
results O
for O
the O
relation B-TaskName
extraction I-TaskName
, O
event B-TaskName
extraction I-TaskName
, O
and O
event B-TaskName
argument I-TaskName
extraction I-TaskName
, O
respectively O
. O
Similar O
to O
the O
NER B-TaskName
task O
. O
Similarly O
, O
our O
system O
also O
includes O
metric O
board O
( O
the O
metrics O
above O
) O
for O
the O
other O
3 O
IE B-TaskName
tasks O
. O
To O
view O
the O
metric O
boards O
for O
these O
tasks O
, O
please O
refer O
to O
our O
demonstration O
video O
. O
The O
UI O
for O
displaying O
NER B-TaskName
extraction O
outputs O
( O
left O
) O
and O
automatically O
calculated O
metrics O
( O
right O
) O
. O
The O
left O
- O
hand O
side O
shows O
the O
named O
entities O
extracted O
from O
an O
user O
- O
input O
sentence O
( O
shown O
on O
the O
top O
) O
. O
The O
user O
can O
click O
on O
" O
+ O
" O
and O
" O
- O
" O
sign O
next O
to O
each O
extraction O
to O
label O
its O
correctness O
. O
The O
right O
- O
hand O
side O
shows O
the O
total B-MetricName
number I-MetricName
of I-MetricName
extracted I-MetricName
named I-MetricName
entities I-MetricName
( O
total O
) O
, O
the O
number B-MetricName
of I-MetricName
correct I-MetricName
and I-MetricName
incorrect I-MetricName
extractions I-MetricName
( O
the O
accuracy B-MetricName
number O
is O
also O
shown O
in O
the O
parenthesis O
next O
to O
" O
correct O
" O
) O
for O
the O
overall O
task O
, O
each O
type O
, O
and O
each O
pattern O
. O
These O
metrics O
are O
calculated O
based O
on O
the O
set O
of O
user O
labels O
. O
8 O
: O
The O
UI O
for O
displaying O
extractions O
for O
relation O
extraction O
, O
event O
extraction O
, O
and O
event O
argument O
extraction O
, O
respectively O
. O
The O
user O
an O
click O
on O
the O
" O
+ O
" O
or O
" O
- O
" O
sign O
next O
to O
each O
extraction O
to O
label O
the O
extraction O
as O
correct O
or O
incorrect O
. O

Acknowledgements O

Oscar O
is O
funded O
by O
a O
PhD O
grant O
from O
the O
Basque O
Government O
( O
PRE_2020_1_0246 O
) O
. O
This O
work O
is O
based O
upon O
work O
partially O
supported O
via O
the O
IARPA O
BETTER O
Program O
contract O
No O
. O
2019 O
- O
19051600006 O
( O
ODNI O
, O
IARPA O
) O
, O
and O
by O
the O
Basque O
Government O
( O
IXA O
excellence O
research O
group O
IT1343 O
- O
19 O
) O
. O

A O
User O
Interface O

We O
present O
more O
details O
on O
our O
user O
interface O
( O
UI O
) O
in O
this O
section O
. O
Our O
system O
supports O
all O
4 O
IE B-TaskName
tasks O
into O
a O
single O
integrated O
interface O
. O

Template O
development O
. O
Figure O
6a O
shows O
the O
main O
template O
development O
UI O
, O
in O
which O
each O
tab O
on O
the O
top O
represents O
one O
of O
the O
entity O
, O
relation O
, O
event O
, O
and O
event O
argument O
tasks O
. O
The O
user O
switch O
between O
tasks O
by O
simply O
clicking O
on O
a O
different O
tab O
( O
the O
tabs O
for O
the O
other O
3 O
tasks O
are O
shown O
in O
Figure O
6b O
, O
6c O
, O
and O
6d O
, O
respectively O
) O
. O

Take O
the O
NER B-TaskName
task O
as O
an O
example O
( O
Figure O
6a O
) O
, O
it O
shows O
an O
overview O
of O
all O
entity O
types O
along O
with O
the O
templates O
defined O
for O
each O
type O
( O
e.g. O
, O
" O
X O
is O
a O
person O
" O
for O
the O
type O
PERSON O
, O
in O
which O
" O
X O
" O
is O
a O
placeholder O
that O
can O
be O
replaced O
with O
a O
noun O
phrase O
" O
New O
York O
City O
" O
) O
. O
If O
the O
user O
clicks O
on O
the O
edit O
button O
( O
the O
pen O
- O
shaped O
button O
) O
, O
the O
pop O
- O
up O
window O
for O
adding O
a O
new O
entity O
type O
( O
the O
right O
- O
hand O
side O
figure O
in O
Figure O
6a O
) O
shows O
up O
. O
The O
user O
can O
add O
a O
template O
by O
clicking O
on O
" O
+ O
" O
sign O
, O
and O
then O
input O
the O
template O
to O
the O
left O
( O
the O
user O
can O
repeat O
this O
several O
times O
to O
add O
more O
templates O
) O
. O
The O
user O
can O
remove O
a O
template O
by O
clicking O
on O
" O
- O
" O
. O
The O
user O
can O
also O
click O
on O
the O
big O
" O
+ O
" O
card O
to O
the O
left O
to O
add O
a O
new O
entity O
type O
. O

Template O
development O
for O
the O
relation O
extraction O
task O
is O
similar O
to O
NER B-TaskName
, O
except O
for O
two O
differences O
: O
first O
, O
as O
shown O
in O
Figure O
6b O
( O
right O
) O
, O
we O
can O
further O
add O
a O
set O
of O
" O
allowed O
type O
" O
pairs O
, O
that O
are O
the O
set O
of O

Understanding O
and O
Bridging O
the O
Modality O
Gap O
for O
Speech B-TaskName
Translation I-TaskName

How O
to O
achieve O
better O
end O
- O
to O
- O
end O
speech B-TaskName
translation I-TaskName
( O
ST B-TaskName
) O
by O
leveraging O
( O
text O
) O
machine B-TaskName
translation I-TaskName
( O
MT B-TaskName
) O
data O
? O
Among O
various O
existing O
techniques O
, O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
is O
one O
of O
the O
effective O
ways O
to O
share O
knowledge O
between O
ST B-TaskName
and O
MT B-TaskName
in O
which O
additional O
MT B-TaskName
data O
can O
help O
to O
learn O
source O
- O
to O
- O
target O
mapping O
. O
However O
, O
due O
to O
the O
differences O
between O
speech O
and O
text O
, O
there O
is O
always O
a O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
. O
In O
this O
paper O
, O
we O
first O
aim O
to O
understand O
this O
modality O
gap O
from O
the O
target O
- O
side O
representation O
differences O
, O
and O
link O
the O
modality O
gap O
to O
another O
well O
- O
known O
problem O
in O
neural B-TaskName
machine I-TaskName
translation I-TaskName
: O
exposure O
bias O
. O
We O
find O
that O
the O
modality O
gap O
is O
relatively O
small O
during O
training O
except O
for O
some O
difficult O
cases O
, O
but O
keeps O
increasing O
during O
inference O
due O
to O
the O
cascading O
effect O
. O
To O
address O
these O
problems O
, O
we O
propose O
the O
Cross B-MethodName
- I-MethodName
modal I-MethodName
Regularization I-MethodName
with I-MethodName
Scheduled I-MethodName
Sampling I-MethodName
( O
CRESS B-MethodName
) O
method O
. O
Specifically O
, O
we O
regularize O
the O
output O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
, O
whose O
target O
- O
side O
contexts O
are O
derived O
by O
sampling O
between O
ground O
truth O
words O
and O
self O
- O
generated O
words O
with O
a O
varying O
probability O
. O
Furthermore O
, O
we O
introduce O
token O
- O
level O
adaptive O
training O
which O
assigns O
different O
training O
weights O
to O
target O
tokens O
to O
handle O
difficult O
cases O
with O
large O
modality O
gaps O
. O
Experiments O
and O
analysis O
show O
that O
our O
approach O
effectively O
bridges O
the O
modality O
gap O
, O
and O
achieves O
promising O
results O
in O
all O
eight O
directions O
of O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
dataset O
. O
1 O

Introduction O

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
speech I-TaskName
translation I-TaskName
( O
ST B-TaskName
) O
aims O
to O
translate O
speech O
signals O
to O
text O
in O
another O
language O
directly O
. O
Compared O
to O
traditional O
cascaded O
methods O
, O
which O
combine O
automatic B-TaskName
speech I-TaskName
recognition I-TaskName
( O
ASR B-TaskName
) O
and O
machine B-TaskName
translation I-TaskName
( O
MT B-TaskName
) O
models O
in O
a O
pipeline O
manner O
, O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
ST I-TaskName
could O
avoid O
error O
propagation O
and O
high O
latency O
( O
Sperber O
and O
Paulik O
, O
2020 O
) O
. O
Recently O
, O
end B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
ST I-TaskName
models O
have O
achieved O
comparable O
or O
even O
better O
results O
than O
cascaded O
ST B-TaskName
models O
( O
Bentivogli O
et O
al O
. O
, O
2021 O
; O
Anastasopoulos O
et O
al O
. O
, O
2021Anastasopoulos O
et O
al O
. O
, O
, O
2022 O
. O

However O
, O
due O
to O
the O
scarcity O
of O
ST B-TaskName
data O
, O
it O
is O
difficult O
to O
directly O
learn O
a O
mapping O
from O
source O
speech O
to O
the O
target O
text O
. O
Previous O
works O
often O
leverage O
MT B-TaskName
data O
to O
help O
the O
training O
with O
multi O
- O
task O
learning O
Tang O
et O
al O
. O
, O
2021a O
) O
. O
By O
sharing O
encoder O
and O
decoder O
between O
ST B-TaskName
and O
MT B-TaskName
, O
the O
model O
tends O
to O
learn O
similar O
representations O
from O
different O
modalities O
. O
In O
this O
way O
, O
the O
auxiliary O
MT B-TaskName
task O
can O
help O
build O
the O
source O
- O
to O
- O
target O
mapping O
. O
However O
, O
there O
remains O
a O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
due O
to O
the O
differences O
between O
speech O
and O
text O
. O
In O
this O
paper O
, O
we O
measure O
the O
modality O
gap O
with O
representation O
differences O
of O
the O
last O
decoder O
layer O
between O
ST B-TaskName
and O
MT B-TaskName
, O
because O
the O
representation O
of O
this O
layer O
will O
be O
mapped O
into O
the O
embedding O
space O
to O
obtain O
the O
final O
translation O
. O
A O
significant O
modality O
gap O
potentially O
causes O
different O
predictions O
, O
which O
makes O
ST B-TaskName
lag O
behind O
MT B-TaskName
. O

Thanks O
to O
multi O
- O
task O
learning O
, O
we O
observe O
that O
when O
training O
with O
teacher O
forcing O
, O
where O
both O
ST B-TaskName
and O
MT B-TaskName
use O
ground O
truth O
words O
as O
target O
- O
side O
contexts O
, O
the O
modality O
gap O
is O
relatively O
small O
except O
for O
some O
difficult O
cases O
. O
However O
, O
the O
exposure O
bias O
problem O
can O
make O
things O
worse O
. O
During O
inference O
, O
both O
ST B-TaskName
and O
MT B-TaskName
predict O
the O
next O
token O
conditioned O
on O
their O
previously O
generated O
tokens O
, O
which O
may O
be O
different O
due O
to O
the O
modality O
gap O
. O
Moreover O
, O
different O
predictions O
at O
the O
current O
step O
may O
lead O
to O
even O
more O
different O
predictions O
at O
the O
next O
step O
. O
As O
a O
result O
, O
the O
modality O
gap O
will O
increase O
step O
by O
step O
due O
to O
this O
cascading O
effect O
. O

To O
solve O
these O
problems O
, O
we O
propose O
the O
Crossmodal B-MethodName
Regularization I-MethodName
with I-MethodName
Scheduled I-MethodName
Sampling I-MethodName
( O
CRESS B-MethodName
) O
method O
. O
To O
reduce O
the O
effect O
of O
exposure O
bias O
, O
we O
introduce O
scheduled O
sampling O
during O
training O
, O
where O
the O
target O
- O
side O
contexts O
are O
sampled O
be O
- O
tween O
ground O
truth O
words O
and O
self O
- O
generated O
words O
with O
a O
changing O
probability O
. O
Based O
on O
this O
, O
we O
propose O
to O
regularize O
ST B-TaskName
and O
MT B-TaskName
in O
the O
output O
space O
to O
bridge O
the O
modality O
gap O
by O
minimizing O
the O
Kullback B-MetricName
- I-MetricName
Leibler I-MetricName
( O
KL B-MetricName
) O
divergence O
between O
their O
predictions O
. O
This O
will O
encourage O
greater O
consistency O
between O
ST B-TaskName
and O
MT O
predictions O
based O
on O
partial O
self O
- O
generated O
words O
, O
which O
is O
closer O
to O
the O
inference O
mode O
. O
Besides O
, O
to O
handle O
the O
difficult O
cases O
, O
we O
introduce O
token O
- O
level O
adaptive O
training O
for O
CRESS B-MethodName
, O
where O
each O
target O
token O
is O
given O
a O
varying O
weight O
during O
training O
according O
to O
the O
scale O
of O
the O
modality O
gap O
. O
In O
this O
way O
, O
those O
cases O
with O
significant O
modality O
gaps O
will O
be O
emphasized O
. O
We O
conduct O
experiments O
on O
the O
ST B-TaskName
benchmark O
dataset O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
( O
Di O
Gangi O
et O
al O
. O
, O
2019a O
) O
. O
Results O
show O
that O
our O
approach O
significantly O
outperforms O
the O
strong O
multi O
- O
task O
learning O
baseline O
, O
with O
1.8 B-MetricValue
BLEU B-MetricName
improvements O
in O
the O
base O
setting O
and O
1.3 B-MetricValue
BLEU B-MetricName
improvements O
in O
the O
expanded O
setting O
on O
average O
. O
Further O
analysis O
shows O
that O
our O
approach O
effectively O
bridges O
the O
modality O
gap O
and O
improves O
the O
translation O
quality O
, O
especially O
for O
long O
sentences O
. O

Background O

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
Speech I-TaskName
Translation I-TaskName

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
speech I-TaskName
translation I-TaskName
( O
ST B-TaskName
) O
directly O
translates O
speech O
in O
the O
source O
language O
to O
text O
in O
the O
target O
language O
. O
The O
corpus O
of O
ST B-TaskName
is O
usually O
composed O
of O
triplet O
data O
D O
= O
{ O
( O
s O
, O
x O
, O
y O
) O
} O
. O

Here O
s O
= O
( O
s O
1 O
, O
... O
, O
s O
|s| O
) O
is O
the O
sequence O
of O
audio O
wave O
, O
x O
= O
( O
x O
1 O
, O
... O
, O
x O
|x| O
) O
is O
the O
transcription O
and O
y O
= O
( O
y O
1 O
, O
... O
, O
y O
|y| O
) O
is O
the O
translation B-TaskName
. O
Similar O
to O
previous O
work O
Fang O
et O
al O
. O
, O
2022 O
) O
, O
our O
ST B-TaskName
model O
is O
composed O
of O
an O
acoustic O
encoder O
and O
a O
translation B-TaskName
model O
. O
The O
acoustic O
encoder O
is O
a O
pre O
- O
trained O
HuBERT O
( O
Hsu O
et O
al O
. O
, O
2021 O
) O
model O
followed O
by O
two O
convolutional O
layers O
, O
which O
are O
used O
to O
reduce O
the O
length O
of O
the O
speech O
sequence O
. O
The O
translation B-TaskName
model O
follows O
standard O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
- O
decoder O
architecture O
, O
where O
the O
encoder O
contains O
N O
Transformer O
encoder O
layers O
, O
and O
the O
decoder O
contains O
N O
Transformer O
decoder O
layers O
. O
We O
first O
pre O
- O
train O
the O
translation B-TaskName
model O
with O
MT B-TaskName
data O
, O
and O
then O
optimize O
the O
whole O
model O
by O
minimizing O
a O
cross O
- O
entropy O
loss O
: O

L O
ST O
= O
− O
|y| O
i=1 O
log O
p O
( O
y O
i O
|s O
, O
y O
< O
i O
) O
, O
( O
1 O
) O

p O
( O
y O
i O
|s O
, O
y O
< O
i O
) O
∝ O
exp O
( O
W O
• O
f O
( O
s O
, O
y O
< O
i O
) O
) O
, O
( O
2 O
) O

where O
f O
is O
a O
mapping O
from O
the O
input O
speech O
s O
and O
target O
prefix O
y O
< O
i O
to O
the O
representation O
of O
the O
last O
decoder O
layer O
at O
step O
i. O
W O
is O
used O
to O
transform O
the O
dimension O
to O
the O
size O
of O
the O
target O
vocabulary O
. O

Multi B-MethodName
- I-MethodName
task I-MethodName
Learning I-MethodName
for O
ST B-TaskName

Multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
( O
MTL B-MethodName
) O
has O
been O
proven O
useful O
for O
sharing O
knowledge O
between O
text B-TaskName
translation I-TaskName
and O
speech B-TaskName
translation I-TaskName
( O
Tang O
et O
al O
. O
, O
2021a O
) O
, O
where O
an O
auxiliary O
MT B-TaskName
task O
is O
introduced O
during O
training O
: O

L O
MT O
= O
− O
|y| O
i=1 O
log O
p O
( O
y O
i O
|x O
, O
y O
< O
i O
) O
, O
( O
3 O
) O

p O
( O
y O
i O
|x O
, O
y O
< O
i O
) O
∝ O
exp O
( O
W O
• O
f O
( O
x O
, O
y O
< O
i O
) O
) O
. O
( O
4 O
) O

Note O
that O
both O
modalities O
( O
i.e. O
, O
speech O
and O
text O
) O
share O
all O
transformer O
encoder O
and O
decoder O
layers O
. O
Finally O
, O
the O
training O
objective O
is O
written O
as O
follows O
: O

L O
MTL O
= O
L O
ST O
+ O
L O
MT O
. O
( O
5 O
) O

Preliminary O
Studies O
on O
Modality O
Gap O

With O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
, O
most O
of O
the O
knowledge O
of O
MT B-TaskName
can O
be O
transferred O
to O
ST B-TaskName
. O
However O
, O
the O
performance O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
still O
exists O
. O
In O
this O
section O
, O
we O
first O
conduct O
some O
preliminary O
studies O
with O
our O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
baseline O
model O
to O
understand O
where O
this O
gap O
comes O
from O
. O

Definition O
of O
the O
Modality O
Gap O

The O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
is O
related O
to O
the O
prediction O
difference O
at O
each O
decoding O
step O
, O
while O
the O
prediction O
depends O
only O
on O
the O
representation O
of O
the O
last O
decoder O
layer O
. O
Therefore O
, O
we O
define O
the O
modality B-MetricName
gap I-MetricName
at O
the O
i O
- O
th O
decoding O
step O
as O
follows O
: O

G O
( O
s O
, O
y O
< O
i O
∥x O
, O
y O
< O
i O
) O
= O
1−cos O
( O
f O
( O
s O
, O
y O
< O
i O
) O
, O
f O
( O
x O
, O
y O
< O
i O
) O
) O
, O
( O
6 O
) O

where O
cos O
is O
the O
cosine O
similarity O
function O
cos O
( O
a O
, O
b O
) O
= O
a O
⊤ O
b O
/ O
∥a∥∥b∥. O
A O
larger O
cosine O
similarity O
indicates O
a O
smaller O
modality B-MetricName
gap I-MetricName
. O

To O
understand O
the O
extent O
of O
the O
modality B-MetricName
gap I-MetricName
, O
we O
count O
the O
distribution O
of O
G O
( O
s O
, O
y O
< O
i O
∥x O
, O
y O
< O
i O
) O
based O
on O
all O
triples O
( O
s O
, O
x O
, O
y O
< O
i O
) O
in O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
( O
Di O
Gangi O
et O
al O
. O
, O
2019a O
) O
En→De O
dev O
set O
. O
As O
shown O
in O
Figure O
1 O
, O
the O
modality B-MetricName
gap I-MetricName
is O
relatively O
small O
( O
< O
10 B-MetricValue
% I-MetricValue
) O
in O
most O
cases O
, O
which O
proves O
the O
effectiveness O
of O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
in O
sharing O
knowledge O
across O
ST B-TaskName
and O
MT B-TaskName
. O
However O
, O
we O
also O
observe O
a O
long O
- O
tail O
problem O
: O
there O
is O
a O
large O
difference O
between O
ST B-TaskName
and O
MT B-TaskName
representations O
in O
some O
difficult O
cases O
. O

Connection O
between O
Exposure O
Bias O
and O
Modality B-MetricName
Gap I-MetricName

Exposure O
bias O
, O
a O
discrepancy O
between O
training O
and O
inference O
, O
is O
a O
well O
- O
known O
problem O
in O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
Bengio O
et O
al O
. O
, O
2015 O
; O
Ranzato O
et O
al O
. O
, O
2016 O
; O
Wang O
and O
Sennrich O
, O
2020 O
; O
Arora O
et O
al O
. O
, O
2022 O
) O
. O
During O
training O
with O
teacher O
forcing O
, O
both O
ST B-TaskName
and O
MT B-TaskName
predict O
the O
next O
token O
conditioned O
on O
the O
ground O
truth O
target O
prefix O
y O
< O
i O
. O
However O
, O
during O
inference O
, O
the O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
depend O
on O
their O
previously O
generated O
tokens O
by O
the O
model O
itself O
( O
denoted O
as O
y O
s O
< O
i O
and O
y O
x O
< O
i O
for O
ST B-TaskName
and O
MT B-TaskName
respectively O
) O
, O
which O
might O
be O
different O
due O
to O
the O
modality B-MetricName
gap I-MetricName
. O
Furthermore O
, O
different O
predictions O
at O
the O
current O
decoding O
step O
result O
in O
different O
target O
prefixes O
for O
ST B-TaskName
and O
MT B-TaskName
, O
potentially O
causing O
even O
more O
different O
predictions O
at O
the O
next O
step O
. O
Such O
cascading O
effect O
will O
enlarge O
the O
modality B-MetricName
gap I-MetricName
step O
by O
step O
during O
inference O
. O

To O
prove O
our O
hypothesis O
, O
we O
present O
the O
curves O
of O
the O
modality B-MetricName
gap I-MetricName
with O
decoding O
steps O
under O
teacher B-MethodName
forcing I-MethodName
, O
beam B-MethodName
search I-MethodName
, O
and O
greedy B-MethodName
search I-MethodName
strategies O
, O
respectively O
. O
As O
shown O
in O
Figure O
2 O
, O
with O
teacher B-MethodName
forcing I-MethodName
, O
there O
is O
no O
significant O
difference O
in O
the O
modality B-MetricName
gap I-MetricName
across O
steps O
, O
as O
both O
ST B-TaskName
and O
MT B-TaskName
depend O
on O
the O
same O
target O
prefix O
at O
any O
step O
. O
Hence O
, O
the O
modality B-MetricName
gap I-MetricName
G O
( O
s O
, O
y O
< O
i O
∥x O
, O
y O
< O
i O
) O
only O
comes O
from O
the O
difference O
between O
input O
speech O
s O
and O
text O
x. O
However O
, O
when O
decoding O
with O
greedy B-MethodName
search I-MethodName
, O
due O
to O
the O
cascading O
effect O
mentioned O
above O
, O
the O
self O
- O
generated O
target O
prefix O
y O
s O
< O
i O
and O
y O
x O
< O
i O
become O
increasingly O
different O
, O
making O
the O
modality B-MetricName
gap I-MetricName
G O
( O
s O
, O
y O
s O
< O
i O
∥x O
, O
y O
x O
< O
i O
) O
keep O
increasing O
with O
decoding O
steps O
. O
A O
simple O
way O
to O
alleviate O
this O
problem O
is O
beam B-MethodName
search I-MethodName
, O
which O
considers O
several O
candidate O
The O
modality B-MetricName
gap I-MetricName
is O
calculated O
with O
the O
average O
representation O
of O
all O
candidates O
. O
We O
set O
a O
beam B-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
. O
tokens O
rather O
than O
a O
single B-HyperparameterValue
one I-HyperparameterValue
at O
each O
decoding O
step O
. O
When O
there O
is O
an O
overlap O
between O
candidate O
tokens O
of O
ST B-TaskName
and O
MT B-TaskName
, O
the O
cascading O
effect O
will O
be O
reduced O
, O
thus O
slowing O
down O
the O
increase O
of O
the O
modality B-MetricName
gap I-MetricName
. O

Method O
: O
CRESS B-MethodName

Our O
preliminary O
studies O
in O
Section O
3 O
show O
that O
: O

• O
The O
modality B-MetricName
gap I-MetricName
will O
be O
enlarged O
during O
inference O
due O
to O
exposure O
bias O
. O

• O
The O
modality B-MetricName
gap I-MetricName
may O
be O
significant O
in O
some O
difficult O
cases O
. O

Inspired O
by O
these O
, O
we O
propose O
the O
Cross B-MethodName
- I-MethodName
modal I-MethodName
Regularization I-MethodName
with I-MethodName
Scheduled I-MethodName
Sampling I-MethodName
( O
CRESS B-MethodName
) O
method O
to O
bridge O
the O
modality O
gap O
, O
especially O
in O
inference O
mode O
( O
Section O
4.1 O
) O
. O
Furthermore O
, O
we O
propose O
a O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
method O
for O
CRESS B-MethodName
to O
handle O
difficult O
cases O
( O
Section O
4.2 O
) O
. O

Cross B-MethodName
- I-MethodName
modal I-MethodName
Regularization I-MethodName
with I-MethodName
Scheduled I-MethodName
Sampling I-MethodName
( O
CRESS B-MethodName
) O

To O
bridge O
the O
modality B-MetricName
gap I-MetricName
during O
inference O
, O
we O
adopt O
scheduled B-MethodName
sampling I-MethodName
for O
both O
ST B-TaskName
and O
MT B-TaskName
to O
approximate O
the O
inference O
mode O
at O
training O
time O
. O

After O
that O
, O
we O
add O
a O
regularization O
loss O
between O
the O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
based O
on O
the O
part O
of O
their O
self O
- O
generated O
words O
as O
context O
. O
This O
allows O
for O
more O
consistent O
predictions O
between O
ST B-TaskName
and O
MT B-TaskName
during O
inference O
, O
thus O
reducing O
the O
performance O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
. O
Figure O
3 O
illustrates O
the O
main O
framework O
of O
our O
method O
. O

Decoder O

( O
predict O
translation O
) O

speechỹ O
s O
i−1ỹ O
s O
i O
Decoder O
( O
select O
predicted O
words O
) O
speech O
y O
i−1 O
y O
î O
y O
s O
iŷ O
s O
i+1 O
p O
* O
1 O
− O
p O
* O
. O
. O
. O
. O
. O
. O
p O
( O
y O
i O
| O
s O
, O
ỹ O
s O
< O
i O
) O
p O
( O
y O
i+1 O
| O
s O
, O
ỹ O
s O
< O
i+1 O
) O
. O
. O
. O
. O
. O
. O

Decoder O

( O
predict O
translation O
) O

textỹ O
x O
i−1ỹ O
x O
i O
Decoder O
( O
select O
predicted O
words O
) O
text O
y O
i−1 O
y O
î O
y O
x O
iŷ O
x O
i+1 O
1 O
− O
p O
* O
. O
. O
. O
. O
. O
. O
p O
( O
y O
i O
| O
x O
, O
ỹ O
x O
< O
i O
) O
p O
( O
y O
i+1 O
| O
x O
, O
ỹ O
x O
< O
i+1 O
) O
. O
. O
. O
. O
. O
. O

D O
KL O
D O
KL O

Speech B-TaskName
Translation I-TaskName
( O
ST B-TaskName
) O
Text B-TaskName
Translation I-TaskName
( O
MT B-TaskName
) O

1 O
. O
Ground O
Truth O
Scheduled B-MethodName
Sampling I-MethodName
Scheduled B-MethodName
sampling I-MethodName
( O
Bengio O
et O
al O
. O
, O
2015 O
) O
, O
which O
samples O
between O
ground O
truth O
words O
and O
self O
- O
generated O
words O
, O
i.e. O
, O
predicted O
words O
, O
with O
a O
certain O
probability O
as O
targetside O
context O
, O
has O
proven O
helpful O
in O
alleviating O
exposure O
bias O
. O
In O
general O
, O
the O
input O
at O
the O
{ O
i O
+ O
1 O
} O
-th O
decoding O
step O
should O
be O
the O
ground O
truth O
word O
y O
i O
during O
training O
. O
With O
scheduled O
sampling O
, O
it O
can O
also O
be O
substituted O
by O
a O
predicted O
word O
. O
Next O
, O
we O
describe O
how O
to O
select O
the O
predicted O
word O
y O
s O
i O
for O
ST B-TaskName
and O
y O
x O
i O
for O
MT B-TaskName
. O
For O
ST B-TaskName
, O
we O
follow O
to O
select O
the O
predicted O
word O
y O
s O
i O
by O
sampling O
from O
the O
word O
distribution O
p O
( O
y O
i O
|s O
, O
y O
< O
i O
) O
in O
Equation O
( O
2 O
) O
with O
Gumbel O
- O
Max O
technique O
( O
Gumbel O
, O
1954 O
; O
Maddison O
et O
al O
. O
, O
2014 O
) O
, O
a O
method O
to O
draw O
a O
sample O
from O
a O
categorical O
distribution O
: O

η O
= O
− O
log O
( O
− O
log O
u O
) O
, O
( O
7 O
) O

y O
s O
i O
= O
arg O
max O
( O
W O
• O
f O
( O
s O
, O
y O
< O
i O
) O
+ O
η O
) O
, O
( O
8 O

where O
η O
is O
the O
Gumbel O
noise O
calculated O
from O
the O
uniform O
noise O
u O
∼ O
U O
( O
0 O
, O
1 O
) O
. O
Similarly O
, O
for O
MT B-TaskName
, O
there O
is O
: O

y O
x O
i O
= O
arg O
max O
( O
W O
• O
f O
( O
x O
, O
y O
< O
i O
) O
+ O
η O
) O
. O
( O
9 O
) O

Note O
that O
we O
may O
omit O
the O
superscript O
and O
denote O
the O
predicted O
word O
for O
both O
ST B-TaskName
and O
MT B-TaskName
by O
y O
i O
in O
the O
following O
. O

How O
to O
select O
between O
the O
ground O
truth O
word O
y O
i O
and O
the O
predicted O
word O
y O
i O
? O
Similar O
to O
Bengio O
et O
al O
. O
( O
2015 O
) O
; O
, O
we O
randomly O
sample O
from O
both O
with O
a O
varying O
probability O
. O
We O
denote O
the O
probability O
of O
selecting O
from O
the O
ground O
truth O
word O
as O
p O
* O
. O
At O
the O
beginning O
of O
training O
, O
since O
the O
model O
is O
not O
yet O
well O
trained O
, O
we O
select O
more O
from O
the O
ground O
truth O
words O
( O
with O
larger O
p O
* O
) O
to O
help O
the O
model O
converge O
. O
In O
the O
later O
stages O
of O
training O
, O
we O
select O
more O
from O
the O
predicted O
words O
( O
with O
smaller O
p O
* O
) O
, O
which O
is O
closer O
to O
the O
situation O
during O
inference O
. O
To O
achieve O
this O
, O
we O
decrease O
p O
* O
with O
a O
function O
of O
the O
index O
of O
training O
epochs O
e O
: O

p O
* O
= O
µ B-HyperparameterName
µ B-HyperparameterName
+ O
exp O
( O
e O
/ O
µ B-HyperparameterName
) O
, O
( O
10 O
) O

where O
µ B-HyperparameterName
is O
a O
hyper O
- O
parameter O
. O
With O
scheduled B-MethodName
sampling I-MethodName
, O
the O
target O
- O
side O
context O
becomes O
y O
= O
( O
y O
1 O
, O
... O
, O
y O
|y| O
) O
, O
where O

y O
i O
= O
y O
i O
, O
p O
≤ O
p O
* O
y O
i O
, O
p O
> O
p O
* O
, O
( O
11 O

where O
p O
is O
sampled O
from O
the O
uniform O
distribution O
U O
( O
0 O
, O
1 O
) O
. O
Using O
y O
s O
and O
y O
x O
to O
denote O
the O
targetside O
context O
of O
ST B-TaskName
and O
MT B-TaskName
respectively O
, O
the O
loss O
functions O
of O
ST B-TaskName
and O
MT B-TaskName
become O
: O

L O
CRESS B-MethodName
ST O
= O
− O
|y| O
i=1 O
log O
p O
( O
y O
i O
|s O
, O
y O
s O
< O
i O
) O
, O
( O
12 O
) O

L O
CRESS B-MethodName
MT O
= O
− O
|y| O
i=1 O
log O
p O
( O
y O
i O
|x O
, O
y O
x O
< O
i O
) O
, O
( O
13 O
) O

Cross O
- O
modal O
Regularization O
To O
bridge O
the O
modality O
gap O
in O
inference O
mode O
, O
we O
expect O
the O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
with O
scheduled B-MethodName
sampling I-MethodName
to O
be O
consistent O
. O
Inspired O
by O
recent O
works O
of O
consistency O
training O
Guo O
et O
al O
. O
, O
2022 O
) O
, O
we O
regularize O
ST B-TaskName
and O
MT B-TaskName
in O
the O
output O
space O
. O
Specifically O
, O
we O
minimize O
the O
bidirectional O
Kullback O
- O
Leibler O
( O
KL O
) O
divergence O
between O
the O
output O
distributions O
of O
ST B-TaskName
and O
MT B-TaskName
at O
each O
step O
: O

L O
CRESS B-MethodName
Reg O
= O
|y| O
i=1 O
1 O
2 O
( O
D O
KL O
( O
p O
( O
y O
i O
|s O
, O
y O
s O
< O
i O
) O
∥p O
( O
y O
i O
|x O
, O
y O
x O
< O
i O
) O
) O
+ O
D O
KL O
( O
p O
( O
y O
i O
|x O
, O
y O
x O
< O
i O
) O
∥p O
( O
y O
i O
|s O
, O
y O
s O
< O
i O
) O
) O
) O
. O
( O
14 O

With O
the O
translation O
loss O
in O
Equation O
( O
12 O
) O
and O
( O
13 O
) O
, O
the O
final O
training O
objective O
is O
: O

L O
CRESS B-MethodName
= O
L O
CRESS B-MethodName
ST O
+ O
L O
CRESS B-MethodName
MT O
+ O
λL O
CRESS B-MethodName
Reg O
, O
( O
15 O

where O
λ B-HyperparameterName
is O
the O
hyper O
- O
parameter O
to O
control O
the O
weight O
of O
L O
CRESS B-MethodName
Reg O
. O

Token O
- O
level O
Adaptive O
Training O
for O
CRESS B-MethodName

As O
mentioned O
above O
, O
the O
modality B-MetricName
gap I-MetricName
might O
be O
significant O
in O
some O
difficult O
cases O
. O
Inspired O
by O
the O
idea O
of O
token O
- O
level O
adaptive O
training O
Zhang O
et O
al O
. O
, O
2022a O
) O
, O
we O
propose O
to O
treat O
each O
token O
adaptively O
according O
to O
the O
scale O
of O
the O
modality B-MetricName
gap I-MetricName
. O
The O
training O
objectives O
in O
Equation O
( O
12 O
) O
, O
( O
13 O
) O
, O
and O
( O
14 O
) O
are O
modified O
as O
follows O
: O

L O
CRESS B-MethodName
ST O
= O
− O
|y| O
i=1 O
w O
i O
• O
log O
p O
( O
y O
i O
|s O
, O
y O
s O
< O
i O
) O
, O
( O
16 O
) O

L O
CRESS B-MethodName
MT O
= O
− O
|y| O
i=1 O
w O
i O
• O
log O
p O
( O
y O
i O
|x O
, O
y O
x O
< O
i O
) O
, O
( O
17 O
) O

L O
CRESS B-MethodName
Reg O
= O
|y| O
i=1 O
1 O
2 O
w O
i O
( O
D O
KL O
( O
p O
( O
y O
i O
|s O
, O
y O
s O
< O
i O
) O
∥p O
( O
y O
i O
|x O
, O
y O
x O
< O
i O
) O
) O
+ O
D O
KL O
( O
p O
( O
y O
i O
|x O
, O
y O
x O
< O
i O
) O
∥p O
( O
y O
i O
|s O
, O
y O
s O
< O
i O
) O
) O
) O
, O
( O
18 O
) O

where O
w O
i O
is O
the O
token O
- O
level O
weight O
defined O
by O
a O
linear O
function O
of O
the O
modality B-MetricName
gap I-MetricName
: O

w O
i O
= O
B B-HyperparameterName
+ O
S B-HyperparameterName
• O
G O
( O
s O
, O
y O
s O
< O
i O
∥x O
, O
y O
x O
< O
i O
) O
, O
( O
19 O
) O

where O
B B-HyperparameterName
( O
base B-HyperparameterName
) O
and O
S B-HyperparameterName
( O
scale B-HyperparameterName
) O
are O
hyper O
- O
parameters O
to O
control O
the O
lower O
bound O
and O
magnitude O
of O
change O
of O
w O
i O
. O
In O
this O
way O
, O
cases O
with O
a O
large O
modality B-MetricName
gap I-MetricName
will O
be O
assigned O
a O
larger O
weight O
and O
thus O
emphasized O
during O
training O
. O
Note O
that O
the O
modality B-MetricName
gap I-MetricName
is O
computed O
on O
- O
the O
- O
fly O
during O
training O
. O
External O
MT B-TaskName
Datasets O
We O
also O
introduce O
external O
MT B-TaskName
datasets O
to O
pre O
- O
train O
our O
translation O
model O
in O
the O
expanded O
setting O
. O
For O
En→De O
/ O
Fr O
/ O
Es O
/ O
Ro O
/ O
Ru O
directions O
, O
we O
introduce O
data O
from O
WMT B-DatasetName
( O
Buck O
and O
Koehn O
, O
2016 O
) O
. O
For O
En→It O
/ O
Pt O
/ O
Nl O
, O
we O
introduce O
data O
from O
OPUS100 B-DatasetName
2 I-DatasetName
. O
Table O
4 O
in O
Appendix O
A O
lists O
the O
statistics O
of O
all O
datasets O
. O

Experimental O
Setups O

Pre O
- O
processing O
For O
speech O
input O
, O
we O
use O
the O
raw O
16 O
- O
bit O
16kHz O
mono O
- O
channel O
audio O
wave O
. O
For O
text O
input O
, O
all O
sentences O
in O
ST B-TaskName
and O
external O
MT B-TaskName
datasets O
are O
tokenized O
and O
segmented O
into O
subwords O
using O
SentencePiece O
3 O
. O
For O
each O
translation O
direction O
, O
the O
vocabulary O
is O
learned O
from O
the O
source O
and O
target O
texts O
from O
the O
ST B-TaskName
dataset O
, O
with O
a O
size O
of O
10K. B-HyperparameterValue
For O
the O
external O
MT O
datasets O
, O
we O
filter O
out O
parallel O
sentence O
pairs O
whose O
length B-HyperparameterName
ratio I-HyperparameterName
exceeds O
1.5 B-HyperparameterValue
. O
is O
set O
to O
1e-4 B-HyperparameterValue
. O
We O
use O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
4k B-HyperparameterValue
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
steps I-HyperparameterName
. O
We O
set O
dropout B-HyperparameterName
to O
0.1 B-HyperparameterValue
and O
label B-HyperparameterName
smoothing I-HyperparameterName
to O
0.1 B-HyperparameterValue
. O
The O
training O
will O
early O
stop O
if O
the O
BLEU B-MetricName
score I-MetricName
on O
the O
dev O
set O
did O
not O
increase O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
. O
During O
inference O
, O
we O
average O
the O
checkpoints O
of O
the O
last O
10 B-HyperparameterValue
epochs B-HyperparameterName
for O
evaluation O
. O
We O
use O
beam O
search O
with O
a O
beam B-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
. O
The O
length B-HyperparameterName
penalty I-HyperparameterName
is O
set O
to O
1.2 B-HyperparameterValue
, O
1.8 B-HyperparameterValue
, O
0.6 B-HyperparameterValue
, O
1.4 B-HyperparameterValue
, O
0.8 B-HyperparameterValue
, O
1.0 B-HyperparameterValue
, O
1.4 B-HyperparameterValue
, O
and O
1.0 B-HyperparameterValue
for O
En→De O
, O
Fr O
, O
Es O
, O
Ro O
, O
Ru O
, O
It O
, O
Pt O
and O
Nl O
, O
respectively O
. O
We O
use O
scareBLEU B-MetricName
5 O
( O
Post O
, O
2018 O
) O
to O
compute O
case O
- O
sensitive O
detokenized O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
scores O
and O
the O
statistical O
significance O
of O
translation O
results O
with O
paired O
bootstrap O
resampling O
6 O
( O
Koehn O
, O
2004 O
) O
. O
We O
implement O
our O
model O
with O
fairseq O
7 O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O
All O
models O
are O
trained O
on O
4 O
Nvidia O
RTX O
3090 O
GPUs O
. O

For O
scheduled B-MethodName
sampling I-MethodName
, O
the O
decay B-HyperparameterName
parameter I-HyperparameterName
is O
set O
to O
µ B-HyperparameterName
= O
15 B-HyperparameterValue
. O
For O
cross B-MethodName
- I-MethodName
modal I-MethodName
regularization I-MethodName
, O
the O
weight B-HyperparameterName
parameter I-HyperparameterName
is O
set O
to O
λ B-HyperparameterName
= O
1.0 B-HyperparameterValue
. O
For O
tokenlevel B-MethodName
adaptive I-MethodName
training I-MethodName
, O
we O
did O
a O
grid O
search O
for O
base B-HyperparameterName
and O
scale B-HyperparameterName
parameters O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
En→De I-DatasetName
dev O
set O
with O
B B-HyperparameterName
∈ O
{ O
0.6 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
, O
0.8 B-HyperparameterValue
, O
0.9 B-HyperparameterValue
, O
1.0 B-HyperparameterValue
} O
and O
S B-HyperparameterName
∈ O
{ O
0.05 B-HyperparameterValue
, O
0.10 B-HyperparameterValue
, O
0.20 B-HyperparameterValue
, O
0.50 B-HyperparameterValue
, O
1.00 B-HyperparameterValue
} O
. O
Finally O
, O
we O
set O
B B-HyperparameterName
= O
0.7 B-HyperparameterValue
and O
S B-HyperparameterName
= O
0.05 B-HyperparameterValue
for O
all O
translation O
directions O
. O
We O
start O
applying O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
after O
the O
20th B-HyperparameterValue
epoch B-HyperparameterName
during O
training O
. O

Baseline O
Systems O
We O
include O
several O
strong O
endto B-TaskName
- I-TaskName
end I-TaskName
ST I-TaskName
models O
for O
comparison O
: O
Chimera B-MethodName
, O
XSTNet B-MethodName
, O
STEMM B-MethodName
( O
Fang O
et O
al O
. O
, O
2022 O
) O
, O
ConST B-MethodName
, O
STPT B-MethodName
( O
Tang O
et O
al O
. O
, O
2022 O
) O
, O
and O
SpeechUT B-MethodName
( O
Zhang O
et O
al O
. O
, O
2022b O
) O
. O
Besides O
, O
the O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
baseline O
in O
Section O
2.2 O
is O
also O
included O
as O
a O
strong O
baseline O
, O
which O
is O
denoted O
as O
MTL B-MethodName
. O
We O
use O
CRESS B-MethodName
to O
denote O
our O
method O
with O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
. O
Among O
these O
models O
, O
Chimera B-MethodName
, O
XSTNet B-MethodName
, O
STEMM B-MethodName
, O
and O
ConST B-MethodName
combine O
pre O
- O
trained O
Wav2vec B-MethodName
2.0 I-MethodName
( O
Baevski O
et O
al O
. O
, O
2020 O
) O
and O
pre O
- O
trained O
translation O
model O
together O
, O
and O
then O
fine O
- O
tune O
the O
whole O
model O
on O
ST B-TaskName
datasets O
. O
Our O
implemented O
MTL B-MethodName
and O
CRESS B-MethodName
follow O
a O
similar O
design O
, O
but O
we O
use O
HuBERT B-MethodName
instead O
of O
Wav2vec B-MethodName
2.0 I-MethodName
as O
we O
find O
HuBERT B-MethodName
gives O
a O
stronger O
baseline O
( O
See O
Table O
5 O
for O
details O
) O
. O
STPT B-MethodName
and O
SpeechUT B-MethodName
jointly O
pre O
- O
train O
the O
model O
on O
speech O
and O
text O
data O
from O
scratch O
, O
which O
achieve O
better O
performance O
but O
also O
bring O
higher O
training O
costs O
8 O
. O

Main O
Results O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
Dataset O

Table O
1 O
shows O
the O
results O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
tst O
- O
COMMON O
set O
in O
all O
eight O
directions O
. O
First O
, O
we O
find O
that O
our O
implemented O
MTL B-MethodName
is O
a O
strong O
baseline O
compared O
with O
existing O
approaches O
. O
Second O
, O
our O
proposed O
CRESS B-MethodName
significantly O
outperforms O
MTL B-MethodName
in O
both O
settings O
, O
with O
1.8 B-MetricValue
BLEU B-MetricName
improvement O
in O
the O
base O
set- O
ting O
and O
1.3 B-MetricValue
BLEU B-MetricName
improvement O
in O
the O
expanded O
setting O
on O
average O
, O
demonstrating O
the O
superiority O
of O
our O
approach O
. O
Besides O
, O
we O
report O
ChrF++ B-MetricName
scores O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
in O
Appendix O
E O
, O
and O
we O
also O
provide O
results O
on O
CoVoST B-DatasetName
2 I-DatasetName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
En→De O
dataset O
in O
Appendix O
C O
. O

Analysis O
and O
Discussion O

Results O
in O
Section O
5.3 O
show O
the O
superiority O
of O
our O
method O
. O
To O
better O
understand O
CRESS B-MethodName
, O
we O
explore O
several O
questions O
in O
this O
section O
. O
All O
analysis O
experiments O
are O
conducted O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
En→De I-DatasetName
dataset O
in O
the O
expanded O
setting O
. O

( O
1 O
) O
Do O
scheduled B-MethodName
sampling I-MethodName
, O
cross B-MethodName
- I-MethodName
modal I-MethodName
regularization I-MethodName
, O
and O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
all O
matter O
? O
Scheduled B-MethodName
sampling I-MethodName
, O
regularization B-MethodName
, O
and O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
are O
effective O
techniques O
to O
improve O
the O
performance O
of O
translation O
models O
. O
To O
understand O
the O
role O
of O
each O
, O
we O
conduct O
ablation O
experiments O
in O
Table O
2 O
. O
When O
only O
applying O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
( O
# O
5 O
) O
, O
we O
observe O
a O
performance O
decline O
of O
0.2 B-MetricValue
BLEU B-MetricName
since O
only O
adaptive B-MethodName
training I-MethodName
can O
not O
bridge O
the O
modality B-MetricName
gap I-MetricName
. O
When O
training O
with O
scheduled B-MethodName
sampling I-MethodName
only O
( O
# O
4 O
) O
, O
we O
observe O
a O
slight O
improvement O
of O
0.3 B-MetricValue
BLEU B-MetricName
, O
probably O
due O
to O
the O
alleviation O
of O
exposure O
bias O
. O
When O
training O
with O
cross B-MethodName
- I-MethodName
modal I-MethodName
regularization I-MethodName
only O
( O
# O
3 O
) O
, O
which O
encourages O
the O
consistency O
between O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
with O
ground O
truth O
target O
contexts O
, O
we O
observe O
an O
improvement O
of O
0.7 B-MetricValue
BLEU B-MetricName
. O
If O
we O
combine O
both O
( O
# O
2 O
) O
, O
we O
obtain O
a O
much O
more O
significant O
boost O
of O
1.3 B-MetricValue
BLEU B-MetricName
, O
proving O
that O
both O
scheduled B-MethodName
sampling I-MethodName
and O
cross B-MethodName
- I-MethodName
modal I-MethodName
regularization I-MethodName
play O
a O
crucial O
role O
in O
our O
method O
. O
Furthermore O
, O
with O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
( O
# O
1 O
) O
, O
the O
improvement O
comes O
to O
1.7 B-MetricValue
BLEU B-MetricName
, O
which O
shows O
the O
benefit O
of O
treating O
different O
tokens O
differently O
according O
to O
the O
modality B-MetricName
gap I-MetricName
. O

( O
2 O
) O
Does O
CRESS B-MethodName
successfully O
bridge O
the O
modal- B-MetricName
ity I-MetricName
gap I-MetricName
? O
To O
validate O
whether O
our O
approach O
successfully O
bridges O
the O
modality B-MetricName
gap I-MetricName
between O
ST B-TaskName
and O
MT B-TaskName
, O
we O
revisit O
the O
experiments O
in O
Section O
3 O
. O
Figure O
4 O
shows O
the O
distribution O
of O
the O
modality B-MetricName
gap I-MetricName
with O
teacher B-MethodName
forcing I-MethodName
. O
We O
observe O
a O
general O
decrease O
in O
the O
modality B-MetricName
gap I-MetricName
compared O
with O
MTL B-MethodName
. O
We O
also O
plot O
the O
curves O
of O
the O
modality B-MetricName
gap I-MetricName
with O
decoding O
steps O
of O
CRESS B-MethodName
under O
teacher B-MethodName
forcing I-MethodName
, O
greedy B-MethodName
search I-MethodName
, O
and O
beam B-MethodName
search I-MethodName
strategies O
. O
As O
shown O
in O
Figure O
5 O
, O
our O
approach O
significantly O
slows O
down O
the O
increase O
of O
the O
modality B-MetricName
gap I-MetricName
compared O
with O
MTL B-MethodName
baseline O
, O
suggesting O
that O
the O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
are O
more O
consistent O
during O
inference O
, O
demonstrating O
the O
effectiveness O
of O
our O
method O
in O
bridging O
the O
modality B-MetricName
gap I-MetricName
. O

( O
in O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
. O
We O
investigate O
how O
different O
combinations O
of O
B B-HyperparameterName
and O
S B-HyperparameterName
influence O
performance O
. O
As O
shown O
in O
Figure O
6 O
, O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
can O
bring O
improvements O
in O
most O
cases O
. O
In O
particular O
, O
it O
usually O
performs O
better O
with O
smaller O
B B-HyperparameterName
and O
smaller O
S B-HyperparameterName
, O
leading O
to O
a O
boost O
of O
up O
to O
0.4 B-MetricValue
BLEU B-MetricName
. O
We O
conclude O
that O
treating O
different O
tokens O
too O
differently O
is O
also O
undesirable O
. O
We O
use O
B B-HyperparameterName
= O
0.7 B-HyperparameterValue
and O
S B-HyperparameterName
= O
0.05 B-HyperparameterValue
for O
all O
translation O
directions O
. O

( O
4 O
) O
Does O
CRESS B-MethodName
successfully O
reduce O
the O
performance O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
? O
As O
shown O
in O
Table O
3 O
, O
our O
method O
not O
only O
brings O
improvements O
to O
ST B-TaskName
, O
but O
also O
gives O
a O
slight O
average O
boost O
of O
0.3 B-MetricValue
BLEU B-MetricName
to O
MT B-TaskName
. O
We O
suggest O
that O
this O
may O
be O
due O
to O
the O
effect O
of O
regularization O
. O
More O
importantly O
, O
we O
find O
that O
the O
performance O
gap O
between O
ST B-TaskName
and O
MT B-TaskName
for O
CRESS B-MethodName
is O
significantly O
reduced O
compared O
to O
the O
MTL B-MethodName
baseline O
( O
6.0→5.0 O
) O
, O
which O
further O
demonstrates O
that O
the O
improvement O
in O
ST B-TaskName
is O
mainly O
due O
to O
the O
effective O
reduction O
of O
the O
modality B-MetricName
gap I-MetricName
. O

( O
5 O
) O
Is O
CRESS B-MethodName
more O
effective O
for O
longer O
sentences O
? O
The O
autoregressive O
model O
generates O
the O
translation O
step O
by O
step O
, O
making O
the O
translation O
of O
long O
sentences O
more O
challenging O
. O
We O
divide O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
En→De I-DatasetName
dev O
set O
into O
several O
groups O
according O
to O
the O
length O
of O
target O
sentences O
, O
and O
compute O
the O
BLEU B-MetricName
scores O
in O
each O
group O
separately O
, O
as O
shown O
in O
Figure O
7 O
. O
We O
observe O
that O
CRESS B-MethodName
achieve O
significant O
improvements O
over O
the O
baseline O
in O
all O
groups O
, O
especially O
for O
sentences O
longer O
than O
45 B-HyperparameterValue
, O
which O
shows O
the O
superiority O
of O
our O
method O
when O
translating O
long O
sentences O
. O

( O
6 O
) O
How O
the O
decay B-HyperparameterName
parameter O
in O
scheduled B-MethodName
sampling I-MethodName
influence O
the O
performance O
? O
In O
scheduled B-MethodName
sampling I-MethodName
, O
the O
probability O
of O
selecting O
the O
ground O
truth O
word O
p O
* O
keeps O
decreasing O
during O
training O
as O
the O
function O
in O
Equation O
( O
10 O
) O
. O
Here O
, O
the O
hyper O
- O
parameter O
µ B-HyperparameterName
is O
used O
to O
control O
the O
shape O
of O
the O
function O
. O
As O
µ B-HyperparameterName
increases O
, O
the O
probability O
p O
* O
decreases O
more O
slowly O
, O
and O
vice O
versa O
. O
We O
investigate O
the O
impact O
of O
µ B-HyperparameterName
in O
Figure O
8 O
, O
and O
find O
that O
( O
1 O
) O
the O
model O
performs O
worse O
when O
p O
* O
drops O
too O
quickly O
, O
and O
( O
2 O
) O
when O
µ B-HyperparameterName
is O
within O
a O
reasonable O
range O
, O
there O
is O
not O
much O
impact O
on O
the O
final O
BLEU B-MetricName
score O
. O
We O
use O
µ B-HyperparameterName
= O
15 B-HyperparameterValue
in O
our O
experiments O
. O

Related O
Work O

End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
Speech I-TaskName
Translation I-TaskName
End B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
end I-TaskName
speech I-TaskName
translation I-TaskName
( O
Bérard O
et O
al O
. O
, O
2016 O
; O
Weiss O
et O
al O
. O
, O
2017 O
) O
( O
Bansal O
et O
al O
. O
, O
2019 O
; O
Stoian O
et O
al O
. O
, O
2020 O
; O
Wang O
et O
al O
. O
, O
2020b O
, O
c O
; O
Alinejad O
and O
Sarkar O
, O
2020 O
; O
Le O
et O
al O
. O
, O
2021 O
; O
Dong O
et O
al O
. O
, O
2021a O
; O
Zheng O
et O
al O
. O
, O
2021 O
; O
Tang O
et O
al O
. O
, O
2022 O
) O
, O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
( O
Le O
et O
al O
. O
, O
2020 O
; O
Dong O
et O
al O
. O
, O
2021b O
; O
Tang O
et O
al O
. O
, O
2021a O
, O
b O
; O
Indurthi O
et O
al O
. O
, O
2021 O
) O
, O
knowledge O
distillation O
Inaguma O
et O
al O
. O
, O
2021 O
) O
, O
and O
data O
augmentation O
( O
Jia O
et O
al O
. O
, O
2019 O
; O
Bahar O
et O
al O
. O
, O
2019b O
; O
Lam O
et O
al O
. O
, O
2022 O
; O
. O
However O
, O
due O
to O
the O
modality B-MetricName
gap I-MetricName
between O
speech O
and O
text O
, O
it O
is O
still O
difficult O
to O
fully O
exploit O
MT B-TaskName
data O
with O
the O
above O
techniques O
. O
To O
overcome O
the O
modality B-MetricName
gap I-MetricName
, O
Exposure O
Bias O
Exposure O
bias O
indicates O
the O
discrepancy O
between O
training O
and O
inference O
. O
Several O
approaches O
employ O
Reinforcement O
Learning O
( O
RL O
) O
( O
Ranzato O
et O
al O
. O
, O
2016 O
; O
Shen O
et O
al O
. O
, O
2016 O
; O
Bahdanau O
et O
al O
. O
, O
2017 O
) O
instead O
of O
Maximum O
Likelihood O
Estimation O
( O
MLE O
) O
to O
avoid O
this O
problem O
. O
However O
, O
Wu O
et O
al O
. O
( O
2018 O
) O
shows O
that O
RL O
- O
based O
training O
is O
unsta O
- O
ble O
due O
to O
the O
high O
variance O
of O
gradient O
estimation O
. O
An O
alternative O
and O
simpler O
approach O
is O
scheduled B-MethodName
sampling I-MethodName
( O
Bengio O
et O
al O
. O
, O
2015 O
) O
, O
which O
samples O
between O
ground O
truth O
words O
and O
self O
- O
generated O
words O
with O
a O
changing O
probability O
. O
extends O
it O
with O
Gumbel O
noise O
for O
more O
robust O
training O
. O

In O
this O
paper O
, O
we O
adopt O
this O
approach O
to O
approximate O
the O
inference O
mode O
due O
to O
its O
training O
stability O
and O
low O
training O
cost O
. O

Output O
Regularization O
for O
MT B-TaskName
Regularization O
in O
the O
output O
space O
has O
proved O
useful O
for O
MT B-TaskName
. O
proposes O
to O
regularize O
the O
output O
predictions O
of O
two O
sub O
- O
models O
sampled O
by O
dropout O
. O
Guo O
et O
al O
. O
( O
2022 O
) O
regularizes O
the O
output O
predictions O
of O
models O
before O
and O
after O
input O
perturbation O
. O
In O
this O
paper O
, O
we O
regularize O
the O
output O
predictions O
across O
modalities O
, O
which O
encourages O
more O
consistent O
predictions O
for O
ST B-TaskName
and O
MT B-TaskName
. O

Token B-MethodName
- I-MethodName
level I-MethodName
Adaptive I-MethodName
Training I-MethodName
Token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
for O
MT B-TaskName
is O
first O
proposed O
in O
, O
which O
assigns O
larger O
weights O
to O
lowfrequency O
words O
to O
prevent O
them O
from O
being O
ignored O
. O
; O
Zhang O
et O
al O
. O
( O
2022a O
) O
computes O
the O
weight O
with O
bilingual O
mutual O
information O
. O
In O
this O
paper O
, O
we O
compute O
the O
weights O
with O
the O
modality B-MetricName
gap I-MetricName
between O
ST B-TaskName
and O
MT B-TaskName
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
a O
simple O
yet O
effective O
method O
CRESS B-MethodName
to O
regularize O
the O
model O
predictions O
of O
ST B-TaskName
and O
MT B-TaskName
, O
whose O
target O
- O
side O
contexts O
contain O
both O
ground O
truth O
words O
and O
self O
- O
generated O
words O
with O
scheduled B-MethodName
sampling I-MethodName
. O
Based O
on O
this O
, O
we O
further O
propose O
a O
token B-MethodName
- I-MethodName
level I-MethodName
adaptive I-MethodName
training I-MethodName
method O
to O
handle O
difficult O
cases O
. O
Our O
method O
achieves O
promising O
results O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
benchmark O
. O
Further O
analysis O
shows O
that O
our O
method O
can O
effectively O
bridge O
the O
modality B-MetricName
gap I-MetricName
and O
improve O
the O
translation O
quality O
, O
especially O
for O
long O
sentences O
. O
In O
the O
future O
, O
we O
will O
explore O
how O
to O
apply O
our O
method O
to O
other O
tasks O
. O

Limitations O

Although O
our O
proposed O
method O
achieves O
promising O
results O
and O
outperforms O
most O
baseline O
systems O
on O
the O
ST B-TaskName
benchmark O
, O
it O
still O
has O
some O
limitations O
: O
( O
1 O
) O
the O
performance O
of O
our O
method O
still O
lags O
behind O
a O
recent O
work O
SpeechUT B-MethodName
, O
although O
our O
approach O
has O
the O
advantage O
of O
consuming O
less O
time O
and O
resources O
; O

( O
2 O
) O
we O
observe O
that O
the O
modality B-MetricName
gap I-MetricName
is O
still O
not O
eliminated O
and O
the O
effect O
of O
exposure O
bias O
on O
the O
modality B-MetricName
gap I-MetricName
still O
exists O
; O

( O
3 O
) O
the O
performance O
of O
our O
approach O
on O
larger O
datasets O
and O
larger O
models O
remains O
to O
be O
explored O
; O
( O
4 O
) O
how O
to O
apply O
our O
approach O
to O
other O
tasks O
also O
needs O
to O
be O
further O
investigated O
. O

Ethics O
Statement O

In O
this O
paper O
, O
we O
present O
an O
effective O
method O
CRESS B-MethodName
for O
speech B-TaskName
translation I-TaskName
. O
While O
our O
model O
achieves O
superior O
performance O
on O
the O
widely O
used O
ST B-TaskName
benchmark O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
, O
applying O
it O
directly O
to O
real O
scenarios O
is O
still O
risky O
. O
This O
is O
due O
to O
the O
fact O
that O
our O
training O
corpus O
only O
contains O
hundreds O
of O
hours O
of O
audio O
recordings O
from O
TED O
talks O
, O
which O
is O
far O
from O
covering O
all O
domains O
of O
the O
real O
world O
. O
Besides O
, O
the O
datasets O
we O
used O
in O
this O
paper O
( O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
, O
WMT B-DatasetName
, O
and O
OPUS-100 B-DatasetName
) O
are O
all O
publicly O
available O
. O
We O
also O
release O
the O
code O
implemented O
with O
a O
wellknown O
framework O
fairseq O
. O
These O
guarantee O
the O
reproducibility O
of O
our O
work O
. O

B O
Impact O
of O
Different O
Acoustic O
Encoders O

Our O
model O
is O
composed O
of O
an O
acoustic O
encoder O
and O
a O
translation O
model O
. O
To O
investigate O
the O
impact O
of O
different O
acoustic O
encoders O
, O
we O
also O
conduct O
experiments O
using O
Wav2vec B-MethodName
2.0 I-MethodName
9 O
( O
Baevski O
et O
al O
. O
, O
2020 O
) O
as O
the O
acoustic O
encoder O
. O
As O
shown O
in O
Table O
5 O
, O
we O
find O
that O
( O
1 O
) O
HuBERT B-MethodName
performs O
slightly O
better O
than O
Wav2vec B-MethodName
2.0 I-MethodName
with O
an O
improvement O
of O
0.5 B-MetricValue
BLEU B-MetricName
, O
and O
( O
2 O
) O
our O
proposed O
CRESS B-MethodName
achieves O
consistent O
improvements O
with O
different O
acoustic O
encoders O
. O
In O
practice O
, O
we O
use O
HuBERT B-MethodName
to O
build O
our O
systems O
, O
since O
we O
believe O
that O
developing O
on O
a O
strong O
baseline O
will O
make O
our O
results O
more O
convincing O
and O
demonstrate O
the O
robustness O
of O
our O
approach O
. O

Acoustic O
Encoder O
MTL B-MethodName
CRESS B-MethodName

HuBERT B-MethodName
( O
Hsu O
et O
al O
. O
, O
2021 O
) O
27.5 O
29.4 O
Wav2vec B-MethodName
2.0 I-MethodName
( O
Baevski O
et O
al O
. O
, O
2020 O
) O
27.0 O
28.9 O

C O
Results O
on O
CoVoST B-DatasetName
2 I-DatasetName
En→De I-DatasetName

We O
also O
conduct O
experiments O
on O
CoVoST B-DatasetName
2 I-DatasetName
( O
Wang O
et O
al O
. O
, O
2020a O
) O
to O
examine O
the O
performance O
of O
our O
approach O
on O
large O
datasets O
. O
CoVoST B-DatasetName
2 I-DatasetName
is O
a O
largescale O
multilingual B-TaskName
speech I-TaskName
translation I-TaskName
corpus O
that O
covers O
translations O
from O
21 O
languages O
into O
English O
and O
from O
English O
into O
15 O
languages O
. O
It O
is O
one O
of O
the O

D O
Discussion O
about O
the O
Training O
Speed O

During O
training O
, O
our O
approach O
requires O
an O
additional O
forward O
pass O
to O
select O
predicted O
words O
compared O
with O
the O
baseline O
, O
which O
will O
impair O
the O
training B-MetricName
speed I-MetricName
. O
Practically O
, O
we O
find O
the O
training O
time O
for O
1 O
epoch O
of O
CRESS B-MethodName
is O
1.12 B-MetricValue
times O
longer O
than O
MTL B-MethodName
, O
which O
is O
actually O
negligible O
. O
This O
is O
because O
the O
step O
of O
selecting O
predicted O
words O
is O
fully O
parallel O
and O
has O
no O
gradient O
calculation O
, O
which O
does O
not O
incur O
a O
significant O
time O
overhead O
. O

E O
ChrF++ B-MetricName
Scores O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
Dataset O

We O
also O
report O
ChrF++ B-MetricName
score O
( O
Popović O
, O
2017 O
) O
using O
sacreBLEU B-MetricName
toolkit O
10 O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
dataset O
in O
Table O
7 O
. O
We O
observe O
that O
CRESS B-MethodName
outperforms O
MTL B-MethodName
with O
1.4 B-MetricValue
ChrF++ B-MetricName
improvement O
in O
the O
base O
setting O
and O
1.0 B-MetricValue
ChrF++ B-MetricName
improvement O
in O
the O
expanded O
setting O
. O
B2 O
. O
Did O
you O
discuss O
the O
license O
or O
terms O
for O
use O
and O
/ O
or O
distribution O
of O
any O
artifacts O
? O

We O
just O
use O
for O
research O
purposes O
, O
no O
commercial O
use O
and O
no O
derivative O
works O
of O
the O
original O
data O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O

Our O
use O
of O
dataset O
and O
pre O
- O
trained O
models O
is O
consistent O
with O
their O
intended O
use O
, O
which O
does O
not O
require O
much O
discussion O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O

The O
data O
we O
used O
are O
publicly O
available O
on O
the O
website O
, O
and O
widely O
used O
in O
the O
research O
community O
. O
We O
can O
not O
change O
the O
training O
/ O
test O
data O
in O
order O
to O
be O
consistent O
with O
previous O
work O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Section O
5 O

B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Appendix O
C O
Did O
you O
run O
computational O
experiments O
? O

Section O
5 O
, O
Section O
6 O
, O
Appendix O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Section O
5 O
, O
Appendix O

Acknowledgements O

We O
thank O
all O
the O
anonymous O
reviewers O
for O
their O
insightful O
and O
valuable O
comments O
. O
This O
work O
was O
supported O
by O
National O
Key O
R O
& O
D O
Program O
of O
China O
( O
NO O
. O
2018AAA0102502 O
) O

Models O

Audio O
Datasets O
# O
Params O
BLEU B-MetricName

Continued B-MethodName
Pretraining I-MethodName
for O
Better O
Zero O
- O
and O
Few O
- O
Shot O
Promptability O

Recently O
introduced O
language O
model O
prompting O
methods O
can O
achieve O
high O
accuracy O
in O
zeroand O
few O
- O
shot O
settings O
while O
requiring O
few O
to O
no O
learned O
task O
- O
specific O
parameters O
. O
Nevertheless O
, O
these O
methods O
still O
often O
trail O
behind O
full O
model O
finetuning O
. O
In O
this O
work O
, O
we O
investigate O
if O
a O
dedicated O
continued B-MethodName
pretraining I-MethodName
stage O
could O
improve O
" O
promptability O
" O
, O
i.e. O
, O
zero O
- O
shot O
performance O
with O
natural O
language O
prompts O
or O
few O
- O
shot O
performance O
with O
prompt O
tuning O
. O
We O
reveal O
settings O
where O
existing O
continued O
pretraining O
methods O
lack O
promptability O
. O
We O
also O
identify O
current O
methodological O
gaps O
, O
which O
we O
fill O
with O
thorough O
large O
- O
scale O
experiments O
. O
We O
demonstrate O
that O
a O
simple O
recipe O
, O
continued B-MethodName
pretraining I-MethodName
that O
incorporates O
a O
trainable O
prompt O
during O
multi O
- O
task O
learning O
, O
leads O
to O
improved O
promptability O
in O
both O
zero O
- O
and O
fewshot O
settings O
compared O
to O
existing O
methods O
, O
up O
to O
31 B-MetricValue
% I-MetricValue
relative O
. O
On O
the O
other O
hand O
, O
we O
find O
that O
continued B-MethodName
pretraining I-MethodName
using O
MAML B-MethodName
- I-MethodName
style I-MethodName
metalearning I-MethodName
, O
a O
method O
that O
directly O
optimizes O
fewshot O
promptability O
, O
yields O
subpar O
performance O
. O
We O
validate O
our O
findings O
with O
two O
prompt O
tuning O
methods O
, O
and O
, O
based O
on O
our O
results O
, O
we O
provide O
concrete O
recommendations O
to O
optimize O
promptability O
for O
different O
use O
cases O
. O

Introduction O

Conditioning O
language O
models O
( O
LMs O
) O
on O
manuallywritten O
or O
learned O
continuous O
prompts O
allows O
them O
to O
solve O
tasks O
with O
high O
accuracy O
and O
minimal O
parameter O
overhead O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Li O
and O
Liang O
, O
2021 O
; O
Lester O
et O
al O
. O
, O
2021 O
, O
i.a O
. O
) O
. O
However O
, O
prompting O
performance O
often O
still O
lags O
behind O
traditional O
full O
finetuning O
. O
Natural O
language O
prompts O
usually O
underperform O
trained O
models O
even O
when O
manually O
curated O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
. O
Similarly O
, O
while O
learned O
prompts O
yield O
higher O
accuracy O
, O
This O
work O
was O
done O
when O
Zhaofeng O
Wu O
was O
at O
AI2 O
, O
and O
Robert O
Logan O
was O
at O
UCI O
. O

We O
release O
our O
code O
and O
models O
at O
https O
: O
/ O
/ O
github O
. O
com O
/ O
allenai O
/ O
better O
- O
promptability O
. O
they O
do O
not O
work O
as O
well O
when O
the O
training O
data O
is O
scarce O
, O
when O
the O
model O
is O
small O
or O
moderately O
sized O
( O
Lester O
et O
al O
. O
, O
2021 O
) O
, O
and O
when O
the O
tasks O
are O
difficult O
( O
He O
et O
al O
. O
, O
2022 O
) O
. O

To O
reduce O
the O
gap O
between O
prompt O
and O
full O
model O
tuning O
, O
past O
work O
has O
shown O
that O
continued B-MethodName
pretraining I-MethodName
on O
data O
that O
resembles O
the O
downstream O
prompting O
setup O
induces O
better O
" O
promptability O
" O
, O
i.e. O
, O
zero O
- O
shot O
performance O
with O
natural O
language O
( O
NL O
) O
prompts O
and O
few O
- O
shot O
performance O
of O
prompt O
tuning O
. O
However O
, O
in O
this O
paper O
, O
we O
identify O
several O
shortcomings O
of O
these O
methods O
. O
First O
, O
continued B-MethodName
pretraining I-MethodName
on O
NL O
prompts O
sometimes O
causes O
performance O
degradation O
with O
prompt O
tuning O
. O
Second O
, O
continued B-MethodName
pretraining I-MethodName
approaches O
that O
learn O
only O
a O
universal O
prompt O
initialization O
Vu O
et O
al O
. O
, O
2022 O
) O
bring O
only O
marginal O
improvement O
on O
the O
P3 B-DatasetName
datasets O
. O

To O
further O
improve O
zero O
- O
shot O
and O
few O
- O
shot O
promptability O
, O
we O
investigate O
gaps O
in O
existing O
methods O
with O
different O
parameter O
configurations O
and O
training O
procedures O
. O
First O
, O
we O
explore O
the O
effect O
of O
incorporating O
a O
learned O
continuous O
prompt O
into O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
( O
MTL B-MethodName
) O
, O
and O
find O
it O
to O
significantly O
improve O
zero O
- O
and O
few O
- O
shot O
promptability O
across O
the O
board O
. O
In O
addition O
, O
we O
explore O
MAMLstyle B-MethodName
meta I-MethodName
- I-MethodName
learning I-MethodName
( O
Finn O
et O
al O
. O
, O
2017 O
; O
Nichol O
et O
al O
. O
, O
2018 O
) O
as O
an O
alternative O
to O
the O
standard O
continued B-MethodName
pretraining I-MethodName
paradigm O
, O
but O
find O
that O
it O
underperforms O
simple O
MTL B-MethodName
, O
despite O
its O
previous O
success O
on O
fewshot O
learning O
tasks O
( O
Li O
et O
al O
. O
, O
2017 O
; O
Gu O
et O
al O
. O
, O
2018 O
; O
Qian O
and O
Yu O
, O
2019 O
, O
i.a O
. O
) O
. O
We O
perform O
an O
analysis O
of O
this O
phenomenon O
and O
present O
several O
explanations O
. O

Through O
large O
- O
scale O
experiments O
, O
each O
involving O
continued O
pretraining O
on O
over O
9B O
tokens O
( O
§ O
A O
) O
, O
we O
make O
several O
contributions O
: O
( O
1 O
) O
we O
thoroughly O
evaluate O
continued B-MethodName
pretraining I-MethodName
methods O
, O
both O
existing O
and O
our O
proposed O
ones O
, O
in O
many O
setups O
; O
( O
2 O
) O
we O
demonstrate O
that O
a O
simple O
continued B-MethodName
pretraining I-MethodName
recipe O
improves O
over O
existing O
methods O
by O
up O
to O
31 B-MetricValue
% I-MetricValue
; O
( O
3 O
) O
we O
show O
that O
MAML B-MethodName
- I-MethodName
style I-MethodName
meta I-MethodName
- I-MethodName
learning I-MethodName
underperforms O
multi B-MethodName
- I-MethodName
task I-MethodName
learning I-MethodName
and O
provide O
explanations O
; O
( O
4 O
) O
we O
provide O
concrete O
recommendations O
to O
improve O
promptability O
in O
various O
use O
cases O
. O

Prompting O

We O
review O
two O
types O
of O
prompting O
that O
we O
use O
: O
natural B-MethodName
language I-MethodName
( I-MethodName
NL I-MethodName
) I-MethodName
prompting I-MethodName
and O
prompt B-MethodName
tuning I-MethodName
. O

Traditionally O
, O
NLP O
tasks O
are O
solved O
by O
taskspecific O
models O
that O
predict O
label O
y O
∈ O
Y O
from O
input O
x O
∈ O
X O
. O
We O
can O
consider O
LMs O
as O
functions O
that O
score O
any O
source O
and O
target O
text O
pair O
, O
LM O
: O
V O
* O
× O
V O
* O
→ O
R O
with O
vocabulary O
V. O
1 O
Past O
work O
found O
that O
large O
LMs O
can O
be O
repurposed O
to O
solve O
many O
tasks O
by O
casting O
x O
, O
y O
into O
a O
text O
format O
using O
a O
template O
function O
f O
: O
X O
∪ O
Y O
→ O
V O
* O
and O
taking O
as O
prediction O
arg O
max O
y O
′ O
∈Y O
LM O
( O
f O
( O
x O
) O
, O
f O
( O
y O
′ O
) O
) O
. O

NL B-MethodName
prompts I-MethodName
, O
or O
instructions O
, O
are O
manually O
constructed O
f O
( O
• O
) O
. O
Without O
task O
- O
specific O
training O
, O
they O
have O
been O
successfully O
used O
to O
elicit O
predictions O
from O
LMs O
to O
perform O
tasks O
with O
high O
accuracy O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Logan O
IV O
et O
al O
. O
, O
2022 O
) O
. O

Sharing O
the O
motivation O
, O
prompt B-MethodName
tuning I-MethodName
learns O
a O
continuous O
prompt O
to O
condition O
the O
model O
. O
It O
takes O
the O
source O
text O
embedded O
by O
the O
LM O
input O
embeddings O
, O
s O
∈ O
R O
N O
×d O
with O
length O
N O
and O
dimension O
d O
, O
and O
prepends O
learnable O
embeddings O
E O
∈ O
R O
L×d B-HyperparameterName
, O
where O
L B-HyperparameterName
is O
a O
hyperparameter O
, O
to O
obtain O
a O
new O
( O
L B-HyperparameterName
+ O
N O
) O
-lengthed O
embedded O
sequence O
. O
We O
consider O
hybrid O
prompt B-MethodName
tuning I-MethodName
, O
where O
s O
is O
the O
embedding O
of O
the O
templatized O
f O
( O
x O
) O
, O
i.e. O
, O
prompt B-MethodName
tuning I-MethodName
is O
always O
performed O
in O
addition O
to O
NL O
templates O
. O
This O
has O
been O
widely O
adopted O
due O
to O
demonstrated O
better O
performance O
Min O
et O
al O
. O
, O
2022 O
) O
. O
We O
also O
study O
a O
variant O
of O
prompt B-MethodName
tuning I-MethodName
, O
sometimes O
called O
prefix B-MethodName
tuning I-MethodName
( O
Li O
and O
Liang O
, O
2021 O
) O
, O
where O
the O
learnable O
vectors O
are O
added O
not O
only O
to O
the O
input O
but O
all O
transformer O
layers O
. O
See O
Lester O
et O
al O
. O
( O
2021 O
) O
and O
Li O
and O
Liang O
( O
2021 O
) O
for O
more O
details O
on O
these O
methods O
. O
Following O
the O
terminology O
of O
Liu O
et O
al O
. O
( O
2022b O
) O
, O
we O
refer O
to O
the O
input O
- O
level O
method O
as O
shallow B-MethodName
prompt I-MethodName
tuning I-MethodName
and O
the O
layer O
- O
specific O
method O
as O
deep B-MethodName
prompt I-MethodName
tuning I-MethodName
. O

Improving O
Promptability O

In O
this O
section O
, O
we O
describe O
existing O
methods O
to O
improve O
promptability O
and O
a O
new O
paradigm O
that O
combines O
their O
advantages O
. O

While O
prompt B-MethodName
tuning I-MethodName
sometimes O
performs O
close O
to O
full O
model O
finetuning O
( O
Lester O
et O
al O
. O
, O
2021 O
; O
Liu O
et O
al O
. O
, O
2022b O
) O
, O
there O
is O
often O
still O
a O
substantial O
gap O
, O
such O
as O
with O
limited O
training O
data O
, O
non O
- O
gigantic O
models O
( O
Lester O
et O
al O
. O
, O
2021 O
) O
, O
or O
challenging O
tasks O
( O
He O
et O
al O
. O
, O
2022 O
) O
. O
We O
therefore O
study O
ways O
to O
improve O
LMs O
' O
" O
promptability O
. O
" O
We O
focus O
on O
a O
low O
resource O
setup O
and O
consider O
zero O
- O
shot O
NL O
prompts O
and O
few O
- O
shot O
learned O
prompts O
( O
which O
, O
again O
, O
are O
in O
conjunction O
with O
NL O
prompts O
; O
§ O
2 O
) O
. O
For O
the O
former O
, O
better O
promptability O
increases O
the O
performance O
when O
LMs O
face O
textual O
prompts O
of O
new O
tasks O
. O
For O
the O
latter O
, O
it O
more O
effectively O
leverages O
limited O
training O
examples O
for O
higher O
accuracy O
. O

We O
investigate O
if O
promptability O
can O
improve O
with O
a O
continued O
pretraining O
stage O
after O
LM O
pretraining O
( O
or O
LM O
adaptation O
for O
LM O
- O
adapted O
T5 O
( O
Lester O
et O
al O
. O
, O
2021 O
) O
) O
and O
before O
task O
- O
specific O
finetuning O
. O
The O
model O
is O
trained O
on O
a O
collection O
of O
tasks O
that O
have O
NL O
prompts O
and O
evaluated O
on O
unseen O
tasks O
. O
The O
methods O
that O
we O
explore O
below O
differ O
in O
how O
the O
continued B-MethodName
pretraining I-MethodName
stage O
is O
performed O
. O
We O
use O
the O
notation O
MTL O
- O
T_P O
_ O
to O
abbreviate O
those O
methods O
that O
are O
based O
on O
multi O
- O
task O
learning O
, O
where O
the O
blanks O
_ O
specify O
different O
configurations O
of O
the O
transformer O
( O
T O
) O
and O
the O
prompt O
( O
P O
) O
components O
during O
MTL O
. O
Architecturally O
, O
a O
method O
may O
continue O
to O
pretrain O
only O
the O
T5 O
model O
without O
prompt O
parameters O
, O
in O
which O
case O
we O
use O
P O
✗ O
to O
denote O
the O
lack O
of O
them O
; O
otherwise O
, O
both O
transformer O
and O
prompt O
parameters O
exist O
during O
MTL O
. O
We O
use O
and O
to O
denote O
if O
the O
corresponding O
component O
is O
trained O
or O
frozen O
in O
MTL O
, O
respectively O
. O
This O
notation O
describes O
the O
continued O
pretraining O
stage O
only O
: O
in O
the O
final O
finetuning O
stage O
, O
all O
methods O
include O
both O
the O
transformer O
and O
prompt O
components O
, O
but O
only O
the O
latter O
is O
updated O
. O

Continued B-MethodName
pretraining I-MethodName
has O
been O
studied O
in O
limited O
settings O
. O
proposed O
T0 O
by O
multi O
- O
task O
training O
a O
T5 O
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
as O
continued B-MethodName
pretraining I-MethodName
. O
They O
updated O
T5 O
parameters O
through O
learning O
on O
continued O
pretraining O
tasks O
, O
not O
including O
a O
prompt O
component O
, O
and O
showed O
that O
this O
training O
improves O
zero O
- O
shot O
NL O
promptability O
. O
Following O
our O
nomenclature O
, O
we O
refer O
to O
this O
paradigm O
as O
MTL O
- O
T O
P O
✗ O
. O
Additionally O
, O
employed O
a O
similar O
stage O
, O
incorporating O
and O
multi O
- O
task O
training O
a O
shallow O
prompt O
as O
continued O
pretraining O
, O
while O
freezing O
the O
transformer O
parameters O
in O
this O
stage O
. O
They O
showed O
that O
this O
strategy O
helps O
few O
- O
shot O
promptability O
during O
finetuning O
. O
We O
refer O
to O
this O
paradigm O
as O
MTL O
- O
T O
P O
. O

In O
this O
work O
, O
we O
study O
the O
gains O
of O
the O
previous O
two O
continued B-MethodName
pretraining I-MethodName
approaches O
, O
as O
well O
as O
a O
model O
that O
synthesizes O
them O
, O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
, O
which O
we O
are O
the O
first O
to O
propose O
. O
For O
few O
- O
shot O
downstream O
tuning O
, O
the O
learned O
prompt O
can O
act O
as O
a O
good O
initialization O
compared O
to O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
. O
In O
the O
zeroshot O
setup O
, O
prior O
work O
has O
discovered O
that O
including O
certain O
text O
in O
a O
prompt O
, O
such O
as O
" O
Let O
's O
think O
step O
by O
step O
, O
" O
can O
adjust O
the O
reasoning O
of O
LMs O
to O
yield O
substantially O
improved O
performance O
across O
tasks O
( O
Kojima O
et O
al O
. O
, O
2022 O
; O
Askell O
et O
al O
. O
, O
2021 O
) O
. O
The O
learned O
prompt O
here O
could O
function O
analogously O
. O
Compared O
to O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
, O
on O
the O
other O
hand O
, O
the O
additional O
capacity O
brought O
by O
more O
updatable O
parameters O
could O
further O
boost O
model O
performance O
. O

MAML B-MethodName
- I-MethodName
style I-MethodName
meta I-MethodName
- I-MethodName
learning I-MethodName
( O
Finn O
et O
al O
. O
, O
2017 O
) O
directly O
optimizes O
for O
the O
downstream O
updates O
and O
can O
outperform O
MTL O
for O
full O
model O
finetuning O
( O
Dou O
et O
al O
. O
, O
2019 O
; O
Bansal O
et O
al O
. O
, O
2020a O
) O
. O
Yet O
, O
it O
similarly O
remains O
unexplored O
for O
prompting O
. O
We O
examine O
first O
- O
order O
MAML B-MethodName
( O
FOMAML O
; O
Finn O
et O
al O
. O
, O
2017 O
) O
, O
performing O
T O
steps O
of O
prompt O
tuning O
in O
the O
inner O
loop O
and O
updating O
all O
parameters O
in O
the O
outer O
loop O
. O
We O
also O
evaluate O
a O
version O
of O
Reptile O
( O
Nichol O
et O
al O
. O
, O
2018 O
) O
adapted O
for O
our O
setting O
that O
performs O
T O
steps O
of O
prompt O
tuning O
followed O
by O
one O
step O
of O
full O
model O
tuning O
, O
and O
use O
the O
resulting O
Reptile O
gradient O
for O
model O
updates O
. O
They O
have O
the O
same O
architecture O
as O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
and O
all O
parameters O
are O
trainable O
too O
. O
We O
provide O
a O
detailed O
description O
and O
theoretical O
discussion O
of O
these O
processes O
in O
§ O
B. O
See O
the O
original O
papers O
for O
more O
details O
. O

Experimental O
Setup O

We O
use O
P3 B-DatasetName
, O
a O
collection O
of O
NL O
- O
templatized O
examples O
for O
a O
variety O
of O
datasets O
, O
for O
training O
and O
evaluation O
using O
the O
standard O
splits O
in O
. O
Not O
only O
is O
there O
no O
dataset O
overlap O
between O
training O
and O
evaluation O
, O
but O
no O
task O
overlap O
either O
( O
e.g. O
, O
sentiment B-TaskName
vs. O
QA B-TaskName
) O
, O
making O
it O
challenging O
. O
We O
report O
dataset O
statistics O
in O
§ O
A. O
We O
perform O
continued B-MethodName
pretraining I-MethodName
for O
one O
epoch O
over O
all O
training O
datasets O
. O
Each O
dataset O
has O
multiple O
templates O
, O
each O
evaluated O
with O
accuracy B-MetricName
. O
As O
different O
datasets O
have O
different O
numbers O
of O
answer O
choices O
and O
hence O
different O
baseline O
accuracy O
, O
we O
report O
Average B-MetricName
Relative I-MetricName
Gain I-MetricName
( O
ARG B-MetricName
; O
Ye O
et O
al O
. O
, O
2021 O
) O
as O
a O
single O
summary O
metric O
by O
averaging O
across O
all O
templates O
the O
relative O
ac O
- O
curacy O
improvement O
over O
a O
random O
baseline O
. O
We O
perform O
significance O
testing O
using O
bootstrap O
with O
1,000 O
iterations O
, O
in O
each O
iteration O
randomly O
sampling O
evaluation O
examples O
and O
comparing O
the O
two O
models O
in O
question O
. O
§ O
D O
reports O
per O
- O
dataset O
results O
. O

Following O
, O
we O
initialize O
the O
continued O
pretraining O
stage O
from O
T5 O
finetuned O
with O
an O
LM O
objective O
( O
Lester O
et O
al O
. O
, O
2021 O
) O
, O
making O
it O
more O
amenable O
to O
prompting O
. O
We O
experiment O
with O
two O
sizes O
: O
T5 O
- O
Large O
with O
770 O
M O
parameters O
and O
T5 O
- O
XL O
with O
3B O
parameters O
. O
We O
retrain O
T0 O
, O
i.e. O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
, O
to O
eliminate O
confounding O
factors O
in O
the O
training O
procedure O
. O
We O
also O
reproduce O
's O
experiment O
in O
our O
setup O
, O
i.e. O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
, O
pretraining O
a O
shallow O
prompt O
with O
other O
parameters O
frozen O
. O
During O
few O
- O
shot O
finetuning O
, O
we O
train O
on O
the O
same O
16 O
examples O
for O
100 B-HyperparameterValue
epochs B-HyperparameterName
. O
§ O
C O
reports O
additional O
hyperparameters O
. O

Results O

Table O
1 O
reports O
our O
results O
. O
From O
No O
Cont O
. O
Pretraining O
, O
we O
find O
that O
continued B-MethodName
pretraining I-MethodName
is O
crucial O
for O
prompt B-MethodName
tuning I-MethodName
with O
low O
resources O
- O
without O
it O
, O
only O
few O
- O
shot O
deep O
prompt O
tuning O
yields O
slightly O
above O
- O
random O
performance O
. O
These O
results O
contradict O
previous O
findings O
that O
few O
- O
shot O
prompt O
tuning O
works O
well O
without O
this O
stage O
( O
Min O
et O
al O
. O
, O
2022 O
) O
. O
We O
believe O
this O
is O
due O
to O
the O
challenging O
nature O
of O
the O
P3 B-DatasetName
evaluation O
datasets O
, O
compared O
to O
the O
simple O
sentence O
classification O
tasks O
previously O
investigated O
. O
This O
is O
consistent O
with O
what O
He O
et O
al O
. O
( O
2022 O
) O
observed O
in O
the O
full O
- O
data O
setting O
where O
deep O
prompt O
tuning O
performs O
sub O
- O
optimally O
on O
difficult O
tasks O
. O

Existing O
methods O
for O
continued B-MethodName
pretraining I-MethodName
have O
their O
drawbacks O
. O
In O
contrast O
to O
, O
we O
found O
that O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
with O
a O
shallow O
prompt O
does O
not O
substantially O
perform O
above O
random O
. O
We O
attribute O
this O
to O
( O
1 O
) O
their O
simpler O
evaluation O
tasks O
which O
, O
unlike O
ours O
, O
have O
decent O
prompt O
- O
tuned O
performance O
without O
continued O
pretraining O
; O
and O
( O
2 O
) O
their O
hand O
- O
designed O
pretraining O
tasks O
that O
match O
their O
evaluation O
tasks O
, O
while O
P3 B-DatasetName
conversely O
avoids O
training O
- O
evaluation O
task O
overlap O
, O
requiring O
generalizability O
. O
Vu O
et O
al O
. O
( O
2022 O
) O
also O
found O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
to O
be O
effective O
, O
though O
with O
high O
resources O
. O
We O
also O
compare O
with O
T0 O
, O
i.e. O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
, O
where O
both O
the O
official O
model O
and O
our O
reproduction O
suffer O
from O
degraded O
performance O
when O
few O
- O
shot O
shallow O
prompt O
tuned O
( O
compared O
to O
0 O
- O
shot O
) O
, O
likely O
because O
the O
prompt O
added O
during O
finetuning O
is O
intrusive O
, O
and O
the O
limited O
gradient O
updates O
are O
not O
sufficient O
to O
re- O
cover O
from O
it O
. O
We O
note O
that O
the O
official O
T0 O
model O
is O
not O
well O
- O
optimized O
: O
even O
without O
hyperparameter O
tuning O
, O
our O
implementation O
is O
significantly O
better O
( O
p O
< O
0.001 O
for O
all O
) O
. O

MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
significantly O
outperforms O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
, O
the O
strongest O
existing O
method O
we O
examine O
, O
across O
all O
settings O
( O
p O
< O
0.005 O
for O
all O
) O
except O
for O
few O
- O
shot O
deep O
prompt O
tuning O
on O
T5 O
- O
XL O
( O
p O
= O
0.21 O
) O
. O
For O
zero O
- O
shot O
NL O
promptability O
, O
the O
improvement O
could O
be O
due O
to O
the O
extra O
model O
capacity O
, O
or O
the O
multi O
- O
task O
trained O
prompt O
adjusting O
the O
reasoning O
of O
the O
LM O
, O
analogous O
to O
the O
text O
- O
based O
" O
Let O
's O
think O
step O
by O
step O
" O
effect O
( O
Kojima O
et O
al O
. O
, O
2022 O
) O
. O
For O
fewshot O
shallow O
prompt O
tuning O
, O
unlike O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
, O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
does O
not O
degrade O
in O
performance O
, O
resulting O
in O
31 B-MetricValue
% I-MetricValue
higher O
ARG B-MetricName
than O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
on O
T5 O
- O
Large O
. O
This O
is O
likely O
because O
of O
the O
model O
's O
familiarity O
with O
the O
prompt O
, O
though O
the O
limited O
capacity O
of O
shallow O
prompt O
tuning O
does O
not O
yield O
benefits O
either O
. O
Nevertheless O
, O
with O
deep B-MethodName
prompt I-MethodName
tuning I-MethodName
, O
which O
gives O
the O
model O
sufficient O
conditioning O
capacity O
, O
few O
- O
shot O
tuning O
does O
lead O
to O
performance O
increase O
, O
again O
outperforming O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ O
. O
Here O
, O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
provides O
a O
good O
prompt O
initialization O
and O
alleviates O
its O
intrusiveness O
. O
These O
results O
emphasize O
the O
importance O
of O
continued O
pretraining O
being O
aware O
of O
the O
downstream O
finetuning O
process O
. O
Interestingly O
, O
however O
, O
the O
gap O
between O
these O
two O
models O
shrinks O
as O
the O
model O
size O
increases O
, O
no O
longer O
significant O
at O
T5 O
- O
XL O
( O
p O
= O
0.21 O
) O
. O
Also O
, O
notably O
, O
pretraining O
with O
a O
shallow O
prompt O
has O
better O
0 O
- O
shot O
performance O
than O
a O
deep O
prompt O
. O
This O
high O
- O
lights O
that O
higher O
pretraining O
capacity O
is O
not O
always O
beneficial O
, O
and O
matches O
our O
motivation O
from O
textbased O
conditioning O
which O
also O
happens O
at O
the O
input O
level O
. O

FOMAML B-MethodName
and O
Reptile B-MethodName
surprisingly O
underperform O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
in O
few O
- O
shot O
prompt O
tuning O
, O
even O
though O
they O
specifically O
optimize O
for O
this O
procedure O
and O
have O
demonstrated O
success O
in O
NLP O
for O
full O
model O
finetuning O
( O
Dou O
et O
al O
. O
, O
2019 O
; O
Bansal O
et O
al O
. O
, O
2020bBansal O
et O
al O
. O
, O
, O
2021 O
and O
few O
- O
shot O
learning O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Qian O
and O
Yu O
, O
2019 O
; O
Mi O
et O
al O
. O
, O
2019 O
, O
i.a O
. O
) O
. O
While O
Ye O
et O
al O
. O
( O
2021 O
) O
also O
found O
FOMAML B-MethodName
to O
underperform O
MTL B-MethodName
, O
they O
sub O
- O
optimally O
only O
performed O
one O
inner O
loop O
update O
. O
Here O
, O
we O
show O
that O
this O
comparison O
holds O
for O
more O
appropriate O
hyperparameters O
. O
This O
could O
be O
due O
to O
the O
fewer O
number O
of O
gradient O
updates O
: O
to O
perform O
one O
gradient O
update O
, O
MTL B-MethodName
uses O
one O
training O
batch O
, O
while O
FOMAML B-MethodName
with O
T O
inner O
loop O
steps O
or O
Reptile O
with O
T O
prompt O
tuning O
steps O
use O
T O
+ O
1 O
batches O
. O
Not O
only O
might O
this O
be O
an O
inefficient O
use O
of O
training O
examples O
, O
but O
compute O
FLOPs O
too O
, O
since O
each O
inner O
loop O
/ O
prompt O
tuning O
step O
involves O
a O
full O
forward O
- O
backward O
pass O
. O
We O
attempt O
using O
a O
T O
+ O
1 O
times O
smaller O
meta O
batch O
size O
( O
see O
§ O
B O
for O
more O
detail O
) O
to O
pretrain O
a O
deep O
T5 O
- O
Large O
- O
sized O
Reptile O
. O
When O
prompt O
- O
tuned O
, O
it O
achieves O
22.8 B-MetricValue
ARG B-MetricName
, O
which O
is O
even O
lower O
, O
possibly O
due O
to O
higher O
gradient O
estimation O
noise O
. O
Alternatively O
, O
other O
factors O
could O
affect O
the O
performance O
of O
meta O
- O
learning O
. O
It O
is O
, O
for O
example O
, O
well O
- O
known O
that O
MAML O
- O
style O
meta O
- O
learning O
can O
be O
unstable O
and O
sensitive O
to O
architectures O
and O
hyperparameters O
( O
Antoniou O
et O
al O
. O
, O
2019 O
) O
. O
This O
instability O
is O
likely O
amplified O
by O
our O
large O
heterogeneous O
multi O
- O
task O
setup O
and O
our O
inability O
to O
afford O
hyperparameter O
search O
. O
Furthermore O
, O
its O
theoretical O
foundation O
has O
mostly O
only O
been O
examined O
through O
simple O
optimizers O
, O
predominantly O
SGD O
( O
Finn O
et O
al O
. O
, O
2017 O
; O
Nichol O
et O
al O
. O
, O
2018 O
) O
. O
How O
it O
interacts O
with O
optimizers O
more O
common O
in O
modern O
NLP O
, O
such O
as O
Adafactor O
( O
which O
we O
use O
) O
, O
remains O
to O
be O
explored O
. O

Recommendations O
. O
Based O
on O
our O
findings O
, O
we O
recommend O
practitioners O
to O
always O
incorporate O
a O
prompt O
during O
continued B-MethodName
pretraining I-MethodName
and O
to O
train O
the O
entire O
model O
. O
Without O
downstream O
taskspecific O
tuning O
, O
such O
as O
when O
there O
is O
no O
training O
data O
or O
sufficient O
compute O
, O
a O
shallow O
prompt O
yields O
better O
accuracy O
. O
When O
few O
- O
shot O
task O
- O
specific O
prompt O
tuning O
is O
affordable O
, O
continued O
pretraining O
with O
a O
deep O
prompt O
enables O
the O
best O
performance O
. O

Conclusion O

We O
demonstrated O
that O
the O
simple O
recipe O
of O
continued B-MethodName
pretraining I-MethodName
with I-MethodName
a I-MethodName
prompt I-MethodName
significantly O
improves O
zero O
- O
shot O
NL O
promptability O
and O
fewshot O
learned O
promptability O
. O
MAML B-MethodName
- I-MethodName
based I-MethodName
metalearning I-MethodName
, O
on O
the O
other O
hand O
, O
obtains O
worse O
performance O
, O
for O
which O
we O
provided O
several O
explanations O
. O
Nonetheless O
, O
we O
believe O
future O
efforts O
to O
leverage O
their O
conceptual O
advantage O
could O
be O
fruitful O
, O
perhaps O
aided O
by O
our O
observations O
. O
We O
also O
hope O
to O
study O
the O
effect O
of O
continued B-MethodName
pretraining I-MethodName
with O
other O
parameter O
injection O
methods O
( O
Houlsby O
et O
al O
. O
, O
2019 O
; O
Hu O
et O
al O
. O
, O
2022 O
; O
Liu O
et O
al O
. O
, O
2022a O
) O
. O

Limitations O

Due O
to O
the O
expensive O
nature O
of O
our O
experiments O
, O
each O
involving O
continued O
pretraining O
on O
over O
9B O
tokens O
( O
§ O
A O
) O
, O
we O
could O
not O
afford O
to O
perform O
hyperparameter O
tuning O
, O
and O
instead O
took O
hyperparameters O
from O
prior O
work O
. O
It O
is O
, O
nevertheless O
, O
possible O
that O
careful O
hyperparameter O
tuning O
might O
yield O
slightly O
different O
trends O
from O
what O
we O
observed O
. O
Furthermore O
, O
because O
of O
computational O
constraints O
, O
we O
were O
unable O
to O
perform O
experiments O
on O
the O
largest O
released O
T5 O
model O
with O
11B O
parameters O
. O
Though O
we O
validated O
our O
findings O
on O
two O
model O
sizes O
, O
it O
has O
been O
found O
that O
larger O
models O
sometimes O
demonstrate O
qualitatively O
different O
results O
( O
Srivastava O
et O
al O
. O
, O
2022 O
; O
Lampinen O
et O
al O
. O
, O
2022 O
; O
Wei O
et O
al O
. O
, O
2022 O
) O
. O
We O
would O
be O
excited O
to O
see O
if O
our O
experiments O
could O
be O
reproduced O
at O
a O
larger O
model O
scale O
. O

global_grad O
= O
0 O
for O
b O
← O
1 O
, O
• O
• O
• O
, O
B O
do O
ϕ O
= O
clone O
( O
ϕ O
orig O
) O
for O
t O
← O
1 O
, O
• O
• O
• O
, O
T O
do O
data O
= O
next_batch O
( O
) O
/ O
/ O
support O
grad O
= O
forward_backward O
( O
ϕ O
, O

A O
Dataset O
Details O

We O
use O
P3 B-DatasetName
as O
our O
training O
and O
evaluation O
datasets O
. O
It O
contains O
35 O
datasets O
grouped O
into O
8 O
tasks O
: O
Multiple B-TaskName
- I-TaskName
Choice I-TaskName
QA I-TaskName
, O
Extractive B-TaskName
QA I-TaskName
, O
Closed B-TaskName
- I-TaskName
Book I-TaskName
QA I-TaskName
, O
Sentiment B-TaskName
, O
Topic B-TaskName
Classification I-TaskName
, O
Structure B-TaskName
- I-TaskName
To I-TaskName
- I-TaskName
Text I-TaskName
, O
Summarization B-TaskName
, O
and O
Paraphrase B-TaskName
Identification I-TaskName
. O
Examples O
in O
each O
dataset O
are O
templatized O
using O
multiple O
human O
- O
written O
templates O
. O
Across O
the O
35 O
datasets O
, O
there O
are O
a O
total O
of O
313 O
templates O
. O
For O
continued B-MethodName
pretraining I-MethodName
, O
we O
follow O
and O
only O
use O
the O
training O
split O
of O
each O
dataset O
. O
Four O
tasks O
are O
held O
out O
for O
evaluation O
in O
P3 B-DatasetName
: O
Sentence B-TaskName
Completion I-TaskName
, O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
, O
Coreference B-TaskName
Resolution I-TaskName
, O
and O
Word B-TaskName
Sense I-TaskName
Disambiguation I-TaskName
. O
They O
consist O
of O
11 O
evaluation O
datasets O
( O
considering O
the O
three O
splits O
of O
ANLI O
as O
separate O
datasets O
) O
and O
116 O
templates O
in O
total O
. O
We O
use O
the O
training O
split O
of O
each O
dataset O
for O
few O
- O
shot O
experiments O
, O
and O
, O
following O
, O
evaluate O
on O
the O
validation O
splits O
. O
The O
only O
exception O
is O
StoryCloze B-DatasetName
which O
does O
not O
have O
a O
training O
split O
, O
so O
we O
use O
its O
validation O
split O
for O
training O
and O
evaluate O
on O
its O
test O
split O
. O
Unlike O
T0 O
, O
we O
do O
not O
evaluate O
on O
the O
BIG O
- O
Bench O
datasets O
( O
Srivastava O
et O
al O
. O
, O
2022 O
) O
as O
they O
had O
not O
stabilized O
as O
a O
collection O
of O
datasets O
at O
the O
time O
of O
this O
work O
. O
All O
the O
prompts O
in O
P3 B-DatasetName
are O
collected O
in O
English O
. O

To O
make O
training O
more O
efficient O
, O
we O
righttruncate O
all O
source O
sequences O
to O
768 B-HyperparameterValue
tokens O
and O
target O
sequences O
to O
192 B-HyperparameterValue
tokens O
. O
For O
the O
continued O
pretraining O
stage O
, O
this O
affects O
2 O
% O
of O
all O
training O
Input O
: O
Number O
of O
inner O
loop O
steps O
T B-HyperparameterName
, O
meta O
batch O
size O
B B-HyperparameterName
, O
inner O
loop O
learning O
rate O
α B-HyperparameterName
Initialize O
LM O
- O
adapted O
T5 O
parameters O

ϕ O
orig O
global_grad O
= O
0 O
for O
b O
← O
1 O
, O
• O
• O
• O
, O
B O
do O
ϕ O
= O
clone O
( O
ϕ O
orig O
) O
for O
t O
← O
1 O
, O
• O
• O
• O
, O
T O
do O
data O
= O
next_batch O
( O
) O
/ O
/ O
support O
grad O
= O
forward_backward O
( O
ϕ O
, O
data O
) O
update O
( O
ϕ O
, O
grad O
, O
prompt_only O
= O
True O
) O
data O
= O
next_batch O
( O
) O

/ O
/ O
query O
grad O
= O
forward_backward O
( O
ϕ O
, O
data O
) O
update O
( O
ϕ O
, O
grad O
, O
prompt_only O
= O
False O
) O
global_grad O
-= O
ϕ O
global_grad O
= O
global_grad O
/ O
( O
αB O
) O
+ O
ϕ O
orig O
update O
( O
ϕ O
orig O
, O
global_grad O
, O
prompt_only O
= O
False O
) O

B O
Meta O
- O
Learning O
Details O

In O
this O
section O
, O
we O
elaborate O
on O
our O
meta O
- O
learning O
training O
procedures O
. O
Algorithm O
1 O
contains O
pseudocode O
for O
our O
first B-MethodName
- I-MethodName
order I-MethodName
MAML I-MethodName
( O
FOMAML B-MethodName
) O
procedure O
. O
In O
the O
inner O
loop O
, O
we O
perform O
T B-HyperparameterName
steps O
of O
prompt O
tuning O
on O
a O
cloned O
model O
using O
support O
data O
. O
In O
the O
outer O
loop O
, O
we O
use O
query O
data O
to O
evaluate O
the O
prompt O
- O
tuned O
model O
and O
compute O
gradients O
. O
We O
use O
the O
first O
- O
order O
approximation O
where O
the O
gradient O
is O
not O
taken O
with O
respect O
to O
the O
entire O
prompt O
tuning O
process O
but O
only O
the O
forward O
pass O
with O
query O
data O
because O
it O
is O
computationally O
more O
tractable O
, O
and O
past O
work O
has O
shown O
that O
this O
first O
- O
order O
approximation O
does O
not O
hurt O
performance O
much O
, O
if O
at O
all O
( O
Finn O
et O
al O
. O
, O
2017 O
; O
Dou O
et O
al O
. O
, O
2019 O
) O
. O
Theoretically O
, O
to O
perfectly O
simulate O
the O
downstream O
prompt O
tuning O
procedure O
, O
we O
should O
use O
the O
same O
batch O
of O
support O
data O
for O
the O
T B-HyperparameterName
steps O
of O
update O
. O
Nevertheless O
, O
this O
would O
traverse O
the O
training O
data O
much O
more O
slowly O
, O
so O
we O
use O
different O
support O
batches O
. O
Our O
theoretical O
analysis O
through O
the O
perspective O
of O
Reptile O
below O
also O
justifies O
this O
. O

In O
preliminary O
experiments O
, O
we O
found O
a O
naïve O
adoption O
of O
Reptile B-MethodName
( O
Nichol O
et O
al O
. O
, O
2018 O
) O
to O
yield O
subpar O
performance O
. O
As O
there O
is O
no O
inner O
- O
and O
outer O
- O
loop O
distinction O
in O
Reptile B-MethodName
, O
doing O
prompt O
tuning O
leads O
to O
only O
the O
prompt O
parameter O
being O
updated O
throughout O
the O
entire O
continued O
pretraining O
stage O
, O
likely O
causing O
the O
performance O
degradation O
. O
This O
effect O
is O
also O
seen O
in O
our O
multi O
- O
task O
learning O
setup O
with O
the O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
model O
. O
Thus O
, O
we O
propose O
to O
adapt O
Reptile B-MethodName
to O
better O
suit O
prompt O
tuning O
, O
which O
we O
illustrate O
in O
Algorithm O
2 O
. O
It O
is O
similar O
to O
FOMAML B-MethodName
, O
but O
instead O
of O
considering O
the O
outer O
loop O
's O
gradient O
as O
the O
meta O
- O
learning O
gradient O
, O
it O
uses O

1 O
αB O
B O
b=1 O
( O
ϕ O
orig O
− O
ϕ O
b O
) O

where O
b O
is O
the O
cloned O
model O
's O
final O
parameter O
for O
meta O
- O
batch O
b O
. O

Now O
we O
theoretically O
justify O
our O
proposed O
Reptile B-MethodName
version O
, O
mostly O
following O
the O
original O
proof O
structure O
in O
Nichol O
et O
al O
. O
( O
2018 O
) O
, O
in O
the O
context O
of O
prompt O
tuning O
. O
We O
can O
think O
of O
the O
downstream O
finetuning O
stage O
as O
starting O
from O
the O
initial O
model O
parameters O
ϕ O
0 O
and O
performing O
T O
steps O
of O
prompt O
tuning O
on O
the O
same O
batch O
of O
training O
data O
which O
produces O
a O
loss O
function O
L O
train O
( O
ϕ O
) O
for O
some O
model O
parameters O
ϕ. O
Then O
this O
model O
is O
evaluated O
on O
some O
test O
data O
that O
similarly O
produces O
a O
loss O
function O
L O
test O
( O
ϕ O
T O
) O
for O
the O
final O
trained O
model O
ϕ O
T O
. O
Let O
us O
first O
abbreviate O
some O
gradients O
and O
Hessians O
: O

g O
i O
= O
L O
′ O
train O
( O
ϕ O
i O
) O
= O
∂ O
∂ϕ O
i O
L O
train O
( O
ϕ O
i O
) O
H O
i O
= O
L O
′′ O
train O
( O
ϕ O
i O
) O
g O
= O
L O
′ O
test O
( O
ϕ O
0 O
) O
H O
= O
L O
′′ O
test O
( O
ϕ O
0 O
) O

Then O
we O
can O
write O
each O
step O
of O
the O
prompt O
tuning O
process O
as O
an O
update O
function O
: O

U O
( O
ϕ O
) O
= O
ϕ O
− O
αm O
• O
L O
′ O
train O
( O
ϕ O
) O
U O
′ O
( O
ϕ O
) O
= O
I O
− O
αM O
• O
L O
′′ O
train O
( O
ϕ O
) O

where O
α B-HyperparameterName
is O
the O
learning B-HyperparameterName
rate I-HyperparameterName
and O
m O
and O
M O
are O
boolean O
masks O
that O
contain O
1 O
for O
the O
prompt O
parameters O
. O
• O
indicates O
element O
- O
wise O
multiplication O
, O
which O
we O
prescribe O
to O
take O
the O
highest O
precedence O
in O
the O
equations O
below O
. O
With O
T O
iterations O
of O
U O
, O
we O
have O
: O

ϕ O
T O
= O
ϕ O
0 O
− O
αm O
• O
T O
−1 O
j=0 O
g O
j O
( O
1 O
) O

Plugging O
in O
Equation O
1 O
, O
by O
Taylor O
's O
theorem O
: O

g O
i O
= O
L O
′ O
train O
( O
ϕ O
i O
) O
= O
L O
′ O
train O
( O
ϕ O
0 O
) O
+ O
L O
′′ O
train O
( O
ϕ O
0 O
) O
( O
ϕ O
i O
− O
ϕ O
0 O
) O
+ O
O O
( O
α O
2 O
) O
= O
g O
0 O
+ O
H O
0 O
( O
ϕ O
i O
− O
ϕ O
0 O
) O
+ O
O O
( O
α O
2 O
) O
= O
g O
0 O
− O
αH O
0 O
m O
• O
i−1 O
j=0 O
g O
j O
+ O
O O
( O
α O
2 O
) O
= O
g O
0 O
− O
αiH O
0 O
m O
• O
g O
0 O
+ O
O O
( O
α O
2 O
) O
( O
2 O
) O

where O
the O
last O
step O
can O
be O
seen O
by O
induction O
, O
iteratively O
applying O
the O
second O
- O
to O
- O
last O
line O
. O

With O
a O
similar O
process O
, O
we O
can O
derive O
: O

H O
i O
= O
H O
0 O
+ O
O O
( O
α O
) O
( O
3 O
) O

The O
FOMAML B-MethodName
gradient O
is O
the O
same O
as O
L O
′ O
test O
( O
ϕ O
T O
) O
. O
Plugging O
in O
Equation O
2 O
but O
sweeping O
its O
nonleading O
terms O
into O
O O
( O
α O
2 O
) O
: O

g O
FOMAML B-MethodName
= O
L O
′ O
test O
( O
ϕ O
T O
) O
= O
L O
′ O
test O
( O
ϕ O
0 O
) O
+ O
L O
′′ O
test O
( O
ϕ O
0 O
) O
( O
ϕ O
T O
− O
ϕ O
0 O
) O
+ O
O O
( O
α O
2 O
) O
= O
g O
+ O
H O
( O
ϕ O
T O
− O
ϕ O
0 O
) O
+ O
O O
( O
α O
2 O
) O
= O
g O
− O
αHm O
• O
T O
−1 O
j=0 O
g O
j O
+ O
O O
( O
α O
2 O
) O
= O
g O
− O
αT O
Hm O
• O
g O
0 O
+ O
O O
( O
α O
2 O
) O

The O
full O
MAML B-MethodName
gradient O
takes O
the O
derivative O
throughout O
the O
entire O
prompt O
tuning O
process O
. O
Plugging O
in O
Equation O
3 O
and O
g O
FOMAML B-MethodName
= O
L O
′ O
test O
( O
ϕ O
T O
) O
and O
sweeping O
terms O
into O
O O
( O
α O
2 O
) O
when O
possible O
: O

g O
MAML B-MethodName
= O
∂ O
∂ϕ O
0 O
L O
test O
( O
ϕ O
T O
) O
= O
∂ O
∂ϕ O
0 O
L O
test O
( O
U O
( O
U O
( O
• O
• O
• O
U O
( O
ϕ O
0 O
) O
) O
) O
) O
= O
U O
′ O
( O
ϕ O
0 O
) O
U O
′ O
( O
ϕ O
1 O
) O
• O
• O
• O
U O
′ O
( O
ϕ O
T O
−1 O
) O
L O
′ O
test O
( O
ϕ O
T O
) O
= O
 O
 O
T O
−1 O
j=0 O
( O
I O
− O
αM O
• O
L O
′′ O
train O
( O
ϕ O
j O
) O
) O
 O
 O
L O
′ O
test O
( O
ϕ O
T O
) O
= O
 O
 O
T O
−1 O
j=0 O
( O
I O
− O
αM O
• O
H O
j O
) O
 O
 O
L O
′ O
test O
( O
ϕ O
T O
) O
= O
 O
 O
I O
− O
αM O
• O
T O
−1 O
j=0 O
H O
j O
 O
 O
L O
′ O
test O
( O
ϕ O
T O
) O
+ O
O O
( O
α O
2 O
) O
= O
 O
 O
I O
− O
αM O
• O
T O
−1 O
j=0 O
H O
j O
 O
 O
g O
− O
αT O
Hm O
• O
g O
0 O
+ O
O O
( O
α O
2 O
) O
= O
( O
I O
− O
αT O
M O
• O
H O
0 O
) O
g O
− O
αT O
Hm O
• O
g O
0 O
+ O
O O
( O
α O
2 O
) O
= O
g O
− O
αT O
M O
• O
H O
0 O
g O
− O
αT O
Hm O
• O
g O
0 O
+ O
O O
( O
α O
2 O
) O

The O
Reptile B-MethodName
gradient O
, O
in O
our O
adaptation O
, O
takes O
the O
prompt O
's O
gradient O
during O
the O
T O
steps O
and O
the O
entire O
model O
's O
gradient O
for O
one O
step O
. O
Taking O
the O
Reptile B-MethodName
gradient O
from O
Algorithm O
2 O
and O
using O
ϕ O
T O
+1 O
to O
represent O
the O
parameters O
after O
the O
outer O
loop O
fullfinetuning O
update O
: O

g O
Reptile B-MethodName
= O
1 O
α O
( O
ϕ O
0 O
− O
ϕ O
T O
+1 O
) O
= O
m O
• O
T O
−1 O
j=0 O
g O
j O
+ O
g O
FOMAML B-MethodName
= O
m O
• O
T O
−1 O
j=0 O
( O
g O
0 O
− O
αjH O
0 O
m O
• O
g O
0 O
) O
+ O
g O
FOMAML B-MethodName
+ O
O O
( O
α O
2 O
) O
= O
T O
m O
• O
g O
0 O
− O
α O
T O
( O
T O
− O
1 O
) O
2 O
m O
• O
( O
H O
0 O
m O
• O
g O
0 O
) O
+ O
g O
FOMAML B-MethodName
+ O
O O
( O
α O
2 O
) O
= O
g O
+ O
T O
m O
• O
g O
0 O
− O
α O
T O
( O
T O
− O
1 O
) O
2 O
m O
• O
( O
H O
0 O
m O
• O
g O
0 O
) O
− O
αT O
Hm O
• O
g O
0 O
+ O
O O
( O
α O
2 O
) O

We O
can O
see O
that O
all O
three O
meta O
- O
learning O
gradients O
have O
a O
similar O
effect O
: O
they O
only O
contain O
a O
mixture O
of O
lone O
gradients O
terms O
( O
g O
, O
g O
0 O
) O
, O
which O
act O
as O
a O
pure O
multi O
- O
task O
learning O
objective O
, O
and O
terms O
that O
are O
Hessian O
times O
gradient O
, O
which O
Nichol O
et O
al O
. O
( O
2018 O
) O
termed O
" O
AvgGradInner O
" O
and O
showed O
to O
encourage O
the O
expected O
similarity O
between O
different O
data O
batches O
, O
improving O
generalization O
. O

Back O
to O
our O
use O
of O
different O
data O
batches O
in O
FO B-MethodName
- I-MethodName
MAML I-MethodName
's O
inner O
loop O
and O
Reptile B-MethodName
's O
prompt O
tuning O
steps O
. O
If O
the O
inner O
loop O
uses O
the O
same O
support O
( O
i.e. O
, O
training O
) O
data O
, O
as O
in O
the O
derivation O
above O
, O
the O
" O
Avg O
- O
GradInner O
" O
terms O
become O
somewhat O
degenerate O
, O
with O
the O
same O
term O
scaled O
T O
or O
T O
( O
T O
−1 O
) O
2 O
times O
. O
With O
different O
inner O
loop O
batches O
, O
on O
the O
other O
hand O
, O
there O
would O
be O
more O
diverse O
Hessian O
- O
gradient O
interactions O
between O
different O
batches O
of O
data O
and O
hence O
encouraging O
generalization O
between O
more O
tasks O
. O

C O
Training O
Details O

Due O
to O
the O
expensiveness O
of O
our O
experiments O
, O
we O
did O
not O
perform O
any O
hyperparameter O
tuning O
. O
For O
all O
continued O
pretraining O
runs O
, O
we O
follow O
Raffel O
et O
al O
. O
( O
2020 O
) O
and O
and O
use O
Adafactor O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
with O
a O
0.001 B-HyperparameterValue
learning B-HyperparameterName
rate I-HyperparameterName
. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
4,096 B-HyperparameterValue
which O
we O
calculated O
to O
be O
close O
to O
what O
used O
. O
2 O
We O
clip O
gradients O
to O
unit O
norm O
. O
For O
shallow O
prompt O
tuning O
, O
we O
follow O
Min O
et O
al O
. O
( O
2022 O
) O
and O
use O
L B-HyperparameterName
= O
20 B-HyperparameterValue
prompt O
tokens O
, O
each O
with O
the O
same O
dimension O
as O
the O
word O
embedding O
size O
, O
on O
the O
source O
side O
only O
. O
For O
deep O
prompt O
tuning O
, O
we O
similarly O
use O
20 B-HyperparameterValue
hidden O
vectors O
that O
are O
prepended O
in O
every O
transformer O
layer O
, O
on O
both O
the O
source O
and O
target O
side O
for O
added O
capacity O
. O
For O
meta O
- O
learning O
, O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
, O
simulating O
our O
16 O
- O
shot O
evaluation O
( O
see O
below O
) O
, O
and O
a O
meta O
batch B-HyperparameterName
size I-HyperparameterName
of O
128 B-HyperparameterValue
. O
We O
perform O
7 B-HyperparameterValue
steps O
of O
inner O
loop O
updates O
( O
FOMAML B-MethodName
) O
/ O
prompt O
tuning O
( O
Reptile B-MethodName
) O
, O
following O
Bansal O
et O
al O
. O
( O
2020b O
) O
and O
Bansal O
et O
al O
. O
( O
2021 O
) O
, O
and O
similarly O
using O
Adafactor O
with O
learning B-HyperparameterName
rate I-HyperparameterName
0.001 B-HyperparameterValue
. O
All O
continued O
pretraining O
experiments O
run O
for O
one O
epoch O
over O
the O
training O
datasets O
with O
no O
checkpoint O
selection O
. O
In O
few O
- O
shot O
finetuning O
, O
we O
train O
on O
one O
batch O
of O
16 B-HyperparameterValue
randomly O
selected O
examples O
for O
100 B-HyperparameterValue
epochs O
( O
the O
same O
batch O
throughout O
training O
) O
, O
following O
Min O
et O
al O
. O
( O
2022 O
) O
. O
Like O
Min O
et O
al O
. O
( O
2022 O
) O
, O
we O
do O
not O
manually O
balance O
the O
label O
distribution O
in O
these O
examples O
, O
unlike O
in O
prior O
work O
( O
Gao O
et O
al O
. O
, O
2021 O
; O
Logan O
IV O
et O
al O
. O
, O
2022 O
) O
. O

Acknowledgments O

We O
appreciate O
Victor O
Sanh O
, O
Albert O
Webson O
, O
Colin O
Raffel O
, O
Zaid O
Alyafeai O
, O
and O
other O
members O
of O
the O
Bigscience O
project O
who O
answered O
many O
of O
our O
questions O
when O
we O
reimplemented O
T0 O
, O
and O
Yuxian O
Gu O
when O
we O
reimplemented O
. O
We O
also O
thank O
Sébastien O
M. O
R. O
Arnold O
whose O
help O
was O
crucial O
for O
our O
meta O
- O
learning O
implementation O
. O
We O
are O
also O
grateful O
for O
the O
members O
at O
AI2 O
and O
UC O
Irvine O
, O
as O
well O
as O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
. O

We O
perform O
all O
experiments O
on O
80 O
GB O
A100 O
GPUs O
. O
Each O
continued B-MethodName
pretraining I-MethodName
run O
takes O
four O
( O
sometimes O
eight O
) O
of O
them O
. O
The O
largest O
MTL O
model O
takes O
10 O
days O
to O
pretrain O
with O
four O
GPUs O
, O
while O
the O
largest O
meta O
- O
learning O
model O
takes O
14 O
days O
. O

D O
Per O
- O
Dataset O
Results O

In O
Figures O
1 O
to O
3 O
, O
we O
compare O
the O
per O
- O
dataset O
accuracy O
of O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
✗ I-MethodName
( O
our O
reproduction O
) O
, O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
, O
FOMAML B-MethodName
, O
and O
Reptile B-MethodName
. O
We O
omit O
MTL B-MethodName
- I-MethodName
T I-MethodName
P I-MethodName
due O
to O
its O
near O
- O
random O
performance O
. O

Multi B-TaskName
- I-TaskName
modal I-TaskName
Action I-TaskName
Chain I-TaskName
Abductive I-TaskName
Reasoning I-TaskName

Abductive B-TaskName
Reasoning I-TaskName
, O
has O
long O
been O
considered O
to O
be O
at O
the O
core O
ability O
of O
humans O
, O
which O
enables O
us O
to O
infer O
the O
most O
plausible O
explanation O
of O
incomplete O
known O
phenomena O
in O
daily O
life O
. O
However O
, O
such O
critical O
reasoning O
capability O
is O
rarely O
investigated O
for O
contemporary O
AI O
systems O
under O
such O
limited O
observations O
. O
To O
facilitate O
this O
research O
community O
, O
this O
paper O
sheds O
new O
light O
on O
Abductive B-TaskName
Reasoning I-TaskName
by O
studying O
a O
new O
vision O
- O
language O
task O
, O
Multi B-TaskName
- I-TaskName
modal I-TaskName
Action I-TaskName
chain I-TaskName
abductive I-TaskName
Reasoning I-TaskName
( O
MAR B-TaskName
) O
, O
together O
with O
a O
large O
- O
scale O
Abductive B-TaskName
Reasoning I-TaskName
dataset O
: O
Given O
an O
incomplete O
set O
of O
language O
described O
events O
, O
MAR B-TaskName
aims O
to O
imagine O
the O
most O
plausible O
event O
by O
spatio O
- O
temporal O
grounding O
in O
past O
video O
and O
then O
infer O
the O
hypothesis O
of O
subsequent O
action O
chain O
that O
can O
best O
explain O
the O
language O
premise O
. O
To O
solve O
this O
task O
, O
we O
propose O
a O
strong O
baseline O
model O
that O
realizes O
MAR B-TaskName
from O
two O
perspectives O
: O
( O
i O
) O
we O
first O
introduce O
the O
transformer O
, O
which O
learns O
to O
encode O
the O
observation O
to O
imagine O
the O
plausible O
event O
with O
explicitly O
interpretable O
event O
grounding O
in O
the O
video O
based O
on O
the O
commonsense O
knowledge O
recognition O
ability O
. O
( O
ii O
) O
To O
complete O
the O
assumption O
of O
a O
follow O
- O
up O
action O
chain O
, O
we O
design O
a O
novel O
symbolic O
module O
that O
can O
complete O
strict O
derivation O
of O
the O
progressive O
action O
chain O
layer O
by O
layer O
. O
We O
conducted O
extensive O
experiments O
on O
the O
proposed O
dataset O
, O
and O
the O
experimental O
study O
shows O
that O
the O
proposed O
model O
significantly O
outperforms O
existing O
video O
- O
language O
models O
in O
terms O
of O
effectiveness O
on O
our O
newly O
created O
MAR B-TaskName
dataset O
. O
Our O
dataset O
is O
available O
1 O
. O

Introduction O

Abductive B-TaskName
Reasoning I-TaskName
typically O
begins O
with O
an O
incomplete O
observation O
or O
several O
observations O
and O
then O
proceeds O
to O
the O
likeliest O
possible O
explanation O
for O
the O
set O
Peirce O
, O
1974 O
) O
. O
Given O
an O
event O
observation O
( O
O O
) O
, O
humans O
can O
find O
some O
related O
information O
in O
the O
recollection O
and O
easily O
trace O
the O
complete O
process O
with O
strong O
reasoning O
ability O
as O
a O
hypothesis O
( O
H O
) O
to O
explain O
the O
observation O
. O
For O
instance O
, O
when O
we O
observe O
the O
O O
: O
" O
The O
man O
in O
a O
T O
- O
shirt O
chocked O
on O
food O
is O
vomiting O
into O
the O
toilet O
" O
and O
remember O
that O
he O
used O
to O
devour O
food O
in O
the O
kitchen O
, O
we O
could O
infer O
the O
complete O
event O
chain O
about O
that O
man O
as O
hypothesis O
H O
: O
the O
man O
devoured O
the O
food O
in O
the O
kitchen O
→ O
he O
was O
choking O
→ O
he O
put O
down O
the O
food O
→ O
he O
left O
the O
kitchen O
in O
a O
hurry O
→ O
he O
ran O
into O
the O
bathroom O
→ O
he O
bent O
over O
the O
toilet O
. O
This O
ability O
enables O
us O
to O
perform O
better O
than O
machines O
in O
high O
- O
level O
reasoning O
and O
would O
be O
the O
most O
precious O
capacity O
for O
modern O
AI O
. O
Therefore O
, O
it O
is O
important O
to O
enhance O
such O
Abductive B-TaskName
Reasoning I-TaskName
capacities O
of O
AI O
models O
, O
i.e. O
, O
complete O
process O
as O
explanation O
. O

Motivated O
by O
the O
aforementioned O
Abductive B-TaskName
Reasoning I-TaskName
scenario O
, O
we O
present O
a O
novel O
visionlanguage O
task O
, O
called O
Multi B-TaskName
- I-TaskName
modal I-TaskName
Action I-TaskName
chain I-TaskName
abductive I-TaskName
Reasoning I-TaskName
( O
MAR B-TaskName
) O
, O
which O
is O
illustrated O
in O
Figure O
1 O
. O
Specifically O
, O
given O
a O
set O
of O
languagedescribed O
observations O
, O
the O
MAR O
task O
targets O
to O
precisely O
localize O
the O
target O
event O
in O
the O
past O
video O
( O
visual O
recollection O
simulation O
of O
human O
) O
about O
the O
language O
- O
described O
person O
and O
rigorously O
reason O
out O
the O
subsequent O
action O
chain O
( O
subsequent O
events O
inference O
) O
, O
to O
explain O
the O
observation O
. O
Different O
from O
the O
previous O
Abductive B-TaskName
Reasoning I-TaskName
task O
( O
Bhagavatula O
et O
al O
. O
, O
2019 O
) O
focusing O
on O
the O
unimodal O
and O
partial O
reasoning O
, O
our O
new O
task O
has O
the O
following O
characteristics O
: O
( O
i O
) O
MAR B-TaskName
needs O
to O
locate O
the O
target O
event O
from O
the O
complex O
video O
information O
to O
explain O
the O
textual O
observation O
; O
( O
ii O
) O
MAR B-TaskName
requires O
rigorous O
recovery O
of O
the O
complete O
action O
chain O
. O

These O
characteristics O
introduce O
two O
challenges O
to O
the O
MAR B-TaskName
task O
: O
( O
1 O
) O
Heterogeneous O
Information O
Alignment O
. O
To O
realize O
the O
event O
grounding O
, O
aligning O
the O
cross O
- O
modal O
information O
is O
necessary O
. O
However O
, O
unlike O
the O
highly O
concise O
language O
description O
, O
the O
videos O
in O
real O
scenes O
usually O
contain O
complex O
and O
redundant O
information O
, O
including O
multiple O
people O
with O
different O
appearances O
, O
actions O
, O
scenes O
, O
etc O
. O
Only O
a O
small O
amount O
of O
information O
in O
videos O
aligns O
with O
the O
text O
- O
described O
observation O
. O
Precisely O
extracting O
information O
from O
the O
complicated O
video O
information O
to O
align O
is O
difficult O
but O
necessary O
for O
the O
AI O
system O
. O
( O
2 O
) O
Action O
Chain O
Reasoning O
. O
Rigorous O
action O
chain O
reasoning O
is O
an O
interlocking O
and O
progressive O
process O
. O
If O
one O
step O
of O
reasoning O
is O
wrong O
, O
the O
correctness O
of O
subsequent O
steps O
can O
not O
be O
guaranteed O
. O
Therefore O
, O
for O
action O
chain O
reasoning O
, O
it O
is O
highly O
required O
to O
correctly O
learn O
the O
logical O
relationship O
between O
actions O
and O
correctly O
select O
from O
multiple O
next O
- O
step O
actions O
in O
each O
step O
of O
reasoning O
. O

We O
contribute O
a O
carefully O
annotated O
large O
- O
scale O
dataset O
, O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
, O
based O
on O
our O
collected O
data O
to O
facilitate O
the O
challenges O
solved O
for O
the O
MAR B-TaskName
task O
. O
It O
contains O
14 O
, O
201 O
cross O
- O
modal O
examples O
based O
on O
the O
videos O
manually O
collected O
from O
the O
TV O
show O
and O
the O
existing O
dataset O
( O
Sigurdsson O
et O
al O
. O
, O
2016 O
) O
. O
To O
address O
the O
MAR B-TaskName
task O
challenges O
, O
these O
examples O
have O
targeted O
manual O
annotations O
: O
( O
i O
) O
Commonsense O
Knowledge O
Annotation O
for O
Assisted O
Alignment O
. O
We O
provide O
the O
full O
annotation O
of O
commonsense O
knowledge O
for O
every O
textual O
observation O
related O
person O
in O
the O
large O
- O
scale O
videos O
, O
including O
the O
character O
's O
appearance O
, O
clothing O
, O
actions O
, O
sentiment O
, O
etc O
. O
( O
ii O
) O
Rigorous O
Annotation O
for O
Action O
Chains O
. O
Expert O
annotators O
with O
strong O
logical O
ability O
are O
asked O
to O
annotate O
the O
language O
- O
described O
observations O
and O
the O
action O
chains O
, O
ensuring O
the O
accuracy O
and O
rigor O
of O
logical O
annotations O
. O

Based O
on O
the O
constructed O
dataset O
, O
we O
propose O
an O
end O
- O
to O
- O
end O
Neural B-MethodName
- I-MethodName
symbOlic I-MethodName
model I-MethodName
Via I-MethodName
commonsense I-MethodName
knowledgE I-MethodName
for I-MethodName
multi I-MethodName
- I-MethodName
modaL I-MethodName
action I-MethodName
chain I-MethodName
Abductive I-MethodName
Reasoning I-MethodName
( O
NOVEL B-MethodName
) O
. O
There O
are O
two O
key O
targeted O
designs O
: O
( O
i O
) O
Knowledge O
- O
guided O
Alignment O
. O
We O
adopt O
the O
multi O
- O
task O
learning O
paradigm O
to O
synchronize O
the O
recognition O
learning O
of O
commonsense O
knowledge O
. O
Based O
on O
such O
knowledge O
recognition O
ability O
, O
our O
NOVEL B-MethodName
can O
minimize O
the O
interference O
of O
the O
inferred O
event O
and O
past O
video O
, O
thereby O
more O
easily O
learning O
to O
generate O
explicit O
event O
grounding O
conditioned O
on O
textual O
observations O
. O
( O
ii O
) O
Graph O
- O
aware O
Symbolic O
Reasoning O
. O
Motivated O
by O
the O
powerful O
reasoning O
ability O
of O
the O
symbolic O
network O
( O
Yi O
et O
al O
. O
, O
2018 O
) O
, O
we O
design O
the O
targeted O
symbolic O
reasoning O
module O
based O
on O
the O
traditional O
graph O
theory O
. O
Specifically O
, O
we O
store O
the O
learned O
action O
association O
graph O
in O
the O
training O
process O
. O
During O
inference O
, O
we O
determine O
the O
intermediate O
action O
chain O
between O
the O
textual O
observation O
and O
the O
grounded O
video O
event O
with O
Dijkstra O
's O
algorithm O
( O
Dijkstra O
, O
1959 O
) O
. O
Our O
contributions O
are O
three O
- O
fold O
: O

• O
We O
introduce O
a O
new O
task O
, O
Multi B-TaskName
- I-TaskName
modal I-TaskName
Action I-TaskName
chain I-TaskName
abductive I-TaskName
Reasoning I-TaskName
( O
MAR B-TaskName
) O
, O
which O
includes O
two O
sub O
- O
parts O
: O
target O
event O
grounding O
and O
sequential O
action O
chain O
reasoning O
. O

• O
A O
carefully O
collected O
large O
- O
scale O
dataset O
, O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
, O
is O
provided O
, O
in O
which O
the O
complete O
observation O
- O
explanation O
pairs O
are O
accurately O
and O
rigorously O
annotated O
. O
In O
addition O
, O
a O
variety O
of O
commonsense O
knowledge O
that O
can O
aid O
in O
training O
is O
annotated O
in O
detail O
. O

• O
An O
end O
- O
to O
- O
end O
neural O
- O
symbolic O
model O
named O
NOVEL B-MethodName
is O
proposed O
for O
MAR B-TaskName
with O
knowledge O
- O
guided O
alignment O
and O
graph O
- O
based O
symbolic O
reasoning O
. O
Extensive O
experiments O
demonstrate O
the O
model O
design O
rationality O
. O

Related O
Work O

Multi O
- O
modal O
Spatio O
- O
temporal O
Grounding O
. O
Our O
MAR O
task O
is O
related O
to O
the O
multi O
- O
modal O
spatiotemporal O
grounding O
task O
, O
which O
aims O
to O
detect O
target O
visual O
information O
described O
by O
the O
sentence O
from O
the O
video O
. O
It O
is O
an O
important O
task O
in O
the O
visual O
understanding O
domain O
( O
Miao O
et O
al O
. O
, O
, O
2021Zhang O
et O
al O
. O
, O
2019 O
. O
For O
video O
grounding O
research O
direction O
, O
most O
researchers O
focus O
their O
research O
on O
temporal O
grounding O
task O
. O
However O
, O
spatio O
- O
temporal O
grounding O
and O
spatial O
grounding O
Su O
et O
al O
. O
, O
2021 O
; O
Li O
et O
al O
. O
, O
2022bLi O
et O
al O
. O
, O
, O
2023 O
have O
received O
less O
attention O
. O
uses O
the O
graph O
neural O
network O
to O
model O
the O
spatio O
- O
temporal O
relationship O
between O
objects O
to O
align O
text O
descriptions O
for O
object O
localization O
. O
In O
addition O
, O
to O
evaluating O
the O
model O
performance O
, O
this O
paper O
proposes O
a O
complete O
large O
- O
scale O
dataset O
. O
( O
Su O
et O
al O
. O
, O
2021 O
) O
designs O
an O
end O
- O
to O
- O
end O
multi O
- O
modal O
grounding O
model O
based O
on O
the O
transformer O
. O
It O
outperforms O
all O
previous O
models O
without O
pre O
- O
training O
. O
Later O
, O
makes O
a O
targeted O
design O
to O
fit O
the O
pre O
- O
trained O
parameters O
and O
achieves O
a O
great O
improvement O
in O
accuracy O
. O

Neural O
- O
symbolic O
Reasoning O
. O
Compared O
with O
the O
pure O
neural O
network O
( O
Li O
et O
al O
. O
, O
2020b O
; O
Li O
et O
al O
. O
, O
2020a O
; O
Wu O
et O
al O
. O
, O
2020 O
; O
Miao O
et O
al O
. O
, O
2022 O
) O
, O
neural O
- O
symbolic O
models O
have O
stronger O
inference O
and O
perception O
capabilities O
. O
( O
Yi O
et O
al O
. O
, O
2018 O
) O
is O
an O
earlier O
paper O
exploring O
this O
direction O
. O
It O
stitches O
symbolic O
models O
behind O
the O
multi O
- O
modal O
neural O
network O
to O
reason O
on O
the O
information O
the O
network O
perceives O
. O
In O
this O
neat O
way O
, O
the O
model O
achieves O
excellent O
results O
. O
( O
Li O
et O
al O
. O
, O
2020c O
) O
combines O
the O
laws O
of O
physics O
with O
deep O
learning O
to O
make O
models O
capable O
of O
fitting O
complex O
physical O
processes O
. O
In O
this O
way O
, O
the O
model O
can O
effectively O
predict O
the O
motion O
trends O
of O
objects O
in O
the O
physical O
world O
. O
Similarly O
, O
uses O
physical O
laws O
such O
as O
collision O
to O
design O
a O
symbolic O
model O
to O
process O
the O
information O
perceived O
by O
the O
neural O
network O
, O
which O
can O
effectively O
predict O
the O
future O
motion O
of O
objects O
such O
as O
balls O
or O
sliders O
. O
( O
Greff O
et O
al O
. O
, O
2019 O
) O
applies O
neural O
network O
and O
symbolic O
model O
to O
highlevel O
and O
low O
- O
level O
visual O
relation O
detection O
, O
respectively O
, O
and O
achieves O
good O
performance O
through O
the O
cooperation O
of O
the O
two O
. O

Abductive B-TaskName
Reasoning I-TaskName
. O
There O
is O
a O
limited O
amount O
of O
existing O
research O
work O
on O
abductive B-TaskName
reasoning I-TaskName
AI O
systems O
. O
Previous O
abductive B-TaskName
reasoning I-TaskName
tasks O
( O
Bhagavatula O
et O
al O
. O
, O
2019 O
; O
Liang O
et O
al O
. O
, O
2022 O
) O
require O
AI O
systems O
to O
provide O
a O
unimodal O
and O
partial O
explanation O
, O
which O
may O
lack O
some O
key O
information O
. O

Dataset O
Description O

To O
advance O
research O
on O
Abductive B-TaskName
Reasoning B-TaskName
, O
we O
propose O
the O
Multi B-TaskName
- I-TaskName
modal I-TaskName
Action I-TaskName
chain I-TaskName
abductive I-TaskName
Reasoning I-TaskName
task O
( O
MAR O
) O
and O
contribute O
a O
large O
- O
scale O
Text O
- O
videO O
dataset O
for O
the O
MAR B-TaskName
task O
( O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
) O
. O
To O
complete O
the O
MAR B-TaskName
task O
, O
the O
AI O
system O
needs O
to O
reason O
out O
the O
complete O
process O
( O
the O
video O
event O
and O
the O
subsequent O
action O
chain O
) O
to O
explain O
the O
observed O
event O
described O
by O
the O
textual O
observation O
O. O
In O
detail O
, O
the O
target O
event O
E O
t O
is O
grounded O
in O
the O
video O
V O
by O
localizing O
the O
temporal O
boundary O
T O
and O
the O
target O
person O
bounding O
boxes O
B O
in O
the O
event O
E O
t O
. O

After O
that O
, O
the O
action O
chain O
A O
= O
{ O
A O
i O
} O
N O
A O
i=1 O
following O
the O
target O
event O
E O
t O
is O
inferred O
, O
where O
the O
N O
A O
is O
the O
number O
of O
actions O
in O
chain O
A O
. O

Dataset O
Preparation O

We O
collect O
and O
annotate O
the O
proposed O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
based O
on O
the O
above O
MAR B-TaskName
's O
definition O
. O
To O
increase O
the O
variety O
of O
data O
, O
a O
two O
- O
source O
dataset O
collection O
is O
conducted O
for O
labeling O
: O
( O
1 O
) O
We O
select O
lifestyle O
videos O
from O
the O
Charades B-DatasetName
dataset O
( O
Sigurdsson O
et O
al O
. O
, O
2016 O
) O
. O
These O
examples O
contain O
diverse O
people O
and O
rich O
activities O
. O
( O
2 O
) O
The O
TV O
show O
videos O
are O
selected O
from O
92 O
well O
- O
known O
American O
dramas O
, O
such O
as O
The O
Big O
Bang O
Theory O
, O
Grey O
's O
Anatomy O
, O
etc O
. O
Notably O
, O
the O
number O
of O
people O
is O
relatively O
limited O
compared O
to O
the O
first O
source O
, O
but O
the O
causal O
links O
among O
events O
are O
clearer O
. O

Dataset O
Annotation O

The O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
contains O
commonsense O
knowledge O
annotations O
and O
MAR B-TaskName
task O
annotations O
. O
More O
details O
are O
in O
the O
appendix O
. O

Commonsense O
Knowledge O
Annotation O
. O
We O
annotate O
the O
commonsense O
knowledge O
that O
is O
related O
to O
the O
person O
conditioned O
on O
the O
observations O
for O
event O
grounding O
, O
including O
the O
appearance O
and O
clothing O
of O
key O
video O
characters O
. O
We O
also O
annotated O
the O
characters O
' O
actions O
, O
sentiments O
and O
located O
scenes O
in O
each O
frame O
. O
Each O
category O
contains O
several O
critical O
subcategories O
( O
e.g. O
, O
Appearance O
: O
Gender O
, O
Hair O
Length O
, O
Age O
. O
) O
recognized O
by O
labeling O
experts O
to O
ensure O
the O
MAR B-TaskName
model O
the O
relationship O
between O
the O
event O
and O
observations O
. O
MAR B-TaskName
Task O
Annotation O
. O
The O
MAR B-TaskName
task O
annotation O
for O
the O
video O
V O
consists O
of O
a O
languagedescribed O
observation O
O O
and O
its O
explanation O
, O
including O
the O
spatio O
- O
temporal O
markers O
( O
the O
bounding O
boxes O
B O
and O
the O
temporal O
boundary O
T O
) O
of O
the O
target O
video O
event O
E O
t O
and O
the O
subsequent O
action O
chain O
A. O
In O
order O
to O
ensure O
the O
quality O
of O
the O
annotations O
, O
we O
employ O
experts O
with O
related O
annotation O
experience O
from O
leading O
AI O
research O
institutions O
. O
The O
annotation O
process O
contains O
two O
steps O
: O

Step O
1 O
: O
Annotating O
. O
The O
annotators O
annotate O
the O
natural O
language O
description O
O O
of O
the O
observation O
referred O
to O
the O
commonsense O
knowledge O
and O
the O
follow O
- O
up O
video O
content O
for O
the O
TV O
show O
video O
clips O
. O
The O
corresponding O
target O
video O
event O
E O
t O
and O
the O
subsequent O
action O
chain O
A O
annotations O
are O
also O
recorded O
. O

Step O
( O
1 O
) O

In O
it O
, O
the O
function O
ξ O
( O
. O
) O
outputs O
the O
ground O
truth O
, O
which O
contains O
: O
( O
1 O
) O
the O
temporal O
boundary O
T O
of O
the O
target O
video O
event O
E O
t O
and O
the O
target O
person O
bounding O
boxes O
B O
in O
the O
event O
E O
t O
; O

( O
2 O
) O
the O
category O
of O
each O
action O
in O
the O
action O
chain O
A. O
The O
δ O
( O
. O
) O
outputs O
the O
prediction O
, O
and O
the O
Θ O
is O
the O
learnable O
parameter O
. O

The O
function O
ϵ O
( O
. O
) O
calculates O
the O
consistency O
of O
ξ O
( O
. O
) O
and O
δ O
( O
. O
) O
. O
Model O
Pipeline O
. O
As O
shown O
in O
the O
Figure O
3 O
, O
after O
extracting O
features O
from O
the O
observation O
O O
and O
the O
frames O
of O
the O
video O
V O
with O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
and O
Resnet101 B-MethodName
( O
He O
et O
al O
. O
, O
2016 O
) O
, O
our O
NOVEL B-MethodName
M O
mainly O
contains O
two O
parts O
to O
process O
the O
features O
. O
( O
1 O
) O
First O
, O
the O
multi O
- O
modal O
features O
is O
reasoned O
by O
the O
transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Based O
on O
the O
recognition O
ability O
of O
the O
commonsense O
knowledge O
( O
e.g. O
, O
the O
character O
's O
actions O
, O
appearance O
, O
etc O
. O
) O
, O
the O
model O
focuses O
on O
the O
key O
video O
information O
aligned O
with O
the O
textual O
observation O
O O
, O
and O
learns O
to O
infer O
the O
temporal O
boundary O
T O
and O
the O
target O
bounding O
boxes O
B O
of O
the O
target O
event O
E O
t O
in O
the O
video O
. O
( O
2 O
) O
Second O
, O
the O
symbolic O
reasoning O
part O
maintains O
a O
relation O
memory O
module O
and O
stores O
the O
learned O
action O
relation O
in O
it O
at O
the O
training O
step O
. O
In O
the O
inference O
phase O
, O
the O
action O
graph O
is O
constructed O
based O
on O
the O
action O
relations O
. O
We O
use O
Dijkstra O
's O
algorithm O
to O
find O
the O
connected O
path O
on O
the O
action O
graph O
so O
as O
to O
infer O
the O
action O
chain O
A O
between O
the O
observation O
O O
and O
the O
target O
video O
event O
E O
t O
. O

Knowledge O
- O
guided O
Alignment O

We O
follow O
the O
general O
training O
and O
prediction O
protocol O
of O
cross O
- O
modal O
transformer O
applied O
in O
other O
video O
grounding O
methods O
( O
Kamath O
et O
al O
. O
, O
2021 O
; O

… O
X O
2 O
X O
1 O

The O
man O
in O
a O
T O
- O
shirt O
choked O
on O
food O
is O
vomiting O
into O
the O
toilet O
. O

. O
. O
. O
Transformer O
Encoder O

Transformer O
Decoder O

T O
D O
V O
C O
B O
. O
. O
. O
X O
A O
G O
Node O
Split O
P O
H O
T O
D O
V O
C O
B O
. O
. O
. O
A O
G O
X O
X O
2 O
X O
1 O
Action O
Chain O
Search O
P O
H O
T O
D O
V O
C O
B O
. O
. O
. O
A O
G O
Devour O
Vomit O

Target O
Action O
Chain O
vomiting O

Information O
Flow O
Action O
Search O

Step O
1 O
: O
Knowledge O
- O
guided O
Alignment O

Video O
Observation O

Step O
2 O
: O
Graph O
- O
aware O
Symbolic O
Reasoning O
( O
1 O
) O
The O
transformer O
- O
based O
neural O
network O
learns O
to O
ground O
the O
target O
video O
event O
E O
t O
and O
predicts O
the O
language O
- O
described O
person O
's O
target O
action O
, O
based O
on O
the O
commonsense O
knowledge O
recognition O
ability O
. O
( O
2 O
) O
We O
construct O
the O
action O
graph O
from O
the O
learned O
action O
relation O
memory O
and O
reason O
out O
the O
connection O
path O
between O
the O
start O
node O
( O
the O
video O
action O
) O
and O
the O
end O
node O
( O
the O
observation O
action O
) O
with O
Dijkstra O
's O
algorithm O
. O
. O
The O
transformer O
decoder O
generates O
the O
features O
for O
all O
video O
frames O
. O
For O
each O
frame O
, O
we O
predict O
the O
bounding O
boxes O
and O
whether O
it O
is O
the O
temporal O
start O
or O
end O
of O
the O
target O
event O
E O
t O
. O

However O
, O
there O
is O
complex O
information O
in O
the O
input O
video O
V. O
We O
need O
to O
guide O
the O
model O
to O
focus O
on O
the O
video O
information O
aligned O
with O
the O
observation O
O O
during O
the O
training O
process O
, O
using O
the O
prediction O
learning O
for O
the O
commonsense O
knowledge O
( O
e.g. O
, O
human O
action O
, O
appearance O
, O
etc O
. O
) O
of O
the O
target O
character O
in O
the O
video O
. O
Specifically O
, O
following O
previous O
transformer O
- O
based O
models O
Carion O
et O
al O
. O
, O
2020 O
) O
, O
several O
query O
vectors O
are O
defined O
: O
frame O
level O
vectors O
Q O
r O
= O
{ O
q O
i O
r O
} O
Nr O
i=1 O
and O
video O
level O
vectors O

Q O
v O
= O
{ O
q O
i O
v O
} O
2 O
i=1 O
. O

The O
N O
r O
is O
the O
frame O
number O
in O
the O
video O
V. O
With O
these O
query O
vectors O
, O
we O
apply O
transformer O
decoder O
D O
to O
analyze O
multi O
- O
modal O
features O
F O
fused O
by O
the O
transformer O
encoder O
: O

[ O
F O
r O
, O
F O
v O
] O
= O
D O
( O
[ O
Q O
r O
, O
Q O
v O
] O
, O
F O
) O
, O
( O
2 O
) O

where O
[ O
. O
] O
represents O
the O
feature O
concatenated O
. O

Next O
, O
we O
predict O
the O
commonsense O
knowledge O
of O
the O
language O
- O
described O
character O
using O
these O
output O
frame O
level O
features O
F O
r O
= O
{ O
f O
i O
r O
} O
Nr O
i=1 O
and O
video O
level O
features O

F O
v O
= O
{ O
f O
i O
v O
} O
2 O
i=1 O
. O

The O
scene O
where O
the O
character O
is O
located O
and O
the O
character O
's O
sentiment O
change O
over O
time O
. O
Thus O
, O
we O
predict O
them O
frame O
by O
frame O
using O
MultiLayer O
Perceptron O
( O
MLP O
) O
. O
Viewing O
the O
i O
- O
th O
frame O
as O
an O
example O
, O
the O
prediction O
process O
is O
represented O
as O
: O

p O
i O
sc O
= O
softmax B-HyperparameterName
( O
M O
LP O
sc O
( O
f O
i O
r O
) O
) O
, O
( O
3 O
) O

p O
i O
se O
= O
softmax O
( O
M O
LP O
se O
( O
f O
i O
r O
) O
) O
. O
( O
4 O
) O

In O
them O
, O
the O
M O
LP O
sc O
and O
M O
LP O
se O
are O
the O
MLP O
applied O
to O
predict O
the O
probabilities O
of O
all O
scene O
and O
sentiment O
classes O
( O
p O
i O
sc O
and O
p O
i O
se O
) O
. O
In O
addition O
, O
we O
apply O
the O
video O
level O
features O
F O
v O
to O
predict O
the O
appearance O
and O
clothing O
of O
the O
target O
character O
described O
by O
the O
language O
sentence O
O O
, O
and O
the O
character O
's O
action O
in O
the O
target O
video O
event O
E O
t O
: O

p O
ap O
= O
softmax O
( O
M O
LP O
ap O
( O
f O
1 O
v O
) O
) O
( O
5 O
) O

p O
cl O
= O
softmax O
( O
M O
LP O
cl O
( O
f O
1 O
v O
) O
) O
, O
( O
6 O
) O

p O
ac O
= O
softmax O
( O
M O
LP O
ac O
( O
f O
2 O
v O
) O
) O
. O
( O
7 O
) O

In O
them O
, O
the O
M O
LP O
ap O
, O
M O
LP O
cl O
and O
M O
LP O
ac O
are O
the O
MLP O
applied O
to O
predict O
the O
probabilities O
of O
all O
appearance O
, O
clothing O
, O
and O
action O
classes O
( O
p O
ap O
, O
p O
cl O
, O
and O
p O
ac O
) O
, O
respectively O
. O
We O
employ O
the O
cross O
- O
entropy O
loss O
function O
to O
train O
the O
prediction O
of O
commonsense O
knowledge O
. O

Graph O
- O
aware O
Symbolic O
Reasoning O

The O
prediction O
for O
the O
action O
chain O
A O
requires O
rigorous O
layer O
- O
by O
- O
layer O
logical O
reasoning O
ability O
. O
Considering O
the O
symbolic O
network O
's O
reasoning O
ability O
( O
Yi O
et O
al O
. O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2020c O
) O
, O
we O
design O
a O
traditional O
graph O
theory O
based O
symbolic O
module O
for O
searching O
targeted O
nodes O
( O
actions O
) O
. O

In O
the O
training O
phase O
, O
we O
divide O
the O
action O
chain O
annotation O
labeled O
for O
each O
training O
example O
into O
several O
single O
- O
step O
action O
mappings O
and O
store O
them O
in O
the O
action O
relation O
memory O
module O
. O
During O
the O
process O
, O
new O
actions O
are O
continuously O
introduced O
. O
We O
initialize O
the O
prototype O
feature O
f O
i O
p O
for O
the O
newly O
added O
action O
category O
with O
index O
i. O
In O
addition O
, O
at O
each O
step O
of O
the O
relation O
memory O
update O
, O
we O
construct O
the O
action O
graph O
based O
on O
the O
action O
relations O
. O
There O
may O
be O
N O
d O
different O
connected O
paths O
between O
two O
nodes O
, O
which O
may O
result O
in O
the O
predicted O
action O
chain O
A O
not O
unique O
. O
To O
address O
the O
problem O
, O
we O
replace O
the O
starting O
node O
f O
s O
p O
with O
N O
d O
different O
nodes O
{ O
f O

s O
j O
p O
} O
N O
d O

j=1 O
and O
view O
them O
as O
the O
starting O
of O
each O
path O
. O

During O
the O
inference O
process O
, O
the O
action O
node O
described O
by O
the O
textual O
observation O
O O
is O
detected O
from O
the O
action O
graph O
. O
Using O
the O
traditional O
graph O
theory O
algorithm O
, O
Dijkstra O
( O
Dijkstra O
, O
1959 O
) O
, O
we O
find O
all O
paths O
ending O
at O
this O
node O
. O
Assuming O
that O
there O
are O
N O
s O
nodes O
on O
these O
paths O
, O
we O
view O
them O
as O
the O
candidate O
starting O
nodes O
and O
calculate O
the O
probability O
of O
being O
selected O
p O
sn O
= O
{ O
p O
i O
sn O
} O
Ns O
i=1 O
, O
using O
the O
transformer O
decoder O
predicted O
feature O
f O
2 O
v O
: O

p O
sn O
= O
softmax B-HyperparameterName
( O
[ O
S O
( O
f O
1 O
p O
, O
f O
2 O
v O
) O
, O
S O
( O
f O
2 O
p O
, O
f O
2 O
v O
) O
, O
... O
, O
S O
( O
f O
Ns O
p O
, O
f O
2 O
v O
) O
] O
) O
, O
( O
8 O
) O

where O
the O
S O
( O
. O
) O
is O
the O
similarity O
calculation O
function O
. O
The O
node O
with O
max O
probability O
is O
chosen O
out O
from O
the O
candidate O
starting O
nodes O
. O
With O
the O
starting O
and O
ending O
nodes O
, O
the O
actions O
corresponding O
to O
the O
middle O
nodes O
between O
them O
constitute O
the O
intermediate O
action O
chain O
A O
. O

Experiments O

We O
evaluate O
the O
effectiveness O
of O
the O
proposed O
NOVEL B-MethodName
on O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
, O
followed O
by O
a O
discus- O
sion O
of O
NOVEL B-MethodName
's O
property O
with O
controlled O
studies O
. O

MAR B-TaskName
Experiments O

Implement O
Detail O
. O
Our O
model O
is O
implemented O
based O
on O
the O
PyTorch O
framework O
, O
which O
is O
trained O
on O
a O
Linux O
server O
. O
The O
implementation O
of O
the O
transformer O
part O
is O
based O
on O
the O
TubeDETR B-MethodName
. O
For O
the O
training O
data O
, O
we O
randomly O
rotate O
and O
resize O
the O
input O
frames O
. O
In O
addition O
, O
random O
horizontal O
flips O
and O
size O
cropping O
are O
applied O
during O
the O
video O
frame O
preprocessing O
. O
For O
the O
validation O
and O
test O
data O
, O
we O
only O
normalize O
and O
randomly O
resize O
each O
frame O
. O
In O
the O
training O
process O
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
1 B-HyperparameterValue
and O
the O
random O
seed O
is O
42 O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
0.00005 B-HyperparameterValue
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
is O
0.0001 B-HyperparameterValue
. O
All O
experimental O
environments O
are O
deployed O
in O
Hikvision O
( O
https O
: O
/ O
/ O
www.hikvision.com O
/ O
en O
/ O
) O
. O
Evaluation O
Metrics O
. O
Following O
the O
evaluation O
protocols O
of O
the O
spatio O
- O
temporal O
grounding O
( O
Su O
et O
al O
. O
, O
2021 O
) O
, O
we O
adopt O
m_vIoU B-MetricName
and O
vIoU B-MetricName
@ I-MetricName
R I-MetricName
to O
evaluate O
the O
model O
performance O
. O
The O
vIoU B-MetricName
is O
calculated O
by O

1 O
|Sp∪Sgt| O
n∈Sp∩Sgt O
IoU O
( O
b O
t O
, O
b O
t O
) O
. O

In O
it O
, O
the O
S O
p O
and O
the O
S O
gt O
are O
the O
frame O
sets O
in O
the O
predicted O
and O
ground O
truth O
tubes O
, O
respectively O
. O
Theb O
t O
and O
the O
b O
t O
are O
the O
predicted O
and O
ground O
truth O
bounding O
boxes O
of O
the O
frame O
t. O
The O
vIoU B-MetricName
@ I-MetricName
R I-MetricName
is O
the O
ratio O
of O
samples O
whose O
vIoU B-MetricName
> O
R. B-MetricName
The O
m_vIoU B-MetricName
is O
the O
mean O
vIoU B-MetricName
of O
all O
samples O
. O
In O
addition O
, O
we O
adopt O
the O
action B-MetricName
chain I-MetricName
accuracy I-MetricName
( O
ACC O
) O
to O
evaluate O
the O
model O
performance O
for O
the O
action O
chain O
prediction O
. O

Baselines O
. O
Existing O
methods O
of O
other O
tasks O
can O
not O
be O
transferred O
directly O
to O
our O
MAR B-TaskName
task O
. O
Thus O
, O
we O
extend O
several O
SOTA O
multi O
- O
modal O
and O
reasoning O
models O
as O
the O
baselines O
to O
compare O
. O
In O
detail O
, O
for O
a O
comprehensive O
comparison O
, O
we O
consider O
: O
( O
1 O
) O
multi O
- O
modal O
video O
grounding O
methods O
, O
including O
TubeDETR B-MethodName
, O
IT B-MethodName
- I-MethodName
OS I-MethodName
, O
and O
STVGBert B-MethodName
( O
Su O
et O
al O
. O
, O
2021 O
) O
; O
( O
2 O
) O
action O
chain O
prediction O
methods O
, O
Cycle_C B-MethodName
( O
Farha O
et O
al O
. O
, O
2020 O
) O
and O
FUTR B-MethodName
( O
Gong O
et O
al O
. O
, O
2022 O
) O
. O
Performance O
Comparison O
. O
We O
compare O
our O
NOVEL B-MethodName
model O
with O
the O
baselines O
on O
the O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
for O
the O
MAR B-TaskName
task O
. O
The O
experiment O
results O
are O
shown O
in O
Table O
2 O
. O
From O
it O
, O
we O
can O
observe O
that O
our O
NOVEL B-MethodName
model O
performs O
better O
than O
all O
previous O
methods O
. O
Specifically O
, O
compared O
to O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
, O
TubeDETR B-MethodName
, O
the O
NOVEL B-MethodName
significantly O
improves O
the O
target O
event O
grounding O
( O
vIoU B-MetricName
@ I-MetricName
0.3 I-MetricName
) O
from O
18.7 B-MetricValue
to O
24.4 B-MetricValue
. O
In O
addition O
, O
the O
NOVEL B-MethodName
model O
improves O
the O
action B-MetricName
chain I-MetricName
prediction I-MetricName
from O
62.1 B-MetricValue
to O
72.0 B-MetricValue
compared O
with O
the O
best O
performance O
baseline O
, O
FUTR B-MethodName
. O
We O
attribute O
the O
performance O
improvement O
of O
our O
model O
to O
commonsense O
knowledge O
- O
driven O
perception O
design O
. O
It O
helps O
the O
model O
focus O
on O
the O
correct O
visual O
semantics O
aligned O
with O
the O
textual O
observation O
. O
The O
graph O
symbol O
model O
rigorously O
describes O
the O
logical O
relationship O
between O
actions O
, O
and O
Dijkstra O
's O
algorithm O
accurately O
reasons O
out O
the O
action O
chain O
. O

Comparison O
using O
Different O
Training O
Data O
Volumes O
. O
We O
are O
interested O
in O
how O
the O
NOVEL B-MethodName
model O
performance O
varies O
with O
the O
amount O
of O
training O
data O
. O
To O
this O
end O
, O
we O
randomly O
select O
different O
proportions O
of O
examples O
from O
the O
training O
set O
and O
compare O
our O
NOVEL B-MethodName
model O
with O
several O
state O
- O
ofthe O
- O
arts O
trained O
on O
them O
. O
The O
experimental O
results O
are O
shown O
in O
Figure O
4 O
. O
From O
it O
, O
we O
can O
observe O
that O
the O
NOVEL B-MethodName
model O
performance O
is O
always O
the O
best O
under O
different O
data O
volumes O
. O
Based O
on O
the O
commonsense O
knowledge O
recognition O
ability O
, O
the O
NOVEL B-MethodName
model O
can O
eliminate O
the O
interference O
of O
irrelevant O
information O
on O
training O
, O
so O
that O
the O
model O
can O
learn O
the O
target O
video O
event O
grounding O
more O
effectively O
. O
Even O
if O
the O
training O
set O
is O
small O
, O
the O
NOVEL B-MethodName
model O
still O
has O
higher O
accuracy O
. O

Ablation O
Study O
. O
To O
fully O
evaluate O
the O
NOVEL B-MethodName
model O
's O
effectiveness O
, O
we O
need O
to O
understand O
how O
different O
components O
contribute O
. O
The O
new O
architectures O
are O
constructed O
by O
removing O
several O
compo- O
5 O
. O
From O
the O
figure O
, O
we O
can O
find O
that O
our O
NOVEL B-MethodName
model O
predicts O
the O
target O
video O
event O
and O
the O
action O
chain O
accurately O
. O
In O
addition O
, O
its O
commonsense O
knowledge O
prediction O
is O
also O
correct O
. O
In O
contrast O
, O
the O
baseline O
predictions O
are O
not O
as O
satisfactory O
. O
This O
intuitively O
reflects O
that O
our O
neural O
- O
symbolic O
model O
, O
NOVEL B-MethodName
, O
is O
reasonably O
designed O
for O
the O
multi O
- O
modal O
action O
chain O
Abductive B-TaskName
Reasoning I-TaskName
task O
. O

Spatio O
- O
Temporal O
Grounding O
Experiments O

Based O
on O
our O
proposed O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
, O
the O
models O
suitable O
for O
another O
similar O
language O
- O
vision O
understanding O
task O
, O
Multi B-TaskName
- I-TaskName
modal I-TaskName
Spatio I-TaskName
- I-TaskName
Temporal I-TaskName
Grounding I-TaskName
( O
MSTG B-TaskName
) O
, O
can O
be O
evaluated O
. O
This O
task O
aims O
to O
detect O
the O
spatio O
- O
temporal O
tube O
described O
by O
the O
concise O
language O
sentence O
from O
the O
complex O
video O
content O
( O
Su O
et O
al O
. O
, O
2021 O
) O
. O
We O
further O
evaluate O
the O
effectiveness O
of O
the O
knowledge O
- O
guided O
alignment O
in O
our O
NOVEL B-MethodName
model O
on O
this O
task O
. O

Dataset O
. O
9 O
, O
143 O
labels O
for O
this O
task O
based O
on O
the O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
are O
annotated O
by O
annota- O
4 O
. O
From O
it O
, O
we O
can O
observe O
that O
our O
NOVEL B-MethodName
model O
performs O
best O
compared O
with O
the O
other O
three O
baselines O
. O
Specifically O
, O
it O
improves O
the O
accuracy B-MetricName
( O
m_vIoU O
/ O
vIoU O
@ O
0.3 O
/ O
vIoU O
@ O
0.5 O
) O
from O
8.9 B-MetricValue
/ O
9.5 B-MetricValue
/ O
4.5 B-MetricValue
to O
11.7 B-MetricValue
/ O
12.0 B-MetricValue
/ O
7.0 B-MetricValue
. O
This O
again O
demonstrates O
the O
power O
of O
commonsense O
knowledge O
guidance O
for O
the O
heterogeneous O
information O
alignment O
problem O
. O
In O
addition O
, O
the O
knowledge O
- O
guided O
alignment O
design O
generalizes O
effectively O
to O
different O
AI O
tasks O
, O
where O
heterogeneous O
alignment O
problem O
exists O
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
a O
new O
task O
, O
multi B-TaskName
- I-TaskName
modal I-TaskName
action I-TaskName
chain I-TaskName
abductive I-TaskName
reasoning I-TaskName
, O
to O
promote O
the O
development O
of O
the O
abductive O
field O
. O
This O
cross O
- O
modal O
task O
targets O
to O
reason O
out O
a O
more O
complete O
explanation O
( O
explanation O
event O
grounding O
and O
sequential O
action O
chain O
inference O
) O
than O
the O
previous O
abductive B-TaskName
reasoning I-TaskName
tasks O
. O
Furthermore O
, O
we O
propose O
a O
largescale O
dataset O
( O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
) O
and O
a O
neural B-MethodName
- I-MethodName
symbolic I-MethodName
model I-MethodName
via I-MethodName
commonsense I-MethodName
knowledge I-MethodName
( O
NOVEL B-MethodName
) O
for O
our O
new O
task O
as O
a O
strong O
baseline O
. O
Extensive O
experiments O
on O
the O
TO B-DatasetName
- I-DatasetName
MAR I-DatasetName
dataset O
and O
the O
TO B-DatasetName
- I-DatasetName
MSTG I-DatasetName
dataset O
demonstrate O
the O
effectiveness O
of O
our O
NOVEL B-MethodName
model O
. O

Limitations O

This O
work O
is O
currently O
limited O
to O
the O
action O
chain O
as O
the O
abstract O
summary O
of O
the O
complete O
explanation O
for O
the O
given O
limited O
observation O
. O
In O
the O
future O
, O
we O
will O
further O
upgrade O
this O
task O
, O
e.g. O
, O
considering O
the O
progressive O
textual O
descriptions O
as O
the O
complete O
explanation O
. O
We O
hope O
our O
work O
can O
advance O
the O
reasoning O
AI O
system O
research O
community O
. O

Acknowledgments O

This O
work O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
under O
Grant O
No O
. O
62037001 O
, O
the O
National O
Key O
RD O
Program O
of O
China O
under O
Grant O
No.2022ZD0162000 O
, O
the O
National O
Natural O
Science O
Foundation O
of O
China O
under O
Grant O
No O
. O
62222211 O
and O
Grant O
No.61836002 O
. O
In O
addition O
, O
our O
research O
is O
funded O
by O
the O
Starry O
Night O
Science O
Fund O
at O
Shanghai O
Institute O
for O
Advanced O
Study O
( O
Zhejiang O
University O
) O
. O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Not O
applicable O
. O
Left O
blank O
. O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Embedding B-MethodName
Hallucination I-MethodName
for O
Few O
- O
Shot O
Language O
Fine O
- O
tuning O

Few O
- O
shot O
language O
learners O
adapt O
knowledge O
from O
a O
pre O
- O
trained O
model O
to O
recognize O
novel O
classes O
from O
a O
few O
- O
labeled O
sentences O
. O
In O
such O
settings O
, O
fine O
- O
tuning O
a O
pre O
- O
trained O
language O
model O
can O
cause O
severe O
over O
- O
fitting O
. O
In O
this O
paper O
, O
we O
propose O
an O
Embedding B-MethodName
Hallucination I-MethodName
( O
EmbedHalluc B-MethodName
) O
method O
, O
which O
generates O
auxiliary O
embedding O
- O
label O
pairs O
to O
expand O
the O
finetuning O
dataset O
. O
The O
hallucinator O
is O
trained O
by O
playing O
an O
adversarial O
game O
with O
the O
discriminator O
, O
such O
that O
the O
hallucinated O
embedding O
is O
indiscriminative O
to O
the O
real O
ones O
in O
the O
finetuning O
dataset O
. O
By O
training O
with O
the O
extended O
dataset O
, O
the O
language O
learner O
effectively O
learns O
from O
the O
diverse O
hallucinated O
embeddings O
to O
overcome O
the O
over O
- O
fitting O
issue O
. O
Experiments O
demonstrate O
that O
our O
proposed O
method O
is O
effective O
in O
a O
wide O
range O
of O
language O
tasks O
, O
outperforming O
current O
fine O
- O
tuning O
methods O
. O
Further O
, O
we O
show O
that O
EmbedHalluc B-MethodName
outperforms O
other O
methods O
that O
address O
this O
over O
- O
fitting O
problem O
, O
such O
as O
common O
data B-MethodName
augmentation I-MethodName
, O
semi B-MethodName
- I-MethodName
supervised I-MethodName
pseudo I-MethodName
- I-MethodName
labeling I-MethodName
, O
and O
regularization B-MethodName
. O
The O
code O
will O
be O
made O
available O
at O
: O
https O
: O
/ O
/ O
github.com O
/ O
yiren-jian O
/ O
EmbedHalluc B-MethodName
. O

Introduction O

Fine O
- O
tuning O
a O
pre O
- O
trained O
language O
model O
( O
LM O
) O
on O
a O
downstream O
task O
with O
the O
labeled O
data O
has O
been O
the O
de O
facto O
approach O
in O
many O
NLP O
tasks O
( O
Wang O
et O
al O
. O
, O
2019 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
Conventional O
finetuning O
has O
been O
shown O
to O
be O
effective O
when O
a O
few O
thousands O
of O
labeled O
examples O
are O
available O
. O
Data B-MethodName
augmentation I-MethodName
( O
Wei O
and O
Zou O
, O
2019 O
) O
, O
regularization B-MethodName
and O
re B-MethodName
- I-MethodName
initialization I-MethodName
further O
improve O
the O
results O
. O

However O
, O
the O
performance O
drops O
drastically O
when O
the O
number O
of O
examples O
falls O
to O
only O
a O
few O
dozens O
. O
Experiments O
from O
recent O
work O
have O
shown O
that O
fine O
- O
tuning O
performs O
poorly O
in O
the O
setting O
where O
only O
16 O
examples O
per O
class O
are O
* O
Both O
authors O
contributed O
equally O
to O
this O
research O
. O

given O
. O
Indeed O
, O
tuning O
a O
language O
model O
with O
hundreds O
of O
millions O
of O
parameters O
( O
e.g. O
, O
BERT O
- O
large O
has O
300 O
M O
parameters O
) O
with O
only O
a O
few O
examples O
inevitably O
faces O
the O
over O
- O
fitting O
problem O
. O

Prior O
work O
have O
proposed O
regularization B-MethodName
methods O
to O
overcome O
this O
problem O
. O
However O
, O
we O
show O
in O
our O
experiments O
that O
these O
methods O
fail O
in O
extreme O
data O
scarce O
setting O
. O
We O
speculate O
that O
the O
key O
to O
solve O
this O
issue O
is O
by O
data B-MethodName
augmentation I-MethodName
. O

Current O
common O
text O
data O
augmentation O
methods O
, O
such O
as O
EDA B-MethodName
( O
Wei O
and O
Zou O
, O
2019 O
) O
( O
which O
have O
been O
used O
in O
recent O
few O
- O
shot O
learning O
papers O
( O
Wei O
et O
al O
. O
, O
2021 O
; O
Basu O
et O
al O
. O
, O
2021 O
) O
) O
and O
AEDA O
( O
Karimi O
et O
al O
. O
, O
2021 O
) O
operate O
at O
the O
lexical O
level O
, O
which O
while O
resulting O
in O
human O
readable O
texts O
, O
lead O
to O
limited O
diversity O
due O
to O
the O
discrete O
nature O
of O
the O
lexical O
space O
. O
In O
this O
work O
, O
we O
propose O
to O
use O
a O
generative O
augmentation O
method O
at O
the O
embedding O
space O
for O
few O
- O
shot O
learning O
. O
The O
underlying O
hypothesis O
is O
that O
the O
intra O
- O
class O
relation O
of O
the O
observed O
examples O
can O
be O
modeled O
and O
that O
this O
can O
be O
learned O
from O
a O
few O
- O
samples O
to O
hallucinate O
diverse O
unseen O
examples O
. O
To O
be O
specific O
, O
we O
adapt O
a O
conditional B-MethodName
Wasserstein I-MethodName
Generative I-MethodName
Adversarial I-MethodName
Network I-MethodName
( O
cW B-MethodName
- I-MethodName
GAN I-MethodName
) O
( O
Arjovsky O
et O
al O
. O
, O
2017 O
) O
as O
our O
hallucinator O
to O
hallucinate O
embeddings O
of O
sentences O
. O
By O
observing O
the O
real O
embeddings O
of O
examples O
from O
the O
fine O
- O
tuning O
dataset O
, O
the O
cWGAN B-MethodName
plays O
an O
adversarial O
game O
to O
hallucinate O
embeddings O
that O
can O
fool O
the O
discriminator O
, O
while O
the O
discriminator O
is O
trying O
to O
classify O
the O
fake O
embeddings O
from O
the O
real O
ones O
. O
Once O
the O
halluciantor O
is O
trained O
, O
we O
condition O
it O
on O
labels O
to O
generate O
diverse O
embeddings O
at O
each O
fine O
- O
tuning O
step O
. O
This O
effectively O
extends O
the O
fine O
- O
tuning O
dataset O
with O
diverse O
embedding O
- O
label O
pairs O
which O
carry O
intra O
- O
class O
variation O
that O
can O
be O
a O
useful O
learning O
signal O
for O
the O
language O
learner O
. O

We O
evaluate O
our O
method O
, O
called O
Embedding B-MethodName
Hallucination I-MethodName
( O
Embedhalluc B-MethodName
) O
, O
on O
15 O
tasks O
and O
show O
that O
it O
generally O
improves O
over O
recent O
fine O
- O
tuning O
methods O
. O
We O
further O
experimentally O
show O
the O
overall O
superiority O
of O
EmbedHalluc B-MethodName
when O
comparing O
to O
regularization O
methods O
proposed O
to O
address O
the O
problem O
of O
over O
- O
fitting O
during O
fine O
- O
tuning O
of O
LMs O
, O
such O
as O
Mixout B-MethodName
and O
Re B-MethodName
- I-MethodName
Init I-MethodName
. O
Finally O
, O
since O
our O
method O
is O
a O
form O
of O
data O
augmentation O
, O
we O
also O
compare O
EmbedHalluc B-MethodName
to O
a O
common O
data O
augmentation O
technique O
EDA B-MethodName
, O
and O
semi O
- O
supervised O
learning O
where O
unlabeled O
data O
is O
already O
available O
. O

Related O
Work O

Fine O
- O
tuning O
of O
Language O
Models O
. O
Better O
finetuning O
of O
language O
models O
can O
be O
achieved O
by O
proper O
initialization O
( O
Dodge O
et O
al O
. O
, O
2020 O
) O
, O
regularization O
or O
prompts O
( O
Schick O
and O
Schütze O
, O
2021 O
) O
. O
Other O
tricks O
include O
bias O
correction O
in O
optimizer O
and O
re O
- O
initialization O
of O
top O
layers O
in O
Transformer O
. O
Instead O
of O
fine O
- O
tuning O
all O
parameters O
in O
a O
model O
, O
other O
work O
explore O
only O
learning O
a O
few O
vectors O
( O
Lester O
et O
al O
. O
, O
2021 O
; O
Li O
and O
Liang O
, O
2021 O
; O
Guo O
et O
al O
. O
, O
2021 O
) O
or O
a O
few O
additional O
parameters O
( O
Houlsby O
et O
al O
. O
, O
2019 O
) O
. O
Hallucination O
Methods O
. O
Feature O
Hallucination O
of O
examples O
is O
first O
introduced O
for O
visual O
recognition O
( O
Hariharan O
and O
Girshick O
, O
2017 O
) O
by O
metalearning O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
variational O
inference O
( O
Luo O
et O
al O
. O
, O
2021 O
; O
Lazarou O
et O
al O
. O
, O
2022 O
) O
, O
and O
adversarial O
learning O
Tjio O
et O
al O
. O
, O
2022 O
) O
. O
Label O
Hallucination O
( O
Jian O
and O
Torresani O
, O
2022 O
) O
assigns O
soft O
pseudo O
- O
labels O
for O
unlabelled O
images O
to O
extend O
the O
fine O
- O
tuning O
few O
- O
shot O
dataset O
. O
Learning O
from O
limited O
labeled O
data O
( O
few O
- O
shot O
learning O
) O
in O
Computer O
Vision O
is O
usually O
achieved O
by O
meta O
- O
learning O
( O
Ren O
et O
al O
. O
, O
2018a O
, O
b O
; O
Jian O
et O
al O
. O
, O
2020 O
; O
Jian O
and O
Gao O
, O
2021 O
) O
or O
transfer O
learning O
( O
Tian O
et O
al O
. O
, O
2020 O
) O
. O
In O
NLP O
, O
few O
- O
shot O
learning O
has O
been O
successfully O
applied O
to O
machine O
translation O
( O
Arthaud O
et O
al O
. O
, O
2021 O
) O
, O
abstract O
summarizing O
( O
Fabbri O
et O
al O
. O
, O
2021 O
) O
, O
question O
and O
answering O
( O
Hua O
et O
al O
. O
, O
2020 O
; O
Ram O
et O
al O
. O
, O
2021 O
) O
, O
and O
entity O
recognition O
( O
de O
Lichy O
et O
al O
. O
, O
2021 O
; O
Tong O
et O
al O
. O
, O
2021 O
; O
Ding O
et O
al O
. O
, O
2021 O
) O
, O
by O
meta O
learning O
( O
Li O
and O
Zhang O
, O
2021 O
; O
Bansal O
et O
al O
. O
, O
2020 O
; O
Sharaf O
et O
al O
. O
, O
2020 O
) O
, O
data O
augmentation O
( O
Wei O
et O
al O
. O
, O
2021 O
; O
Wei O
and O
Zou O
, O
2019 O
; O
Karimi O
et O
al O
. O
, O
2021 O
; O
, O
and O
prompts O
Tam O
et O
al O
. O
, O
2021 O
) O
. O

Our O
method O
is O
a O
generative O
data O
augmentation O
method O
in O
the O
embedding O
space O
. O
Different O
from O
( O
Wei O
et O
al O
. O
, O
2021 O
) O
which O
uses O
EDA O
( O
Wei O
and O
Zou O
, O
2019 O
) O
to O
augment O
examples O
at O
the O
discrete O
input O
space O
, O
we O
hallucinate O
auxiliary O
examples O
at O
the O
embedding O
space O
. O
Our O
method O
shares O
similarity O
to O
FDA O
( O
Kumar O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
also O
a O
generative O
data O
augmentation O
method O
, O
but O
at O
the O
feature O
space O
. O
Also O
, O
different O
from O
FDA O
which O
is O
focused O
on O
two O
intent O
classification O
tasks O
, O
our O
method O
can O
be O
applied O
to O
a O
wide O
- O
range O
of O
NLP O
task O
as O
shown O
by O
our O
experiments O
on O
15 O
diverse O
tasks O
. O

Method O

Conditional B-MethodName
Wasserstein I-MethodName
GAN I-MethodName

GAN B-MethodName
( O
Goodfellow O
et O
al O
. O
, O
2014 O
) O
has O
led O
the O
revolution O
of O
generative O
models O
to O
achieve O
impressive O
results O
in O
synthesizing O
images O
( O
Zhu O
et O
al O
. O
, O
2017 O
) O
and O
higher O
dimensional O
data O
. O
Wasserstein B-MethodName
GAN I-MethodName
( O
WGAN B-MethodName
) O
( O
Arjovsky O
et O
al O
. O
, O
2017 O
) O
uses O
the O
Wasserstein O
distance O
as O
the O
objective O
function O
to O
stabilize O
the O
training O
of O
GAN B-MethodName
. O

Our O
hallucinator O
is O
trained O
under O
the O
conditional O
WGAN B-MethodName
framework O
. O
After O
the O
training O
, O
we O
use O
it O
to O
generate O
pseudo O
- O
embeddings O
of O
examples O
by O
feeding O
it O
with O
random O
noisy O
vectors O
z O
sampled O
from O
N O
( O
0 O
, O
1 O
) O
and O
the O
corresponding O
condition O
class O
labels O
c O
i O
. O
The O
hallucinated O
embeddings O
s O
halluc O
, O
in O
principal O
, O
are O
indiscriminative O
to O
the O
embeddings O
of O
observed O
examples O
in O
that O
class O
. O

Fine B-MethodName
- I-MethodName
tuning I-MethodName
with I-MethodName
Hallucinated I-MethodName
Embedding I-MethodName

For O
a O
single O
input O
sentence O
, O
we O
first O
pass O
it O
through O
the O
embedding O
layer O
to O
get O
the O
sentence O
embedding O
s O
sent O
. O
We O
then O
concatenate O
s O
sent O
with O
s O
halluc O
( O
c O
i O
) O
to O
form O
a O
batch O
of O
mixture O
of O
real O
and O
fake O
embeddings O
[ O
s O
sent O
, O
s O
halluc O
( O
c O
i O
) O
] O
. O
The O
encoder O
learns O
from O
the O
batch O
with O
the O
corresponding O
labels O
[ O
c O
sent O
, O
c O
i O
] O
. O

Label B-MethodName
Calibration I-MethodName
. O
The O
hallucinated O
embedding O
s O
halluc O
( O
c O
i O
) O
is O
conditioned O
on O
its O
label O
c O
i O
. O
However O
, O
this O
hard O
label O
may O
not O
best O
represent O
the O
class O
information O
of O
the O
hallucinated O
embedding O
. O
We O
propose O
Label B-MethodName
Calibration I-MethodName
( O
LabelCalib B-MethodName
) O
by O
pseudolabeling O
from O
a O
teacher O
model O
F O
GEN0 O
( O
LM O
1 O
in O
Algorithm O
1 O
) O
, O
where O
F O
GEN0 O
is O
first O
fine O
- O
tuned O
on O
the O
original O
training O
set O
( O
without O
augmentation O
) O
. O
The O
soft O
- O
label O
of O
the O
embedding O
s O
halluc O
( O
c O
i O
) O
is O
then O
c O
pseudo O
, O
i O
= O
F O
GEN0 O
( O
s O
halluc O
( O
c O
i O
) O
) O
. O
Finally O
, O
the O
language O
model O
M O
learns O
from O
the O
hallucinated O
embedding O
by O
KL O
- O
divergence O

L O
halluc O
= O
KL O
( O
M O
( O
s O
halluc O
( O
c O
i O
) O
) O
, O
c O
pseudo O
, O
i O
) O
( O
1 O
) O

The O
total O
loss O
of O
our O
method O
is O

L O
total O
= O
L O
real O
+ O
L O
halluc O
( O
2 O
) O

where O
L O
real O
is O
the O
loss O
learning O
from O
real O
embedding O
- O
label O
pairs O
. O
The O
pseudo O
- O
code O
for O
finetuning O
of O
few O
- O
shot O
language O
learners O
with O
hallucinated O
embeddings O
is O
shown O
in O
Algorithm O
1 O
. O

Note O
that O
baselines O
considered O
in O
this O
paper O
use O
total O
loss O
L O
total O
= O
L O
real O
. O
Computing O
L O
halluc O
requires O
one O
additional O
forward O
pass O
of O
the O
hallucinator O
and O
one O
more O
forward O
pass O
and O
backward O
pass O
of O
the O
language O
model O
. O
Thus O
, O
our O
method O
has O
about O
×2 O
computational O
overhead O
compared O
to O
the O
baselines O
. O

Experiments O

Evaluation O
Datasets O
and O
Protocol O

We O
evaluate O
our O
method O
on O
15 O
classification O
tasks O
. O
The O
evaluations O
are O
conducted O
by O
averaging O
results O
on O
5 O
different O
train O
test O
splits O
. O
We O
sample O
16 O
examples O
per O
class O
to O
form O
a O
training O
set O
and O
construct O
a O
validation O
set O
with O
the O
same O
size O
as O
the O
training O
set O
. O

Training O
Details O
for O
Embedding O
Hallucinators O

The O
sent O
, O
y O
= O
Sample O
( O
T O
rain_Set O
) O

10 O
: O

output O
1 O
= O
LM O
1 O
( O
sent O
) O
L O
halluc O
= O
KL O
( O
prob O
2 O
, O
output O
2 O
) O
25 O
: O

L O
halluc O
.backward O
( O
) O

26 O
: O

optimizer.step O
( O
) O
27 O
: O
end O
for O
28 O
: O
return O
LM O
2 O
L B-HyperparameterName
is O
set O
to O
be O
128 B-HyperparameterValue
. O
The O
discriminator O
is O
a O
3blocks O
model O
, O
each O
bock O
having O
a O
sequence O
of O
FullyConnect O
- O
BatchNorm O
- O
LeakyReLU O
with O
the O
same O
hidden B-HyperparameterName
dimension I-HyperparameterName
of O
512 B-HyperparameterValue
. O

We O
train O
the O
Embedding O
Hallucinators O
for O
150 B-HyperparameterValue
epochs B-HyperparameterName
using O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
, O
the O
Adam O
optimizer B-HyperparameterName
( O
β B-HyperparameterName
= O
( O
0.5 B-HyperparameterValue
, O
0.999 B-HyperparameterValue
) O
) O
, O
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.0002 B-HyperparameterValue
. O
The O
real O
embeddings O
are O
collected O
from O
the O
language O
few O
- O
shot O
training O
set O
by O
passing O
text O
into O
the O
embedding O
layer O
of O
the O
language O
model O
. O
We O
apply O
gradient B-HyperparameterName
penalty I-HyperparameterName
with I-HyperparameterName
weight I-HyperparameterName
of I-HyperparameterName
loss I-HyperparameterName
100 B-HyperparameterValue
for O
training O
the O
cWGAN B-MethodName
. O

Training O
Details O
for O
Few O
- O
Shot O
Language O
Learners O

We O
draw O
two O
mini O
- O
batches O
during O
the O
training O
of O
our O
few O
- O
shot O
language O
learners O
, O
i.e. O
, O
one O
from O
the O
real O
language O
few O
- O
shot O
training O
set O
, O
another O
one O
by O
sampling O
the O
hallucinators O
( O
see O
Algorithm O
1 O
) O
. O

To O
fairly O
compare O
our O
method O
with O
baselines O
and O
other O
methods O
, O
when O
learning O
with O
real O
sentences O
, O
we O
use O
the O
same O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e B-HyperparameterValue
−5 I-HyperparameterValue
( O
further O
justification O
of O
using O
this O
learning O
rate O
can O
be O
found O
in O
Appendix O
D O
) O
. O
Our O
method O
learns O
from O
hallucinated O
embeddings O
with O
a O
grid O
search O
of O
learning O
rate O
of O
1e B-HyperparameterValue
−5 I-HyperparameterValue
, O
5e B-HyperparameterValue
−6 I-HyperparameterValue
, O
1e B-HyperparameterValue
−6 I-HyperparameterValue
, O
and O
batch B-HyperparameterName
size I-HyperparameterName
of O
4 B-HyperparameterValue
, O
6 B-HyperparameterValue
, O
8 B-HyperparameterValue
. O
We O
use O
the O
same O
search O
for O
EDA O
( O
Wei O
and O
Zou O
, O
2019 O
) O
and O
semi O
- O
supervised O
pseduo O
- O
labeling O
( O
SSL O
) O
when O
learning O
with O
additional O
augmented O
or O
pseudo O
- O
labeled O
data O
. O

The O
models O
are O
selected O
based O
on O
the O
validation B-MetricName
accuracy I-MetricName
every O
100 O
steps O
. O
Finally O
, O
results O
are O
reported O
by O
testing O
the O
models O
on O
the O
testing O
dataset O
. O
The O
algorithm O
is O
implemented O
in O
PyTorch-1.10 O
and O
experiments O
are O
conducted O
on O
Nvidia O
RTX-6000 O
and O
RTX O
- O
A6000 O
GPU O
. O

Main O
Results O
on O
15 O
Tasks O

We O
compare O
our O
method O
EmbedHalluc B-MethodName
( O
w B-MethodName
/ I-MethodName
o I-MethodName
or I-MethodName
w I-MethodName
/ I-MethodName
LabelCalib I-MethodName
) O
using O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
on O
15 O
tasks O
with O
two O
fine O
- O
tuning O
methods O
: O
conventional O
( O
Table O
1 O
) O
and O
prompt O
- O
based O
fine O
- O
tuning O
( O
Table O
2 O
) O
. O
Results O
for O
BERT B-MethodName
- I-MethodName
large I-MethodName
- I-MethodName
cased I-MethodName
can O
be O
found O
in O
Appendix O
B. O
In O
conventional O
fine O
- O
tuning O
, O
EmbedHalluc B-MethodName
improves O
over O
the O
baseline O
in O
14 O
tasks O
, O
only O
marginally O
under O
- O
performs O
in O
.6 O
of O
baseline O
) O
. O
When O
combining O
with O
LabelCalib B-MethodName
, O
our O
method O
outperforms O
in O
all O
tasks O
. O
When O
applying O
to O
prompt O
- O
based O
fine O
- O
tuning O
, O
while O
our O
method O
under O
- O
performs O
in O
MNLI B-TaskName
, O
MNLI B-TaskName
- I-TaskName
mm I-TaskName
and O
RTE B-TaskName
, O
it O
outperforms O
for O
all O
other O
tasks O
, O
with O
substantial O
improvements O
over O
the O
baseline O
in O
CoLA B-TaskName
, O
TREC B-TaskName
, O
QNLI B-TaskName
, O
MRPC B-TaskName
. O

The O
relatively O
smaller O
improvements O
for O
promptbased O
methods O
may O
be O
due O
to O
the O
inconsistency O
and O
EDA B-MethodName
edits O
the O
input O
sentences O
by O
applying O
synonym O
replacement O
, O
random O
swap O
, O
random O
deletion O
and O
random O
insertion O
for O
a O
default O
10 O
% O
( O
α O
) O
of O
tokens O
. O
EDA B-MethodName
either O
greatly O
change O
the O
sentence O
with O
a O
large O
α O
or O
fails O
to O
introduce O
substantial O
variations O
( O
which O
is O
crucial O
in O
the O
extreme O
low O
data O
setting O
) O
of O
inputs O
with O
a O
small O
α O
. O
Since O
it O
operates O
in O
the O
continuous O
embedding O
space O
, O
EmbedHalluc B-MethodName
hallucinates O
diverse O
embeddings O
that O
follow O
the O
distribution O
of O
few O
- O
shot O
set O
. O
Thus O
, O
we O
observe O
in O
Table O
3 O
that O
EmbedHalluc B-MethodName
is O
overall O
superior O
to O
EDA B-MethodName
. O

EmbedHalluc B-MethodName
is O
still O
competitive O
when O
comparing O
against O
SSL B-MethodName
which O
assumes O
to O
have O
additional O
64 O
examples O
per O
class O
from O
the O
task O
distribution O
. O

Negative O
Results O
from O
Regularizations O

Our O
method O
can O
also O
be O
viewed O
as O
an O
implicit O
regularization O
method O
. O
Thus O
, O
we O
also O
compare O
to O
two O
latest O
methods O
for O
better O
fine O
- O
tuning O
language O
models O
with O
regularization O
. O
find O
that O
fine O
- O
tuning O
can O
be O
achieved O
by O
: O
correcting O
bias O
in O
the O
optimizer O
, O
re O
- O
initialization O
of O
top O
layers O
, O
and O
training O
longer O
. O
Correcting O
bias O
in O
the O
optimizer O
is O
already O
fixed O
by O
the O
default O
optimizer O
in O
Huggingface O
Transformer O
and O
training O
longer O
surely O
will O
lead O
to O
further O
over O
- O
fitting O
in O
our O
extreme O
data O
scarce O
scenario O
. O
Thus O
, O
we O
consider O
reinitialization B-MethodName
( O
Re B-MethodName
- I-MethodName
Init I-MethodName
) O
of O
top O
layers O
as O
one O
of O
our O
comparisons O
. O
We O
further O
compare O
against O
Mixout B-MethodName
, O
which O
is O
shown O
to O
be O
an O
effective O
regularization O
when O
fine O
- O
tuning O
with O
a O
few O
thousand O
examples O
. O
We O
used O
the O
public O
code O
for O
both O
of O
these O
methods O
. O
Since O
we O
adapt O
their O
code O
to O
our O
extreme O
data O
deficient O
setting O
, O
we O
re O
- O
search O
the O
hyper O
- O
parameters O
of O
both O
methods O
( O
including O
their O
suggested O
values O
) O
. O
For O
Re B-MethodName
- I-MethodName
Init I-MethodName
, O
we O
search O
the O
top O
1,2,3,4,5 O
layers O
; O
and O
for O
Mixout B-MethodName
, O
we O
search O
mixout O
rate O
from O
0.1 O
, O
0.2 O
, O
... O
, O
0.9 O
and O
report O
their O
best O
results O
in O
Table O
4 O
, O
using O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
. O
Results O
for O
BERT B-MethodName
- I-MethodName
large I-MethodName
- I-MethodName
cased I-MethodName
can O
be O
found O
in O
Appendix O
C. O
We O
find O
that O
those O
two O
methods O
fail O
to O
alleviate O
the O
over O
- O
fitting O
problem O
in O
such O
extreme O
setting O
, O
though O
they O
have O
been O
to O
be O
effective O
when O
given O
a O
few O
thousands O
examples O
. O

Comparing O
to O
Adversarial O
Training O

Adversarial O
training O
adds O
noise O
into O
the O
training O
data O
to O
increase O
the O
robustness O
of O
a O
model O
. O
It O
has O
been O
shown O
that O
adversarial O
training O
can O
also O
improve O
the O
performance O
of O
language O
models O
. O
Here O
, O
we O
compare O
EmbedHalluc B-MethodName
to O
two O
recent O
adversarial O
training O
methods O
, O
freeLB B-MethodName
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
and O
SMART B-MethodName
( O
Jiang O
et O
al O
. O
, O
2020 O
) O
adapted O
to O
our O
setting O
. O
For O
freeLB B-MethodName
, O
we O
use O
the O
publicly O
available O
code O
and O

Limitations O

While O
EmbedHalluc B-MethodName
works O
well O
empirically O
, O
it O
relies O
on O
hallucinating O
non O
- O
interpretable O
embeddings O
to O
facilitate O
the O
learning O
process O
. O
Besides O
, O
the O
learning O
of O
cWGAN B-MethodName
requires O
careful O
human O
attention O
to O
maintain O
a O
stable O
training O
. O

Conclusion O

In O
this O
paper O
, O
we O
introduce O
an O
embedding O
hallucination O
method O
for O
data O
augmentation O
for O
few O
- O
shot O
learning O
, O
based O
on O
cWGAN B-MethodName
. O
The O
proposed O
method O
improves O
over O
the O
baselines O
in O
15 O
tasks O
and O
outperforms O
a O
common O
augmentation O
method O
, O
and O
two O
recent O
regularization O
methods O
. O

Ethics O
Statement O

As O
far O
as O
we O
are O
aware O
, O
our O
proposed O
work O
does O
not O
have O
any O
explicit O
ethical O
concerns O
. O
However O
, O
our O
work O
relies O
on O
pre O
- O
trained O
language O
models O
, O
which O
have O
been O
shown O
to O
be O
biased O
in O
prior O
work O
. O
As O
such O
, O
users O
of O
such O
models O
, O
specially O
for O
sensitive O
applications O
, O
should O
be O
aware O
of O
and O
if O
possible O
address O
such O
issues O
. O

A O
Best O
Learning B-HyperparameterName
Rate I-HyperparameterName
for O
RoBERTa B-MethodName
- I-MethodName
prompt I-MethodName

Here O
, O
we O
provide O
best O
learning B-HyperparameterName
rates I-HyperparameterName
( O
LR B-HyperparameterName
, O
searched O
from O
1e B-HyperparameterValue
−5 I-HyperparameterValue
, O
5e B-HyperparameterValue
−6 I-HyperparameterValue
, O
1e B-HyperparameterValue
−6 I-HyperparameterValue
as O
discussed O
in O
main O
paper O
) O
for O
L O
halluc O
of O
EmbedHalluc B-MethodName
for O
each O
task O
used O
in O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
prompt I-MethodName
- I-MethodName
based I-MethodName
fine O
- O
tuning O
. O

B O
EmbedHalluc B-MethodName
with O
BERT B-MethodName

In O
addition O
to O
the O
experiments O
using O
RoBERTa B-MethodName
shown O
in O
the O
main O
paper O
, O
here O
we O
show O
the O
results O
of O
BERT B-MethodName
- I-MethodName
large I-MethodName
- I-MethodName
cased I-MethodName
with O
conventional O
fine O
- O
tuning O
as O
a O
further O
check O
on O
robustness O
of O
our O
method O
with O
respect O
to O
the O
choice O
of O
model O
. O

D O
Learning B-HyperparameterName
Rate I-HyperparameterName
for O
Baselines O

The O
baseline O
has O
only O
one O
loss O
L O
real O
, O
whereas O
we O
are O
learning O
with O
an O
additional O
loss O
L O
halluc O
, O
making O
the O
total O
loss O
to O
be O
L O
real O
+ O
L O
halluc O
. O
The O
learning O
rate O
for O
L O
real O
in O
the O
baselines O
and O
ours O
are O
kept O
the O
same O
. O
Note O
that O
we O
do O
not O
search O
for O
this O
learning O
rate O
for O
our O
method O
. O
We O
choose O
1e B-HyperparameterValue
−5 I-HyperparameterValue
, O
which O
is O
the O
most O
common O
learning B-HyperparameterName
rate I-HyperparameterName
to O
finetune O
BERT B-MethodName
/ O
RoBERTa B-MethodName
. O
As O
we O
show O
in O
Table O
D.1 O
, O
this O
learning O
rate O
produces O
reasonably O
good O
results O
for O
the O
baselines O
, O
being O
the O
best O
for O
13 O
tasks O
and O
only O
marginally O
under O
- O
performing O
in O
the O
other O
2 O
tasks O
. O
The O
results O
in O
Table O
D.1 O
are O
generated O
by O
running O
the O
baselines O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
2 B-HyperparameterValue
and O
different O
learning B-HyperparameterName
rates I-HyperparameterName
1e B-HyperparameterValue
−5 I-HyperparameterValue
, O
2e B-HyperparameterValue
−5 I-HyperparameterValue
, O
5e B-HyperparameterValue
−5 I-HyperparameterValue
suggested O
by O
. O

Learning O
Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
via O
Topic O
- O
informed O
Discrete O
Latent O
Variables O

Recently O
, O
discrete O
latent O
variable O
models O
have O
received O
a O
surge O
of O
interest O
in O
both O
Natural O
Language O
Processing O
( O
NLP O
) O
and O
Computer O
Vision O
( O
CV O
) O
, O
attributed O
to O
their O
comparable O
performance O
to O
the O
continuous O
counterparts O
in O
representation O
learning O
, O
while O
being O
more O
interpretable O
in O
their O
predictions O
. O
In O
this O
paper O
, O
we O
develop O
a O
topic B-MethodName
- I-MethodName
informed I-MethodName
discrete I-MethodName
latent I-MethodName
variable I-MethodName
model I-MethodName
for O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
, O
which O
learns O
a O
shared O
latent O
space O
for O
sentence O
- O
pair O
representation O
via O
vector O
quantization O
. O
Compared O
with O
previous O
models O
limited O
to O
local O
semantic O
contexts O
, O
our O
model O
can O
explore O
richer O
semantic O
information O
via O
topic O
modeling O
. O
We O
further O
boost O
the O
performance O
of O
semantic B-TaskName
similarity I-TaskName
by O
injecting O
the O
quantized O
representation O
into O
a O
transformer O
- O
based O
language O
model O
with O
a O
well O
- O
designed O
semanticdriven O
attention O
mechanism O
. O
We O
demonstrate O
, O
through O
extensive O
experiments O
across O
various O
English O
language O
datasets O
, O
that O
our O
model O
is O
able O
to O
surpass O
several O
strong O
neural O
baselines O
in O
semantic O
textual O
similarity O
tasks O
. O

Introduction O

Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
( O
STS B-TaskName
) O
, O
which O
concerns O
the O
problem O
of O
measuring O
and O
scoring O
the O
relationships O
or O
relevance O
of O
pairs O
of O
text O
on O
real O
- O
valued O
scales O
, O
is O
a O
fundamental O
task O
in O
NLP O
. O
It O
has O
further O
driven O
many O
other O
important O
NLP O
tasks O
such O
as O
machine O
translation O
( O
Zou O
et O
al O
. O
, O
2013 O
) O
, O
text O
summarization O
( O
Mohamed O
and O
Oussalah O
, O
2019 O
) O
, O
question O
answering O
( O
Bordes O
et O
al O
. O
, O
2014 O
) O
, O
and O
etc O
. O

Recently O
, O
deep O
neural O
language O
models O
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
STS B-TaskName
. O
The O
success O
of O
these O
models O
is O
attributed O
to O
their O
adoption O
of O
self O
- O
supervised O
learning O
on O
text O
representations O
, O
which O
overcomes O
the O
absence O
of O
large O
labeled O
STS B-TaskName
data O
in O
many O
domains O
. O
The O
text O
representations O
learned O
by O
these O
models O
, O
usually O
consist O
of O
continuous O
latent O
variables O
, O
and O
can O
be O
fed O
pairwise O
* O
Corresponding O
Author O
into O
some O
functions O
( O
e.g. O
a O
multi O
- O
layer O
perceptron O
) O
to O
compute O
their O
semantic O
similarity O
. O

Compared O
to O
the O
continuous O
ones O
, O
discrete O
latent O
variables O
have O
drawn O
much O
less O
attention O
in O
language O
modeling O
, O
despite O
that O
natural O
languages O
are O
discrete O
in O
nature O
, O
and O
there O
is O
growing O
evidence O
, O
from O
several O
NLP O
tasks O
( O
Jin O
et O
al O
. O
, O
2020 O
; O
Bao O
et O
al O
. O
, O
2020 O
) O
, O
that O
they O
are O
equally O
good O
, O
if O
not O
more O
suitable O
, O
as O
the O
continuous O
counterparts O
. O
As O
a O
widelyused O
training O
technique O
for O
learning O
discrete O
latent O
variables O
, O
vector B-MethodName
- I-MethodName
quantized I-MethodName
variational I-MethodName
autoencoder I-MethodName
( O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
) O
( O
Oord O
et O
al O
. O
, O
2017 O
) O
computes O
the O
values O
for O
the O
variables O
through O
the O
nearest O
neighbor O
lookup O
across O
the O
quantized O
vectors O
from O
the O
shared O
latent O
embedding O
space O
. O
Despite O
its O
success O
in O
speech O
recognition O
and O
computer O
vision O
, O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
has O
yet O
to O
be O
investigated O
in O
its O
use O
and O
viability O
in O
general O
NLP O
tasks O
. O
In O
this O
paper O
, O
we O
intend O
to O
explore O
the O
use O
of O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
and O
its O
generated O
discrete O
latent O
variables O
for O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
. O

Two O
issues O
need O
to O
be O
resolved O
in O
order O
for O
the O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
to O
work O
effectively O
on O
the O
STS B-TaskName
task O
. O
First O
, O
the O
codebook O
embeddings O
need O
to O
be O
carefully O
initialized O
to O
prevent O
the O
so O
- O
called O
codebook O
collapse O
, O
in O
which O
only O
a O
few O
of O
the O
embeddings O
are O
selected O
and O
learned O
by O
the O
model O
, O
causing O
a O
reduction O
in O
the O
representational O
capacity O
of O
the O
codebook O
. O
Past O
research O
has O
indicated O
that O
sufficiently O
informed O
and O
sophisticated O
manipulation O
of O
the O
embedding O
initialization O
can O
mitigate O
this O
problem O
. O
A O
second O
issue O
is O
that O
most O
STS B-TaskName
scenarios O
are O
designed O
to O
teach O
a O
language O
model O
to O
measure O
the O
textual O
similarity O
from O
a O
local O
perspective O
( O
e.g. O
over O
contexts O
within O
each O
sentence O
) O
. O
However O
, O
global O
semantics O
underlying O
a O
broader O
context O
has O
already O
been O
shown O
to O
benefit O
a O
number O
of O
NLP O
tasks O
, O
including O
language O
generation O
( O
Guo O
et O
al O
. O
, O
2020a O
) O
, O
textual O
similarity O
calculation O
( O
Peinelt O
et O
al O
. O
, O
2020 O
) O
, O
keyphrase O
generation O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
, O
etc O
. O
For O
STS B-TaskName
, O
it O
can O
provide O
perspectives O
on O
the O
global O
correlations O
and O
dependencies O
among O
sentences O
, O
which O
can O
be O
leveraged O
to O
better O
distinguish O
the O
semantic O
nuances O
of O
sentence O
- O
level O
contexts O
. O

In O
this O
paper O
, O
we O
propose O
to O
leverage O
topic O
modeling O
to O
tackle O
both O
issues O
. O
The O
topic O
information O
can O
provide O
informative O
guidance O
on O
the O
initialization O
and O
the O
learning O
of O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
's O
codebook O
. O
Furthermore O
, O
since O
the O
discovered O
topics O
can O
capture O
global O
semantics O
( O
e.g. O
, O
semantics O
as O
distributions O
over O
the O
vocabulary O
shared O
by O
the O
whole O
corpus O
) O
, O
they O
can O
be O
used O
to O
calibrate O
the O
bias O
of O
local O
semantics O
on O
the O
measure O
of O
textual O
similarity O
. O

Our O
proposed O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
language I-MethodName
model I-MethodName
features O
two O
major O
components O
to O
enrich O
the O
codebook O
representational O
capacity O
with O
rich O
contextual O
information O
. O
One O
is O
the O
topic O
- O
aware O
sentence O
encoder O
, O
which O
computes O
the O
sentence O
embedding O
using O
its O
( O
local O
) O
topic O
distribution O
and O
the O
corresponding O
global O
topic O
- O
word O
distributions O
. O
The O
other O
is O
a O
topic O
- O
guided O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
codebook O
, O
where O
we O
align O
its O
latent O
codes O
/ O
embeddings O
with O
the O
topic O
embeddings O
jointly O
learned O
by O
an O
NTM O
throughout O
the O
training O
phase O
. O
We O
further O
incorporate O
the O
quantized O
sentence O
representations O
learned O
by O
our O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
into O
transformer O
- O
based O
language O
models O
to O
guide O
the O
learning O
of O
contextual O
embeddings O
. O
Built O
on O
top O
of O
the O
multi O
- O
head O
attention O
, O
a O
semantics O
- O
driven O
attention O
mechanism O
is O
proposed O
to O
compute O
the O
attention O
scores O
based O
on O
the O
quantized O
sentence O
representations O
. O
The O
proposed O
attention O
mechanism O
is O
a O
flexible O
plugand O
- O
play O
module O
that O
requires O
a O
small O
number O
of O
extra O
parameters O
and O
a O
minimum O
change O
to O
the O
existing O
multi O
- O
head O
attention O
implementation O
. O
Our O
contributions O
are O
summarised O
as O
: O

• O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
language O
model O
that O
explores O
the O
use O
of O
discrete O
latent O
variables O
learned O
by O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
for O
STS B-TaskName
. O
• O
We O
introduce O
a O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
model O
that O
combines O
the O
merits O
of O
topic O
modeling O
and O
vector O
quantization O
, O
where O
the O
quantization O
is O
informed O
by O
the O
topic O
information O
jointly O
learned O
by O
an O
NTM O
to O
enhance O
its O
robustness O
against O
the O
collapsing O
problem O
and O
its O
capacity O
of O
capturing O
both O
the O
global O
and O
local O
semantics O
. O
• O
We O
present O
a O
simple O
yet O
effective O
semanticsdriven O
attention O
mechanism O
to O
inject O
the O
quantized O
representations O
into O
transformer O
models O
, O
which O
can O
serve O
as O
a O
plug O
- O
and O
- O
play O
module O
for O
almost O
all O
the O
existing O
transformer O
- O
based O
models O
. O
• O
Comprehensive O
experiments O
on O
six O
real O
- O
world O
datasets O
for O
semantic B-TaskName
textual I-TaskName
matching I-TaskName
demon O
- O
strate O
the O
effectiveness O
of O
our O
model O
and O
the O
significance O
of O
its O
major O
components O
. O

2 O
Related O
Work O

Neural O
Language O
Models O
for O
STS B-TaskName

Neural O
language O
models O
have O
played O
a O
key O
role O
in O
fulfilling O
the O
STS B-TaskName
task O
. O
Among O
them O
, O
the O
earlier O
CNN O
- O
based O
model O
( O
Hu O
et O
al O
. O
, O
2014 O
; O
Yin O
et O
al O
. O
, O
2016 O
) O
and O
the O
RNN O
- O
based O
model O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
used O
to O
achieve O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
before O
the O
emergence O
of O
the O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
and O
the O
large O
- O
scale O
pretraining O
of O
its O
corresponding O
models O
. O
These O
pretrained O
models O
, O
with O
their O
ability O
to O
be O
fine O
- O
tuned O
on O
various O
domains O
and O
tasks O
, O
have O
dominated O
the O
entire O
NLP O
area O
, O
significantly O
improving O
the O
task O
performance O
in O
the O
specific O
domains O
. O
As O
the O
most O
prominent O
model O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
along O
with O
its O
variants O
including O
RoBERTa B-MethodName
, O
BERT B-MethodName
- I-MethodName
sim I-MethodName
( O
Xia O
et O
al O
. O
, O
2021 O
) O
, O
AL B-MethodName
- I-MethodName
BERT I-MethodName
( O
Lan O
et O
al O
. O
, O
2020 O
) O
and O
SemBERT B-MethodName
, O
have O
achieved O
superior O
results O
in O
the O
STS B-TaskName
task O
. O
Despite O
the O
achieved O
success O
, O
current O
language O
models O
are O
mostly O
" O
black O
- O
box O
" O
models O
with O
low O
interpretability O
, O
which O
require O
additional O
explanatory O
components O
for O
their O
predictions O
in O
needed O
fields O
such O
as O
Biomedicine O
and O
Finance O
. O

Neural O
Topic O
Models O

The O
success O
of O
Variational O
AutoEncoder O
( O
VAE O
) O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
has O
led O
to O
the O
development O
of O
a O
series O
of O
neural O
topic O
models O
( O
NTM O
) O
( O
Zhao O
et O
al O
. O
, O
2017 O
( O
Zhao O
et O
al O
. O
, O
, O
2021b O
, O
( O
Refer O
to O
Zhao O
et O
al O
. O
( O
2021a O
) O
for O
a O
comprehensive O
survey O
on O
NTMs O
) O
, O
where O
the O
posterior O
distribution O
of O
document O
topics O
is O
approximated O
by O
a O
neural O
network O
during O
the O
variational O
inference O
process O
, O
known O
as O
the O
neural O
variational O
inference O
( O
NVI O
) O
( O
Miao O
et O
al O
. O
, O
2016 O
) O
. O
The O
NVI O
also O
enables O
NTMs O
to O
be O
easily O
combined O
with O
various O
language O
models O
for O
capturing O
the O
global O
semantics O
information O
. O
These O
hybrid O
models O
are O
known O
as O
the O
topic O
- O
aware O
language O
models O
. O
Their O
combination O
strategies O
can O
be O
largely O
characterized O
by O
either O
concatenation O
of O
( O
document O
) O
topic O
distributions O
with O
the O
local O
word O
embeddings O
, O
or O
( O
multi O
- O
head O
) O
attention O
mechanisms O
towards O
the O
inferred O
topics O
. O
These O
models O
have O
been O
shown O
to O
achieve O
higher O
performance O
than O
the O
plain O
language O
models O
on O
various O
tasks O
, O
such O
as O
text O
summarization O
, O
keyphrase O
generation O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
and O
document O
classification O
( O
Zeng O
et O
al O
. O
, O
2018 O
) O
. O

VQ B-MethodName
- I-MethodName
VAE I-MethodName

Rather O
than O
learning O
continuous O
representations O
, O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
( O
Oord O
et O
al O
. O
, O
2017 O
) O
learns O
quantized O
representations O
via O
vector O
quantization O
. O
It O
was O
first O
applied O
to O
speech O
representation O
( O
Chorowski O
et O
al O
. O
, O
2019 O
) O
and O
video O
generation O
( O
Yan O
et O
al O
. O
, O
2021 O
) O
. O
Recently O
, O
it O
has O
attracted O
lots O
of O
attention O
in O
a O
variety O
of O
NLP O
tasks O
. O
In O
machine O
translation O
, O
Kaiser O
et O
al O
. O
( O
2018 O
) O

Modelling O
Framework O

Given O
a O
pair O
of O
sentence O
X O
= O
{ O
X O
i O
, O
X O
j O
} O
as O
the O
input O
, O
where O
each O
sentence O
is O
represented O
as O
the O
bag O
- O
of O
- O
words O
( O
bow O
) O
vectors O
and O
contextualized O
embedding O
, O
our O
modelling O
framework O
, O
as O
shown O
in O
Figure O
1 O
, O
predicts O
the O
semantic O
similarity O
Y O
between O
X O
i O
and O
X O
j O
. O

Neural O
Topic O
Component O

We O
adopt O
a O
VAE B-MethodName
- I-MethodName
based I-MethodName
neural I-MethodName
topic I-MethodName
model I-MethodName
( O
Miao O
et O
al O
. O
, O
2017 O
) O
to O
learn O
latent O
topics O
. O
Different O
from O
LDA O
( O
Blei O
et O
al O
. O
, O
2003 O
) O
, O
NTM O
parameterises O
the O
latent O
topics O
θ O
using O
a O
neural O
network O
conditioned O
on O
a O
draw O
from O
a O
Gaussian O
Softmax O
Construction O
. O

Here O
, O
θ O
∈ O
R O
K O
represents O
the O
topic O
proportions O
of O
a O
sentence O
X O
, O
where O
K O
denotes O
the O
number O
of O
topics O
. O
Let O
β O
k O
∈ O
R O
V O
be O
a O
distribution O
over O
a O
vocabulary O
V O
associated O
with O
a O
topic O
k. O
Following O
( O
Wang O
and O
YANG O
, O
2020 O
) O
, O
we O
use O
word O
embeddings O
ω O
∈ O
R O
V O
×E O
, O
topic O
embeddings O
ϕ O
∈ O
R O
K×E O
to O
compute O
β O
k O
as O
β O
k O
= O
Sof O
tmax O
( O
ω O
• O
ϕ O
T O
k O
) O
. O
With O
NVI O
, O
we O
can O
use O
an O
encoder O
network O
to O
approximate O
the O
true O
posterior O
. O
Specifically O
, O
the O
encoder O
generates O
the O
variational O
parameters O
µ O
and O
σ O
2 O
through O
neural O
networks O
and O
the O
latent O
vari O
- O
able O
θ O
is O
sampled O
with O
the O
Gaussian O
reparameterization O
trick O
. O
The O
decoder O
reconstructs O
the O
BoW O
representation O
of O
a O
document O
by O
maximizing O
the O
log O
- O
likelihood O
of O
the O
input O
. O
The O
loss O
function O
( O
i.e. O
, O
ELBO O
) O
contains O
a O
reconstruction O
error O
term O
and O
a O
KL O
divergence O
term O
as O

L O
N O
T O
M O
=D O
KL O
[ O
q O
( O
θ|x O
) O
||p O
( O
θ|x O
) O
] O
−E O
q O
( O
θ|x O
) O
[ O
M O
m=1 O
log O
p O
( O
wm|θ O
) O
] O
, O

where O
q O
( O
θ|x O
) O
denotes O
the O
variational O
posterior O
distribution O
of O
θ O
given O
the O
sentence O
x O
= O
( O
w O
1 O
, O
... O
, O
w O
m O
) O
, O
approximating O
the O
true O
posterior O
p O
( O
θ|x O
) O
. O

Topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName

VQ B-MethodName
- I-MethodName
VAE I-MethodName
( O
Oord O
et O
al O
. O
, O
2017 O
) O
takes O
the O
contextualized O
embeddings O
of O
words O
within O
a O
sentence O
: O
X O
seq O
= O
( O
x O
seq O
1 O
, O
... O
, O
x O
seq O
m O
) O
into O
its O
encoder O
to O
produce O
the O
corresponding O
latent O
representations O
Z O
e O
. O
Then O
, O
the O
quantized O
representations O
Z O
q O
are O
calculated O
by O
the O
nearest O
neighbor O
look O
- O
up O
using O
the O
predefined O
and O
shared O
embedding O
space O
E O
, O
and O
further O
used O
as O
an O
input O
to O
the O
decoder O
to O
reconstruct O
the O
original O
sentence O
text O
. O

However O
, O
since O
the O
original O
embedding O
space O
of O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
is O
randomly O
initialized O
, O
these O
learned O
embeddings O
can O
be O
arbitrary O
without O
clear O
semantic O
meaning O
. O
To O
guide O
the O
codebook O
learning O
, O
we O
incorporate O
the O
topic O
information O
into O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
by O
designing O
the O
following O
two O
components O
: O
Topic O
Sensitive O
Encoder O
. O
We O
use O
a O
standard O
single O
- O
layer O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
as O
our O
encoder O
. O
Specifically O
, O
for O
a O
sentence O
consisting O
of O
a O
sequence O
of O
m O
words O
X O
= O
( O
w O
1 O
, O
... O
, O
w O
m O
) O
, O
its O
corresponding O
word O
representations O
X O
seq O
are O
generated O
by O
the O
transformer O
embedding O
layer O
, O
which O
concatenates O
the O
word O
embedding O
and O
position O
embedding O
. O
To O
incorporate O
the O
topic O
information O
, O
we O
leverage O
the O
sentence O
topic O
distribution O
θ O
and O
the O
word O
topic O
weight O
vector O
β O
wm O
∈ O
R O
K O
from O
the O
NTM O
model O
to O
compute O
the O
topical O
embedding O
of O
the O
words O
X O
t O
= O
x O
t O
1 O
, O
... O
, O
x O
t O
m O
∈ O
R O
m×K O
, O
where O
x O
t O
m O
∈ O
R O
K O
= O
θ O
⊗ O
β O
wm O
. O
We O
concatenate O
the O
X O
seq O
and O
X O
t O
and O
feed O
them O
into O
the O
encoder O
as O
Z O
e O
∈ O
R O
m×E O
= O
f O
enc O
( O
X O
seq O
⊕ O
X O
t O
) O
. O

Topical O
Latent O
Embedding O
. O
We O
get O
final O
word O
representations O
Z O
e O
= O
{ O
z O
e O
1 O
, O
... O
, O
z O
e O
m O
} O
from O
the O
topic O
sensitive O
encoder O
. O
These O
representations O
are O
mapped O
onto O
the O
nearest O
element O
of O
embedding O
E. O
Different O
from O
previous O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
models O
that O
use O
a O
randomly O
initialized O
embedding O
E O
, O
we O
leverage O
the O
topic O
embedding O
ϕ O
∈ O
R O
K×E O
from O
NTM O
. O
The O
discretization O
process O
is O
defined O
as O
: O
For O
each O
i O
∈ O
{ O
1 O
, O
. O
. O
. O
, O
m O
} O
, O
z O
q O
i O
= O
ϕ O
k O
∈ O
R O
E O
, O
where O
k O
= O
arg O
min O
j∈ O
{ O
1 O
, O
... O
, O
k O
} O
∥z O
e O
i O
− O
ϕ O
j O
∥ O
2 O
. O
Thus O
, O
we O
have O
quantized O
representations O
Z O
q O
= O
{ O
z O
q O
1 O
, O
... O
, O
z O
q O
m O
} O
∈ O
R O
m×E O
for O
a O
sentence O
. O
We O
then O
feed O
Z O
q O
into O
the O
decoder O
which O
is O
also O
a O
singlelayer O
transformer O
to O
reconstruct O
the O
sentence O
X. O
The O
overall O
training O
objective O
is O
thus O
defined O
as O

LV O
Q−V O
AE O
= O
m O
i=1 O
( O
− O
log O
p O
( O
xi|z O
q O
i O
) O
+ O
∥sg O
[ O
z O
e O
i O
] O
− O
z O
q O
i O
∥ O
2 O
2 O
+ O
λ O
∥z O
e O
i O
− O
sg O
[ O
z O
q O
i O
] O
∥ O
2 O
2 O
) O
. O
( O
1 O
) O

The O
first O
term O
above O
is O
the O
reconstruction O
error O
of O
the O
decoder O
given O
Z O
q O
. O
The O
last O
two O
terms O
are O
used O
to O
minimize O
the O
distance O
between O
the O
latent O
embedding O
E O
and O
the O
encoder O
output O
Z O
e O
, O
where O
λ O
is O
a O
commitment O
loss O
and O
sg O
( O
• O
) O
means O
the O
stop O
- O
gradient O
operation O
. O
It O
is O
noteworthy O
that O
our O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
model O
jointly O
trained O
with O
NTM O
can O
explicitly O
learn O
the O
topic O
assignment O
of O
each O
individual O
words O
, O
which O
is O
not O
feasible O
for O
NTM O
alone O
. O

Semantics O
- O
driven O
Multi O
- O
head O
Attention O

The O
multi O
- O
head O
attention O
in O
transformer O
- O
based O
models O
explores O
the O
relationships O
among O
tokens O
by O
calculating O
the O
token O
similarities O
, O
defined O
as O

Q O
= O
K O
= O
V O
= O
h O
l−1 O
MultiHead O
( O
Q O
, O
K O
, O
V O
) O
= O
Concat O
( O
head1 O
, O
• O
• O
• O
, O
headn O
) O
W O
O O
headi O
= O
Attention O
( O
Q O
i O
W O
Q O
i O
, O
K O
i O
W O
K O
i O
, O
V O
i O
W O
V O
i O
) O
( O
2 O
) O

where O
h O
l−1 O
∈ O
R O
L× O
( O
n•E O
) O
is O
the O
output O
of O
last O
layer O
, O
L O
is O
the O
length O
of O
sentence O
pair O
, O
n O
is O
the O
number O
of O
attention O
head O
, O
E O
is O
the O
hidden O
size O
of O
each O
attention O
head O
. O
W O
Q O
i O
, O
W O
K O
i O
, O
W O
V O
i O
and O
W O
O O
i O
∈ O
R O
E×E O
are O
projection O
matrices O
. O
Scaled O
Dot O
- O
Product O
method O
is O
adopted O
to O
calculate O
the O
attention O
function O
: O

Attention O
( O
Q O
, O
K O
, O
V O
) O
= O
Softmax O
( O
QK O
T O
√ O
d O
k O
) O
V O
( O
3 O
) O

To O
enable O
the O
transformer O
- O
based O
model O
to O
learn O
discrete O
semantics O
between O
two O
sentences O
, O
we O
design O
a O
semantics O
- O
driven O
attention O
mechanism O
. O
Specifically O
, O
given O
the O
quantized O
representations O
Z O
q O
i O
, O
Z O
q O
j O
for O
sentence O
X O
i O
and O
X O
j O
respectively O
, O
we O
compute O
the O
quantized O
representation O
- O
based O
query O
and O
key O
matrices O
as O
follows O
: O

Qq O
= O
ZW O
Q O
q O
, O
Kq O
= O
ZW O
K O
q O
, O
Z O
= O
FFN O
[ O
Z O
q O
i O
⊕ O
Z O
q O
j O
] O
, O
( O
4 O
) O

where O
Z O
, O
Q O
q O
, O
K O
q O
∈ O
R O
L×E O
, O
W O
Q O
q O
and O
W O
K O
q O
∈ O
R O
E×E O
, O
FFN O
means O
a O
fully O
connected O
feed O
- O
forward O
network O
. O
The O
semantics O
- O
driven O
attention O
can O
be O
defined O
as O
: O

Attention O
( O
Q O
, O
K O
, O
V O
) O
= O
Softmax O
( O
QK O
T O
+ O
QqK O
T O
q O
√ O
d O
k O
) O
V O
. O
( O
5 O
) O

It O
is O
different O
from O
the O
topic O
select O
- O
attention O
block O
introduced O
by O
( O
Lu O
et O
al O
. O
, O
2022 O
) O
, O
which O
is O
an O
attention O
block O
built O
separately O
on O
top O
of O
the O
multi O
- O
head O
attention O
block O
. O
In O
addition O
to O
the O
introduction O
of O
the O
quantized O
representations O
into O
the O
self O
- O
attention O
mechanism O
, O
we O
also O
leverage O
those O
representations O
to O
enhance O
the O
final O
output O
h O
l O
of O
multi O
- O
layer O
trans- O

former O
, O
i.e. O
, O
[ O
h O
l O
; O
Z O
q O
i O
⊕ O
Z O
q O
j O
] O
∈ O
R O
L× O
( O
n+1 O
) O
E O

, O
which O
is O
further O
fed O
into O
a O
non O
- O
linear O
layer O
to O
predict O
label O
Y O
for O
semantic B-TaskName
textual I-TaskName
matching I-TaskName
. O

Joint O
Training O

Our O
proposed O
framework O
integrates O
the O
three O
modules O
in O
Figure O
1 O
, O
i.e. O
, O
the O
NTM O
, O
the O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
, O
and O
the O
semantics O
- O
attended O
Transformer O
equipped O
with O
the O
semantics O
- O
driven O
attention O
mechanism O
. O
We O
first O
pretrain O
the O
NTM O
to O
obtain O
meaningful O
topic O
information O
used O
to O
initialize O
the O
code- O

L O
= O
L O
V O
Q−V O
AE O
+ O
γL O
N O
T O
M O
, O
( O
6 O

where O
γ B-HyperparameterName
is O
the O
trade B-HyperparameterName
- I-HyperparameterName
off I-HyperparameterName
parameter I-HyperparameterName
controlling O
the O
balance O
between O
the O
topic O
model O
and O
the O
topicenhanced B-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
model I-MethodName
. O
Finally O
, O
we O
incorporate O
the O
learned O
quantized O
representations O
from O
topicenhanced B-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
into O
the O
transformer O
block O
and O
fine O
- O
tune O
the O
transformer O
block O
on O
the O
STS B-TaskName
task O
. O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
. O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
is O
a O
collection O
of O
sentence O
pairs O
extracted O
from O
news O
headlines O
and O
other O
sources O
. O
It O
comprises O
a O
selection O
of O
the O
English O
datasets O
used O
in O
the O
STS B-TaskName
task O
which O
were O
annotated O
with O
a O
score O
from O
1 O
to O
5 O
denoting O
how O
similar O
the O
two O
sentences O
are O
. O
The O
SemEval B-DatasetName
community O
question O
answering O
has O
three O
subtasks O
: O
( O
A O
) O
Question O
- O
Comment O
Similarity O
, O
( O
B O
) O
Question O
- O
Question O
Similarity O
, O
( O
C O
) O
Question O
- O
External O
Comment O
Similarity O
( O
Nakov O
et O
al O
. O
, O
2015 O
( O
Nakov O
et O
al O
. O
, O
, O
2016 O
( O
Nakov O
et O
al O
. O
, O
, O
2017 O
. O
Following O
their O
settings O
, O
we O
use O
the O
2016 O
test O
set O
as O
the O
development O
set O
and O
the O
2017 O
test O
set O
as O
the O
test O
set O
. O
Table O
1 O
summarises O
their O
statistics O
. O

Baseline O
Models O

We O
implemented O
several O
strong O
baselines O
in O
the O
STS B-TaskName
task O
for O
comparison O
. O
To O
evaluate O
the O
semanticsattended O
Transformer O
, O
we O
select O
two O
widely O
- O
used O
pretrained O
language O
models O
, O
Bert B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
. O
We O
used O
the O
small O
version O
to O
fine O
- O
tune O
the O
STS B-TaskName
task O
, O
i.e. O
, O
Bert B-MethodName
- I-MethodName
base I-MethodName
and O
RoBERTa B-MethodName
- I-MethodName
base I-MethodName
. O
And O
we O
modified O
these O
two O
models O
by O
adding O
the O
semanticsdriven O
multi O
- O
head O
attention O
to O
the O
original O
multihead O
attention O
, O
which O
is O
denoted O
as O
DisBert B-MethodName
and O
DisRoBERTa B-MethodName
respectively O
. O
We O
selected O
tBERT B-MethodName
( O
Peinelt O
et O
al O
. O
, O
2020 O
) O
as O
an O
important O
baseline O
that O
also O
makes O
use O
of O
topic O
information O
. O
Different O
from O
tBert B-MethodName
which O
directly O
concatenates O
topic O
representation O
and O
sentence O
pair O
vector O
, O
we O
leverage O
the O
topic O
model O
to O
help O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
capture O
global O
semantics O
through O
the O
topic O
sensitive O
encoder O
and O
topical O
latent O
embedding O
. O
We O
also O
select O
the O
knowledge O
- O
enhanced O
Bert O
model O
ERNIE B-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
that O
incorporates O
knowledge O
graph O
into O
Bert B-MethodName
and O
achieves O
improvements O
on O
language O
understanding O
tasks O
. O
To O
further O
verify O
the O
performance O
on O
the O
STS B-TaskName
task O
, O
we O
also O
compared O
with O
Semantic B-MethodName
- I-MethodName
aware I-MethodName
Bert I-MethodName
( O
SemBert B-MethodName
) O
, O
which O
incorporates O
explicit O
contextual O
semantics O
and O
outperforms O
other O
Bert O
- O
based O
models O
on O
the O
STS B-TaskName
task O
. O

Experimental O
Settings O

For O
all O
methods O
, O
we O
performed O
a O
greedy O
search O
to O
find O
their O
optimal O
hyper O
- O
parameters O
using O
the O
development O
set O
. O
For O
NTM O
, O
we O
processed O
the O
datasets O
with O
gensim B-HyperparameterValue
tokenizer B-HyperparameterName
1 O
, O
and O
a O
vocabulary O
for O
each O
dataset O
was O
built O
based O
on O
its O
training O
set O
with O
stop O
words O
and O
words O
occurring O
less O
than O
3 B-HyperparameterValue
times O
removed O
. O
The O
BoW O
input O
of O
topic O
model O
X O
BoW O
on O
each O
dataset O
was O
constructed O
based O
on O
the O
corresponding O
vocabulary O
. O
For O
the O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
model O
, O
we O
set O
the O
dimension B-HyperparameterName
of I-HyperparameterName
each I-HyperparameterName
code I-HyperparameterName
E O
to O
64 B-HyperparameterValue
and O
commitment O
loss O
λ B-HyperparameterName
= O
0.0001 B-HyperparameterValue
. O
2 O
For O
joint O
training O
, O
we O
pretrained O
the O
NTM O
and O
selected O
the O
best O
model O
based O
on O
their O
reconstruction O
perplexity O
. O
Then O
we O
jointly O
trained O
the O
selected O
NTM O
and O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
with O
γ B-HyperparameterName
= O
1 B-HyperparameterValue
. O
We O
fine O
- O
tuned O
our O
semantics O
- O
driven O
language O
model O
and O
the O
vanilla O
one O
with O
the O
same O
parameters O
for O
a O
fair O
comparison O
, O
following O
their O
settings O
with O
the O
publicly O
available O
code O
. O
We O
use O
Pearson B-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
PC B-MetricName
) O
and O
Spearman B-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
SC B-MetricName
) O
for O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
datasets O
. O
For O
other O
datasets O
, O
we O
report O
accuracy B-MetricName
( O
ACC B-MetricName
) O
and O
F1 B-MetricName
score I-MetricName
. O
More O
detailed O
parameter O
settings O
are O
described O
in O
the O
appendix O
. O

Performance O
on O
Six O
STS B-TaskName
Datasets O

Table O
2 O
shows O
the O
results O
drove O
on O
the O
six O
datasets O
, O
for O
which O
we O
have O
the O
following O
observations O
. O
• O
Semantics B-MethodName
- I-MethodName
attended I-MethodName
transformer I-MethodName
is O
effective O
. O

Our O
model O
, O
DisBert B-MethodName
, O
which O
introduces O
the O
quantized O
representations O
into O
the O
transformer O
, O
significantly O
outperforms O
Bert B-MethodName
, O
with O
the O
improvement O
ranging O
from O
0.1 B-MetricValue
% I-MetricValue
to O
5.5 B-MetricValue
% I-MetricValue
on O
the O
six O
datasets O
. O
The O
results O
demonstrate O
the O
usefulness O
of O
our O
semantics B-MethodName
- I-MethodName
driven I-MethodName
multi I-MethodName
- I-MethodName
head I-MethodName
attention I-MethodName
mechanism I-MethodName
. O
Similarly O
, O
RoBERTa B-MethodName
equipped O
with O
vector O
quantized O
representations O
learned O
by O
our O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
also O
achieves O
improved O
performance O
. O
Thus O
, O
the O
results O
on O
both O
Bert B-MethodName
and O
RoBERTa B-MethodName
verify O
that O
our O
semantics O
- O
driven O
attention O
is O
an O
effective O
plug O
- O
and O
- O
play O
module O
that O
can O
be O
readily O
applied O
to O
various O
transformer O
- O
based O
language O
models O
. O
Q O
q O
K O
t O
q O
) O
in O
Eq O
( O
5 O
) O
. O
w O
/ O
o O
Output O
- O
enhanced O
is O
the O
variant O
does O
not O
concatenate O
quantized O
variable O
Z O
q O
with O
h O
l O
in O
the O
output O
layer O
. O

As O
shown O
in O
Figure O
2 O
we O
have O
the O
following O
observation O
: O
( O
1 O
) O
Topic O
information O
is O
informative O
for O
the O
STS B-TaskName
task O
for O
both O
DisBert B-MethodName
and O
DisRoBERTa B-MethodName
. O
The O
performance O
of O
DisBert B-MethodName
has O
dropped O
0.96 B-MetricValue
, O
0.04 B-MetricValue
, O
and O
0.28 B-MetricValue
on O
MRPC B-DatasetName
datasets O
without O
the O
topic O
model O
, O
topic O
sensitive O
encoder O
, O
and O
topical O
latent O
embedding O
respectively O
. O
( O
2 O
) O
The O
well O
- O
designed O
semantics O
- O
driven O
multi O
- O
head O
attention O
benefits O
the O
STS B-TaskName
task O
. O
Without O
such O
attention O
, O
performance O
dropped O
for O
both O
DisBert B-MethodName
and O
DisRoBERTa B-MethodName
on O
both O
datasets O
. O
Meanwhile O
, O
when O
the O
output O
is O
equipped O
with O
quantized O
representations O
, O
performance O
is O
significantly O
improved O
, O
which O
further O
verifies O
that O
the O
learned O
topic O
- O
informed O
discrete O
latent O
variable O
can O
capture O
semantics O
and O
improve O
the O
STS B-TaskName
task O
. O

Joint O
Training O
vs O
Multistage O
Training O

After O
we O
pretrained O
the O
NTM O
, O
there O
are O
two O
ways O
of O
training O
our O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
. O
One O
is O
to O
utilize O
the O
topic O
embeddings O
from O
the O
pretrained O
NTM O
as O
the O
codebook O
initialization O
in O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
and O
train O
the O
latter O
while O
holding O
the O
former O
fixed O
( O
denoted O
by O
Multistage O
in O
Table O
3 O
) O
, O
the O
other O
is O
to O
joint O
train O
the O
pretrained O
NTM O
and O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
( O
denoted O
by O
Joint O
) O
. O
The O
results O
of O
these O
two O
training O
methods O
derived O
from O
the O
six O
STS B-TaskName
datasets O
are O
shown O
in O
suggests O
that O
we O
can O
leverage O
the O
topic O
- O
enhanced O
discrete O
variable O
in O
all O
data O
sizes O
, O
even O
when O
the O
training O
data O
is O
scarce O
. O

Impact O
of O
Topic O
Numbers O
. O
Figure O
4 O
shows O
the O
performance O
of O
DisBert B-MethodName
given O
varying O
topic O
numbers O
. O
As O
we O
can O
see O
, O
the O
curves O
on O
all O
datasets O
are O
not O
monotonic O
and O
the O
best O
accuracy B-MetricName
is O
achieved O
with O
different O
numbers B-HyperparameterName
of I-HyperparameterName
topics I-HyperparameterName
, O
e.g. O
, O
k B-HyperparameterName
= O
30 B-HyperparameterValue
on O
the O
MRPC B-DatasetName
dataset O
. O
We O
also O
found O
that O
on O
larger O
training O
datasets O
DisBert B-MethodName
requires O
larger O
topic B-HyperparameterName
numbers I-HyperparameterName
and O
vice O
versa O
, O
e.g. O
, O
k B-HyperparameterName
= O
90 B-HyperparameterValue
for O
Quora B-DatasetName
and O
k B-HyperparameterName
= O
30 B-HyperparameterValue
for O
SemEval B-DatasetName
- I-DatasetName
B. I-DatasetName
It O
is O
not O
unexpected O
, O
as O
larger O
datasets O
can O
cover O
more O
topics O
. O
However O
, O
how O
to O
let O
the O
data O
choose O
the O
right O
number O
of O
topics O
in O
NTM O
is O
beyond O
the O
scope O
of O
this O
paper O
. O

Impact O
of O
Bert B-MethodName
Layers O
. O
Figure O
5 O
shows O
the O
effect O
of O
semantics O
- O
driven O
multi O
- O
head O
attention O
on O
different O
layers O
. O
We O
found O
that O
the O
aforesaid O
attention O
is O
not O
always O
helpful O
for O
all O
layers O
and O
best O
performances O
are O
achieved O
when O
applying O
such O
attention O
to O
the O
last O
layer O
of O
DisBert B-MethodName
/ O
DisRoBERTa B-MethodName
on O
MRPC B-DatasetName
datasets O
. O
Meanwhile O
, O
the O
performance O
drops O
when O
such O
attention O
is O
applied O
to O
all O
layers O
. O

Case O
Studies O

In O
Table O
4 O
, O
we O
conduct O
a O
case O
study O
on O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
datasets O
to O
show O
how O
the O
model O
works O
with O
topical O
information O
. O
The O
left O
part O
of O
the O
table O
shows O
four O
word O
clusters O
generated O
by O
Topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
. O
As O
described O
in O
Section O
3.2 O
, O
the O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
model O
clusters O
words O
in O
the O
discrete O
space O
given O
by O
the O
codebook O
. O
It O
can O
be O
seen O
that O
words O
in O
the O
same O
cluster O
form O
a O
meaningful O
topic O
: O
Cluster O
7 O
is O
about O
protest O
, O
Cluster O
6 O
is O
about O
transportation O
, O
Cluster O
20 O
is O
about O
government O
election O
, O
and O
Cluster O
26 O
is O
one O
of O
the O
clusters O
capturing O
all O
the O
functional O
words O
. O
The O
right O
part O
of O
the O
table O
shows O
two O
pairs O
of O
sentences O
used O
in O
the O
STS B-TaskName
tasks O
with O
the O
assignments O
of O
the O
four O
topics O
to O
their O
words O
. O
We O
can O
observe O
that O
sentences O
, O
which O
are O
predicted O
to O
be O
similar O
by O
our O
model O
, O
tend O
to O
have O
more O
words O
assigned O
to O
similar O
topics O
, O
even O
if O
those O
words O
are O
morphologically O
different O
. O
For O
instance O
, O
" O
Killed O
" O
and O
" O
dead O
" O
are O
both O
from O
Cluster O
7 O
, O
" O
coach O
" O
and O
" O
bus O
" O
are O
from O
Cluster O
16 O
, O
and O
" O
elections O
" O
and O
" O
vote O
" O
are O
from O
Cluster O
20 O
. O
The O
heat O
map O
calculated O
by O
semantics O
- O
driven O
multihead O
attention O
is O
shown O
on O
Appendix O
A.3 O
for O
more O
discussion O
. O

Conclusion O

In O
this O
paper O
, O
we O
developed O
a O
topic B-MethodName
- I-MethodName
enhanced I-MethodName
VQ I-MethodName
- I-MethodName
VAE I-MethodName
model O
to O
effectively O
train O
discrete O
latent O
variables O
by O
informing O
the O
vector O
quantization O
with O
semantics O
( O
i.e. O
, O
latent O
topics O
) O
learned O
from O
a O
broader O
context O
by O
a O
neural O
topic O
model O
. O
Then O
we O
further O
designed O
a O
semantics O
- O
driven O
multi O
- O
head O
attention O
mechanism O
for O
enriching O
the O
contextual O
embeddings O
learned O
by O
Transformer O
with O
topical O
information O
, O
which O
is O
calculated O
based O
on O
the O
quantized O

Limitations O

The O
limitations O
of O
this O
work O
, O
to O
the O
best O
of O
our O
knowledge O
, O
can O
be O
summarized O
into O
two O
aspects O
: O

( O
1 O
) O
Compared O
to O
the O
origin O
Bert B-MethodName
/ O
RoBERTa B-MethodName
, O
our O
model O
DisBert B-MethodName
/ O
DisRoBERTa B-MethodName
needs O
a O
longer O
time O
to O
train O
since O
there O
are O
three O
components O
in O
our O
framework O
, O
i.e. O
, O
topic O
model O
, O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
model O
, O
and O
the O
transformer O
- O
based O
model O
. O
From O
table O
5 O
, O
we O
can O
observe O
that O
in O
a O
small O
dataset O
like O
MRPC B-DatasetName
and O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
, O
the O
training O
time O
is O
almost O
three O
times O
longer O
. O
While O
in O
the O
large O
datasets O
Quora B-DatasetName
, O
our O
model O
took O
an O
extra O
2.5 O
hours O
. O
Therefore O
in O
future O
work O
, O
we O
need O
to O
improve O
the O
efficiency O
of O
our O
model O
. O

( O
2 O
) O
The O
improvement O
of O
our O
method O
is O
limited O
when O
sentences O
are O
long O
enough O
or O
training O
datasets O
are O
large O
, O
e.g. O
, O
Quora B-DatasetName
and O
SemEvalC B-DatasetName
shown O
in O
table O
2 O
. O
Our O
model O
is O
capable O
of O
complete O
global O
semantics O
, O
hence O
it O
works O
better O
for O
small O
datasets O
and O
short O
sentences O
which O
contain O
limited O
semantics O
. O

A O
Example O
Appendix O

A.1 O
Dataset O
Examples O

Table O
6 O
shows O
examples O
from O
different O
datasets O
. O
Labels O
indicate O
if O
the O
second O
sentence O
is O
a O
paraphrase O
( O
for O
paraphrasing O
tasks O
) O
or O
relevant O
( O
for O
QA O
tasks O
) O
. O
In O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
datasets O
, O
the O
label O
stands O
for O
sentence B-MetricName
pair I-MetricName
similarity I-MetricName
score I-MetricName
, O
and O
the O
higher O
score O
, O
the O
more O
similar O
the O
two O
sentences O
are O
. O

A.2 O
Hyper O
- O
Parameter O
settings O

Table O
7 O
shows O
the O
hyperparameter O
we O
chose O
in O
our O
model O
. O
All O
hyper O
- O
parameters O
were O
chosen O
through O
greedy O
search O
based O
on O
development O
set O
performance O
. O

For O
topic O
model O
, O
we O
choose O
number B-HyperparameterName
of I-HyperparameterName
topics I-HyperparameterName
in O
( O
20,30,40,50,60,70,80,90,100,150,200 B-HyperparameterValue
) O
, O
batch B-HyperparameterName
size I-HyperparameterName
in O
( O
128,256,512 B-HyperparameterValue
) O
, O
and O
learning B-HyperparameterName
rate I-HyperparameterName
in O
( O
1e-3 B-HyperparameterValue
, O
2e-3 B-HyperparameterValue
, O
3e-3 B-HyperparameterValue
) O
. O

For O
the O
VQ B-MethodName
- I-MethodName
VAE I-MethodName
model O
, O
we O
choose O
the O
batch B-HyperparameterName
size I-HyperparameterName
in O
( O
32,64,128 B-HyperparameterValue
) O
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
in O
( O
2e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
) O
. O

For O
the O
Bert B-MethodName
model O
, O
We O
choose O
the O
batch B-HyperparameterName
size I-HyperparameterName
in O
( O
32,64,128 B-HyperparameterValue
) O
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
in O
( O
2e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
) O
. O

A.3 O
Heat O
Map O

Figure O
6 O
shows O
the O
heat O
map O
of O
sentence O
pair O
1 O
in O
the O
case O
study O
, O
which O
is O
calculated O
by O
our O
semantics O
- O
driven O
multi O
- O
head O
attention O
. O
We O
can O
observe O
that O
words O
from O
the O
same O
cluster O
usually O
pay O
high O
attention O
to O
each O
other O
because O
they O
correspond O
to O
the O
same O
hidden O
vector O
in O
the O
codebook O
. O
Such O
attention O
could O
help O
Bert B-MethodName
model O
focus O
on O
the O
words O
which O
may O
have O
similar O
semantics O
. O

Acknowledgments O

This O
work O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No.61976102 O
and O
No O
. O
U19A2065 O
) O
and O
the O
Science O
and O
Technology O
Development O
Program O
of O
Jilin O
Province O
( O
No.20210508060RQ O
) O
, O
and O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
, O
JLU O
. O

Dataset O

Sentence B-DatasetName
Pair I-DatasetName
Label I-DatasetName
MRPC I-DatasetName
S1 I-DatasetName
: O
The O
world O
's O
two O
largest O
automakers O
said O
their O
U.S. O
sales O
declined O
more O
than O
predicted O
last O
month O
1 O
as O
late O
summer O
sales O
frenzy O
caused O
more O
of O
an O
industry O
backlash O
than O
expected O
. O
S2 O
: O
Domestic O
sales O
at O
both O
GM O
and O
No O
. O
2 O
Ford O
Motor O
Co. O
declined O
more O
than O
predicted O
as O
a O
late O
summer O
sales O
frenzy O
prompted O
a O
larger O
- O
than O
- O
expected O
industry O
backlash O
. O

Quora B-DatasetName
S1 I-DatasetName
: O
Currently O
, O
all O
Supreme O
Court O
Justices O
come O
from O
very O
elite O
law O
schools O
, O
is O
it O
similar O
for O
the O
best O
1 O
lawyers O
in O
private O
practice O
. O
S2 O
: O
What O
's O
your O
type O
of O
jungle O
-LRB O
- O
concrete O
or O
nature O
-RRB O
- O
and O
why O
? O

STS B-DatasetName
- I-DatasetName
B I-DatasetName
S1 I-DatasetName
: O

A O
man O
is O
spreading O
shreded O
cheese O
on O
a O
pizza O
. O

S2 O
: O

A O
man O
is O
spreading O
shredded O
cheese O
on O
an O
uncooked O
pizza O
. O
SemEval B-DatasetName
- I-DatasetName
C I-DatasetName
S1 I-DatasetName
: O
Best O
sunglass O
store O
in O
Qatar O
Can O
somebody O
suggest O
the O
best O
store O
where O
i O
can O
1 O
get O
the O
best O
sunglasses O
for O
ladies O
. O
Where O
can O
i O
find O
a O
store O
with O
the O
best O
variety O
? O
S2 O
: O
Fashion O
wear O
-Primark O
Lingerie O
-Agent O
Provocateur O
Lingerie O
-Intimo O
Lingerie O
-Peach O
John O

How O
well O
can O
Text O
- O
to O
- O
Image O
Generative O
Models O
understand O
Ethical O
Natural O
Language O
Interventions O
? O

Text O
- O
to O
- O
image O
generative O
models O
have O
achieved O
unprecedented O
success O
in O
generating O
highquality O
images O
based O
on O
natural O
language O
descriptions O
. O
However O
, O
it O
is O
shown O
that O
these O
models O
tend O
to O
favor O
specific O
social O
groups O
when O
prompted O
with O
neutral O
text O
descriptions O
( O
e.g. O
, O
' O
a O
photo O
of O
a O
lawyer O
' O
) O
. O
Following O
Zhao O
et O
al O
. O
( O
2021 O
) O
, O
we O
study O
the O
effect O
on O
the O
diversity O
of O
the O
generated O
images O
when O
adding O
ethical O
intervention O
that O
supports O
equitable O
judgment O
( O
e.g. O
, O
' O
if O
all O
individuals O
can O
be O
a O
lawyer O
irrespective O
of O
their O
gender O
' O
) O
in O
the O
input O
prompts O
. O
To O
this O
end O
, O
we O
introduce O
an O
Ethical B-DatasetName
NaTural I-DatasetName
Language I-DatasetName
Interventions I-DatasetName
in I-DatasetName
Text I-DatasetName
- I-DatasetName
to I-DatasetName
- I-DatasetName
Image I-DatasetName
GENeration I-DatasetName
( O
ENTIGEN B-DatasetName
) O
benchmark O
dataset O
to O
evaluate O
the O
change O
in O
image O
generations O
conditional O
on O
ethical O
interventions O
across O
three O
social O
axesgender O
, O
skin O
color O
, O
and O
culture O
. O
Through O
ENTI B-DatasetName
- I-DatasetName
GEN I-DatasetName
framework O
, O
we O
find O
that O
the O
generations O
from O
minDALL•E B-MethodName
, O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
and O
Stable B-MethodName
Diffusion I-MethodName
cover O
diverse O
social O
groups O
while O
preserving O
the O
image O
quality O
. O
Preliminary O
studies O
indicate O
that O
a O
large O
change O
in O
the O
model O
predictions O
is O
triggered O
by O
certain O
phrases O
such O
as O
' O
irrespective O
of O
gender O
' O
in O
the O
context O
of O
gender O
bias O
in O
the O
ethical O
interventions O
. O
We O
release O
code O
and O
annotated O
data O
at O
https O
: O
/ O
/ O
github O
. O
com O
/ O
Hritikbansal O
/ O
entigen_emnlp O
. O

Introduction O

Recent O
Text O
- O
to O
- O
Image O
generative O
models O
( O
Ramesh O
et O
al O
. O
, O
, O
2022Saharia O
et O
al O
. O
, O
2022 O
; O
Nichol O
et O
al O
. O
, O
2021 O
; O
Rombach O
et O
al O
. O
, O
2022 O
) O
can O
synthesize O
high O
- O
quality O
photo O
- O
realistic O
images O
conditional O
on O
natural O
language O
text O
descriptions O
in O
a O
zero O
- O
shot O
fashion O
. O
For O
instance O
, O
they O
can O
generate O
an O
image O
of O
' O
an O
armchair O
in O
the O
shape O
of O
an O
avocado O
' O
which O
appears O
rarely O
in O
the O
real O
world O
. O
However O
, O
despite O
the O
unprecedented O
zero O
- O
shot O
abilities O
of O
the O
text O
- O
to O
- O
image O
generative O
models O
, O
recent O
experiments O
with O
small O
- O
scale O
instantiations O
( O
such O
* O
* O
Equal O
Contribution O
as O
minDALL•E B-MethodName
) O
have O
shown O
that O
prompting O
the O
model O
with O
neutral O
texts O
( O
' O
a O
photo O
of O
a O
lawyer O
' O
) O
, O
devoid O
of O
any O
cues O
towards O
a O
social O
group O
, O
still O
generates O
images O
that O
are O
biased O
towards O
white O
males O
( O
Cho O
et O
al O
. O
, O
2022 O
) O
. O

In O
our O
work O
, O
we O
consider O
three O
bias O
axis O
-1 O
) O
{ O
man O
, O
woman O
} O
grouping O
across O
gender O
axis O
, O
2 O
) O
{ O
light O
- O
skinned O
, O
dark O
- O
skinned O
} O
grouping O
across O
skin O
color O
axis O
, O
and O
3 O
) O
{ O
Western O
, O
Non O
- O
Western O
} O
grouping O
across O
cultural O
axis O
. O
1 O
The O
existence O
of O
any O
gender O
2 O
and O
skin O
color O
bias O
3 O
( O
see O
Ethical O
Statements O
for O
more O
discussion O
) O
causes O
potential O
harms O
to O
underrepresented O
groups O
by O
amplifying O
bias O
present O
in O
the O
dataset O
( O
Birhane O
et O
al O
. O
, O
2021 O
; O
Barocas O
et O
al O
. O
, O
2018 O
) O
. O
Hence O
, O
it O
is O
essential O
for O
a O
text O
- O
to O
- O
image O
system O
to O
generate O
diverse O
set O
of O
images O
. O

To O
this O
end O
, O
we O
study O
if O
the O
presence O
of O
addi O
- O
tional O
knowledge O
that O
supports O
equitable O
judgment O
help O
in O
diversifying O
model O
generations O
. O
Being O
part O
of O
text O
input O
, O
this O
knowledge O
acts O
as O
an O
ethical O
intervention O
over O
the O
original O
neutral O
prompt O
( O
Zhao O
et O
al O
. O
, O
2021 O
) O
. O
Ethical O
interventions O
provide O
models O
with O
ethical O
advice O
and O
do O
not O
emanate O
any O
visual O
cues O
towards O
a O
specific O
social O
group O
. O
For O
instance O
, O
in O
the O
context O
of O
generating O
' O
a O
photo O
of O
a O
lawyer O
' O
that O
tends O
to O
be O
biased O
towards O
' O
light O
- O
skinned O
man O
' O
, O
we O
wish O
to O
study O
if O
prompting O
the O
model O
with O
ethically O
intervened O
prompt O
( O
e.g. O
, O
' O
a O
photo O
of O
a O
lawyer O
if O
all O
individuals O
can O
be O
a O
lawyer O
irrespective O
of O
their O
gender O
' O
) O
can O
diversify O
the O
outputs O
. O

We O
introduce O
an O
Ethical B-DatasetName
NaTural I-DatasetName
Language I-DatasetName
Interventions I-DatasetName
in I-DatasetName
Text I-DatasetName
- I-DatasetName
to I-DatasetName
- I-DatasetName
Image I-DatasetName
GENeration I-DatasetName
( O
ENTI B-DatasetName
- I-DatasetName
GEN I-DatasetName
) O
benchmark O
dataset O
to O
study O
the O
change O
in O
the O
perceived O
societal O
bias O
of O
the O
text O
- O
to O
- O
image O
generative O
models O
in O
the O
presence O
of O
ethical O
interventions O
. O
ENTIGEN B-DatasetName
covers O
prompts O
to O
study O
the O
bias O
across O
three O
axes O
-gender O
, O
skin O
color O
and O
culture O
. O
The O
neutral O
prompts O
in O
ENTIGEN B-DatasetName
dataset O
are O
intervened O
with O
corresponding O
ethical O
knowledge O
as O
illustrated O
in O
Figure O
1 O
. O
We O
evaluate O
ENTIGEN B-DatasetName
on O
publicly O
available O
models O
-minDALL•E B-MethodName
( O
Kim O
et O
al O
. O
, O
2021 O
) O
, O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
( O
Dayma O
et O
al O
. O
, O
2021 O
) O
, O
and O
Stable B-MethodName
Diffusion I-MethodName
( O
Rombach O
et O
al O
. O
, O
2022 O
) O
automatically O
with O
CLIP O
model O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
manually O
with O
human O
annotators O
from O
MTurk O
. O

Through O
our O
experiments O
, O
( O
1 O
) O
we O
show O
that O
a O
few O
ethical O
interventions O
lead O
to O
the O
diversification O
of O
the O
image O
generations O
across O
different O
groups O
while O
preserving O
the O
image O
generation O
quality O
. O
Interestingly O
, O
in O
some O
cases O
, O
we O
observe O
the O
bias O
can O
be O
flipped O
towards O
the O
originally O
underrepresented O
groups O
with O
ethical O
interventions O
( O
Appendix O
Figure O
6 O
) O
. O
( O
2 O
) O
Moreover O
, O
we O
find O
that O
the O
interventions O
containing O
keywords O
such O
as O
' O
irrespective O
of O
gender O
' O
and O
' O
culture O
' O
tend O
to O
trigger O
a O
large O
change O
in O
model O
generations O
. O
We O
further O
analyze O
the O
pretraining O
data O
to O
analyze O
the O
context O
in O
which O
these O
keywords O
are O
used O
to O
better O
understand O
how O
they O
may O
affect O
the O
diversity O
in O
generation O
. O

Dataset O
and O
Evaluation O
Methods O

In O
this O
section O
, O
we O
introduce O
the O
process O
of O
building O
an O
ethical O
intervention O
benchmark O
ENTIGEN B-DatasetName
and O
evaluating O
the O
images O
generated O
by O
text O
- O
to O
- O
image O
generative O
models O
. O

ENTIGEN B-DatasetName
Benchmark O
Construction O

Initially O
, O
we O
determine O
three O
axes O
of O
societal O
bias O
to O
be O
studied O
: O
gender O
, O
skin O
color O
, O
and O
culture O
. O
Specifically O
, O
to O
investigate O
the O
gender O
and O
skin O
color O
bias O
, O
ENTIGEN B-DatasetName
consists O
of O
prompts O
belonging O
to O
the O
category O
of O
professions O
and O
objects O
. O
For O
assessing O
the O
cultural O
bias O
, O
it O
consists O
of O
prompts O
surrounding O
the O
wedding O
ceremonies O
as O
it O
is O
ubiquitous O
and O
diverse O
across O
different O
regions O
( O
Bell O
et O
al O
. O
, O
1997 O
; O
Xu O
and O
Xu O
, O
2018 O
; O
Acharya O
et O
al O
. O
, O
2020 O
) O
. O

Based O
on O
the O
selected O
axes O
, O
we O
design O
neutral O
prompts O
without O
any O
ethical O
interventions O
as O
the O
original O
prompts O
. O
Subsequently O
, O
we O
append O
ethical O
interventions O
to O
the O
original O
prompts O
that O
can O
perhaps O
amend O
the O
model O
's O
behaviour O
towards O
more O
diverse O
generations O
. O
We O
further O
include O
irrelevant O
interventions O
in O
ENTIGEN B-DatasetName
. O
These O
interventions O
also O
provide O
ethical O
advice O
, O
but O
do O
not O
correspond O
to O
any O
social O
axes O
we O
study O
in O
ENTIGEN B-DatasetName
. O
For O
example O
, O
' O
if O
honesty O
is O
the O
best O
policy O
' O
is O
an O
irrelevant O
intervention O
since O
it O
is O
unrelated O
to O
gender O
, O
skin O
color O
and O
culture O
. O
Ideally O
, O
these O
interventions O
can O
not O
help O
in O
diversifying O
image O
generations O
on O
either O
of O
studied O
social O
axes O
. O

In O
total O
, O
we O
create O
246 O
prompts O
based O
on O
an O
attribute O
set O
containing O
diverse O
professions O
, O
objects O
, O
and O
cultural O
scenarios O
. O
4 O

Image O
Generation O
. O

Each O
prompt O
in O
ENTIGEN B-DatasetName
is O
used O
to O
generate O
9 O
images O
from O
each O
text O
- O
to O
- O
image O
generation O
model O
9 O
times O
. O
We O
choose O
the O
publicly O
available O
models O
, O
minDALL•E B-MethodName
, O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
, O
and O
Stable B-MethodName
Diffusion I-MethodName
for O
analysis O
. O
It O
is O
mainly O
because O
these O
three O
models O
can O
generate O
high O
- O
quality O
images O
efficiently O
. O
We O
provide O
more O
details O
in O
Appendix O
B O
. O

Evaluation O
Metrics O
. O

We O
evaluate O
the O
diversity B-MetricName
among O
the O
generated O
images O
of O
the O
models O
. O
We O
focus O
on O
the O
gap O
between O
the O
number O
of O
images O
associated O
with O
the O
different O
groups O
( O
mentioned O
in O
§ O
1 O
) O
which O
measure O
the O
demographic O
disparity O
across O
various O
social O
axes O
. O
Specifically O
, O
for O
one O
of O
the O
prompts O
( O
e.g. O
, O
' O
a O
photo O
of O
a O
[ O
profession O
] O
if O
all O
genders O
can O
be O
a O
[ O
profession O
] O
' O
) O
filled O
with O
each O
attribute O
k O
( O
e.g. O
, O
police O
officer O
) O
in O
category O
P O
( O
e.g. O
, O
profession O
) O
, O
we O
count O
s O
g O
k O
, O
a O

( O
number O
of O
images O
with O
man O
) O
and O
s O
g O
k O
, O
b O
( O
number O
of O
images O
with O
woman O
) O
, O
associated O
with O
the O
two O
groups O
a O
( O
man O
) O
and O
b O
( O
woman O
) O
across O
a O
specific O
social O
axis O
g O
( O
gender O
) O
. O
Finally O
, O
the O
diversity B-MetricName
score I-MetricName
for O
axis O
g O
towards O
its O
groups O
for O
category O
P O
is O
: O

diversity B-MetricName
g O
P O
= O
k∈P O
|s O
g O
k O
, O
a O
− O
s O
g O
k O
, O
b O
| O
k∈P O
( O
s O
g O
k O
, O
a O
+ O
s O
g O
k O
, O
b O
) O
, O
( O
1 O

where O
g O
is O
one O
of O
{ O
gender O
, O
skin O
color O
, O
culture O
} O
, O
P O
is O
one O
of O
{ O
profession O
, O
object O
, O
wedding O
} O
and O
k O
can O
be O
any O
attribute O
according O
to O
the O
category O
P O
we O
select O
. O
The O
generations O
that O
could O
not O
have O
been O
assigned O
gender O
or O
skin O
color O
due O
to O
uncertainity O
in O
the O
judgements O
of O
the O
agents O
are O
not O
included O
in O
this O
metric O
. O
5 O
Smaller O
scores O
represent O
more O
diverse O
outputs O
. O
The O
normalization O
factor O
in O
the O
denominator O
of O
the O
Eq O
. O
( O
1 O
) O
allows O
us O
to O
compare O
model O
generations O
from O
two O
different O
prompts O
-original O
and O
ethically O
intervened O
as O
they O
could O
have O
different O
number O
of O
image O
generations O
that O
belong O
to O
either O
of O
the O
two O
social O
groups O
. O
To O
quantify O
the O
bias B-MetricName
and O
its O
direction O
, O
given O
one O
specific O
attribute O
k O
, O
we O
directly O
compute O
the O
normalized O
difference O
of O
the O
two O
counts O
, O

bias B-MetricName
g O
k O
= O
s O
g O
k O
, O
a O
− O
s O
g O
k O
, O
b O
/ O
s O
g O
k O
, O
a O
+ O
s O
g O
k O
, O
b O
, O
( O
2 O
) O

belonging O
to O
two O
groups O
a O
and O
b. O
6 O
Greater O
absolute O
value O
of O
bias B-MetricName
g O
k O
indicates O
greater O
bias O
and O
vice O
versa O
. O
Built O
upon O
these O
metrics O
, O
CLIP O
- O
based O
and O
human O
evaluations O
are O
used O
to O
assess O
output O
diversity B-MetricName
and O
bias B-MetricName
. O
Due O
to O
limited O
budget O
, O
we O
select O
part O
of O
the O
professions O
and O
objects O
for O
human O
annotators O
to O
evaluate O
. O
7 O
For O
the O
entire O
set O
of O
images O
, O
we O
use O
auto-5 O
Details O
on O
assigning O
a O
social O
group O
to O
a O
model O
generation O
are O
in O
Appendix O
C O
. O

6 O
E.g. O
, O
a O
is O
man O
, O
light O
- O
skinned O
and O
Western O
for O
gender O
, O
skin O
color O
and O
culture O
axes O
. O
b O
is O
woman O
, O
dark O
- O
skinned O
and O
Non O
- O
Western O
. O

7 O
professions O
: O
police O
officer O
, O
doctor O
; O
objects O
: O
suit O
, O
scarf O
, O
makeup O
; O
cultural O
scenarios O
: O
bride O
, O
groom O
, O
wedding O
. O
matic O
CLIP O
- O
based O
evaluation O
8 O
as O
a O
complementary O
method O
. O
Appendix O
C O
provides O
more O
details O
about O
our O
evaluations O
. O

Note O
that O
we O
are O
aware O
of O
the O
possibility O
that O
CLIP O
model O
may O
be O
biased O
towards O
certain O
groups O
( O
Zhang O
et O
al O
. O
, O
2022 O
) O
. O
We O
measure O
the O
consistency O
between O
the O
gender O
and O
skin O
color O
determined O
by O
the O
CLIP O
model O
and O
human O
annotators O
in O
the O
images O
generated O
for O
a O
subset O
of O
attributes O
. O
We O
find O
that O
CLIP O
- O
based O
determinations O
agree O
with O
the O
human O
annotations O
with O
a O
rate O
of O
78 O
- O
85 O
% O
for O
gender O
recognition O
while O
for O
skin O
color O
, O
the O
rate O
is O
down O
to O
67 O
- O
78 O
% O
. O
We O
finally O
decide O
to O
apply O
CLIP O
- O
based O
evaluation O
on O
gender O
axis O
only O
as O
the O
predictions O
on O
gender O
are O
more O
consistent O
with O
the O
humans O
. O

Results O

CLIP O
- O
based O
Results O

We O
investigate O
the O
effect O
of O
the O
ethical O
interventions O
on O
the O
gender O
diversity B-MetricName
score I-MetricName
Eq O
. O
( O
1 O
) O
for O
the O
profession O
category O
in O
Table O
1 O
( O
Column O
3 O
- O
5 O
) O
. O
We O
observe O
that O
gender O
- O
specific O
ethical O
intervention O
causes O
the O
promotion O
of O
gender O
diversity O
( O
Row O
2 O
- O
3 O
) O
for O
all O
the O
models O
. O
We O
also O
find O
that O
the O
prompt O
with O
' O
irrespective O
of O
their O
gender O
' O
improves O
the O
gender O
diversity B-MetricName
score I-MetricName
much O
more O
than O
the O
prompt O
simply O
stating O
that O
' O
all O
genders O
can O
be O
[ O
profession O
] O
' O
. O
Additionally O
, O
we O
observe O
that O
an O
ethical O
intervention O
with O
respect O
to O
skin O
color O
does O
not O
have O
significant O
effect O
on O
the O
gender O
diversity B-MetricName
of O
the O
model O
generations O
( O
Row O
4 O
- O
5 O
) O
. O
Even O
though O
the O
irrelevant O
interventions O
should O
not O
change O
the O
diversity B-MetricName
scores I-MetricName
, O
we O
observe O
that O
diversity B-MetricName
scores I-MetricName
are O
affected O
by O
their O
presence O
( O
Row O
6 O
- O
7 O
) O
. O
We O
present O
the O
gender O
diversity B-MetricName
score I-MetricName
evaluated O
through O
CLIP O
for O
the O
object O
category O
in O
Appendix O
Table O
6 O
. O
To O
ensure O
the O
reliability O
of O
our O
evaluation O
, O
we O
also O
perform O
human O
annotations O
for O
better O
assessment O
. O

Human O
Evaluation O
Results O

We O
present O
human O
evaluation O
results O
for O
the O
profession O
category O
in O
Table O
1 O
( O
Column O
5 O
- O
8 O
) O
. O
We O
observe O
that O
axis O
- O
specific O
ethical O
instructions O
with O
' O
irrespective O
of O
{ O
gender O
, O
skin O
color O
} O
' O
produce O
better O
diversity B-MetricName
scores I-MetricName
( O
Row O
2 O
and O
4 O
) O
. O
We O
also O
find O
that O
the O
diversity B-MetricName
scores I-MetricName
do O
not O
improve O
for O
most O
cases O
as O
ethical O
interventions O
do O
when O
adding O
irrelevant O
in- O
structions O
. O
We O
can O
draw O
similar O
conclusions O
from O
Appendix O
Table O
6 O
for O
the O
objects O
category O
. O

We O
also O
present O
the O
human O
evaluation O
results O
along O
the O
cultural O
axis O
in O
Table O
2 O
. O
We O
observe O
that O
the O
generations O
of O
all O
the O
models O
become O
more O
diverse O
when O
prompted O
in O
the O
presence O
of O
cultural O
intervention O
. O
Additionally O
, O
the O
cultural O
diversity B-MetricName
is O
not O
influenced O
by O
the O
irrelevant O
instructions O
. O

Till O
now O
, O
we O
have O
focused O
at O
the O
effect O
on O
the O
diversity B-MetricName
scores I-MetricName
. O
However O
, O
it O
is O
only O
the O
uniformity O
in O
image O
generations O
across O
groups O
but O
does O
not O
indicate O
the O
direction O
of O
the O
bias O
. O
Hence O
, O
we O
also O
calculate O
the O
bias B-MetricName
score I-MetricName
Eq O
. O
( O
2 O
) O
. O
Our O
results O
reveal O
that O
the O
presence O
of O
ethical O
interventions O
may O
flip O
the O
direction O
of O
model O
's O
bias B-MetricName
. O
For O
instance O
, O
DALL•Emini B-MethodName
generates O
man O
and O
dark O
- O
skinned O
individuals O
with O
makeup O
( O
Appendix O
Fig O
. O
6 O
) O
. O
Similarly O
, O
Stable B-MethodName
Diffusion I-MethodName
generates O
more O
woman O
images O
than O
man O
images O
for O
the O
police O
profession O
when O
prompted O
with O
the O
gender O
ethical O
intervention O
. O

Further O
visual O
inspection O
of O
Figure O
4 O
suggests O
that O
the O
Stable B-MethodName
Diffusion I-MethodName
model O
synthesizes O
multiple O
humans O
in O
a O
single O
image O
that O
prevents O
the O
human O
annotators O
to O
assign O
a O
particular O
gender O
or O
skin O
color O
to O
them O
. O
Such O
model O
generations O
are O
disregarded O
during O
diversity B-MetricName
score I-MetricName
generation O
, O
thus O
preventing O
us O
to O
make O
reliable O
estimate O
of O
the O
stable B-MethodName
diffusion I-MethodName
generations O
through O
diversity B-MetricName
score I-MetricName
alone O
. O
We O
believe O
that O
our O
work O
motivates O
further O
studies O
on O
the O
sensitivity O
of O
text O
- O
to O
- O
image O
model O
generations O
to O
ethical O
instructions O
. O

Quality O
of O
Image O
Generation O

Do O
these O
abstract O
interventions O
bring O
side O
effect O
such O
as O
hurting O
the O
quality O
of O
generations O
? O
We O
ask O
human O
annotators O
to O
select O
if O
generated O
images O
are O
of O
good O
quality O
9 O
conditional O
on O
the O
original O
9 O
The O
criteria O
are O
whether O
the O
images O
can O
be O
recognized O
as O
a O
person O
and O
whether O
the O
images O
are O
generated O
as O
input O
prompts O
describe O
. O

prompt O
and O
the O
ethical O
intervention O
. O
We O
present O
our O
analysis O
in O
Table O
3 O
for O
the O
same O
five O
subset O
of O
attributes O
( O
police O
, O
doctor O
, O
makeup O
, O
suit O
, O
scarf O
) O
for O
gender O
and O
skin O
color O
bias O
study O
, O
and O
three O
attributes O
( O
bride O
, O
groom O
, O
wedding O
) O
for O
cultural O
bias O
study O
( O
§ O
3.2 O
) O
. O
Compared O
to O
generating O
with O
original O
prompts O
, O
except O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
and O
Stable B-MethodName
Diffusion I-MethodName
on O
profession O
category O
, O
the O
number O
of O
good O
quality O
generations O
reduce O
slightly O
for O
both O
the O
models O
( O
0 O
- O
1.5 O
images O
per O
attribute O
) O
in O
the O
presence O
of O
the O
ethical O
interventions O
. O
This O
presents O
a O
positive O
case O
towards O
using O
ethical O
interventions O
for O
model O
diversification O
while O
preserving O
the O
quality O
of O
the O
generations O
. O

4 O
How O
important O
are O
phrases O
present O
in O
an O
ethical O
intervention O
? O

In O
§ O
3 O
, O
we O
observed O
that O
ethical O
interventions O
would O
elicit O
large O
changes O
in O
the O
diversity B-MetricName
scores I-MetricName
in O
some O
cases O
. O
However O
, O
it O
is O
still O
unclear O
as O
to O
which O
phrases O
in O
an O
ethical O
intervention O
lead O
to O
such O
changes O
in O
the O
model O
's O
behaviour O
. O
To O
this O
end O
, O
we O
perform O
a O
preliminary O
analysis O
on O
the O
model O
generations O
with O
' O
a O
photo O
of O
a O
{ O
person O
wearing O
a O
makeup O
/ O
police O
officer O
} O
if O
all O
individuals O
can O
{ O
wear O
a O
makeup O
/ O
be O
a O
police O
officer O
} O
irrespective O
of O
their O
gender O
' O
prompt O
with O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
. O

We O
find O
that O
removing O
' O
irrespective O
of O
their O
gender O
' O
phrase O
from O
the O
ethical O
intervention O
leads O
to O
generations O
biased O
towards O
' O
woman O
' O
and O
' O
man O
' O
for O
the O
' O
makeup O
' O
and O
' O
police O
officer O
' O
attributes O
respectively O
. O
This O
trend O
is O
identical O
to O
what O
we O
observe O
for O
original O
prompts O
without O
intervention O
. O
It O
shows O
that O
the O
model O
may O
have O
captured O
the O
semantics O
of O
the O
phrase O
based O
on O
its O
usage O
in O
the O
pre O
- O
training O
dataset O
. O
Further O
analyzing O
the O
pre O
- O
training O
data O
( O
Sharma O
et O
al O
. O
, O
2018 O
) O
, O
we O
observe O
' O
irrespective O
of O
' O
phrase O
is O
used O
37 O
times O
to O
elicit O
equitable O
judgment O
based O
on O
the O
context O
in O
the O
captions O
( O
Table O
7 O
) O
. O
But O
the O
entire O
phrase O
' O
irrespective O
of O
their O
gender O
' O
appears O
only O
once O
. O

There O
is O
also O
a O
possibility O
that O
the O
captions O
containing O
word O
' O
gender O
' O
and O
' O
makeup O
' O
are O
associated O
with O
images O
with O
' O
man O
' O
person O
in O
pre O
- O
training O
dataset O
images O
( O
Changpinyo O
et O
al O
. O
, O
2021 O
; O
Sharma O
et O
al O
. O
, O
2018 O
) O
and O
thus O
contribute O
to O
generating O
more O
men O
. O
However O
, O
we O
find O
that O
the O
six O
images O
with O
' O
gender O
' O
and O
' O
makeup O
' O
words O
in O
their O
captions O
only O
contain O
people O
who O
are O
perceived O
as O
woman O
by O
the O
humans O
. O
We O
also O
find O
that O
there O
is O
only O
one O
image O
, O
without O
any O
person O
clearly O
visible O
, O
with O
' O
gender O
' O
and O
' O
police O
' O
in O
its O
caption O
. O
Hence O
, O
we O
further O
verify O
the O
effect O
of O
phrase O
' O
irrespective O
of O
their O
gender O
' O
on O
generating O
diverse O
images O
despite O
its O
absence O
in O
pre O
- O
training O
data O
. O
Why O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
can O
generate O
anti O
- O
stereotype O
images O
with O
such O
ethical O
interventions O
needs O
further O
exploration O
in O
future O
work O
. O

Additionally O
, O
further O
analysis O
on O
the O
cooccurrence O
of O
the O
word O
' O
culture O
' O
with O
' O
Western O
' O
( O
75 O
) O
, O
' O
Indian O
' O
( O
394 O
) O
, O
and O
' O
Chinese O
' O
( O
322 O
) O
explains O
the O
generation O
of O
images O
belonging O
to O
these O
Non O
- O
Western O
cultures O
when O
the O
original O
prompts O
are O
intervened O
with O
ethical O
interventions O
containing O
the O
' O
culture O
' O
keyword O
( O
Appendix O
Fig O
. O
7 O
, O
8 O
) O
. O

Discussion O
and O
Conclusion O

We O
present O
a O
framework O
along O
with O
an O
associated O
ENTIGEN B-DatasetName
dataset O
to O
evaluate O
the O
change O
in O
the O
diversity O
of O
the O
text O
- O
to O
- O
image O
generations O
in O
the O
presence O
of O
the O
ethical O
interventions O
. O
We O
observe O
that O
without O
any O
fine O
- O
tuning O
, O
models O
can O
generate O
images O
of O
diverse O
groups O
with O
prompts O
containing O
ethical O
interventions O
. O
Our O
preliminary O
study O
finds O
evidence O
that O
a O
large O
change O
in O
image O
generation O
can O
be O
caused O
by O
certain O
keyphrases O
such O
as O
' O
irrespective O
of O
gender O
' O
in O
the O
context O
of O
the O
gender O
bias O
and O
' O
culture O
' O
in O
the O
context O
of O
the O
cultural O
bias O
. O

Limitations O

We O
note O
that O
even O
with O
ethics O
intervention O
, O
textto O
- O
image O
models O
may O
not O
always O
generate O
diverse O
output O
in O
a O
reliable O
way O
. O
Therefore O
, O
our O
goal O
of O
this O
study O
is O
not O
arguing O
ethical O
intervention O
is O
an O
effective O
way O
to O
reduce O
bias O
in O
practice O
; O
rather O
our O
study O
analyzes O
how O
the O
current O
systems O
respond O
to O
these O
interventions O
. O
As O
a O
future O
work O
, O
we O
aim O
to O
explore O
deeper O
reasons O
behind O
the O
diverse O
and O
anti O
- O
stereotype O
generations O
beyond O
the O
association O
between O
words O
and O
images O
. O
Our O
work O
motivates O
further O
studies O
for O
developing O
more O
inclusive O
and O
reliable O
text O
- O
to O
- O
image O
systems O
. O

The O
creation O
of O
large O
number O
of O
ethical O
interventions O
and O
their O
human O
evaluations O
is O
a O
current O
limitation O
and O
an O
important O
future O
direction O
. O
Additionally O
, O
we O
consider O
binary O
categorization O
of O
the O
model O
generations O
that O
has O
technical O
as O
well O
as O
ethical O
limitations O
. O
It O
would O
be O
important O
to O
study O
mechanisms O
to O
assign O
non O
- O
binary O
labels O
to O
model O
generations O
and O
develop O
diversity O
metrics O
beyond O
binary O
groups O
in O
the O
future O
work O
. O

Our O
work O
is O
also O
limited O
by O
the O
perceptual O
bias O
of O
the O
human O
annotators O
from O
US O
and O
UK O
as O
well O
as O
the O
CLIP O
model O
. O
To O
obtain O
more O
reliable O
evaluation O
results O
, O
we O
plan O
to O
involve O
annotators O
from O
diverse O
regions O
in O
human O
evaluation O
and O
less O
biased O
computer O
vision O
models O
in O
automatic O
evaluation O
. O

Ethics O
Statement O

ENTIGEN B-DatasetName
is O
proposed O
for O
evaluating O
the O
change O
in O
the O
model O
generations O
in O
the O
presence O
of O
ethical O
interventions O
. O
We O
limit O
our O
work O
to O
selected O
categories O
( O
such O
as O
profession O
and O
objects O
) O
within O
the O
gender O
and O
social O
axis O
even O
though O
there O
might O
be O
other O
categories O
such O
as O
politics O
where O
equal O
representation O
is O
desired O
. O
Even O
though O
there O
are O
a O
wide O
range O
of O
groups O
within O
the O
gender O
and O
skin O
color O
axis O
, O
we O
only O
consider O
categorizing O
individuals O
into O
{ O
man O
, O
woman O
} O
and O
{ O
light O
- O
skinned O
, O
dark O
- O
skinned O
} O
. O

We O
are O
aware O
of O
the O
negative O
impact O
brought O
from O
limited O
binary O
categories O
. O
It O
is O
offensive O
for O
underrepresented O
groups O
and O
possibly O
causes O
cyclical O
erasure O
of O
non O
- O
binary O
gender O
identities O
. O
However O
, O
assessing O
any O
individual O
's O
gender O
identity O
or O
sex O
is O
impossible O
based O
on O
their O
appearance O
; O
hence O
we O
limit O
our O
work O
on O
classifying O
individuals O
into O
man O
/ O
woman O
based O
on O
the O
perceptual O
bias O
and O
gender O
assumptions O
of O
the O
human O
annotators O
and O
the O
CLIP O
model O
. O
We O
also O
emphasize O
that O
our O
analysis O
is O
based O
on O
generated O
images O
not O
the O
images O
containing O
real O
individuals O
. O

We O
also O
understand O
that O
there O
are O
numerous O
skin O
colors O
but O
we O
limit O
our O
study O
to O
classify O
individuals O
into O
light O
- O
skinned O
or O
dark O
- O
skinned O
. O
Additionally O
, O
we O
do O
not O
instruct O
the O
annotators O
to O
use O
Fitzpatrick O
scale O
( O
Fitzpatrick O
, O
1986 O
) O
to O
determine O
skin O
- O
color O
, O
rather O
the O
decision O
is O
left O
to O
their O
own O
perception O
. O

The O
imperfect O
image O
- O
to O
- O
text O
generative O
modeling O
can O
run O
into O
the O
hazard O
of O
missing O
certain O
data O
modes O
that O
eventually O
compound O
the O
social O
biases O
present O
in O
the O
pre O
- O
trained O
dataset O
( O
Saharia O
et O
al O
. O
, O
2022 O
) O
. O
There O
are O
harms O
associated O
with O
the O
models O
ability O
to O
change O
predictions O
drastically O
based O
on O
the O
prompts O
as O
it O
can O
lead O
to O
the O
generation O
of O
objectionable O
contents O
. O
We O
encourage O
the O
practice O
of O
having O
sophisticated O
Not O
Safe O
For O
Work O
( O
NSFW O
) O
filters O
before O
image O
generations O
. O
A O
CLIP O
- O
based O
filter O
used O
by O
Stable B-MethodName
Diffusion I-MethodName
implementations O
is O
a O
positive O
step O
in O
this O
direction O
. O

Extensions O
of O
our O
work O
can O
focus O
on O
increasing O
the O
representation O
of O
more O
groups O
as O
well O
as O
designing O
text O
- O
to O
- O
image O
generative O
models O
that O
output O
images O
of O
people O
belonging O
to O
diverse O
groups O
conditional O
on O
the O
neutral O
prompt O
. O

As O
we O
annotate O
a O
new O
dataset O
ENTIGEN B-DatasetName
, O
we O
compensate O
annotators O
with O
a O
fair O
rate O
. O
We O
recruit O
annotators O
from O
Amazon O
MTurk O
. O
We O
provide O
a O
fair O
compensation O
rate O
with O
$ O
10 O
per O
hour O
and O
spent O
around O
$ O
60 O
in O
total O
to O
the O
annotators O
on O
human O
evaluation O
. O
Each O
HIT O
costs O
several O
seconds O
according O
to O
the O
statistics O
in O
Amazon O
MTurk O
. O
Human O
Evaluation O
. O
In O
evaluation O
survey O
, O
we O
first O
ask O
annotators O
whether O
the O
images O
can O
be O
recognized O
as O
a O
person O
and O
whether O
the O
images O
are O
generated O
as O
input O
prompts O
describe O
. O
Once O
, O
they O
recognize O
the O
image O
as O
of O
a O
person O
, O
they O
get O
to O
decide O
the O
gender O
and O
skin O
color O
of O
the O
person O
in O
the O
image O
. O
To O
account O
for O
uncertainty O
in O
assigning O
a O
gender O
and O
skin O
color O
, O
we O
allow O
the O
annotators O
to O
choose O
an O
" O
Ca O
n't O
judge O
" O
option O
. O
To O
guarantee O
the O
reliability O
of O
the O
evaluation O
results O
, O
we O
recruit O
three O
annotators O
to O
evaluate O
each O
image O
and O
the O
evaluation O
results O
depend O
on O
majority O
views O
. O
The O
annotation O
interface O
is O
shown O
in O
Figure O
2 O
. O

D O
List O
of O
Studied O
Professions O
, O
Objects O
and O
Cultural O
Scenarios O

E O
Prompts O
and O
Ethical O
Interventions O
in O
ENTIGEN B-DatasetName

In O
ENTIGEN B-DatasetName
dataset O
, O
there O
are O
two O
types O
of O
prompts O
: O
original O
prompts O
and O
prompts O
with O
ethical O
interventions O
. O
For O
each O
original O
prompt O
, O
such O
as O
' O
a O
photo O
of O
a O
[ O
X O
] O
' O
, O
there O
are O
4 O
- O
7 O
variants O
with O
ethical O
interventions O
. O
Among O
them O
, O
there O
are O
1 O
- O
2 O
irrelevant O
ethical O
interventions O
. O
The O
rest O
interventions O
correspond O
to O
the O
three O
bias O
axes O
-gender O
, O
skin O
color O
and O
culture O
. O

F O
More O
on O
Bias B-MetricName
Results O

We O
present O
the O
formulation O
of O
bias B-MetricName
along O
the O
social O
axis O
g O
in O
Eq O
. O
( O
2 O
) O
. O
Bias B-MetricName
results O
based O
on O
human O
evaluations O
are O
shown O
in O
Figure O
3 O
. O
We O
first O
observe O
that O
in O
most O
cases O
, O
adding O
ethical O
interventions O
can O
help O
in O
reducing O
the O
bias B-MetricName
because O
the O
absolute O
value O
of O
bias B-MetricName
g O
becomes O
smaller O
. O
We O
further O
find O
that O
in O
some O
cases O
, O
for O
example O
, O
outputting O
a O
person O
with O
makeup O
by O
DALL•E B-MethodName
- I-MethodName
mini I-MethodName
, O
the O
bias B-MetricName
direction O
is O
flipped O
oppositely O
towards O
person O
who O
looks O
like O
a O
man O
. O

G O
Case O
Study O

Figure O
4 O
to O
Figure O
8 O
showcase O
the O
generated O
images O
based O
on O
different O
prompt O
variants O
. O
From O
Figure O
8 O
, O
we O
observe O
that O
original O
prompts O
about O
bride O
can O
only O
generate O
brides O
in O
Western O
weddings O
, O
but O
the O
generations O
are O
diversified O
with O
ethical O
intervention O
' O
from O
diverse O
cultures O
' O
. O

Acknowledgement O

We O
thank O
annotators O
for O
tremendous O
efforts O
on O
annotation O
and O
evaluation O
. O
We O
also O
thank O
the O
anonymous O
reviewers O
and O
ethical O
reviewers O
for O
their O
great O
comments O
. O
We O
also O
greatly O
appreciate O
Ashima O
Suvarna O
, O
Xiao O
Liu O
, O
and O
members O
of O
UCLA O
- O
NLP O
group O
for O
their O
helpful O
comments O
. O
This O
work O
was O
partially O
supported O
by O
NSF O
IIS-1927554 O
, O
Sloan O
Research O
Fellow O
, O
Amazon O
AWS O
credits O
, O
and O
a O
DARPA O
MCS O
program O
under O
Cooperative O
Agreement O
N66001 O
- O
19 O
- O
2 O
- O
4032 O
. O
The O
views O
and O
conclusions O
are O
those O
of O
the O
authors O
and O
should O
not O
reflect O
the O
official O
policy O
or O
position O
of O
DARPA O
or O
the O
U.S. O
Government O
. O

Appendix O
A O
Related O
Work O

Recently O
, O
text O
- O
to O
- O
image O
generative O
models O
such O
as O
DALL•E O
, O
DALL•E O
2 O
( O
Ramesh O
et O
al O
. O
, O
2022 O
) O
, O
GLIDE O
( O
Nichol O
et O
al O
. O
, O
2021 O
) O
, O
IMAGEN O
( O
Saharia O
et O
al O
. O
, O
2022 O
) O
and O
Stable B-MethodName
Diffusion I-MethodName
( O
Rombach O
et O
al O
. O
, O
2022 O
) O
have O
been O
capable O
of O
generating O
photorealistic O
images O
according O
to O
text O
prompts O
. O
However O
, O
Cho O
et O
al O
. O
( O
2022 O
) O
discover O
that O
these O
models O
expose O
societal O
bias O
when O
fed O
with O
prompts O
involving O
professions O
and O
objects O
. O

As O
the O
scale O
of O
models O
and O
their O
training O
data O
greatly O
expands O
, O
with O
single O
textual O
instructions O
, O
models O
can O
rapidly O
learn O
how O
to O
accomplish O
the O
corresponding O
tasks O
with O
a O
few O
or O
even O
zero O
examples O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O
In O
the O
context O
of O
fairness O
issue O
, O
ethical O
intervention O
( O
Zhao O
et O
al O
. O
, O
2021 O
) O
is O
proposed O
to O
mitigate O
bias O
of O
predictions O
made O
by O
large O
language O
models O
. O
Different O
from O
Zhao O
et O
al O
. O
( O
2021 O
) O
, O
we O
find O
that O
ethical O
interventions O
can O
adjust O
model O
behaviour O
towards O
generating O
images O
regarding O
minority O
groups O
, O
and O
provide O
preliminary O
study O
on O
why O
the O
intervention O
can O
work O
. O

B O
Image O
Generation O
Details O

Each O
prompt O
in O
ENTIGEN B-DatasetName
is O
used O
to O
generate O
9 O
images O
from O
each O
text O
- O
to O
- O
image O
generation O
model O
9 O
times O
. O
In O
this O
work O
, O
we O
choose O
the O
publicly O
available O
generation O
models O
, O
minDALL•E B-MethodName
and O
DALL•Emini B-MethodName
for O
analysis O
. O
It O
is O
mainly O
because O
the O
two O
models O
can O
generate O
high O
- O
quality O
images O
. O
Based O
on O
our O
experiments O
, O
the O
quality O
of O
image O
generations O
containing O
humans O
from O
other O
available O
instantiations O
such O
as O
ruDALL•E O
- O
XL O
( O
https O
: O
/ O
/ O
rudalle.ru O
/ O
) O
can O
not O
generate O
high O
- O
quality O
images O
. O
More O
powerful O
models O
like O
DALL•E O
2 O
and O
IMA O
- O
GEN O
are O
not O
publicly O
released O
. O
minDALL•E B-MethodName
and O
DALL•E B-MethodName
also O
allow O
us O
to O
perform O
inference O
more O
time O
efficiently O
. O
minDALL•E B-MethodName
and O
DALL•Emini B-MethodName
can O
generate O
a O
image O
in O
10 O
seconds O
on O
a O
RTX1080Ti O
GPU O
. O
But O
models O
like O
Disco O
Diffusion O
( O
http O
: O
/ O
/ O
discodiffusion.com O
/ O
) O
took O
20 O
minutes O
to O
generate O
a O
single O
image O
. O
We O
use O
the O
publicly O
available O
Stable B-MethodName
diffusion I-MethodName
v1 O
- O
4 O
from O
HuggingFace O
library O
( O
https O
: O
/ O
/ O
huggingface.co O
/ O
CompVis O
/ O
stable O
- O
diffusion O
- O
v1 O
- O
4 O
) O
. O

Text O
Names O
Contents O

T O
man O
' O
A O
photo O
of O
a O
person O
who O
looks O
like O
a O
man O
' O
T O
woman O
' O
A O
photo O
of O
a O
person O
who O
looks O
like O
a O
woman O
' O
T O
uncertain O
' O
A O
photo O
of O
a O
person O
with O
an O
uncertain O
gender O
' O

Captions O

A O
great O
team O
is O
all O
the O
time O
humble O
and O
have O
the O
ability O
to O
listen O
to O
everyone O
, O
facilitating O
freedom O
to O
communicate O
each O
member O
's O
thoughts O
and O
perspectives O
irrespective O
of O
hierarchies O
, O
which O
in O
turn O
.. O
Faux O
Leather O
Toddler O
Jacket O
-Leather O
jackets O
irrespective O
of O
the O
colour O
, O
style O
and O
material O
... O

According O
to O
the O
ornithologists O
, O
the O
parrots O
would O
help O
out O
irrespective O
of O
whether O
the O
other O
individual O
was O
their O
' O
friend O
' O
or O
not O
. O

Banquet O
Outfits O
for O
Women O
: O
For O
Banquet O
events O
, O
irrespective O
of O
whether O
it O
will O
be O
a O
formal O
or O
informal O
occasion O
, O
you O
need O
to O
appear O
regal O
and O
elegant O
... O
.. O
served O
to O
more O
than O
10,000 O
people O
every O
day O
. O
It O
is O
now O
a O
tradition O
followed O
by O
more O
than O
30 O
million O
PERSON O
worldwide O
. O
Nearly O
every O
gurdwara O
in O
the O
world O
, O
irrespective O
of O
size O
, O
has O
a O
kitchen O
and O
serves O
langar O
... O
Material O
Risk O
Willmott O
Dixon O
appeal O
-Any O
work O
with O
asbestos O
presents O
a O
material O
risk O
irrespective O
of O
the O
number O
of O
fibres O
released O
( O
if O
any O
) O
or O
the O
length O
of O
exposure O
. O

Secure O
pipes O
to O
prevent O
movement O
irrespective O
of O
slope O
of O
surface O
, O
secure O
pipes O
to O
prevent O
movement O
e.g O
sand O
bags O
, O
star O
pickets O
, O
place O
against O
fixed O
objects O
which O
will O
prevent O
the O
movement O
of O
pipes O
. O

Advertising O
is O
one O
of O
the O
most O
important O
parts O
of O
marketing O
irrespective O
of O
brands O
, O
companies O
and O
products O
... O
The O
starter O
relay O
switch O
will O
be O
replaced O
free O
of O
cost O
in O
the O
identified O
units O
irrespective O
of O
the O
warranty O
status O
of O
the O
vehicle O
across O
Honda O
's O
India O
network O
. O
Photo O
: O
Bloomberg O

The O
Leh O
- O
Karakoram O
road O
is O
also O
a O
part O
of O
this O
project O
. O
It O
has O
37 O
bridges O
and O
is O
motorable O
all O
through O
the O
year O
irrespective O
of O
weather O
conditions O
. O

Air O
pollution O
is O
one O
such O
form O
that O
refers O
to O
the O
contamination O
of O
the O
air O
, O
irrespective O
of O
indoors O
or O
outside O
... O
The O
Salish O
Sea O
joins O
together O
more O
than O
7 O
million O
inhabitants O
, O
which O
work O
together O
on O
a O
wide O
range O
of O
issuesirregardless O
and O
irrespective O
of O
national O
border O
. O

PERSON O
's O
Vases O
The O
fluid O
levels O
are O
the O
same O
in O
all O
each O
tube O
irrespective O
of O
their O
shape O
East O
Or O
West O
India O
is O
the O
best O
. O
These O
fans O
continue O
to O
cheer O
for O
India O
irrespective O
of O
any O
state O
at O
the O
IPL O
6 O
match O
between O
Kings O
XI O
Punjab O
and O
Kolkata O
Knight O
Riders O
in O
Mohali O
. O
( O
PTI O
) O

CLCL B-MethodName
: O
Non B-TaskName
- I-TaskName
compositional I-TaskName
Expression I-TaskName
Detection I-TaskName
with O
Contrastive B-MethodName
Learning I-MethodName
and O
Curriculum B-MethodName
Learning I-MethodName

Non O
- O
compositional O
expressions O
present O
a O
substantial O
challenge O
for O
natural O
language O
processing O
( O
NLP O
) O
systems O
, O
necessitating O
more O
intricate O
processing O
compared O
to O
general O
language O
tasks O
, O
even O
with O
large O
pre O
- O
trained O
language O
models O
. O
Their O
non O
- O
compositional O
nature O
and O
limited O
availability O
of O
data O
resources O
further O
compound O
the O
difficulties O
in O
accurately O
learning O
their O
representations O
. O
This O
paper O
addresses O
both O
of O
these O
challenges O
. O
By O
leveraging O
contrastive B-MethodName
learning I-MethodName
techniques O
to O
build O
improved O
representations O
it O
tackles O
the O
non O
- O
compositionality O
challenge O
. O
Additionally O
, O
we O
propose O
a O
dynamic B-MethodName
curriculum I-MethodName
learning I-MethodName
framework O
specifically O
designed O
to O
take O
advantage O
of O
the O
scarce O
available O
data O
for O
modeling O
non O
- O
compositionality O
. O
Our O
framework O
employs O
an O
easy O
- O
to O
- O
hard O
learning O
strategy O
, O
progressively O
optimizing O
the O
model O
's O
performance O
by O
effectively O
utilizing O
available O
training O
data O
. O
Moreover O
, O
we O
integrate O
contrastive B-MethodName
learning I-MethodName
into O
the O
curriculum B-MethodName
learning I-MethodName
approach O
to O
maximize O
its O
benefits O
. O
Experimental O
results O
demonstrate O
the O
gradual O
improvement O
in O
the O
model O
's O
performance O
on O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
tasks O
. O
Our O
evaluation O
encompasses O
six O
datasets O
, O
consistently O
affirming O
the O
effectiveness O
of O
the O
proposed O
framework O
. O
Our O
models O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
zhjjn O
/ O
CLCL.git O
. O

Introduction O

As O
a O
ubiquitous O
yet O
special O
class O
of O
expressions O
in O
natural O
languages O
, O
non O
- O
compositional O
expressions O
( O
e.g. O
, O
the O
idiom O
under O
the O
weather O
) O
have O
specific O
communicative O
intents O
( O
Moon O
et O
al O
. O
, O
1998 O
; O
Baldwin O
and O
Kim O
, O
2010 O
) O
and O
are O
individually O
rare O
but O
collectively O
frequently O
appearing O
widely O
across O
genres O
( O
Moon O
et O
al O
. O
, O
1998 O
; O
Haagsma O
et O
al O
. O
, O
2020 O
) O
. O
They O
are O
characterized O
by O
non O
- O
compositionality O
in O
their O
meaning O
because O
of O
which O
, O
their O
meaning O
can O
not O
be O
inferred O
by O
composing O
the O
meaning O
of O
their O
constituent O
words O
( O
Baldwin O
and O
Kim O
, O
2010 O
) O
. O
In O
addition O
, O
many O
non O
- O
compositional O
expressions O
can O
be O
used O
either O
figuratively O
or O
literally O
, O
in O
a O
context O
dependent O
manner O
. O
For O
example O
, O
the O
phrase O
" O
clean O
house O
" O
can O
be O
interpreted O
literally O
, O
as O
in O
We O
can O
not O
promise O
you O
good O
weather O
but O
we O
can O
promise O
you O
a O
clean O
house O
and O
a O
really O
good O
breakfast O
and O
can O
be O
understood O
figuratively O
, O
as O
in O
Indeed O
, O
the O
Kursk O
crisis O
may O
provide O
him O
with O
an O
opportunity O
to O
further O
clean O
house O
in O
the O
military O
. O

NLP O
systems O
intending O
to O
process O
these O
noncompositional O
expressions O
need O
to O
decide O
if O
these O
expressions O
are O
used O
in O
the O
figurative O
or O
literal O
sense O
before O
modeling O
their O
meaning O
. O
This O
is O
the O
traditional O
and O
popular O
non O
- O
compositional O
language O
processing O
task O
called O
usage B-TaskName
disambiguation I-TaskName
1 O
which O
aims O
to O
differentiate O
the O
literal O
( O
i.e. O
, O
compositional O
) O
from O
the O
figurative O
( O
i.e. O
, O
non O
- O
compositional O
) O
usage O
of O
these O
expressions O
in O
given O
contexts O
, O
dubbed O
as O
idiom O
usage O
recognition O
for O
idiomatic O
expressions O
and O
metaphor O
detection O
for O
metaphorical O
expressions O
( O
Peng O
and O
Feldman O
, O
2015 O
; O
Köper O
and O
i O
m O
Walde O
, O
2016 O
; O
Hwa O
, O
2017 O
, O
2018 O
; O
Chen O
et O
al O
. O
, O
2017 O
; O
Jiang O
et O
al O
. O
, O
2022 O
) O
. O
However O
, O
compared O
to O
the O
abundance O
of O
resources O
for O
tasks O
related O
to O
compositional O
expressions O
, O
the O
available O
resources O
for O
idiom O
usage O
recognition O
and O
metaphor O
detection O
are O
very O
limited O
. O

Successful O
disambiguation B-TaskName
of I-TaskName
the I-TaskName
usages I-TaskName
of O
the O
non O
- O
compositional O
expressions O
involves O
overcoming O
two O
challenges O
: O
( O
1 O
) O
the O
linguistic O
challenge O
of O
handling O
non O
- O
compositionality O
and O
( O
2 O
) O
the O
resourcerelated O
challenge O
of O
learning O
from O
scarce O
training O
data O
. O
Previous O
works O
( O
Peng O
and O
Feldman O
, O
2015 O
; O
Köper O
and O
i O
m O
Walde O
, O
2016 O
; O
Hwa O
, O
2017 O
, O
2018 O
) O
primarily O
focus O
on O
designing O
complex O
architectures O
for O
modeling O
non O
- O
compositionality O
, O
while O
also O
ignoring O
the O
representational O
aspect O
to O
model O
non O
- O
compositionality O
under O
a O
limited O
- O
resource O
scenario O
to O
address O
the O
second O
challenge O
. O
The O
focus O
of O
this O
work O
is O
a O
method O
to O
solve O
the O
above O
two O
challenges O
jointly O
and O
find O
sense O
- O
specific O
representations O
of O
the O
idiomatic O
expressions O
. O

With O
the O
same O
idioms O
used O
in O
different O
ways O
as O
natural O
positive O
and O
negative O
examples O
whose O
representations O
could O
be O
better O
by O
using O
contrastive B-MethodName
learning I-MethodName
, O
we O
utilize O
contrastive B-MethodName
learning I-MethodName
to O
address O
the O
first O
challenge O
to O
produce O
a O
better O
representation O
of O
non O
- O
compositional O
expressions O
for O
recognizing O
their O
usage O
. O
Successful O
idiom O
usage O
recognition O
and O
metaphor O
detection O
require O
different O
representations O
of O
the O
same O
expression O
when O
they O
are O
used O
in O
a O
literal O
and O
figurative O
way O
, O
respectively O
. O
Therefore O
, O
we O
incorporate O
a O
contrastive O
objective O
to O
enhance O
the O
difference O
between O
the O
contextualized O
representations O
of O
the O
figurative O
sense O
and O
the O
literal O
sense O
for O
the O
same O
expression O
. O
In O
this O
way O
, O
we O
enable O
the O
classifier O
to O
make O
context O
- O
dependent O
decisions O
in O
the O
embedding O
space O
. O
Secondly O
, O
to O
make O
better O
use O
of O
the O
scarce O
available O
data O
, O
we O
use O
curriculum B-MethodName
learning I-MethodName
( O
Bengio O
et O
al O
. O
, O
2009 O
) O
, O
which O
enables O
the O
models O
to O
gradually O
proceed O
from O
easy O
training O
instances O
to O
harder O
ones O
, O
when O
the O
instances O
are O
themselves O
ordered O
according O
to O
a O
difficulty O
measure O
. O
Therefore O
, O
curriculum O
learning O
naturally O
consists O
of O
( O
1 O
) O
measuring O
the O
difficulty O
level O
for O
each O
training O
example O
, O
and O
( O
2 O
) O
scheduling O
training O
examples O
based O
on O
their O
difficulty O
levels O
. O
Furthermore O
, O
we O
combine O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
together O
by O
utilizing O
contrastive O
objectives O
to O
measure O
the O
difficulty O
level O
of O
the O
training O
examples O
. O
During O
model O
training O
, O
the O
contrastive O
objective O
is O
dynamically O
updated O
, O
and O
thus O
the O
difficulty O
levels O
of O
the O
training O
examples O
are O
also O
updated O
in O
accordance O
with O
the O
current O
ability O
of O
the O
model O
. O

Our O
study O
is O
the O
first O
to O
jointly O
alleviate O
the O
problems O
caused O
by O
non O
- O
compositionality O
and O
limited O
data O
resources O
by O
strategically O
and O
dynamically O
combining O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
, O
and O
deploying O
it O
for O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
. O
Our O
proposed O
framework O
enables O
the O
model O
to O
first O
learn O
from O
simple O
non O
- O
compositional O
expressions O
and O
then O
from O
harder O
ones O
by O
building O
better O
representations O
of O
non O
- O
compositional O
expressions O
via O
contrastive B-MethodName
learning I-MethodName
. O
The O
contributions O
of O
our O
work O
are O
as O
follows O
: O

• O
We O
propose O
a O
novel O
framework O
that O
combines O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
for O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
. O
The O
difficulty O
levels O
obtained O
from O
contrastive O
objectives O
are O
dynamically O
updated O
with O
the O
training O
, O
based O
on O
which O
the O
training O
examples O
are O
dynamically O
scheduled O
. O

• O
Empirical O
evaluations O
of O
our O
proposed O
framework O
on O
the O
tasks O
of O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
affirm O
the O
effectiveness O
of O
our O
framework O
. O
Detailed O
ablation O
studies O
and O
analyses O
are O
provided O
to O
support O
our O
claims O
. O
As O
a O
result O
, O
we O
treat O
both O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
under O
the O
same O
computational O
umbrella O
. O

• O
Our O
proposed O
framework O
also O
shows O
better O
cross O
- O
task O
transfer O
between O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
compared O
to O
the O
baseline O
models O
. O

Related O
Prior O
Work O

Idiom B-TaskName
Usage I-TaskName
Recognition I-TaskName
. O
Like O
other O
noncompositional O
expressions O
, O
the O
meaning O
of O
many O
idiomatic O
expressions O
is O
contextually O
ambiguous O
. O

Prior O
studies O
mainly O
focus O
on O
disambiguating O
their O
figurative O
/ O
literal O
use O
( O
Salehi O
et O
al O
. O
, O
2014 O
; O
Senaldi O
et O
al O
. O
, O
2016 O
; O
Flor O
and O
Klebanov O
, O
2018 O
; O
Amin O
et O
al O
. O
, O
2021 O
; O
Peng O
and O
Feldman O
, O
2015 O
; O
Köper O
and O
i O
m O
Walde O
, O
2016 O
; O
Hwa O
, O
2017 O
, O
2018 O
) O
, O
i.e. O
, O
performing O
the O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
task O
. O
Early O
works O
heavily O
rely O
on O
designing O
representative O
features O
, O
e.g. O
, O
canonical O
form O
( O
Fazly O
et O
al O
. O
, O
2009 O
) O
, O
to O
decide O
literal O
and O
figurative O
usages O
. O
With O
the O
emergence O
of O
word O
embeddings O
and O
neural O
networks O
, O
richer O
features O
are O
encoded O
into O
word O
embeddings O
and O
utilized O
for O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
Hwa O
, O
2017 O
, O
2018 O
) O
. O
Recently O
proposed O
pre O
- O
trained O
language O
models O
have O
shown O
great O
improvement O
on O
various O
NLP O
tasks O
leading O
to O
efforts O
that O
leverage O
the O
power O
of O
large O
pre O
- O
trained O
language O
models O
for O
this O
task O
( O
Zeng O
and O
Bhat O
, O
2021 O
) O
. O
However O
, O
due O
to O
non O
- O
compositionality O
and O
scarcity O
of O
available O
data O
resources O
, O
previous O
works O
mainly O
focused O
on O
designing O
complex O
architectures O
while O
ignoring O
the O
representational O
aspect O
to O
model O
noncompositionality O
under O
a O
limited O
- O
resource O
scenario O
. O

Our O
study O
is O
the O
first O
to O
focus O
on O
solving O
both O
of O
these O
two O
challenges O
to O
fill O
this O
research O
gap O
. O
Metaphor B-TaskName
Detection I-TaskName
. O
Like O
other O
figurative O
expressions O
, O
metaphors O
play O
a O
crucial O
role O
in O
cognitive O
and O
communicative O
functions O
( O
Choi O
et O
al O
. O
, O
2021 O
) O
, O
because O
of O
which O
computationally O
recognizing O
and O
understanding O
the O
metaphorical O
meanings O
of O
words O
becomes O
important O
. O
Early O
approaches O
utilized O
various O
linguistic O
features O
to O
detect O
metaphors O
, O
such O
as O
word O
imageability O
( O
Broadwell O
et O
al O
. O
, O
2013 O
) O
, O
semantic O
supersenses O
( O
Tsvetkov O
et O
al O
. O
, O
2014 O
) O
, O
andunigrams O
( O
Klebanov O
et O
al O
. O
, O
2014 O
) O
. O
In O
recent O
years O
, O
different O
neural O
architectures O
have O
been O
widely O
used O
for O
metaphor O
detection O
, O
including O
CNN O
( O
Wu O
et O
al O
. O
, O
2018 O
) O
, O
LSTM O
( O
Gao O
et O
al O
. O
, O
2018 O
) O
. O
Beyond O
these O
, O
the O
prominence O
of O
large O
pre O
- O
trained O
language O
models O
on O
various O
NLP O
tasks O
has O
prompted O
their O
use O
for O
metaphor O
detection O
. O
Choi O
et O
al O
. O
( O
2021 O
) O
uses O
RoBERTa O
as O
the O
backbone O
model O
to O
get O
contextualized O
representations O
of O
words O
and O
( O
Gong O
et O
al O
. O
, O
2020 O
) O
combines O
other O
linguistic O
features O
in O
a O
RoBERTa O
architecture O
for O
the O
purpose O
of O
metaphor O
detection O
. O
The O
subpar O
performance O
of O
large O
pretrained O
models O
when O
labeled O
data O
are O
scarce O
has O
led O
to O
studies O
exploring O
data O
augmentation O
( O
Lin O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
utilizing O
augmented O
data O
with O
pseudo O
labels O
could O
be O
even O
more O
detrimental O
to O
the O
performance O
due O
to O
the O
noise O
in O
the O
augmented O
data O
. O
Our O
proposed O
curriculum B-MethodName
learning I-MethodName
framework O
can O
potentially O
alleviate O
data O
scarcity O
by O
using O
the O
limited O
data O
more O
effectively O
without O
introducing O
additional O
noise O
. O
This O
is O
the O
first O
work O
to O
show O
its O
positive O
impact O
on O
both O
tasks O
of O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
. O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
sentence O
embeddings O
( O
Logeswaran O
and O
Lee O
, O
2018 O
) O
. O
More O
recently O
, O
with O
the O
dominance O
of O
transformer O
- O
based O
models O
, O
contrastive B-MethodName
learning I-MethodName
is O
also O
being O
used O
to O
train O
transformer O
models O
( O
Fang O
et O
al O
. O
, O
2020 O
; O
Giorgi O
et O
al O
. O
, O
2021 O
; O
Wu O
et O
al O
. O
, O
2020 O
) O
. O
Similarly O
, O
in O
this O
work O
, O
for O
a O
given O
non O
- O
compositional O
expression O
, O
we O
use O
contrastive B-MethodName
learning I-MethodName
to O
pull O
the O
expression O
embeddings O
that O
are O
used O
in O
the O
same O
figurative O
/ O
literal O
sense O
closer O
while O
pushing O
the O
embeddings O
between O
figurative O
and O
literal O
senses O
apart O
. O
Thereby O
we O
set O
a O
precedence O
of O
utilizing O
contrastive B-MethodName
learning I-MethodName
to O
enhance O
the O
representation O
quality O
of O
idiomatic O
expressions O
for O
modeling O
non O
- O
compositionality O
. O
Besides O
, O
we O
also O
propose O
to O
utilize O
the O
contrastive O
objective O
to O
design O
curriculum B-MethodName
learning I-MethodName
, O
for O
reducing O
the O
training O
data O
quantity O
needed O
for O
transformers O
. O
Curriculum B-MethodName
Learning I-MethodName
First O
proposed O
by O
( O
Bengio O
et O
al O
. O
, O
2009 O
) O
, O
curriculum B-MethodName
learning I-MethodName
aims O
to O
enable O
the O
models O
to O
gradually O
learn O
from O
easy O
to O
harder O
examples O
according O
to O
a O
difficulty O
measure O
for O
each O
example O
during O
training O
. O
Therefore O
, O
curriculum B-MethodName
learning I-MethodName
enables O
the O
model O
to O
better O
utilize O
available O
data O
. O
With O
growing O
research O
interests O
, O
curriculum B-MethodName
learning I-MethodName
has O
been O
applied O
in O
different O
fields O
. O
In O
computer O
vision O
, O
curriculum B-MethodName
learning I-MethodName
has O
been O
applied O
to O
a O
range O
of O
tasks O
, O
such O
as O
image O
classification O
( O
Weinshall O
et O
al O
. O
, O
2018 O
) O
, O
human O
attribute O
analysis O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
, O
and O
visual O
question O
answering O
( O
Li O
et O
al O
. O
, O
2020 O
) O
, O
however O
, O
its O
NLP O
application O
is O
mainly O
limited O
to O
neural O
machine O
translation O
( O
Platanios O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2021 O
) O
. O
So O
, O
prior O
works O
on O
curriculum B-MethodName
learning I-MethodName
on O
NLP O
, O
including O
their O
difficulty O
measurement O
and O
scheduling O
strategy O
, O
are O
mainly O
designed O
for O
compositional O
language O
processes O
, O
which O
are O
largely O
different O
from O
non O
- O
compositional O
expressions O
, O
i.e. O
, O
idioms O
and O
metaphors O
. O
In O
this O
study O
, O
we O
propose O
a O
new O
curriculum B-MethodName
learning I-MethodName
method O
specifically O
designed O
for O
non B-TaskName
- I-TaskName
compositional I-TaskName
expression I-TaskName
recognition I-TaskName
. O
Moreover O
, O
for O
the O
first O
time O
we O
show O
how O
curriculum B-MethodName
learning I-MethodName
based O
on O
contrastive B-MethodName
learning I-MethodName
, O
results O
in O
performance O
gains O
in O
the O
idiomaticity O
- O
related O
tasks O
. O

Framework O

In O
this O
section O
, O
we O
introduce O
our O
proposed O
framework O
as O
a O
combination O
of O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
. O
Overall O
, O
we O
first O
utilize O
contrastive B-MethodName
learning I-MethodName
to O
obtain O
the O
contrastive O
objective O
, O
which O
is O
then O
used O
as O
a O
measurement O
of O
the O
difficulty O
level O
for O
each O
sentence O
containing O
idioms O
or O
metaphors O
. O
Then O
, O
our O
proposed O
dynamic O
scheduling O
strategy O
is O
used O
to O
re O
- O
arrange O
the O
training O
examples O
. O
Finally O
, O
the O
model O
is O
trained O
via O
the O
classification O
objective O
and O
the O
contrastive O
objective O
. O
On O
the O
other O
hand O
, O
the O
same O
non O
- O
compositional O
expressions O
used O
in O
different O
senses O
in O
different O
sentences O
are O
semantically O
different O
examples O
. O
Training O
with O
contrastive B-MethodName
learning I-MethodName
allows O
the O
model O
to O
learn O
higher O
- O
quality O
representations O
by O
grouping O
the O
embeddings O
of O
a O
given O
non O
- O
compositional O
expression O
into O
two O
distinct O
clusters O
in O
the O
embedding O
space O
, O
corresponding O
to O
its O
figurative O
and O
literal O
meaning O
. O

Contrastive B-MethodName
Learning I-MethodName

More O
specifically O
, O
for O
a O
sentence O
Y O
i O
( O
anchor O
example O
) O
with O
a O
non O
- O
compositional O
expression O
i O
, O
its O
meaning O
should O
be O
similar O
to O
another O
sentence O
Y O
+ O
i O
( O
positive O
example O
) O
with O
the O
same O
expression O
i O
used O
in O
the O
same O
sense O
because O
they O
both O
contain O
the O
same O
non O
- O
compositional O
expression O
used O
in O
the O
same O
way O
( O
figuratively O
or O
literally O
) O
. O
However O
, O
the O
meaning O
of O
Y O
i O
will O
be O
different O
from O
the O
sentence O
Y O
− O
i O
( O
negative O
example O
) O
with O
the O
same O
expression O
i O
but O
used O
differently O
. O
Therefore O
, O
the O
distance O
between O
the O
appropriate O
representations O
of O
Y O
i O
and O
Y O
+ O
i O
( O
x O
i O
and O
x O
+ O
i O
) O
is O
expected O
to O
be O
small O
, O
while O
the O
distance O
between O
the O
appropriate O
representations O
of O
Y O
i O
and O
Y O
− O
i O
( O
x O
i O
and O
x O
− O
i O
) O
is O
expected O
to O
be O
large O
. O
Thus O
, O
we O
develop O
a O
contrastive O
objective O
by O
considering O

( O
Y O
i O
, O
Y O
+ O
i O
) O
a O
positive O
pair O
and O
( O
Y O
i O
, O
Y O
− O
i O
) O
a O
negative O
pair O
: O
L O
cts O
= O
− O
Y O
∈Y O
log O
f O
( O
x O
i O
, O
x O
+ O
i O
) O
f O
( O
x O
i O
, O
x O
+ O
i O
) O
+ O
f O
( O
x O
i O
, O
x O
− O
i O
) O
( O
1 O
) O

where O
f O
represents O
the O
distance O
function O
. O
Therefore O
, O
our O
final O
loss O
is O
: O

L O
= O
L O
cts O
+ O
L O
cls O
( O
2 O
) O

where O
L O
cts O
is O
the O
contrastive O
loss O
and O
L O
cls O
is O
the O
cross O
- O
entropy O
loss O
based O
on O
the O
ground O
truth O
class O
label O
for O
the O
sense O
( O
literal O
or O
figurative O
) O
of O
the O
expression O
in O
Y O
i O
. O

To O
prepare O
for O
training O
, O
for O
each O
training O
example O
Y O
i O
( O
anchor O
) O
, O
we O
randomly O
sample O
a O
Y O
+ O
i O
to O
form O
the O
positive O
pair O
and O
randomly O
sample O
a O
Y O
− O
i O
to O
form O
the O
negative O
pair O
, O
converting O
the O
training O
example O
Y O
i O
into O
a O
triplet O
of O
anchor O
, O
positive O
, O
and O
negative O
examples O
, O
i.e. O
, O

< O
Y O
i O
, O
Y O
+ O
i O
, O
Y O
− O
i O
> O
. O

We O
use O
the O
triplets O
to O
train O
the O
models O
with O
the O
aforementioned O
final O
loss O
. O

Curriculum B-MethodName
Learning I-MethodName

Difficulty O
Metrics O

This O
section O
defines O
the O
difficulty O
metric O
used O
by O
our O
curriculum O
learning O
framework O
. O
We O
correlate O
the O
classification O
difficulty O
for O
each O
example O
Y O
i O
to O
its O
position O
in O
the O
embedding O
space O
relative O
to O
its O
corresponding O
positive O
Y O
+ O
i O
and O
negative O
example O
Y O
− O
i O
because O
the O
contextualized O
representation O
for O
the O
figurative O
and O
literal O
meaning O
of O
the O
noncompositional O
expression O
should O
be O
different O
. O
Noncompositionality O
means O
that O
the O
meaning O
of O
a O
figurative O
expression O
is O
not O
derivable O
from O
its O
constituent O
words O
, O
but O
rather O
, O
the O
expression O
has O
a O
conventionalized O
figurative O
meaning O
. O
Therefore O
, O
the O
differentiation O
between O
figurative O
and O
literal O
semantics O
demands O
a O
distinction O
between O
an O
expression O
's O
figurative O
and O
literal O
embedding O
. O
If O
the O
figurative O
and O
literal O
embeddings O
for O
the O
same O
expression O
are O
really O
separable O
, O
i.e. O
, O
they O
are O
further O
apart O
in O
the O
embedding O
space O
, O
a O
classifier O
should O
be O
able O
to O
classify O
the O
figurative O
and O
literal O
senses O
more O
easily O
. O
Conversely O
, O
if O
the O
embeddings O
of O
an O
expression O
's O
figurative O
and O
literal O
semantics O
are O
not O
distinctive O
, O
it O
would O
be O
harder O
for O
the O
model O
to O
classify O
the O
expression O
into O
its O
figurative O
and O
literal O
senses O
based O
Algorithm O
1 O
: O
CLCL B-MethodName

Input O
: O
Dataset O
P O
= O
{ O
Y O
i O
} O
K O
i=1 O
, O
Model O
M O
and O
number O
of O
epochs O
N O
Output O
: O
Fine O
- O
tuned O
Model O
M O
* O
1 O
P O
* O
= O
{ O
( O
Y O
i O
, O
Y O
+ O
i O
, O
Y O
− O
i O
) O
} O
K O
i=1 O
; O
2 O
D O
0 O
= O
CTS O
( O
P O
* O
, O
M O
) O
; O
3 O
Sort O
P O
* O
based O
on O
each O
difficulty O
level O
in O
D O
0 O
, O

resulting O
in O
a O
re O
- O
arranged O
P O
* O
0 O
; O
on O
its O
embedding O
. O
Therefore O
, O
it O
makes O
sense O
to O
use O
the O
degree O
to O
which O
the O
figurative O
and O
literal O
embeddings O
are O
separable O
in O
the O
embedding O
space O
as O
a O
measure O
of O
classification O
difficulty O
. O
Intuitively O
, O
if O
Y O
i O
is O
easy O
for O
the O
model O
to O
classify O
, O
then O
x O
i O
, O
the O
embedding O
of O
Y O
i O
, O
should O
already O
encode O
certain O
semantic O
features O
and O
thus O
be O
located O
closer O
to O
x O
+ O
i O
than O
x O
− O
i O
in O
the O
embedding O
space O
. O
Hence O
, O
given O
the O
< O
Y O
i O
, O
Y O
+ O
i O
, O
Y O
− O
i O
> O
triplets O
, O
we O
assess O
the O
difficulty O
of O
a O
training O
example O
Y O
i O
based O
on O
the O
models O
' O
contrastive O
objective O
as O

4 O
for O
n O
= O
1 O
; O
n O
≤ O
N O
do O
5 O
M O
n O
⇐ O
TRAIN O
( O
P O
* O
n−1 O
) O
; O
6 O
D O
n O
= O
∅ O
, O
P O
n O
= O
∅ O
; O
7 O
for O
( O
Y O
, O
Y O
+ O
, O
Y O
− O
) O
∈ O
P O
* O
do O
8 O
d O
Mn O
( O
Y O
) O
= O
CTS O
( O
Y O
; O
M O
n O
) O
; O
9 O
if O
d O
Mn O
( O
Y O
) O
̸ O
= O
d O
M O
n−1 O
( O
Y O
) O
then O
10 O
D O
n O
⇐ O
D O
n O
{ O
d O
Mn O
( O
Y O
) O
} O
; O
11P O
n O
⇐ O
P O
n O
( O
Y O
, O
Y O
+ O
, O
Y O
− O
) O
; O

d O
M O
( O
Y O
i O
) O
= O
CTS O
( O
Y O
i O
; O
M O
) O
= O
f O
( O
x O
i O
, O
x O
+ O
i O
) O
f O
( O
x O
i O
, O
x O
+ O
i O
) O
+ O
f O
( O
x O
i O
, O
x O
− O
i O
) O
( O
3 O
) O
where O
M O
is O
the O
model O
and O
d O
M O
( O
Y O
i O
) O
is O
the O
diffi- O
culty O
measure O
for O
Y O
i O
. O

Scheduling O
Strategy O

d O
M O
( O
Y O
i O
) O
for O
each O
example O
Y O
i O
is O
updated O
as O
: O
d O
Mn O
( O
Y O
i O
) O
= O
CTS O
( O
Y O
i O
; O
M O
n O
) O
( O
4 O
) O

where O
M O
n O
refers O
to O
our O
model O
fine O
- O
tuned O
for O
n O
epochs O
in O
our O
task O
. O

Experiments O

Datasets O

Idiom B-TaskName
Usage I-TaskName
Recognition I-TaskName
. O
We O
conduct O
experiments O
on O
three O
datasets O
for O
idiom O
usage O
recognition O
: O
MAGPIE B-DatasetName
( O
Haagsma O
et O
al O
. O
, O
2020 O
) O
SemEval5B B-DatasetName
( O
Korkontzelos O
et O
al O
. O
, O
2013 O
) O
and O
VNC B-DatasetName
( O
Cook O
et O
al O
. O
, O
2008 O
) O
. O
To O
test O
the O
models O
' O
ability O
to O
recognize O
the O
usage O
of O
unseen O
idioms O
, O
each O
dataset O
was O
split O
into O
train O
and O
test O
sets O
in O
two O
ways O
: O
random O
and O
typebased O
. O
In O
the O
random O
split O
, O
the O
sentences O
are O
randomly O
divided O
, O
and O
the O
same O
idiom O
can O
appear O
in O
both O
train O
and O
test O
sets O
, O
whereas O
in O
the O
typebased O
split O
, O
the O
idioms O
in O
the O
test O
set O
and O
the O
train O
set O
do O
not O
overlap O
. O
For O
MAGPIE B-DatasetName
and O
SemEval5B B-DatasetName
, O
we O
use O
their O
respective O
official O
random O
/ O
typebased O
and O
train O
/ O
test O
splits O
. O
For O
VNC B-DatasetName
, O
the O
official O
dataset O
did O
not O
have O
the O
typebased O
split O
. O
Therefore O
, O
to O
create O
the O
typebased O
split O
, O
we O
randomly O
split O
the O
idiom O
types O
by O
an O
80 O
/ O
20 O
ratio O
, O
leaving O
43 O
idiom O
types O
in O
the O
train O
set O
and O
ten O
idiom O
types O
in O
the O
test O
set O
. O
Metaphor B-TaskName
Detection I-TaskName
. O
Following O
previous O
works O
on O
metaphor B-TaskName
detection I-TaskName
, O
we O
conduct O
experiments O
on O
three O
datasets O
for O
metaphor B-TaskName
detection I-TaskName
: O
( O
1 O
) O
VUA-18 B-DatasetName
( O
Leong O
et O
al O
. O
, O
2018 O
) O
, O
( O
2 O
) O
VUA B-DatasetName
- I-DatasetName
verb I-DatasetName
( O
Steen O
et O
al O
. O
, O
2010 O
) O
, O
and O
( O
3 O
) O
MOH B-DatasetName
- I-DatasetName
X I-DatasetName
dataset O
( O
Mohammad O
et O
al O
. O
, O
2016 O
) O
. O
The O
original O
train O
/ O
dev O
/ O
test O
splits O
provided O
by O
the O
official O
datasets O
are O
used O
in O
our O
experiments O
. O

Baselines O

We O
show O
the O
effectiveness O
of O
our O
method O
via O
a O
comparison O
between O
the O
vanilla O
RoBERTa O
classification O
model O
and O
the O
RoBERTa O
classification O
model O
fine O
- O
tuned O
using O
our O
method O
. O
Besides O
, O
we O
also O
choose O
different O
SOTA O
models O
for O
different O
tasks O
as O
baselines O
. O
Idiom B-TaskName
Usage I-TaskName
Recognition I-TaskName
. O
DISC B-MethodName
( O
Zeng O
and O
Bhat O
, O
2021 O
) O
is O
the O
current O
SOTA O
model O
for O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
. O
Therefore O
, O
we O
choose O
this O
model O
as O
the O
baseline O
for O
this O
task O
. O

Metaphor B-TaskName
Detection I-TaskName
. O
Based O
on O
previous O
works O
, O
MelBERT B-MethodName
( O
Choi O
et O
al O
. O
, O
2021 O
) O
, O
MisNet B-MethodName
( O
Zhang O
and O
Liu O
, O
2022 O
) O
and O
CATE O
( O
Lin O
et O
al O
. O
, O
2021 O
) O
are O
current O
SOTA O
models O
for O
metaphor O
detection O
. O
However O
, O
CATE O
not O
only O
requires O
external O
data O
resources O
as O
augmentation O
, O
but O
also O
does O
not O
have O
a O
publicly O
accessible O
implementation O
, O
which O
makes O
it O
reproduction O
difficult O
. O
Therefore O
, O
we O
only O
choose O
Mel B-MethodName
- I-MethodName
BERT I-MethodName
and O
MisNet B-MethodName
as O
our O
baselines O
and O
report O
the O
performance O
using O
their O
released O
code O
. O

Experimental O
Settings O

We O
implement O
our O
framework O
using O
a O
pre O
- O
trained O
RoBERTa O
Base O
model O
from O
Huggingface O
. O
The O
model O
is O
trained O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
for O
three O
epochs O
, O
using O
the O
Adam O
optimizer B-HyperparameterName
, O
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
3e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
. O
During O
training O
, O
for O
each O
training O
example O
, O
we O
randomly O
select O
its O
positive O
example O
and O
negative O
example O
for O
contrastive O
learning O
. O
The O
classification O
loss O
is O
calculated O
based O
only O
on O
the O
original O
training O
example O
's O
label O
. O

Evaluation O
Metrics O

Considering O
that O
the O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
task O
is O
a O
binary O
classification O
problem O
, O
we O
use O
accuracy B-MetricName
and O
macro B-MetricName
F1 I-MetricName
score O
to O
evaluate O
the O
performance O
. O
We O
also O
include O
the O
F1 B-MetricName
score O
that O
treats O
the O
figurative O
class O
as O
the O
positive O
class O
, O
denoted O
as O
F1 O
- O
fig O
. O

For O
metaphor B-TaskName
detection I-TaskName
, O
we O
follow O
the O
evaluation O
metrics O
( O
accuracy B-MetricName
, O
precision B-MetricName
, O
recall B-MetricName
, O
and O
F1 B-MetricName
) O
in O
previous O
studies O
for O
a O
fair O
comparison O
. O
For O
metaphor B-TaskName
detection I-TaskName
, O
F1 B-MetricName
refers O
to O
the O
F1 B-MetricName
score O
that O
treats O
the O
figurative O
class O
as O
the O
positive O
class O
. O
As O
shown O
in O
Table O
2 O
, O
for O
the O
task O
of O
metaphor B-TaskName
, O
RoBERTa O
classification O
model O
using O
our O
proposed O
method O
achieves O
the O
best O
performance O
on O
all O
the O
datasets O
in O
F1 B-MetricName
score O
. O
For O
VUA18 B-DatasetName
dataset O
, O
compared O
with O
the O
performance O
of O
SOTA O
MelBERT B-MethodName
, O
our O
framework O
achieves O
competitive O
performance O
without O
utilizing O
POS O
taggings O
and O
other O
linguistic O
features O
except O
for O
the O
original O
RoBERTa O
model O
's O
parameters O
. O
For O
the O
VUA B-DatasetName
- I-DatasetName
verb I-DatasetName
dataset O
, O
our O
method O
outperforms O
MelBERT B-MethodName
by O
4.0 B-MetricValue
absolute O
points O
in O
accuracy B-MetricName
, O
10.3 B-MetricValue
in O
Precision B-MetricName
, O
and O
3.4 B-MetricValue
in O
F1 B-MetricName
score O
. O
Besides O
, O
our O
model O
outperforms O
MisNet B-MethodName
by O
5.6 B-MetricValue
points O
in O
Recall B-MetricName
, O
and O
2.0 B-MetricValue
points O
in O
F1 B-MetricName
score O
. O
On O
the O
MOH B-DatasetName
- I-DatasetName
X I-DatasetName
dataset O
, O
our O
method O
achieves O
the O
best O
performance O
by O
outperforming O
MelBERT B-MethodName
by O
2.7 B-MetricValue
points O
in O
Accuracy B-MetricName
and O
2.3 B-MetricValue
points O
in O
F1 B-MetricName
score O
and O
outperforming O
MisNet B-MethodName
by O
1.2 B-MetricValue
in O
Accuracy B-MetricName
and O
0.9 B-MetricValue
in O
F1 B-MetricName
score O
. O
As O
a O
result O
, O
our O
method O
not O
only O
performs O
the O
best O
on O
the O
task O
of O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
but O
also O
on O
the O
task O
of O
metaphor B-TaskName
detection I-TaskName
. O

Results O

As O
shown O
in O

Analysis O

Ablation O
Study O
To O
investigate O
the O
effects O
of O
the O
different O
components O
in O
our O
method O
, O
i.e. O
, O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
, O
we O
compare O
variants O
of O
our O
method O
without B-MethodName
curriculum I-MethodName
learning I-MethodName
( O
w B-MethodName
/ I-MethodName
o I-MethodName
CL I-MethodName
) O
and O
without B-MethodName
contrastive I-MethodName
learning I-MethodName
( O
w B-MethodName
/ I-MethodName
o I-MethodName
CTS I-MethodName
) O
. O
As O
shown O
in O
Table O
3 O
, O
both O
have O
worse O
performance O
than O
the O
complete O
version O
. O
Without B-MethodName
curriculum I-MethodName
learning I-MethodName
, O
the O
accuracy B-MetricName
drops O
by O
more O
than O
1 B-MetricValue
point O
, O
and O
the O
F1 B-MetricName
score O
drops O
by O
more O
than O
2 B-MetricValue
points O
on O
all O
the O
datasets O
across O
both O
random O
and O
typebased O
settings O
. O
It O
should O
be O
noted O
that O
the O
curriculum B-MethodName
learning I-MethodName
and O
contrastive B-MethodName
learning I-MethodName
are O
more O
effective O
under O
a O
typebased O
setting O
as O
shown O
in O
Table O
3 O
. O
For O
metaphor B-TaskName
detection I-TaskName
, O
the O
results O
presented O
in O
Table O
4 O
show O
a O
similar O
trend O
that O
each O
component O
is O
important O
for O
our O
method O
. O
Besides O
, O
we O
also O
observe O
in O
the O
Table O
3 O
and O
4 O
that O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
can O
individually O
improve O
model O
performance O
. O
Furthermore O
, O
when O
combined O
together O
, O
they O
complement O
and O
boost O
each O
other O
to O
further O
improve O
the O
performance O
. O

Analysis O
on O
Data O
Splits O
. O
Our O
method O
's O
effectiveness O
is O
most O
prominent O
on O
unseen O
idiomatic O
expressions O
, O
as O
shown O
in O
Table O
1 O
. O
The O
improvement O
brought O
about O
by O
our O
curriculum B-MethodName
learning I-MethodName
method O
is O
always O
more O
prominent O
in O
a O
typebased O
setting O
compared O
with O
the O
gain O
in O
a O
random O
setting O
. O
Therefore O
, O
with O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
, O
our O
method O
can O
enable O
the O
RoBERTa O
model O
to O
generalize O
over O
unseen O
idioms O
and O
transfer O
knowledge O
on O
recognizing O
non O
- O
compositionality O
to O
unseen O
non O
- O
compositional O
expressions O
. O
For O
the O
transfer O
study O
, O
we O
use O
the O
random O
split O
of O
MAGPIE B-DatasetName
dataset O
and O
VUA18 B-DatasetName
. O
When O
trained O
on O
the O
dataset O
for O
one O
task O
and O
tested O
on O
the O
dataset O
for O
another O
task O
, O
our O
method O
always O
outperforms O
the O
baseline O
method O
, O
MelBERT B-MethodName
. O
Besides O
, O
we O
observe O
that O
the O
models O
achieve O
good O
results O
in O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
when O
trained O
in O
metaphor B-TaskName
detection I-TaskName
. O
However O
, O
when O
trained O
on O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
, O
the O
models O
' O
performance O
on O
metaphor B-TaskName
detection I-TaskName
is O
much O
worse O
. O
Therefore O
, O
the O
symbolic O
knowledge O
learned O
during O
the O
task O
of O
metaphor B-TaskName
detection I-TaskName
could O
be O
transferred O
to O
perform O
the O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
while O
the O
idiomatic O
knowledge O
can O
not O
help O
with O
the O
metaphor B-TaskName
detection I-TaskName
. O
We O
leave O
the O
deeper O
study O
of O
this O
phenomenon O
to O
future O
research O
. O

Embedding O
Visualization O
In O
Figures O
2 O
and O
3 O
, O
we O
visualize O
for O
SemEval5B B-DatasetName
sample O
contextual O
embeddings O
for O
sentences O
from O
two O
idioms O
under O
different O
data O
split O
settings O
. O
As O
shown O
in O
Figure O
2 O
, O
under O
the O
random O
- O
split O
setting O
, O
with O
simple O
fine O
- O
tuning O
and O
contrastive B-MethodName
learning I-MethodName
, O
the O
literal O
and O
figurative O
representations O
are O
already O
separated O
with O
a O
few O
points O
mis O
- O
clustered O
. O
However O
, O
with O
our O
method O
, O
all O
the O
points O
are O
correctly O
separated O
. O
In O
Figure O
3 O
, O
under O
the O
typebased O
- O
split O
setting O
, O
simple O
fine O
- O
tuning O
fails O
to O
separate O
senses O
in O
the O
embeddings O
space O
into O
differentiable O
groups O
. O
We O
observe O
that O
even O
with O
contrastive B-MethodName
learning I-MethodName
, O
there O
are O
still O
points O
clustered O
into O
the O
wrong O
group O
. O
However O
, O
with O
both O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
, O
all O
the O
points O
are O
distinctly O
separated O
. O

Conclusion O
and O
Future O
Work O

In O
this O
paper O
, O
we O
propose O
a O
novel O
method O
specifically O
for O
non O
- O
compositional O
expression O
detection O
, O
including O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
and O
metaphor B-TaskName
detection I-TaskName
. O
Our O
proposed O
method O
combines O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
. O
Besides O
, O
our O
method O
finds O
it O
challenging O
to O
transfer O
from O
the O
task O
of O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
to O
that O
of O
metaphor B-TaskName
detection I-TaskName
. O
Therefore O
, O
more O
advanced O
methods O
for O
learning O
the O
broad O
nature O
of O
non O
- O
compositionality O
, O
including O
those O
of O
idioms O
and O
those O
of O
metaphors O
are O
needed O
. O
We O
leave O
this O
to O
a O
future O
study O
. O

Acknowledgements O

This O
material O
is O
based O
upon O
work O
supported O
by O
the O
National O
Science O
Foundation O
under O
Grant O
No O
. O
IIS O
22 O
- O
30817 O
. O

A O
Implementation O

Our O
experiments O
and O
implementation O
are O
based O
on O
the O
Transformers O
library O
and O
PyTorch O
. O

B O
Experimental O
Details O

All O
of O
our O
experiments O
were O
conducted O
using O
two O
GPUs O
with O
16 O
GB O
RAM O
( O
NVIDIA O
V100 O
) O
. O

B.1 O
Hyperparameter O
Choices O

For O
the O
task O
of O
idiom B-TaskName
usage I-TaskName
recognition I-TaskName
, O
we O
use O
the O
Adam O
optimizer B-HyperparameterName
during O
the O
training O
with O
batch B-HyperparameterName
size I-HyperparameterName
32 B-HyperparameterValue
. O
The O
maximum B-HyperparameterName
input I-HyperparameterName
length I-HyperparameterName
is O
set O
to O
128 B-HyperparameterValue
. O
We O
use O
a O
constant O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-5 B-HyperparameterValue
for O
finetuning O
. O
For O
all O
the O
experiments O
, O
we O
fine O
- O
tune O
the O
models O
for O
30 B-HyperparameterValue
epochs B-HyperparameterName
and O
select O
the O
model O
with O
the O
best O
performance O
on O
the O
development O
set O
for O
testing O
. O
For O
the O
task O
of O
metaphor B-TaskName
detection I-TaskName
, O
we O
used O
the O
Adam O
optimizer B-HyperparameterName
during O
the O
training O
with O
batch B-HyperparameterName
size I-HyperparameterName
16 B-HyperparameterValue
. O
All O
the O
other O
hyperparameters O
are O
set O
to O
default O
values O
used O
in O
( O
Choi O
et O
al O
. O
, O
2021 O
) O
. O
All O
of O
our O
experiments O
are O
performed O
for O
five O
times O
. O
The O
mean O
results O
are O
reported O
. O

B.2 O
Number O
of O
Parameters O

Considering O
that O
our O
proposed O
contrastive B-MethodName
learning I-MethodName
and O
curriculum B-MethodName
learning I-MethodName
do O
not O
introduce O
more O
parameters O
, O
the O
number O
of O
parameters O
is O
identical O
to O
the O
number O
of O
parameters O
in O
the O
underlying O
language O
model O
: O
125 O
M O
for O
RoBERTa O
( O
base O
) O
. O

B.3 O
Average O
Runtime O

The O
training O
process O
for O
one O
epoch O
on O
two O
GPUs O
took O
approximately O
40 O
minutes O
, O
including O
10 O
minutes O
for O
evaluating O
difficulties O
and O
30 O
for O
finetuning O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
4 O
B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Not O
applicable O
. O
Left O
blank O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
4 O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
B O
C O
Did O
you O
run O
computational O
experiments O
? O

4 O
, O
B O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
4.3 O
, O
B O

MetaLogic B-DatasetName
: O
Logical O
Reasoning O
Explanations O
with O
Fine O
- O
Grained O
Structure O

In O
this O
paper O
, O
we O
propose O
a O
comprehensive O
benchmark O
to O
investigate O
models O
' O
logical O
reasoning O
capabilities O
in O
complex O
real O
- O
life O
scenarios O
. O
Current O
explanation O
datasets O
often O
employ O
synthetic O
data O
with O
simple O
reasoning O
structures O
. O
Therefore O
, O
it O
can O
not O
express O
more O
complex O
reasoning O
processes O
, O
such O
as O
the O
rebuttal O
to O
a O
reasoning O
step O
and O
the O
degree O
of O
certainty O
of O
the O
evidence O
. O
To O
this O
end O
, O
we O
propose O
a O
comprehensive O
logical O
reasoning O
explanation O
form O
. O
Based O
on O
the O
multi O
- O
hop O
chain O
of O
reasoning O
, O
the O
explanation O
form O
includes O
three O
main O
components O
: O
( O
1 O
) O
The O
condition O
of O
rebuttal O
that O
the O
reasoning O
node O
can O
be O
challenged O
; O
( O
2 O
) O
Logical O
formulae O
that O
uncover O
the O
internal O
texture O
of O
reasoning O
nodes O
; O
( O
3 O
) O
Reasoning O
strength O
indicated O
by O
degrees O
of O
certainty O
. O
The O
fine O
- O
grained O
structure O
conforms O
to O
the O
real O
logical O
reasoning O
scenario O
, O
better O
fitting O
the O
human O
cognitive O
process O
but O
, O
simultaneously O
, O
is O
more O
challenging O
for O
the O
current O
models O
. O
We O
evaluate O
the O
current O
best O
models O
' O
performance O
on O
this O
new O
explanation O
form O
. O
The O
experimental O
results O
show O
that O
generating O
reasoning O
graphs O
remains O
a O
challenging O
task O
for O
current O
models O
, O
even O
with O
the O
help O
of O
giant O
pre O
- O
trained O
language O
models O
. O

Introduction O

Being O
able O
to O
generate O
reasonable O
explanations O
is O
a O
crucial O
capability O
for O
a O
reliable O
reasoning O
system O
. O
Most O
current O
works O
try O
to O
ask O
models O
to O
generate O
reasoning O
chains O
as O
profound O
explanations O
. O
From O
simple O
rationales O
( O
DeYoung O
et O
al O
. O
, O
2020 O
) O
to O
more O
complex O
multi O
- O
step O
explanations O
( O
Inoue O
et O
al O
. O
, O
2020 O
; O
Jhamtani O
and O
Clark O
, O
2020 O
; O
Saha O
et O
al O
. O
, O
2021 O
) O
and O
deductive O
chains O
of O
reasoning O
Tafjord O
et O
al O
. O
, O
2021 O
; O
Dalvi O
et O
al O
. O
, O
2021 O
) O
, O
previous O
works O
attempt O
to O
encompass O
comprehensive O
information O
. O
However O
, O
the O
current O
explanation O
design O
still O
has O
limitations O
for O
logical O
reasoning O
texts O
Figure O
1 O
: O
A O
logical O
passage O
and O
the O
corresponding O
logic O
metagraph O
in O
the O
proposed O
MetaLogic B-DatasetName
. O
Given O
a O
logical O
passage O
, O
the O
goal O
is O
to O
generate O
the O
full O
metagraph O
including O
the O
chain O
of O
reasoning O
with O
conditions O
of O
rebuttal O
, O
the O
node O
formulae O
, O
and O
the O
degrees O
of O
certainty O
. O
in O
real O
scenarios O
. O
As O
current O
explanations O
lack O
a O
fine O
- O
grained O
structure O
, O
three O
remarkable O
features O
are O
not O
included O
in O
current O
explanations O
for O
the O
sake O
of O
real O
- O
world O
logical O
reasoning O
: O
multiple O
relation O
types O
, O
hierarchical O
structure O
, O
and O
certainty O
. O
As O
a O
result O
, O
we O
can O
not O
comprehensively O
evaluate O
models O
' O
reasoning O
capabilities O
in O
real O
- O
life O
scenarios O
. O

Figure O
1 O
shows O
examples O
of O
the O
crucial O
reasoning O
components O
that O
are O
well O
studied O
by O
previous O
cognitive O
science O
literature O
( O
Toulmin O
, O
2003 O
; O
Garson O
, O
2021 O
) O
but O
overlooked O
by O
previous O
work O
in O
the O
machine O
learning O
community O
. O
First O
, O
the O
inference O
rebuttal O
. O
Previous O
work O
( O
Tafjord O
et O
al O
. O
, O
2021 O
) O
mostly O
only O
focuses O
on O
the O
inferences O
of O
conjunction O
and O
entailment O
among O
different O
statements O
while O
ignoring O
the O
rebuttal O
ones O
, O
which O
could O
be O
crucial O
in O
real O
applications O
. O
For O
example O
, O
sent5 O
counters O
sent4 O
as O
a O
condition O
of O
exception O
and O
we O
can O
not O
construct O
the O
correct O
reasoning O
graph O
without O
the O
rebuttal O
relation O
. O
Second O
, O
there O
could O
exist O
internal O
logical O
relations O
inside O
each O
statement O
. O
For O
example O
, O
sent5 O
contains O
two O
atomic O
sentences O
connected O
by O
a O
logical O
implication O
relation O
. O
Third O
, O
real O
- O
life O
statements O
could O
have O
different O
degrees O
of O
certainty O
. O
For O
example O
, O
" O
He O
is O
hungry O
" O
and O
" O
He O
is O
likely O
to O
be O
hungry O
" O
are O
not O
identical O
but O
relevant O
because O
of O
the O
certainty O
. O
However O
, O
most O
previous O
work O
simply O
treats O
them O
completely O
separately O
instead O
of O
considering O
their O
relevance O
and O
trying O
to O
model O
the O
difference O
( O
i.e. O
, O
certainty O
) O
. O

Motivated O
by O
previous O
cognitive O
science O
work O
( O
i.e. O
, O
Toulmin O
Model O
1 O
( O
Toulmin O
, O
2003 O
) O
and O
modal O
logic O
theory O
2 O
( O
Garson O
, O
2021 O
) O
) O
, O
we O
propose O
a O
new O
explanation O
form O
, O
logic O
metagraphs O
, O
to O
address O
the O
aforementioned O
limitations O
of O
previous O
work O
. O
As O
demonstrated O
in O
Figure O
1 O
, O
the O
logical O
metagraphs O
are O
directed O
acyclic O
graphs O
with O
meta O
nodes O
connected O
by O
two O
types O
of O
edges O
, O
support O
and O
rebut O
, O
representing O
the O
inferences O
between O
the O
statements O
over O
a O
logical O
passage O
. O
The O
meta O
structure O
uncovers O
the O
chain O
of O
reasoning O
from O
evidence O
to O
the O
conclusion O
, O
along O
with O
the O
challenges O
from O
the O
rebuttal O
sentences O
. O
Each O
meta O
node O
stores O
information O
about O
a O
logically O
sound O
statement O
formulated O
as O
a O
propositional O
formula O
in O
a O
standard O
modal O
logic O
S5 O
system O
( O
Hughes O
et O
al O
. O
, O
1996 O
) O
, O
a O
direct O
extension O
of O
first O
- O
order O
propositional O
logic O
with O
two O
certainty O
operators O
. O
The O
formulae O
have O
atomic O
sentences O
as O
logical O
variables O
that O
denote O
events O
or O
beliefs O
, O
which O
are O
modified O
by O
three O
unary O
operators O
on O
their O
certainty O
( O
negation O
¬ O
, O
necessity O
2 O
, O
and O
possibility O
3 O
) O
and O
are O
joined O
by O
three O
binary O
operators O
on O
their O
logical O
relations O
( O
implication O
→ O
, O
conjunction O
∧ O
, O
disjunction O
∨ O
) O
. O
As O
a O
result O
, O
the O
logic O
metagraphs O
are O
comprehensive O
with O
multi O
- O
hop O
reasoning O
paths O
, O
inference O
rebuttal O
, O
the O
internal O
structure O
of O
the O
statements O
, O
and O
reasoning O
strength O
denoted O
by O
the O
degrees O
of O
certainty O
. O
We O
collect O
1,000 O
logical O
passages O
from O
the O
ReClor B-DatasetName
dataset O
( O
Yu O
et O
al O
. O
, O
1 O
The O
Toulmin O
Model O
is O
a O
canonical O
theory O
that O
helps O
format O
and O
understand O
arguments O
. O
It O
provides O
a O
general O
pattern O
to O
assign O
logical O
roles O
to O
the O
sentences O
in O
the O
argument O
, O
which O
clarify O
the O
overall O
logical O
relations O
. O
Especially O
, O
the O
rebuttal O
components O
challenge O
the O
derivation O
from O
existing O
evidence O
to O
the O
conclusion O
by O
providing O
additional O
information O
such O
as O
giving O
a O
counterexample O
or O
proposing O
an O
additional O
condition O
. O
2 O
The O
modal O
logic O
theory O
extends O
classic O
first O
- O
order O
propositional O
logic O
with O
two O
modal O
operators O
about O
certainty O
and O
several O
corresponding O
rules O
. O
This O
facilitates O
us O
to O
keep O
the O
logical O
variables O
and O
relations O
found O
in O
the O
text O
and O
, O
at O
the O
same O
time O
, O
introduce O
degrees O
of O
certainty O
to O
the O
graph O
. O
2020 O
) O
and O
build O
the O
MetaLogic B-DatasetName
dataset O
. O

Based O
on O
our O
new O
explanation O
form O
, O
we O
examine O
the O
current O
best O
models O
' O
ability O
to O
understand O
logical O
reasoning O
profoundly O
. O
The O
models O
need O
to O
generate O
the O
logic O
metagraphs O
given O
a O
logical O
passage O
. O
Performances O
are O
evaluated O
by O
matching O
scores O
for O
the O
overall O
structure O
as O
well O
as O
the O
three O
fine O
- O
grained O
components O
: O
( O
1 O
) O
The O
inference O
steps O
between O
meta O
nodes O
; O
( O
2 O
) O
The O
per O
- O
statement O
formulae O
with O
multiple O
logical O
triples O
; O
( O
3 O
) O
The O
degrees O
of O
certainty O
. O
Our O
evaluation O
results O
indicate O
that O
generating O
a O
comprehensive O
logical O
reasoning O
structure O
is O
still O
challenging O
for O
existing O
giant O
models O
. O

Our O
contributions O
are O
three O
- O
fold O
: O

1 O
. O
We O
propose O
a O
new O
explanation O
form O
, O
the O
logic O
metagraphs O
, O
with O
a O
comprehensive O
logical O
structure O
and O
rich O
logical O
information O
, O
and O
the O
corresponding O
metagraph B-TaskName
generation I-TaskName
task O
. O

2 O
. O
We O
build O
a O
high O
- O
quality O
dataset O
, O
MetaLogic B-DatasetName
, O
on O
real O
- O
world O
logical O
passages O
. O

3 O
. O
We O
conduct O
experiments O
on O
three O
generative O
models O
in O
different O
frameworks O
and O
locate O
the O
challenges O
for O
current O
models O
. O

Related O
Works O

Explanations O
Explanation O
in O
the O
context O
of O
natural O
language O
understanding O
tasks O
( O
e.g. O
, O
QA O
) O
provides O
interpretability O
about O
how O
models O
solve O
the O
problem O
. O
The O
strategies O
include O
asking O
the O
models O
to O
generate O
rationales O
while O
answering O
the O
questions O
( O
DeYoung O
et O
al O
. O
, O
2020 O
; O
Inoue O
et O
al O
. O
, O
2020 O
) O
, O
and O
deriving O
multi O
- O
hop O
chains O
of O
reasoning O
( O
Jhamtani O
and O
Clark O
, O
2020 O
; O
Dalvi O
et O
al O
. O
, O
2021 O
) O
. O
The O
single O
- O
sentence O
rationale O
provides O
justification O
for O
the O
question O
answering O
but O
does O
not O
uncover O
the O
reasoning O
procedure O
. O
While O
the O
form O
of O
multi O
- O
hop O
chains O
of O
reasoning O
uncovers O
the O
reasoning O
procedure O
and O
remedies O
the O
simple O
justification O
of O
rationale O
, O
it O
still O
lacks O
critical O
clues O
about O
the O
mechanism O
within O
the O
reasoning O
steps O
. O
Our O
proposed O
fine O
- O
grained O
explanation O
form O
extends O
the O
chain O
of O
reasoning O
by O
unwrapping O
the O
fine O
- O
grained O
texture O
within O
each O
reasoning O
step O
. O
As O
a O
result O
, O
it O
allows O
the O
reasoning O
chains O
to O
include O
multiple O
inference O
types O
( O
e.g. O
, O
rebuttal O
) O
and O
broader O
reasoning O
types O
such O
as O
abductive O
reasoning O
with O
the O
hidden O
worldknowledge O
assumption O
. O

Logical O
Reasoning O
Machine O
logical O
reasoning O
requires O
models O
to O
conduct O
hidden O
symbolic O
reasoning O
processes O
through O
question O
answering O
( O
Yu O
et O
al O
. O
, O
sent1 O
: O
v1 O
: O
doctor O
: O
v2 O
: O
recent O
pharmaceutical O
advances O
will O
lead O
the O
way O
in O
weight O
loss O
. O
sent2 O
: O
v1 O
: O
prior O
to O
these O
advancements O
, O
v2 O
: O
obesity O
-related O
deaths O
outnumbered O
all O
other O
causes O
of O
death O
by O
a O
wide O
margin O
. O
sent3 O
: O
v1 O
: O
the O
new O
drugs O
will O
v2 O
: O
curb O
appetite O
and O
v3 O
: O
increase O
metabolism O
. O
sent4 O
: O
v1 O
: O
thanks O
to O
v2 O
: O
these O
advancements O
, O
v3 O
: O
obesity O
will O
dramatically O
decline O
in O
the O
near O
future O
. O
sent5 O
: O
v1 O
: O
most O
people O
will O
not O
be O
able O
to O
afford O
these O
prescriptions O
v2 O
: O
since O
v3 O
: O
the O
majority O
of O
health O
care O
plans O
will O
not O
cover O
the O
new O
drugs O
. O

Binary O
logical O
operators O
→ O
, O
⋀ O
, O
⋁ O

Unary O
logical O
operators O
¬ O
, O
□ O
( O
necessity O
) O
, O
◇ O
( O
possibility O
) O

Sent4 O

Certainty O
: O
necessary O

Node O
Formula O
: O
□ O
( O
v O
→ O
□ O
v O
) O
Sent3 O
Certainty O
: O
necessary O
Node O
Formula O
: O
□ O
( O
v O
⋀v O
) O

Sent2 O

Certainty O
: O
contingent O
Node O
Formula O
: O
N O
/ O
A O
Sent1 O
Certainty O
: O
contingent O
Node O
Formula O
: O
N O
/ O
A O

Certainty O
: O
unnecessary O

Sent5 O
: O
v1 O
: O
most O
people O
will O
not O
be O
able O
to O
afford O
these O
prescriptions O
v2 O
: O
since O
v3 O
: O
the O
majority O
of O
health O
care O
plans O
will O
not O
cover O
the O
new O
drugs O
. O

Figure O
2 O
: O

The O
overall O
logical B-TaskName
reasoning I-TaskName
explanation I-TaskName
task O
is O
defined O
as O
follows O
. O
Given O
a O
passage O
, O
a O
model O
reconstructs O
the O
fine O
- O
grained O
logical O
structure O
with O
the O
meta O
support O
or O
rebut O
relations O
, O
the O
inner O
node O
formulae O
, O
and O
degrees O
of O
certainty O
for O
each O
node O
. O
Given O
a O
logical O
statement O
, O
the O
formula O
is O
constructed O
from O
the O
labeled O
logical O
triples O
with O
the O
parsed O
unary O
operators O
, O
which O
can O
then O
be O
reduced O
to O
canonical O
forms O
. O
The O
certainty O
label O
should O
follow O
the O
global O
operators O
. O
2020 O
; O
, O
or O
explicitly O
perform O
symbolic O
reasoning O
via O
natural O
language O
Tafjord O
et O
al O
. O
, O
2021 O
; O
Dalvi O
et O
al O
. O
, O
2021 O
) O
. O
The O
QA O
- O
based O
reasoning O
data O
is O
mostly O
collected O
from O
real O
- O
life O
scenarios O
without O
corresponding O
structural O
information O
. O
To O
perform O
reasoning O
, O
symbolic O
modules O
( O
Huang O
et O
al O
. O
, O
2021 O
; O
Ouyang O
et O
al O
. O
, O
2021 O
) O
or O
learning O
strategies O
( O
Wang O
et O
al O
. O
, O
2022 O
) O
are O
designed O
to O
approximate O
the O
reasoning O
structure O
. O
On O
the O
other O
hand O
, O
explicitly O
generating O
chains O
of O
reasoning O
can O
better O
uncover O
models O
' O
reasoning O
processes O
. O
However O
, O
recent O
work O
mostly O
focuses O
on O
deductive O
reasoning O
, O
where O
models O
with O
iterative O
strategy O
( O
Tafjord O
et O
al O
. O
, O
2021 O
) O
or O
reasoning O
modules O
( O
Hong O
et O
al O
. O
, O
2022 O
) O
show O
superior O
performances O
. O
To O
encourage O
more O
advanced O
reasoning O
capabilities O
, O
we O
propose O
a O
comprehensive O
reasoning O
structure O
with O
fine O
- O
grained O
factors O
. O

Argumentation O
/ O
Discourse O
Structures O
Previous O
works O
( O
Lawrence O
and O
Reed O
, O
2019 O
; O
Li O
et O
al O
. O
, O
2022 O
) O
such O
as O
argumentation O
mining O
( O
Stab O
andGurevych O
, O
2014b O
, O
a O
, O
2017 O
) O
or O
discourse O
parsing O
( O
Carlson O
et O
al O
. O
, O
2001 O
; O
Webber O
et O
al O
. O
, O
2019 O
) O
study O
document O
structure O
prediction O
. O
Given O
a O
passage O
, O
a O
model O
is O
required O
to O
predict O
the O
argument O
components O
or O
the O
discourse O
relations O
between O
them O
. O
Instead O
of O
identifying O
the O
rhetorical O
structure O
of O
a O
passage O
, O
the O
proposed O
logic O
metagraphs O
aim O
at O
simulating O
the O
logical O
reasoning O
process O
, O
where O
the O
model O
needs O
to O
select O
the O
relevant O
knowledge O
out O
of O
a O
pool O
to O
finish O
the O
reasoning O
. O
Besides O
, O
unlike O
directly O
con O
- O
sidering O
a O
sentence O
or O
a O
text O
span O
as O
a O
reasoning O
node O
, O
MetaLogic O
explores O
a O
schema O
with O
finer O
granularity O
. O
Each O
reasoning O
node O
is O
further O
decomposed O
into O
logical O
variables O
with O
relations O
and O
modal O
operators O
so O
that O
the O
inner O
structure O
as O
well O
as O
the O
certainty O
are O
considered O
. O

Task O
Definition O

Overall O
Generation O
Task O
The O
desideratum O
is O
that O
a O
model O
reconstructs O
the O
fine O
- O
grained O
logic O
explanation O
for O
a O
given O
passage O
, O
which O
uncovers O
the O
model O
's O
understanding O
of O
the O
logic O
between O
the O
lines O
. O
The O
logic O
explanation O
is O
formatted O
as O
logic O
metagraphs O
with O
support O
or O
rebut O
inference O
steps O
, O
per O
- O
node O
logical O
formulae O
, O
and O
degrees O
of O
certainty O
, O
as O
demonstrated O
in O
Figure O
2 O
. O

The O
input O
for O
the O
models O
is O
a O
passage O
with O
multiple O
statements O
( O
S O
( O
0 O
) O
, O
S O
( O
1 O
) O
, O
... O
, O
S O
( O
N O
) O
) O
and O
atomic O
sentences O
p O
( O
n O
) O
, O
according O
to O
which O
they O
generate O
the O
logic O
metagraph O
. O
The O
logic O
metagraph O
has O
three O
main O
components O
: O
( O
1 O
) O
The O
meta O
structure O
G O
= O
( O
V O
, O
E O
) O
, O
where O
E O
= O
E O
S O
E O
R O
, O
and O
E O
S O
and O
E O
R O
are O
the O
two O
meta O
edge O
types O
, O
support O
and O
rebut O
, O
respectively O
, O
between O
the O
meta O
nodes O
u O

( O
n O
) O
* O
⊆ O
S O

( O
n O
) O
∈ O
V O
, O
n O
≤ O
N O
. O
( O
2 O
) O

The O
set O
of O
node O
formulae O
F O
, O
where O
u O
( O
n O
) O
: O
= O
f O
n O
∈ O
F. O
Each O
formula O
is O
joined O
by O
logical O
triples O
. O
f O
n O
= O
r O
( O
m O
( O
p O

( O
n O
) O
i O
) O
, O
m O
( O
p O
( O
n O
) O
j O
) O
) O

, O
where O
i O
̸ O
= O
j O
, O
r O
∈ O
{ O
→ O
, O
∧ O
, O
∨ O
} O
, O
and O
m O
is O
a O
combination O
in O
{ O
¬ O
, O
2 O
, O
3 O
} O
. O
( O
3 O
) O
The O
set O
of O
degrees O
of O
certainty O
C O
, O
defined O
by O
the O
combination O
format O
of O
{ O
¬ O
, O
2 O
, O
3 O
} O
. O

Senses O

Classic O

Morality O
Tense O
Belief O
2p O

The O
proposition O
p O
is O
necessary O
. O
p O
is O
morally O
obligatory O
. O
It O
will O
always O
be O
the O
case O
that O
p. O
Things O
a O
person O
knows O
to O
be O
true O
. O

3p O

The O
proposition O
p O
is O
possible O
. O
p O
is O
morally O
permissible O
. O
It O
will O
sometimes O
be O
the O
case O
that O
p O
. O

Things O
that O
may O
be O
true O
as O
far O
as O
a O
person O
knows O
. O

Definitions O
2p O
: O
= O
¬3¬p O

It O
is O
necessary O
that O
p. O
: O
= O
It O
is O
not O
possible O
that O
not O
- O
p. O
3p O
: O
= O
¬2¬p O
It O
is O
possible O
that O
p. O
: O
= O
It O
is O
not O
necessary O
that O
not O
- O
p O
. O

Reduction O
Rules O

2¬p O
= O
¬3p O
, O
3¬p O
= O
¬2p O
, O
22p O
= O
2p O
, O
33p O
= O
3p O
, O
23p O
= O
3p O
, O
32p O
= O
2p O
. O

Degrees O
of O
Certainty O

2 O
: O
= O
4 O
( O
necessary O
) O
, O
3 O
: O
= O
3 O
( O
possible O
) O
, O
N O
/ O
A O
: O
= O
2 O
( O
contingent O
) O
, O
¬2 O
: O
= O
1 O
( O
unnecessary O
) O
, O
¬3 O
: O
= O
0 O
( O
impossible O
) O

The O
Logic O
Metagraph O

In O
this O
section O
, O
we O
introduce O
the O
proposed O
logic O
metagraph O
in O
details O
. O
3 O

Meta O
Node O
and O
Edge O

Each O
meta O
node O
corresponds O
to O
a O
logically O
sound O
statement O
( O
e.g. O
, O
premise O
, O
or O
conclusion O
) O
. O
The O
meta O
edges O
are O
either O
support O
or O
rebut O
, O
relating O
to O
a O
single O
step O
of O
inference O
. O
The O
support O
edges O
join O
the O
meta O
nodes O
to O
form O
a O
chain O
of O
reasoning O
to O
the O
conclusion O
, O
whereas O
the O
rebut O
edges O
indicate O
challenges O
from O
the O
condition O
of O
rebuttal O
to O
one O
of O
the O
meta O
nodes O
in O
the O
chain O
, O
which O
are O
evidence O
or O
claims O
about O
exceptional O
conditions O
. O
Each O
inference O
step O
allows O
multiple O
premises O
. O

Internal O
Structure O
of O
Meta O
Node O

The O
internal O
structure O
of O
a O
statement O
is O
formulated O
as O
a O
propositional O
logic O
formula O
. O
The O
logical O
variables O
denote O
the O
atomic O
sentences O
in O
the O
statement O
that O
corresponds O
to O
separate O
events O
or O
beliefs O
. O
The O
logical O
relations O
between O
such O
events O
or O
beliefs O
are O
denoted O
by O
binary O
propositional O
operators O
. O
There O
are O
three O
logical O
relations O
: O
logical O
implication O
, O
conjunction O
, O
and O
disjunction O
( O
→ O
, O
∧ O
, O
∨ O
) O
. O
Multiple O
such O
logical O
triples O
are O
joined O
by O
conjunctions O
( O
∧ O
) O
. O
Furthermore O
, O
each O
logical O
variable O
and O
the O
overall O
formula O
are O
modified O
by O
negation O
( O
¬ O
) O
and O
modal O
( O
2 O
and O
3 O
) O
operators O
, O
representing O
the O
degrees O
of O
certainty O
of O
each O
atomic O
sentence O
as O
well O
as O
the O
whole O
statement O
, O
respectively O
. O
A O
more O
detailed O
introduction O
can O
be O
found O
in O
Section O
4.3 O
. O

3 O
An O
example O
is O
shown O
on O
the O
left O
side O
of O
Figure O
2 O
. O

Certainty O
with O
Modal O
Operators O

Modal O
logic O
( O
Garson O
, O
2021 O
) O
is O
an O
extension O
of O
firstorder O
propositional O
logic O
with O
two O
modal O
operators O
, O
necessity O
( O
2 O
) O
and O
possibility O
( O
3 O
) O
. O
They O
are O
unary O
operators O
, O
and O
Table O
1 O
presents O
examples O
of O
their O
senses O
in O
natural O
language O
( O
Hughes O
et O
al O
. O
, O
1996 O
) O
. O
For O
example O
, O
2p O
denotes O
that O
the O
proposition O
p O
is O
necessary O
, O
while O
3p O
means O
p O
is O
possible O
, O
in O
the O
classic O
definition O
. O
In O
another O
sense O
of O
tense O
, O
2p O
represents O
that O
the O
evidence O
p O
is O
true O
at O
all O
times O
, O
whereas O
3P O
represents O
that O
p O
is O
only O
true O
sometimes O
. O
In O
general O
, O
the O
modal O
operators O
indicate O
certainty O
information O
of O
the O
propositions O
. O
The O
two O
modal O
operators O
can O
define O
each O
other O
with O
the O
negation O
operator O
( O
¬ O
) O
. O
Multiple O
reduction O
rules O
are O
defined O
. O
As O
a O
result O
, O
any O
complex O
formulae O
composed O
of O
modal O
operators O
could O
be O
reduced O
to O
one O
of O
the O
five O
degree O
- O
of O
- O
certainty O
forms O
listed O
in O
Table O
1 O
, O
which O
is O
also O
known O
as O
the O
classic O
S5 O
system O
( O
Hughes O
et O
al O
. O
, O
1996 O
) O
and O
makes O
the O
logic O
metagraph O
defined O
in O
a O
complete O
set O
. O

MetaLogic B-DatasetName

In O
this O
section O
, O
we O
introduce O
the O
construction O
details O
of O
the O
MetaLogic B-DatasetName
dataset O
. O
Since O
the O
logic O
metagraphs O
have O
fine O
- O
grained O
structures O
with O
multiple O
evaluation O
dimensions O
, O
which O
are O
all O
dispensable O
and O
supplement O
each O
other O
, O
we O
design O
a O
rigorous O
annotation O
process O
for O
the O
construction O
. O

Preparation O

Source O
Data O
We O
use O
ReClor B-DatasetName
( O
Yu O
et O
al O
. O
, O
2020 O
) O
as O
the O
source O
data O
, O
where O
the O
multiple O
- O
choice O
questions O
are O
collected O
from O
GMAT O
and O
LSAT O
. O
As O
a O
pilot O
study O
on O
logical B-TaskName
reasoning I-TaskName
explanation I-TaskName
, O
we O
start O
with O
the O
standard O
text O
questions O
so O
that O
the O
explanation O
form O
can O
benefit O
from O
precise O
and O
comprehensive O
logical O
information O
. O
Each O
question O
contains O
a O
logical O
passage O
, O
a O
question O
, O
and O
multiple O
answer O
options O
. O
The O
original O
dataset O
contains O
17 O
reasoning O
types O
, O
which O
can O
be O
mainly O
categorized O
into O
two O
folds O
: O
complete O
reasoning O
composed O
of O
the O
logical O
passage O
and O
the O
option O
( O
e.g. O
, O
the O
types O
Necessary O
Assumptions O
, O
Sufficient O
Assumptions O
, O
Strengthen O
, O
Weaken O
) O
; O
flawed O
in O
- O
context O
reasoning O
structure O
( O
e.g. O
, O
the O
types O
Technique O
, O
Identify O
a O
Flaw O
, O
or O
Dispute O
) O
. O
As O
we O
aim O
to O
study O
models O
' O
understanding O
of O
the O
complete O
reasoning O
process O
over O
the O
whole O
passage O
, O
we O
consider O
data O
from O
the O
first O
category O
, O
from O
which O
we O
randomly O
choose O
1,000 O
samples O
. O
Examples O
of O
the O
selected O
questions O
can O
be O
found O
in O
Appendix O
A. O
Data O
Preprocessing O
We O
first O
filter O
out O
incoherent O
options O
from O
the O
questions O
for O
logical O
structure O
coherence O
. O
For O
ordinary O
questions O
, O
the O
incoherent O
options O
are O
the O
distracting O
ones O
. O
Conversely O
, O
for O
the O
inverse O
questions O
with O
" O
EXCEPT O
" O
, O
we O
randomly O
select O
one O
of O
the O
distracting O
options O
and O
remove O
the O
others O
. O
We O
further O
split O
the O
passage O
into O
sentences O
as O
the O
initial O
meta O
nodes O
and O
per O
meta O
node O
sentence O
into O
clauses O
as O
the O
initial O
logical O
variables O
. O
This O
follows O
the O
convention O
of O
applying O
linguistic O
- O
based O
segments O
as O
reasoning O
components O
in O
related O
studies O
( O
Dalvi O
et O
al O
. O
, O
2021 O
; O
Huang O
et O
al O
. O
, O
2021 O
; O
Wang O
et O
al O
. O
, O
2022 O
; O
Xu O
et O
al O
. O
, O
2022 O
) O
. O
Besides O
, O
considering O
the O
label O
hierarchy O
that O
the O
logical O
variables O
are O
conditioned O
on O
the O
meta O
nodes O
, O
the O
initial O
segments O
help O
build O
the O
desired O
metagraph O
sketch O
. O
Moreover O
, O
the O
initial O
delimitation O
is O
trivial O
with O
punctuation O
marks O
and O
provides O
the O
least O
machine O
guidance O
to O
the O
annotators O
, O
who O
are O
free O
to O
modify O
the O
segments O
on O
their O
understanding O
of O
reasoning O
units O
, O
which O
will O
be O
demonstrated O
in O
Section O
5.2 O
. O
From O
the O
experts O
' O
view O
, O
27 O
of O
30 O
randomly O
sampled O
annotated O
graphs O
are O
of O
high O
quality O
, O
which O
indicates O
the O
high O
reliability O
of O
starting O
with O
the O
initial O
segments O
. O

As O
a O
result O
, O
the O
text O
presented O
to O
the O
annotators O
contains O
the O
original O
text O
with O
the O
passage O
, O
the O
question O
, O
and O
the O
coherent O
option O
, O
along O
with O
a O
list O
of O
delimited O
sentences O
. O

Annotation O

As O
all O
annotation O
tasks O
require O
a O
global O
understanding O
of O
the O
overall O
passage O
, O
we O
recruit O
the O
same O
annotator O
to O
finish O
all O
tasks O
in O
the O
same O
passage O
. O

The O
annotation O
procedure O
has O
four O
steps O
. O
( O
1 O
) O
Read O
through O
the O
text O
and O
have O
a O
rough O
idea O
about O
the O
logical O
role O
of O
each O
initial O
meta O
node O
( O
e.g. O
, O
being O
a O
conclusion O
or O
rebuttal O
) O
. O
If O
an O
initial O
meta O
node O
does O
not O
provide O
complete O
evidence O
, O
then O
the O
annotator O
needs O
to O
merge O
it O
with O
another O
node O
to O
form O
complete O
evidence O
. O

( O
2 O
) O
Annotate O
the O
inference O
types O
between O
the O
meta O
nodes O
. O
After O
this O
stage O
, O
we O
obtain O
the O
chain O
of O
reasoning O
and O
the O
rebuttal O
steps O
. O

( O
3 O
) O
For O
each O
meta O
node O
, O
annotate O
the O
logical O
variables O
by O
refining O
the O
span O
boundaries O
of O
the O
given O
initial O
logical O
variables O
. O
( O
4 O
) O
Annotate O
the O
logical O
binary O
operator O
between O
the O
logical O
variables O
. O
The O
annotation O
platform O
is O
demonstrated O
in O
Appendix O
D. O
We O
recruit O
annotators O
from O
crowd O
- O
sourcing O
platforms O
. O
We O
first O
train O
annotators O
with O
a O
carefully O
designed O
annotation O
guideline O
4 O
and O
require O
them O
to O
pass O
an O
exam O
before O
the O
annotation O
to O
guarantee O
the O
annotation O
quality O
. O
For O
each O
passage O
, O
we O
invite O
two O
annotators O
5 O
. O
On O
average O
, O
we O
pay O
$ O
2.2 O
for O
each O
logical O
passage O
. O

For O
unary O
logical O
operators O
( O
¬ O
, O
2 O
, O
3 O
) O
, O
as O
discussed O
by O
( O
Toulmin O
, O
2003 O
) O
, O
there O
exist O
conventional O
clue O
words O
for O
the O
negation O
and O
modality O
. O
Following O
that O
, O
we O
leverage O
such O
in O
- O
context O
clue O
words O
for O
the O
annotation O
. O
Given O
a O
set O
of O
conventional O
indicators O
( O
demonstrated O
in O
Table O
13 O
in O
Appendix O
C O
) O
, O
we O
parse O
each O
meta O
node O
sentence O
into O
a O
dependency O
parsing O
tree O
, O
then O
detect O
those O
words O
within O
3 B-HyperparameterValue
- O
hops B-HyperparameterName
to O
the O
root O
node O
, O
and O
assign O
the O
corresponding O
operators O
to O
the O
formula O
. O
The O
consecutive O
unary O
operators O
are O
ordered O
by O
the O
distance O
from O
the O
indicators O
to O
the O
parsing O
root O
node O
. O
This O
results O
in O
the O
global O
unary O
operators O
. O
For O
local O
unary O
operators O
of O
the O
logical O
variable O
spans O
, O
we O
parse O
the O
spans O
and O
evaluate O
the O
indicator O
- O
root O
distance O
. O
The O
repeatedly O
detected O
indicators O
are O
reduced O
, O
as O
a O
result O
, O
the O
operators O
are O
kept O
by O
the O
global O
formula O
and O
removed O
before O
the O
local O
variable O
. O
To O
evaluate O
the O
labels O
, O
the O
annotators O
check O
391 O
unary O
operators O
from O
200 O
randomly O
sampled O
passages O
. O
As O
a O
result O
, O
92.6 O
% O
of O
them O
are O
consistent O
with O
human O
cognition O
, O
which O
indicates O
that O
the O
operators O
are O
valid O
and O
consistent O
. O

Inter O
- O
Annotator O
Agreement O

We O
evaluate O
the O
inter O
- O
annotator O
agreement O
in O
multiple O
dimensions O
with O
Cohen B-HyperparameterName
's I-HyperparameterName
Kappa I-HyperparameterName
Coefficient I-HyperparameterName
( O
Cohen O
, O
1960 O
) O
. O

Meta O
Node O
The O
IAA O
of O
meta O
nodes O
reflects O
one O
's O
understanding O
of O
the O
logical O
role O
of O
each O
statement O
. O
We O
evaluate O
the O
annotators O
' O
agreement O
of O
each O
meta O
node O
of O
being O
one O
of O
the O
five O
characters O
: O
conclusion O
, O
rebuttal O
, O
beginning O
of O
the O
chain O
, O
an O
intermediate O
conclusion O
in O
the O
chain O
, O
and O
irrelevant O
node O
. O
Meta O
Edge O
We O
consider O
the O
exhaustive O
meta O
node O
pairs O
except O
the O
reflexive O
ones O
. O
Consequently O
, O
the O
agreement O
is O
calculated O
on O
the O
adjacency O
matrix O
of O
the O
meta O
edges O
regarding O
the O
three O
labels O
: O
support O
, O
rebut O
, O
and O
without O
- O
an O
- O
edge O
. O
The O
diagonal O
elements O
in O
the O
matrix O
are O
excluded O
. O
Logical O
Variable O
As O
the O
logical O
variables O
are O
text O
spans O
, O
the O
annotators O
vote O
for O
each O
token O
for O
being O
in O
a O
logical O
variable O
or O
not O
. O
The O
agreement O
is O
average O
over O
the O
per O
- O
token O
agreement O
. O

Logical O
Relation O
Similar O
to O
meta O
edge O
, O
we O
consider O
the O
exhaustive O
logical O
variable O
pairs O
except O
for O
the O
reflexive O
ones O
. O
Considering O
the O
logical O
variables O
as O
vertices O
, O
the O
agreement O
is O
calculated O
on O
the O
adjacency O
matrix O
regarding O
the O
four O
labels O
: O
logical O
implication O
, O
logical O
conjunction O
, O
logical O
disjunction O
, O
and O
without O
- O
a O
- O
relation O
. O
The O
diagonal O
elements O
are O
regarded O
. O
We O
present O
the O
results O
in O
Table O
2 O
. O
The O
agreement O
is O
consistently O
high O
, O
which O
indicates O
the O
high O
quality O
of O
MetaLogic B-DatasetName
. O
Moreover O
, O
the O
high O
IAA O
also O
indicates O
that O
humans O
could O
easily O
solve O
the O
logical O
reasoning O
explanation O
and O
provide O
consistent O
logical O
reasoning O
graphs O
. O

Dataset O
Statistics O

The O
final O
annotated O
MetaLogic B-DatasetName
contains O
1,000 O
logic O
metagraphs O
with O
over O
3,609 O
meta O
nodes O
and O
1,500 O
formulae O
. O
In O
the O
MetaLogic B-DatasetName
, O
416 O
out O
of O
1,000 O
logic O
metagraphs O
have O
rebuttal O
steps O
. O
Around O
40 O
% O
of O
metagraphs O
have O
multi O
- O
hop O
reasoning O
chains O
. O
On O
average O
, O
each O
logic O
metagraph O
has O
more O
than O
three O
meta O
nodes O
, O
and O
about O
40 O
% O
are O
mapped O
to O
formulae O
. O
Moreover O
, O
each O
metagraph O
has O
an O
average O
of O
2.19 O
global O
operators O
. O
More O
statistics O
can O
be O
found O
in O
Table O
4 O
. O
We O
randomly O
split B-HyperparameterName
the O
data O
with O
60 B-HyperparameterValue
% I-HyperparameterValue
training O
, O
20 B-HyperparameterValue
% I-HyperparameterValue
development O
, O
and O
20 B-HyperparameterValue
% I-HyperparameterValue
testing O
. O

Experiment O

We O
evaluate O
the O
performance O
of O
the O
following O
explanation O
generative O
models O
on O
MetaLogic B-DatasetName
: O
( O
1 O
) O
All B-MethodName
- I-MethodName
at I-MethodName
- I-MethodName
Once I-MethodName
( O
Once B-MethodName
) O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
which O
performs O
sequence O
- O
to O
- O
sequence O
generation O
via O
generating O
the O
whole O
metagraph O
in O
a O
linearized O
sequence O
given O
the O
overall O
passages O
with O
the O
sentence O
and O
variable O
denotations O
; O
( O
2 O
) O
Multitask B-MethodName
T5 I-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
which O
complete O
the O
whole O
generation O
task O
with O
a O
combination O
of O
three O
sub O
- O
tasks O
: O
meta O
structure O
generation O
, O
formula O
generation O
, O
and O
certainty O
prediction O
; O

( O
3 O
) O
MetGen B-MethodName
( O
Hong O
et O
al O
. O
, O
2022 O
) O
, O
which O
is O
a O
module O
- O
based O
framework O
for O
structured O
explanation O
generation O
. O
It O
further O
introduces O
a O
reasoning O
controller O
and O
two O
modules O
for O
meta O
structure O
generation O
. O
To O
the O
best O
of O
our O
knowledge O
, O
MetGen B-MethodName
is O
the O
current O
state O
- O
of O
- O
the O
- O
art O
explanation O
generative O
model O
. O
Further O
model O
details O
are O
in O
Appendix O
E. O
Following O
Dalvi O
et O
al O
. O
( O
2021 O
) O
, O
we O
report O
the O
F1 B-MetricName
and O
AllCorrect B-MetricName
scores I-MetricName
for O
each O
dimension O
and O
the O
overall O
AllCorrect B-MetricName
score I-MetricName
. O
For O
certainty O
, O
we O
report O
the O
accuracy O
of O
a O
five O
- O
label O
classification O
and O
an O
extra O
macro B-MetricName
- I-MetricName
F1 I-MetricName
due O
to O
the O
unbalance O
of O
the O
degree O
labels O
. O
The O
overall O
AllCorrect B-MetricName
is O
the O
strictest O
metric O
since O
any O
difference O
in O
the O
predicted O
metagraph O
will O
make O
the O
prediction O
a O
wrong O
one O
. O
Details O
can O
be O
found O
in O
Appendix O
F O
. O

Implementation O
Details O

We O
fine O
- O
tune O
Once B-MethodName
( O
large B-MethodName
) O
, O
Multitask B-MethodName
( O
large B-MethodName
) O
, O
and O
MetGen B-MethodName
( O
large B-MethodName
) O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
for O
300 B-HyperparameterValue
epochs B-HyperparameterName
on O
1 O
Tesla O
V100 O
GPU O
, O
and O
fine O
- O
tune O
Once B-MethodName
( O
11b B-MethodName
) O
, O
Multitask B-MethodName
( O
11b B-MethodName
) O
, O
MetGen B-MethodName
( O
11b B-MethodName
) O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
4 B-HyperparameterValue
for O
300 B-HyperparameterValue
epochs B-HyperparameterName
on O
8 O
Tesla O
V100 O
GPUs O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
1e-5 B-HyperparameterValue
for O
all O
models O
. O
The O
model O
parameters O
are O
optimized O
by O
Adafactor O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
. O
The O
models O
are O
evaluated O
per O
10 B-HyperparameterValue
epochs B-HyperparameterName
on O
the O
development O
set O
, O
and O
the O
best O
checkpoints O
are O
saved O
for O
test O
set O
evaluation O
. O

Main O
Results O

From O
the O
results O
shown O
in O
Table O
3 O
, O
we O
can O
find O
out O
that O
generating B-TaskName
the I-TaskName
comprehensive I-TaskName
logic I-TaskName
graph I-TaskName
is O
still O
a O
very O
challenging O
task O
, O
even O
for O
current O
giant O
models O
as O
all O
models O
achieve O
low O
AllCorrect B-MetricName
performance O
. O
Specifically O
, O
we O
can O
make O
the O
following O
observations O
: O

1 O
. O
From O
the O
experiments O
in O
certainty O
prediction O
, O
we O
can O
see O
that O
all O
models O
are O
struggling O
, O
which O
shows O
that O
knowing O
certainty O
is O
still O
not O
a O
trivial O
task O
for O
current O
models O
given O
that O
there O
are O
explicit O
indicators O
in O
context O
. O

2 O
. O
We O
notice O
that O
using O
larger O
pre O
- O
trained O
models O
( O
e.g. O
, O
T5 O
- O
11B O
) O
can O
help O
improve O
the O
performance O
of O
all O
models O
, O
this O
indicates O
that O
big O
models O
can O
help O
better O
model O
the O
statement O
semantics O
such O
that O
they O
can O
better O
identify O
and O
link O
statements O
. O

3 O
. O
We O
also O
notice O
that O
the O
module O
- O
based O
method O
MetGen B-MethodName
can O
outperform O
the O
Once B-MethodName
and O
Multitask B-MethodName
method O
, O
which O
indicates O
that O
iteratively O
generating O
the O
explanation O
graph O
with O
basic O
modules O
is O
a O
more O
reliable O
logical B-TaskName
reasoning I-TaskName
framework O
. O

4 O
. O
From O
the O
experiments O
on O
Component O
1 O
, O
we O
can O
see O
that O
the O
models O
could O
obtain O
high O
node O
scores O
and O
mediocre O
step O
scores O
, O
but O
the O
step O
All B-MetricName
- I-MetricName
Correct I-MetricName
results O
are O
inferior O
. O
This O
indicates O
that O
with O
the O
help O
of O
giant O
pre O
- O
trained O
LMs O
, O
current O
models O
could O
effectively O
learn O
to O
identify O
the O
nodes O
, O
but O
they O
may O
not O
know O
the O
true O
logical O
reasoning O
because O
they O
can O
not O
precisely O
predict O
the O
inference O
types O
among O
these O
nodes O
. O

5 O
. O
The O
models O
achieve O
around O
or O
over O
60 B-MetricValue
% I-MetricValue
of O
F1 B-MetricName
and O
AllCorrect B-MetricName
scores O
in O
predicting O
the O
formula O
, O
showing O
their O
awareness O
of O
the O
inner O
logical O
structure O
. O
This O
makes O
sense O
because O
the O
majority O
of O
the O
inner O
structure O
is O
triggered O
by O
connective O
words O
such O
as O
" O
so O
. O
" O

In O
the O
rest O
of O
this O
section O
, O
we O
present O
a O
more O
detailed O
inspection O
from O
different O
perspectives O
. O

Performances O
on O
Metagraph O
Parts O

We O
further O
inspect O
models O
' O
performance O
on O
detailed O
components O
. O
The O
evaluation O
results O
on O
separate O
inference O
types O
( O
support O
and O
rebut O
) O
are O
demonstrated O
in O
Figure O
3 O
. O
Overall O
, O
identifying O
rebut O
is O
easier O
than O
support O
, O
according O
to O
the O
exact O
match O
scores O
F1 B-MetricName
and O
AllCorrect B-MetricName
. O
This O
makes O
sense O
because O
most O
rebut O
nodes O
could O
contain O
informative O
keywords O
such O
as O
" O
however O
" O
but O
the O
majority O
of O
nodes O
in O
support O
edges O
do O
not O
. O
Besides O
, O
the O
average O
F1 B-MetricName
scores O
per O
operator O
are O
shown O
in O
Figure O
4 O
. O
From O
the O
results O
, O
we O
can O
see O
that O
the O
trend O
of O
models O
' O
performance O
are O
generally O
consistent O
on O
different O
operators O
, O
which O
indicates O
that O
different O
operators O
may O
have O
different O
intrinsic O
difficulty O
. O

Data O
Scale O
for O
Logical O
Inference O

To O
investigate O
how O
well O
current O
models O
can O
learn O
to O
generate O
the O
reasoning O
graphs O
, O
We O
use O
different O
ratios O
of O
training O
data O
to O
train O
the O
models O
and O
present O
the O
results O
in O
Figure O
5 O
. O
Overall O
, O
the O
model O
performances O
show O
a O
rapid O
increase O
within O
20 O
% O
of O
training O
data O
, O
then O
a O
flat O
and O
steady O
increase O
and O
do O
not O
reach O
a O
platform O
, O
indicating O
that O
the O
models O
can O
still O
benefit O
from O
more O
structural O
reasoning O
data O
. O
Among O
the O
models O
, O
MetGen B-MethodName
has O
the O
most O
significant O
growth O
trend O
and O
performs O
data O
efficiently O
with O
small O
data O
, O
showing O
the O
advantages O
of O
the O
module O
- O
based O
learning O
framework O
in O
symbolic O
reasoning O
. O
Interestingly O
, O
we O
find O
out O
that O
the O
performance O
of O
multitask B-MethodName
T5 I-MethodName
decreases O
after O
seeing O
half O
of O
the O
training O
data O
. O
A O
possible O
explanation O
is O
that O
the O
decomposed O
logical O
structure O
as O
independent O
sub O
- O
tasks O
prevents O
the O
models O
from O
a O
holistic O
understanding O
of O
the O
logical O
passages O
. O
Besides O
that O
, O
the O
flat O
increasing O
rate O
after O
seeing O
20 O
% O
of O
the O
training O
data O
also O
suggests O
that O
blindly O
increasing O
the O
training O
data O
scale O
may O
not O
be O
the O
most O
efficient O
way O
of O
teaching O
models O
to O
conduct O
such O
a O
complex O
reasoning O
task O
. O

Error O
Analysis O

To O
better O
understand O
current O
models O
' O
errors O
, O
we O
randomly O
sample O
50 O
instances O
from O
the O
development O
set O
and O
collect O
the O
predictions O
from O
the O
All B-MethodName
- I-MethodName
at I-MethodName
- I-MethodName
Once I-MethodName

Graph O

Formula O
Certainty O
( O
G1 O
- O
G5 O
) O
( O
F1 O
- O
F4 O
) O
( O
C1 O
- O
C4 O
) O

13 O
/ O
13 O
/ O
11 O
/ O
10 O
/ O
12 O
32 O
/ O
6 O
/ O
8 O
/ O
4 O
5 O
/ O
30 O
/ O
10 O
/ O
5 O
( O
T5 B-MethodName
- I-MethodName
11b I-MethodName
) O
model O
. O
We O
manually O
evaluate O
the O
predictions O
and O
categorize O
4 O
to O
5 O
error O
types O
for O
each O
component O
, O
as O
shown O
here O
in O
the O
Table O
5 O
. O
Specifically O
, O
the O
meta O
graph O
structure O
mainly O
has O
five O
error O
types O
: O
( O
G1 O
) O
Incorrect O
inference O
type O
: O
the O
model O
predicts O
the O
correct O
structure O
, O
but O
over O
one O
of O
the O
inference O
steps O
has O
the O
inverse O
type O
( O
i.g O
. O
, O
predicted O
support O
but O
should O
be O
rebut O
or O
vice O
versa O
) O
; O
( O
G2 O
) O
Incorrect O
rebuttal O
: O
missing O
or O
incorrectly O
predict O
a O
rebuttal O
step O
; O
( O
G3 O
) O
Incorrect O
conclusion O
: O
mismatched O
conclusion O
node O
at O
the O
end O
of O
the O
reasoning O
chain O
; O
( O
G4 O
) O
Incorrect O
inference O
step O
: O
missing O
or O
predicting O
redundant O
inference O
step O
; O
( O
G5 O
) O
Other O
structural O
mismatches O
: O
Including O
different O
chain O
branches O
and O
so O
forth O
. O
The O
four O
error O
types O
in O
formulae O
are O
( O
F1 O
) O
Incorrect O
logical O
variable O
: O
missing O
or O
predicting O
redundant O
logical O
variable O
, O
or O
predicting O
a O
wrong O
variable O
; O
( O
F2 O
) O
Incorrect O
unary O
operator O
: O
The O
variables O
are O
correct O
but O
are O
bound O
by O
incorrect O
unary O
operators O
; O
( O
F3 O
) O
Incorrect O
binary O
operator O
: O
The O
variables O
and O
unary O
operators O
are O
correct O
, O
but O
predict O
incorrect O
binary O
operator O
. O
( O
F4 O
) O
Incorrect O
implication O
direction O
: O
The O
variables O
, O
unary O
and O
binary O
operator O
types O
are O
correct O
, O
but O
the O
implication O
operator O
has O
an O
inverse O
direction O
. O
The O
four O
error O
types O
in O
certainty O
: O
( O
C1 O
) O
Incorrect O
polarity O
: O
Predicting O
the O
certainty O
in O
an O
opposite O
polarity O
; O
( O
C2 O
) O
Other O
polarities O
to O
contingent O
; O
( O
C3 O
) O
Contingent O
is O
predicted O
as O
other O
polarities O
; O
( O
C4 O
) O
Unresolved O
degree O
of O
certainty O
. O

From O
the O
results O
we O
can O
see O
that O
, O
the O
model O
tends O
to O
predict O
the O
operator O
quite O
well O
( O
F2 O
/ O
F3 O
) O
, O
but O
not O
the O
variable O
( O
F1 O
) O
, O
which O
suggests O
that O
even O
though O
current O
deep O
models O
can O
identify O
the O
correct O
relations O
with O
some O
trigger O
words O
( O
e.g. O
, O
" O
so O
that O
" O
) O
, O
they O
may O
not O
fully O
understand O
it O
because O
they O
can O
not O
find O
the O
correct O
variable O
span O
in O
the O
context O
. O
Be- O
sides O
that O
, O
we O
also O
notice O
that O
the O
model O
tends O
to O
predict O
the O
wrong O
polarities O
, O
which O
is O
typically O
irrelevant O
towards O
the O
conclusion O
, O
as O
the O
important O
certainty O
feature O
. O
This O
suggests O
that O
the O
model O
may O
learn O
to O
answer O
questions O
with O
the O
wrong O
reason O
( O
i.e. O
, O
short O
path O
( O
Lovering O
et O
al O
. O
, O
2021 O
) O
) O
, O
which O
further O
demonstrates O
the O
importance O
of O
our O
task O
for O
constructing O
a O
reliable O
and O
trustworthy O
reasoning O
system O
. O

Conclusion O

This O
paper O
extends O
the O
boundary O
of O
current O
research O
on O
logical B-TaskName
graph I-TaskName
generation I-TaskName
for O
reliable O
reasoning O
systems O
. O
Specifically O
, O
we O
carefully O
design O
a O
complete O
logic O
explanation O
form O
following O
previous O
research O
on O
cognitive O
science O
. O
Accordingly O
, O
we O
built O
MetaLogic B-DatasetName
with O
a O
comprehensive O
annotation O
task O
design O
and O
quality O
examination O
. O
We O
also O
evaluate O
several O
recent O
models O
and O
show O
that O
the O
performance O
of O
current O
models O
is O
still O
unsatisfactory O
, O
even O
with O
giant O
pre O
- O
trained O
language O
models O
. O
We O
hope O
that O
this O
paper O
could O
motivate O
more O
future O
works O
on O
reliable O
reasoning O
systems O
that O
could O
generate O
the O
correct O
logical O
graphs O
to O
support O
their O
reasoning O
. O
The O
MetaLogic B-DatasetName
data O
and O
implementation O
code O
are O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
tencent O
- O
ailab O
/ O
MetaLogic O
. O

Limitation O

The O
major O
limitation O
of O
MetaLogic B-DatasetName
is O
that O
we O
can O
not O
annotate O
a O
large O
enough O
dataset O
for O
data O
- O
driven O
methods O
. O
However O
, O
considering O
that O
humans O
could O
learn O
to O
conduct O
logical O
reasoning O
after O
seeing O
a O
few O
examples O
, O
we O
argue O
that O
it O
is O
meaningful O
to O
investigate O
whether O
machines O
can O
learn O
the O
same O
level O
of O
reasoning O
capability O
with O
limited O
data O
. O

Ethical O
Considerations O

During O
the O
annotation O
process O
, O
we O
follow O
the O
minimum O
payment O
requirement O
of O
the O
united O
states O
. O
No O
personal O
or O
confidential O
information O
is O
collected O
. O
Hence O
, O
to O
the O
best O
of O
our O
knowledge O
, O
there O
is O
no O
ethical O
concern O
. O

A O
Example O
Source O
Data O
from O
ReClor B-DatasetName

Tables O
6 O
, O
7 O
, O
8 O
, O
and O
9 O
demonstrate O
the O
example O
source O
data O
of O
different O
reasoning O
types O
from O
the O
ReClor B-DatasetName
dataset O
. O

B O
Guideline O
for O
Logical O
Relation O
Annotation O

Tables O
20 O
, O
21 O
, O
and O
22 O
show O
mappings O
from O
natural O
language O
patterns O
to O
binary O
logical O
operators O
referring O
to O
the O
PDTB3 O
senses O
( O
Webber O
et O
al O
. O
, O
2019 O
) O
. O

For O
the O
logical O
implication O
, O
the O
order O
of O
arguments O
is O
provided O
. O
The O
three O
tables O
are O
provided O
to O
the O
annotators O
as O
their O
references O
during O
annotation O
. O
The O
final O
annotation O
is O
subject O
to O
human O
understanding O
. O

C O
Indicators O
of O
Unary O
Operators O

Table O
13 O
demonstrates O
the O
indicators O
for O
extracting O
the O
unary O
operators O
. O

D O
Annotation O
Interface O

The O
annotation O
interface O
is O
demonstrated O
in O
Figure O
6 O
. O

E O
Model O
Details O

E.1 O
All B-MethodName
- I-MethodName
at I-MethodName
- I-MethodName
Once I-MethodName
T5 B-MethodName

The O
input O
of O
the O
All B-MethodName
- I-MethodName
at I-MethodName
- I-MethodName
Once I-MethodName
T5 I-MethodName
is O
the O
overall O
logical O
passages O
with O
sentences O
and O
variable O
denotations O
. O
The O
output O
is O
the O
linearized O
metagraph O
as O
shown O
in O
Table O
10 O
. O
The O
linearized O
metagraph O
consists O
of O
three O
parts O
: O
the O
meta O
structure O
, O
node O

Reasoning O
Type O
: O
Necessary O
Assumptions O
Context O
: O
A O
recent O
study O
showed O
that O
people O
who O
address O
problems O
quickly O
and O
directly O
are O
significantly O
less O
likely O
to O
have O
gum O
disease O
than O
are O
people O
who O
react O
to O
problems O
by O
refusing O
to O
think O
about O
them O
. O
Since O
stress O
can O
have O
a O
negative O
effect O
on O
the O
immune O
system O
, O
the O
study O
' O
s O
results O
clearly O
indicate O
that O
some O
forms O
of O
gum O
disease O
are O
caused O
or O
aggravated O
by O
suppression O
of O
the O
immune O
system O
. O
Question O
: O
The O
argument O
requires O
the O
assumption O
that O
Options O
: O

A O
: O
people O
who O
tend O
to O
address O
problems O
quickly O
and O
directly O
will O
invariably O
seek O
dental O
care O
at O
the O
first O
sign O
of O
problems O
B O
: O
painful O
conditions O
will O
interfere O
with O
a O
person O
's O
ability O
to O
address O
problems O
quickly O
and O
directly O
C O
: O
people O
who O
have O
highly O
stressful O
lives O
tend O
to O
address O
problems O
quickly O
and O
directly O
D O
: O
refusing O
to O
think O
about O
something O
troubling O
contributes O
to O
a O
person O
's O
level O
of O
stress O
formula O
, O
and O
sentence O
certainty O
( O
denoted O
with O
$ O
graph O
$ O
, O
$ O
formula O
$ O
, O
and O
$ O
degree O
$ O
, O
respectively O
) O
. O
For O
the O
meta O
structure O
, O
we O
use O
the O
semicolon O
to O
connect O
different O
edges O
. O
For O
the O
node O
formula O
, O
we O
map O
the O
operator O
to O
word O
using O
the O
mapping O
{ O
2 O
: O

[ O
necessary O
] O
, O
3 O
: O
[ O
possible O
] O
, O
¬ O
: O
[ O
negative O
] O
, O
∧ O
: O
[ O
and O
] O
, O
∨ O
: O
[ O
or O
] O
, O
→ O
: O
[ O
entail O
] O
} O
. O
We O
use O
the O
semicolon O
to O
connect O
the O
triples O
for O
the O
same O
sentence O
. O
We O
connect O
the O
certainties O
and O
formulae O
of O
sentences O
with O
" O
| O
" O
. O

Reasoning O
Type O
: O
Weaken O
Context O
: O
Many O
people O
suffer O
an O
allergic O
reaction O
to O
sulfites O
, O
including O
those O
that O
are O
commonly O
added O
to O
wine O
as O
preservatives O
. O
However O
, O
since O
there O
are O
several O
winemakers O
producing O
wine O
to O
which O
no O
sulfites O
are O
added O
, O
those O
who O
would O
like O
to O
drink O
wine O
but O
are O
allergic O
to O
sulfites O
can O
drink O
these O
wines O
without O
risking O
an O
allergic O
reaction O
to O
sulfites O
. O

Question O
: O
Which O
of O
the O
following O
, O
if O
true O
, O
most O
seriously O
weakens O
the O
argument O
? O
Options O
: O
A O
: O
Sulfites O
occur O
naturally O
in O
most O
wine O
. O
B O
: O
The O
sulfites O
that O
can O
produce O
an O
allergic O
reaction O
are O
also O
commonly O
found O
in O
beverages O
other O
than O
wine O
. O
C O
: O
Wine O
without O
added O
sulfites O
sometimes O
becomes O
undrinkable O
even O
before O
the O
wine O
is O
sold O
to O
consumers O
. O
D O
: O
Apart O
from O
sulfites O
, O
there O
are O
other O
substances O
commonly O
present O
in O
wine O
that O
can O
trigger O
allergic O
reactions O
. O

Table O
9 O
: O
Question O
from O
ReClor O
with O
logical O
reasoning O
type O
: O
Weaken O
. O
Correct O
answer O
option O
in O
bold O
. O

Input O
: O
sent1 O
: O
v1 O
: O
measurements O
of O
the O
motion O
of O
the O
planet O
uranus O
seem O
to O
show O
uranus O
being O
tugged O
by O
a O
force O
pulling O
it O
away O
from O
the O
sun O
and O
the O
inner O
planets O
. O
sent2 O
: O
v1 O
: O
neptune O
and O
pluto O
, O
v2 O
: O
the O
two O
known O
planets O
whose O
orbits O
are O
farther O
from O
the O
sun O
than O
is O
the O
orbit O
of O
uranus O
, O
v3 O
: O
do O
not O
have O
enough O
mass O
to O
exert O
the O
force O
that O
the O
measurements O
indicate O
. O
sent3 O
: O
v1 O
: O
therefore O
, O
v2 O
: O
in O
addition O
to O
the O
known O
planets O
, O
v3 O
: O
there O
must O
be O
at O
least O
one O
planet O
in O
our O
solar O
system O
that O
we O
have O
yet O
to O
discover O
. O
sent4 O
: O
v1 O
: O
there O
is O
a O
belt O
of O
comets O
beyond O
the O
orbit O
of O
pluto O
with O
powerful O
gravitational O
pull O
. O
We O
train O
the O
All O
- O
at O
- O
Once O
T5 O
model O
with O
a O
batch O
size O
of O
32 O
and O
a O
learning O
rate O
of O
1e-5 O
for O
300 O
epochs O
. O

E.2 O
Multitask B-MethodName
T5 I-MethodName

The O
Multitask B-MethodName
T5 I-MethodName
decomposes O
the O
whole O
generation O
task O
into O
three O
sub O
- O
tasks O
: O
meta O
structure O
generation O
, O
formula O
generation O
, O
and O
certainty O
prediction O
. O
We O
train O
a O
single O
T5 B-MethodName
model O
on O
these O
three O
sub O
- O
tasks O
simultaneously O
. O
We O
follow O
Raffel O
et O
al O
. O
( O
2020 O
) O
to O
add O
a O
task O
- O
specific O
prefix O
to O
the O
input O
before O
feeding O
it O
to O
the O
model O
. O
Table O
11 O
shows O
some O
specific O
input O
and O
output O
examples O
of O
each O
sub O
- O
task O
. O
We O
train O
the O
Multitask B-MethodName
T5 I-MethodName
model O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-5 B-HyperparameterValue
for O
300 B-HyperparameterValue
epochs B-HyperparameterName
. O
We O
use O
the O
examples O
- O
proportional O
mixing O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
and O
simply O
concatenate O
the O
data O
for O
all O
sub O
- O
tasks O
as O
Task O
: O
Meta O
structure O
generation O
Input O
: O
GRAPH O
: O
sent1 O
: O
to O
reduce O
waste O
of O
raw O
materials O
, O
the O
government O
of O
sperland O
is O
considering O
requiring O
household O
appliances O
to O
be O
broken O
down O
for O
salvage O
when O
discarded O
. O
[ O
AND O
] O
imposing O
the O
fee O
at O
the O
time O
of O
salvage O
would O
reduce O
waste O
more O
effectively O
, O
however O
, O
because O
consumers O
tend O
to O
keep O
old O
appliances O
longer O
if O
they O
are O
faced O
with O
a O
fee O
for O
discarding O
them O
. O
sent2 O
: O
to O
cover O
the O
cost O
of O
salvage O
, O
the O
government O
is O
planning O
to O
charge O
a O
fee O
, O
which O
would O
be O
imposed O
when O
the O
appliance O
is O
first O
sold O
. O
sent4 O
: O
increasing O
the O
cost O
of O
disposing O
of O
an O
appliance O
properly O
increases O
the O
incentive O
to O
dispose O
of O
it O
improperly O
. O

Output O
: O
sent4 O
= O
> O
sent1 O
; O
sent1 O
- O
> O
sent2 O
; O

Task O
: O
Formula O
generation O
Input O
: O
FORMULAE O
: O
v1 O
: O
grammarians O
have O
for O
years O
condemned O
as O
ungrammatical O
the O
english O
phrase O
" O
between O
you O
and O
i O
" O
, O
insisting O
that O
the O
correct O
phrasing O
is O
" O
between O
you O
and O
me O
, O
" O
with O
v2 O
: O
the O
objective O
case O
after O
v3 O
: O
a O
preposition O
. O
v4 O
: O
such O
condemnations O
, O
however O
, O
are O
obviously O
unfounded O
, O
because O
v5 O
: O
shakespeare O
himself O
, O
in O
the O
merchant O
of O
venice O
, O
wrote O
, O
" O
all O
debts O
are O
cleared O
between O
you O
and O
i. O
the O
training O
data O
for O
Multitask B-MethodName
T5 I-MethodName
. O

E.3 O
MetGen B-MethodName

MetGen B-MethodName
( O
Hong O
et O
al O
. O
, O
2022 O
) O
is O
a O
module O
- O
based O
framework O
for O
structured O
explanation O
generation O
. O

Modules O
. O
We O
use O
two O
types O
of O
modules O
: O
the O
conclusion O
module O
and O
the O
rebuttal O
module O
. O
The O
conclusion O
module O
takes O
two O
sentences O
as O
input O
( O
e.g. O
, O
sent1 O
: O
... O
sent2 O
: O
... O
) O
and O
outputs O
the O
inference O
relation O
type O
between O
them O
( O
e.g. O
, O
sent1 O
- O
> O
sent2 O
or O
sent2 O
- O
> O
sent1 O
) O
. O
If O
there O
is O
no O
conclusive O
relationship O
between O
the O
two O
input O
sentences O
, O
the O
module O
would O
output O
the O
word O
none O
. O

The O
rebuttal O
module O
is O
defined O
similarly O
. O
Controller O
. O
The O
controller O
decides O
the O
reasoning O
direction O
based O
on O
the O
current O
reasoning O
state O
. O
Specifically O
, O
given O
the O
current O
partially O
metagraph O
and O
all O
the O
sentences O
, O
the O
controller O
predicts O
which O
combinations O
of O
two O
sentences O
( O
e.g. O
, O
sent1 O
sent2 O
) O
should O
be O
considered O
in O
the O
next O
step O
. O
If O
the O
current O
proof O
is O
complete O
, O
the O
controller O
would O
output O
the O
word O
done O
. O

Reasoning O
Process O
. O
MetGen B-MethodName
generates O
the O
meta O
- O
graphs O
in O
an O
iterative O
manner O
. O
It O
iteratively O
repeats O
the O
following O
reasoning O
iteration O
to O
grow O
the O
graph O
until O
either O
the O
controller O
returns O
done O
or O
the O
maximum O
number O
of O
iteration O
steps O
is O
reached O
. O
It O
takes O
several O
iterations O
before O
completing O
the O
generation O
. O

In O
each O
iteration O
, O
MetGen B-MethodName
generates O
one O
step O
. O
It O
first O
uses O
the O
controller O
to O
predict O
some O
possible O
sentence O
combinations O
. O
Then O
, O
each O
combination O
is O
sent O
to O
the O
reasoning O
modules O
to O
generate O
candidate O
steps O
that O
indicate O
the O
detailed O
inference O
relation O
type O
between O
them O
. O
The O
candidate O
step O
with O
the O
highest O
score O
( O
the O
lowest O
perplexity O
) O
is O
picked O
for O
the O
next O
iteration O
. O
Implementations O
. O
To O
compare O
with O
other O
methods O
under O
the O
same O
number O
of O
parameters O
, O
we O
implement O
MetGen B-MethodName
using O
a O
single O
T5 B-MethodName
model O
. O
Table O
12 O
shows O
some O
input O
and O
output O
examples O
of O
MetGen B-MethodName
. O
The O
MetGen B-MethodName
is O
trained O
on O
five O
sub O
- O
tasks O
simultaneously O
: O
controller O
task O
, O
conclusion O
module O
task O
, O
rebuttal O
module O
task O
, O
formula O
generation O
task O
, O
and O
certainty O
prediction O
tasks O
. O
We O
train O
the O
MetGen B-MethodName
model O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-5 B-HyperparameterValue
for O
300 B-HyperparameterValue
epochs B-HyperparameterName
. O
We O
set O
the O
maximum B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
iteration I-HyperparameterName
steps I-HyperparameterName
as O
3 B-HyperparameterValue
. O

E.4 O
Experimental O
Details O

We O
use O
the O
pre O
- O
trained O
models O
from O
HuggingFace O
Transformers O
7 O
. O
We O
use O
the O
Adafactor O
optimizer O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
. O
We O
run O
the O
experiments O
based O
on O
T5 B-MethodName
- I-MethodName
large I-MethodName
3 O
times O
with O
different O
random O
seeds O
and O
report O
the O
average O
performances O
. O
The O
experiments O
based O
on O
T5 B-MethodName
- I-MethodName
11b I-MethodName
are O
run O
only O
once O
considering O
the O
computational O
cost O
. O

F O
Evaluation O
Metrics O

Meta O
structure O
: O
Does O
the O
predicted O
metagraph O
use O
the O
correct O
sentences O
and O
have O
the O
correct O
structure O
? O
For O
meta O
nodes O
, O
we O
report O
a O
node O
F1 B-MetricName
score I-MetricName
by O
comparing O
the O
set O
of O
sentences O
used O
in O
the O
predicted O
and O
gold O
metagraph O
. O
For O
meta O
structure O
, O
we O
decompose O
the O
metagraph O
into O
one O
- O
premise O
steps O
( O
e.g. O
, O
sent1 O
- O
> O
sent2 O
) O
. O
We O
compare O
the O
set O
of O
steps O
in O
the O
predicted O
and O
gold O
metagraph O
and O
report O
the O
step O
F1 B-MetricName
score I-MetricName
. O
A O
predicted O
step O
is O
correct O
if O
its O
premise O
, O
conclusion O
, O
and O
step O
type O
match O
the O
gold O
one O
. O
The O
AllCorrect B-MetricName
score O
is O
1 B-MetricValue
if O
the O
F1 B-MetricName
is O
1 B-MetricValue
, O
0 B-MetricValue
otherwise O
. O
Formula O
: O
Does O
the O
predicted O
metagraph O
have O
the O
correct O
internal O
structure O
of O
meta O
nodes O
? O
For O
each O
7 O
https O
: O
/ O
/ O
github.com O
/ O
huggingface O
/ O
transformers O
Task O
: O
Controller O
Input O
: O
CONTROL O
: O
proof O
: O
sent1 O
- O
> O
sent3 O
; O
context O
: O
sent1 O
: O
measurements O
of O
the O
motion O
of O
the O
planet O
uranus O
seem O
to O
show O
uranus O
being O
tugged O
by O
a O
force O
pulling O
it O
away O
from O
the O
sun O
and O
the O
inner O
planets O
. O
sent2 O
: O
neptune O
and O
pluto O
, O
the O
two O
known O
planets O
whose O
orbits O
are O
farther O
from O
the O
sun O
than O
is O
the O
orbit O
of O
uranus O
, O
do O
not O
have O
enough O
mass O
to O
exert O
the O
force O
that O
the O
measurements O
indicate O
. O
sent3 O
: O
therefore O
, O
in O
addition O
to O
the O
known O
planets O
, O
there O
must O
be O
at O
least O
one O
planet O
in O
our O
solar O
system O
that O
we O
have O
yet O
to O
discover O
. O
sent4 O
: O
there O
is O
a O
belt O
of O
comets O
beyond O
the O
orbit O
of O
pluto O
with O
powerful O
gravitational O
pull O
. O
Output O
: O
sent2 O
sent3 O
Task O
: O
Conclusion O
Module O
Input O
: O
CONCLUSION O
: O
sent2 O
: O
the O
dna O
of O
contemporary O
humans O
is O
significantly O
different O
from O
that O
of O
the O
neanderthal O
. O
sent3 O
: O
the O
dna O
of O
prehistoric O
homo O
sapiens O
ancestors O
of O
contemporary O
humans O
was O
not O
significantly O
more O
similar O
to O
that O
of O
neanderthals O
than O
is O
the O
dna O
of O
contemporary O
humans O
. O
Output O
: O
sent3 O
- O
> O
sent2 O
Task O
: O
Rebuttal O
Module O
Input O
: O
REBUTTAL O
: O
sent1 O
: O
recent O
unexpectedly O
heavy O
rainfalls O
in O
the O
metropolitan O
area O
have O
filled O
the O
reservoirs O
and O
streams O
; O
water O
rationing O
, O
therefore O
, O
will O
not O
be O
necessary O
this O
summer O
. O
sent2 O
: O
the O
water O
company O
's O
capacity O
to O
pump O
water O
to O
customers O
has O
not O
kept O
up O
with O
the O
increased O
demand O
created O
by O
population O
growth O
in O
the O
metropolitan O
area O
. O
Output O
: O
sent2 O
= O
> O
sent1 O
sentence O
, O
we O
measure O
the O
formula O
F1 B-MetricName
score I-MetricName
by O
comparing O
all O
formulae O
in O
the O
predictions O
and O
gold O
annotations O
. O
A O
predicted O
formula O
is O
considered O
correct O
if O
its O
certainty O
operators O
, O
binary O
operators O
, O
and O
variables O
match O
the O
gold O
one O
. O
For O
the O
certainty O
operators O
, O
we O
reduce O
them O
to O
the O
standard O
form O
( O
one O
of O
the O
five O
degree O
- O
of O
- O
certainty O
forms O
listed O
in O
Table O
1 O
) O
before O
comparison O
. O
For O
the O
binary O
operator O
, O
we O
consider O
its O
symmetry O
. O
For O
example O
, O
¬v O
1 O
∧ O
3v O
2 O
is O
equivalent O
to O
3v O
2 O
∧ O
¬v O
1 O
, O
but O
¬v O
1 O
→ O
3v O
2 O
is O
not O
equivalent O
to O
3v O
2 O
→ O
¬v O
1 O
. O
The O
AllCorrect B-MetricName
score O
is O
1 B-MetricValue
if O
the O
formula O
F1 B-MetricName
is O
1 B-MetricValue
, O
0 B-MetricValue
otherwise O
. O
Since O
each O
sample O
contains O
multiple O
sentences O
, O
we O
average O
the O
formula O
F1 B-MetricName
scores I-MetricName
of O
all O
sentences O
in O
the O
sample O
as O
the O
formula O
F1 B-MetricName
score I-MetricName
for O
this O
sample O
. O

Certainty O
: O
Are O
the O
certainties O
of O
the O
sentence O
correct O
? O
For O
each O
sample O
, O
we O
compute O
the O
accuracy B-MetricName
of O
the O
predicted O
certainties O
of O
the O
sentences O
. O
The O
AllCorrect B-MetricName
score O
is O
1 B-MetricValue
if O
the O
accuracy B-MetricName
is O
1 B-MetricValue
, O
and O
0 B-MetricValue
otherwise O
. O
We O
report O
the O
accuracy O
and O
AllCorrect B-MetricName
score O
of O
the O
testing O
dataset O
, O
which O
is O
the O
average O
accuracy O
and O
AllCorrect B-MetricName
score O
of O
all O
samples O
in O
Negation O
( O
¬ O
) O

" O
no O
" O
, O
" O
not O
" O
, O
" O
none O
" O
, O
" O
nobody O
" O
, O
" O
nothing O
" O
, O
" O
neither O
" O
, O
" O
nor O
" O
, O
" O
nowhere O
" O
, O
" O
never O
" O
, O
" O
hardly O
" O
, O
" O
scarcely O
" O
, O
" O
barely O
" O
, O
" O
does O
n't O
" O
, O
" O
is O
n't O
" O
, O
" O
was O
n't O
" O
, O
" O
should O
n't O
" O
, O
" O
would O
n't O
" O
, O
" O
could O
n't O
" O
, O
" O
wo O
n't O
" O
, O
" O
ca O
n't O
" O
, O
" O
do O
n't O
" O
, O
" O
impossible O
" O
Box O
( O
2 O
) O

" O
necessarily O
" O
, O
" O
must O
" O
, O
" O
definitely O
" O
, O
" O
certainly O
" O
, O
" O
clearly O
" O
, O
" O
obviously O
" O
, O
" O
undoubtedly O
" O
, O
" O
surely O
" O
, O
" O
will O
" O
, O
" O
all O
" O
, O
" O
every O
" O
, O
" O
always O
" O
Diamond O
( O
3 O
) O
" O
likely O
" O
, O
" O
approximately O
" O
, O
" O
possibly O
" O
, O
" O
perhaps O
" O
, O
" O
probably O
" O
, O
" O
maybe O
" O
, O
" O
few O
" O
, O
" O
may O
" O
, O
" O
might O
" O
, O
" O
could O
" O
, O
" O
many O
" O
, O
" O
most O
" O
, O
" O
some O
" O
, O
" O
numerous O
" O
, O
" O
countless O
" O
, O
" O
majority O
" O
, O
" O
often O
" O
, O
" O
frequently O
" O
, O
" O
commonly O
" O
, O
" O
usually O
" O
, O
" O
sometimes O
" O
, O
" O
repeatedly O
" O
, O
" O
appears O
" O
, O
" O
seems O
" O
, O
" O
suggests O
" O
, O
" O
indicates O
" O
the O
dataset O
. O
Due O
to O
the O
unbalance O
of O
the O
certainty O
labels O
, O
we O
gather O
the O
predictions O
for O
all O
sentences O
in O
the O
dataset O
( O
ignoring O
which O
sample O
the O
sentence O
comes O
from O
) O
and O
report O
the O
macro B-MetricName
- I-MetricName
F1 I-MetricName
score O
. O

Overall O
: O
The O
overall O
AllCorrect B-MetricName
score I-MetricName
of O
a O
predicted O
metagraph O
is O
1 B-MetricValue
only O
if O
all O
of O
the O
meta O
structure O
, O
formulae O
, O
and O
certainties O
are O
correct O
. O
This O
is O
a O
strict O
metric O
since O
any O
error O
would O
result O
in O
a O
score O
of O
0 O
. O
G1 O
: O
Incorrect O
Inference O
Type O
Passage O
: O
sent1 O
: O
for O
similar O
cars O
and O
drivers O
, O
automobile O
insurance O
for O
collision O
damage O
has O
always O
cost O
more O
in O
greatport O
than O
in O
fairmont O
. O

G O
Detailed O
Analysis O
Results O

[ O
AND O
] O
police O
studies O
, O
however O
, O
show O
that O
cars O
owned O
by O
greatport O
residents O
are O
, O
on O
average O
, O
slightly O
less O
likely O
to O
be O
involved O
in O
a O
collision O
than O
cars O
in O
fairmont O
. O
sent3 O
: O
clearly O
, O
therefore O
, O
insurance O
companies O
are O
making O
a O
greater O
profit O
on O
collision O
-damage O
insurance O
in O
greatport O
than O
in O
fairmont O
. O
sent4 O
: O
repairing O
typical O
collision O
damage O
does O
not O
cost O
more O
in O
greatport O
than O
in O
fairmont O
. O
Gold O
: O
sent4 O
- O
> O
sent3 O
; O
sent1 O
- O
> O
sent3 O
; O
Pred O
: O
sent1 O
- O
> O
sent3 O
; O
sent4 O
= O
> O
sent3 O
; O

G2 O
: O
Incorrect O
Rebuttal O
Passage O
: O
sent1 O
: O
there O
should O
be O
a O
greater O
use O
of O
gasohol O
. O
sent2 O
: O
gasohol O
is O
a O
mixture O
of O
alcohol O
and O
gasoline O
, O
and O
has O
a O
higher O
octane O
rating O
and O
fewer O
carbon O
monoxide O
emissions O
than O
straight O
gasoline O
. O

[ O
AND O
] O
burning O
gasohol O
adds O
no O
more O
carbon O
dioxide O
to O
the O
atmosphere O
than O
plants O
remove O
by O
photosynthesis O
. O
sent4 O
: O
cars O
burn O
on O
the O
average O
slightly O
more O
gasohol O
per O
kilometer O
than O
they O
do O
gasoline O
. O
Gold O
: O
sent2 O
- O
> O
sent1 O
; O
sent4 O
= O
> O
sent2 O
; O
Pred O
: O
sent4 O
- O
> O
sent1 O
; O
sent2 O
- O
> O
sent1 O
; O

G3 O
: O
Incorrect O
Conclusion O
Passage O
: O
sent1 O
: O
healthy O
lungs O
produce O
a O
natural O
antibiotic O
that O
protects O
them O
from O
infection O
by O
routinely O
killing O
harmful O
bacteria O
on O
airway O
surfaces O
. O

[ O
AND O
] O
people O
with O
cystic O
fibroses O
, O
however O
, O
are O
unable O
to O
fight O
off O
such O
bacteria O
, O
even O
though O
their O
lungs O
produce O
normal O
amounts O
of O
the O
antibiotic O
. O
sent3 O
: O
since O
the O
fluid O
on O
airway O
surfaces O
in O
the O
lungs O
of O
people O
with O
cystic O
fibrosis O
has O
an O
abnormally O
high O
salt O
concentration O
, O
scientists O
hypothesize O
that O
in O
high O
salt O
environments O
the O
antibiotic O
becomes O
ineffective O
at O
killing O
harmful O
bacteria O
. O
sent4 O
: O
the O
lungs O
of O
people O
who O
suffer O
from O
cystic O
fibrosis O
are O
unable O
to O
fight O
off O
harmful O
bacteria O
even O
when O
the O
salt O
concentration O
is O
reduced O
to O
levels O
typical O
of O
healthy O
lungs O
. O
Gold O
: O
sent3 O
- O
> O
sent1 O
; O
sent4 O
= O
> O
sent3 O
; O
Pred O
: O
sent4 O
- O
> O
sent3 O
; O
sent1 O
- O
> O
sent3 O
; O

G4 O
: O
Incorrect O
Inference O
Step O
Passage O
: O
sent1 O
: O
spokesperson O
: O
the O
major O
school O
lunch O
vendors O
recently O
agreed O
to O
stop O
selling O
high O
-calorie O
beverages O
in O
elementary O
and O
middle O
schools O
because O
studies O
show O
that O
children O
of O
ages O
7 O
to O
8 O
who O
substitute O
one O
low O
-calorie O
beverage O
for O
one O
high O
-calorie O
soft O
drink O
in O
their O
daily O
diets O
will O
, O
on O
average O
, O
weigh O
20 O
pounds O
less O
than O
they O
would O
have O
by O
the O
time O
they O
reach O
high O
school O
. O
sent2 O
: O
since O
only O
low O
-calorie O
beverages O
will O
be O
sold O
in O
schools O
, O
within O
six O
to O
eight O
years O
, O
we O
can O
expect O
to O
see O
a O
reduction O
in O
the O
percentage O
of O
overweight O
high O
-school O
children O
. O
sent3 O
: O
elementary O
and O
middle O
school O
students O
who O
used O
to O
buy O
high O
-calorie O
soft O
drinks O
at O
school O
will O
not O
bring O
them O
to O
school O
or O
drink O
extra O
high O
-calorie O
beverages O
at O
home O
as O
a O
substitute O
. O
Gold O
: O
sent3 O
- O
> O
sent2 O
; O
sent1 O
- O
> O
sent2 O
; O
Pred O
: O
sent3 O
- O
> O
sent2 O
; O

G5 O
: O
Other O
Structural O
Mismatch O
Passage O
: O
sent1 O
: O
employer O
: O
in O
the O
current O
economic O
climate O
, O
the O
best O
way O
to O
run O
a O
business O
is O
to O
pay O
employees O
the O
least O
amount O
possible O
to O
do O
the O
job O
. O
sent2 O
: O
the O
supply O
of O
labor O
is O
far O
outpacing O
demand O
since O
the O
number O
of O
college O
graduates O
increases O
every O
year O
and O
the O
average O
age O
of O
retirement O
is O
also O
increasing O
. O

[ O
AND O
] O
applicants O
will O
typically O
take O
the O
first O
job O
offer O
on O
the O
table O
, O
and O
any O
employee O
who O
demands O
a O
raise O
can O
be O
easily O
replaced O
from O
the O
labor O
pool O
. O
sent4 O
: O
even O
if O
the O
employee O
is O
unhappy O
, O
he O
or O
she O
will O
often O
remain O
on O
the O
job O
due O
to O
the O
competition O
in O
the O
job O
market O
. O

[ O
AND O
] O
keeping O
payroll O
costs O
low O
allows O
more O
resources O
to O
be O
devoted O
to O
innovation O
, O
delivering O
a O
higher O
quality O
product O
to O
customers O
. O
sent6 O
: O
automation O
is O
the O
leading O
cause O
for O
unemployment O
. O
Gold O
: O
sent4 O
- O
> O
sent1 O
; O
sent2 O
- O
> O
sent4 O
; O
Pred O
: O
sent6 O
= O
> O
sent1 O
; O
sent2 O
- O
> O
sent1 O
; O

Acknowledgements O

We O
appreciate O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O
We O
thank O
Dr. O
Su O
Wu O
for O
reviewing O
the O
annotation O
manual O
, O
thank O
Dr. O
Xingchi O
Su O
for O
double O
- O
checking O
the O
operator O
reduction O
, O
and O
thank O
Jianheng O
Tang O
, O
Zhicheng O
Yang O
, O
and O
Xinran O
Zhao O
for O
their O
constructive O
advice O
for O
the O
manuscript O
. O
This O
work O
was O
supported O
in O
part O
by O
National O
Key O
R O
& O
D O
Program O
of O
China O
under O
Grant O
No O
. O
2020AAA0109700 O
, O
National O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
under O
Grant O
No.61976233 O
, O
Guangdong O
Province O
Basic O
and O
Applied O
Basic O
Research O
( O
Regional O
Joint O
Fund O
- O
Key O
) O
Grant O
No.2019B1515120039 O
, O
Guangdong O
Outstanding O
Youth O
Fund O
( O
Grant O
No O
. O

2021B1515020061 O
) O
, O
Shenzhen O
Fundamental O
Research O
Program O
( O
Project O
No O
. O
JCYJ20190807154211365 O
) O
and O
CAAI O
- O
Huawei O
MindSpore O
Open O
Fund O
. O
We O
thank O
MindSpore O
for O
the O
partial O
support O
of O
this O
work O
, O
which O
is O
a O
new O
deep O
learning O
computing O
framework O
6 O
. O

F1 O
: O
Incorrect O
Logical O
Variable O

Sentence O
: O
v1 O
: O
legislators O
considering O
a O
proposed O
law O
for O
which O
they O
have O
v2 O
: O
repugnance O
or O
v3 O
: O
enthusiasm O
v4 O
: O
do O
not O
consider O
the O
consequences O
that O
it O
will O
actually O
have O
. O
Gold O
: O
v3 O
[ O
or O
] O
v2 O
; O
Pred O
: O
v2 O
[ O
or O
] O
v3 O
; O
v4 O
[ O
entail O
] O
v1 O
; O
F2 O
: O
Incorrect O
Unary O
Operator O
Sentence O
: O
v1 O
: O
auditor O
: O
xyz O
, O
v2 O
: O
a O
construction O
company O
, O
purchased O
20 O
new O
trucks O
3 O
years O
ago O
, O
and O
v3 O
: O
there O
is O
no O
record O
of O
any O
of O
those O
trucks O
being O
sold O
last O
year O
. O
Gold O
: O
v2 O
[ O
and O
] O
v3 O
; O
Pred O
: O
v1 O
[ O
and O
] O
v3 O
; O
F3 O
: O
Incorrect O
Binary O
Operator O
Sentence O
: O
v1 O
: O
travaillier O
corporation O
has O
recently O
hired O
employees O
with O
experience O
in O
the O
bus O
tour O
industry O
v2 O
: O
its O
executives O
have O
also O
been O
negotiating O
with O
charter O
bus O
companies O
that O
subcontract O
with O
bus O
tour O
companies O
. O
[ O
AND O
] O
but O
v3 O
: O
travaillier O
has O
traditionally O
focused O
on O
serving O
consumers O
who O
travel O
primarily O
by O
air O
, O
and O
v4 O
: O
marketing O
surveys O
show O
that O
travaillier O
' O
s O
traditional O
consumers O
have O
not O
changed O
their O
vacation O
preferences O
. O
Gold O
: O
v1 O
[ O
and O
] O
v2 O
; O
v3 O
[ O
and O
] O
v4 O
; O
Pred O
: O
v3 O
[ O
and O
] O
v4 O
; O
v2 O
[ O
entail O
] O
v1 O
; O
F4 O
: O
Incorrect O
Implication O
Direction O
Sentence O
: O
v1 O
: O
now O
some O
politicians O
are O
saying O
that O
, O
in O
order O
to O
v2 O
: O
cause O
another O
similarly O
sized O
increase O
in O
exports O
, O
v3 O
: O
the O
government O
should O
allow O
the O
pundra O
to O
become O
weak O
again O
. O
Gold O
: O
v3 O
[ O
entail O
] O
v2 O
; O
Pred O
: O
v2 O
[ O
entail O
] O
v3 O
; O
Table O
18 O
: O
Error O
cases O
for O
the O
formulae O
. O

C1 O
: O
Incorrect O
Polarity O

Sentence O
: O
the O
chemistry O
department O
's O
funding O
for O
basic O
science O
research O
is O
not O
likely O
to O
increase O
if O
its O
funding O
from O
sources O
other O
than O
profit O
-driven O
institutions O
does O
not O
increase O
. O
Gold O
: O
impossible O
Pred O
: O
possible O
C2 O
: O
Other O
Polarities O
to O
Contingent O
Sentence O
: O
if O
legislators O
are O
to O
enact O
laws O
that O
benefit O
constituents O
, O
they O
must O
be O
sure O
to O
consider O
what O
the O
consequences O
of O
enacting O
a O
proposed O
law O
will O
actually O
be O
. O
[ O
AND O
] O
concerned O
primarily O
with O
advancing O
their O
own O
political O
careers O
, O
legislators O
present O
legislation O
in O
polemical O
terms O
; O
this O
arouses O
in O
their O
colleagues O
either O
repugnance O
or O
enthusiasm O
for O
the O
legislation O
. O
Gold O
: O
necessary O
Pred O
: O
contingent O
C3 O
: O
Contingent O
to O
Other O
Polarities O
Sentence O
: O
making O
decisions O
about O
patterns O
of O
work O
organization O
, O
resource O
allocation O
, O
and O
location O
of O
industry O
is O
not O
the O
core O
of O
a O
public O
official O
's O
job O
. O
Gold O
: O
contingent O
Pred O
: O
unnecessary O
C4 O
: O
Unresolved O
Certainty O
Sentence O
: O
the O
link O
between O
jogging O
and O
certain O
structural O
disorders O
appears O
to O
be O
a O
causal O
one O
. O
Gold O
: O
contingent O
Pred O
: O
causal O

Bi B-MethodName
- I-MethodName
SimCut I-MethodName
: O
A O
Simple O
Strategy O
for O
Boosting O
Neural B-TaskName
Machine I-TaskName
Translation I-TaskName

We O
introduce O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
: O
a O
simple O
but O
effective O
training O
strategy O
to O
boost O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
NMT B-TaskName
) O
performance O
. O
It O
consists O
of O
two O
procedures O
: O
bidirectional O
pretraining O
and O
unidirectional O
finetuning O
. O
Both O
procedures O
utilize O
SimCut B-MethodName
, O
a O
simple O
regularization O
method O
that O
forces O
the O
consistency O
between O
the O
output O
distributions O
of O
the O
original O
and O
the O
cutoff O
sentence O
pairs O
. O
Without O
leveraging O
extra O
dataset O
via O
back O
- O
translation O
or O
integrating O
large O
- O
scale O
pretrained O
model O
, O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
achieves O
strong O
translation O
performance O
across O
five O
translation O
benchmarks O
( O
data O
sizes O
range O
from O
160 O
K O
to O
20.2 O
M O
) O
: O
BLEU B-MetricName
scores O
of O
31.16 B-MetricValue
for O
en O
→ O
de O
and O
38.37 B-MetricValue
for O
de O
→ O
en O
on O
the O
IWSLT14 B-DatasetName
dataset O
, O
30.78 B-MetricValue
for O
en O
→ O
de O
and O
35.15 B-MetricValue
for O
de O
→ O
en O
on O
the O
WMT14 B-DatasetName
dataset O
, O
and O
27.17 B-MetricValue
for O
zh O
→ O
en O
on O
the O
WMT17 B-DatasetName
dataset O
. O
Sim B-MethodName
- I-MethodName
Cut I-MethodName
is O
not O
a O
new O
method O
, O
but O
a O
version O
of O
Cutoff B-MethodName
( O
Shen O
et O
al O
. O
, O
2020 O
) O
simplified O
and O
adapted O
for O
NMT B-TaskName
, O
and O
it O
could O
be O
considered O
as O
a O
perturbation O
- O
based O
method O
. O
Given O
the O
universality O
and O
simplicity O
of O
SimCut B-MethodName
and O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
, O
we O
believe O
they O
can O
serve O
as O
strong O
baselines O
for O
future O
NMT B-TaskName
research O
. O

Introduction O

The O
state O
of O
the O
art O
in O
machine O
translation O
has O
been O
dramatically O
improved O
over O
the O
past O
decade O
thanks O
to O
the O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
NMT B-TaskName
) O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
, O
and O
Transformer B-MethodName
- O
based O
models O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
often O
deliver O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
translation O
performance O
with O
large O
- O
scale O
corpora O
( O
Ott O
et O
al O
. O
, O
2018 O
) O
. O
Along O
with O
the O
development O
in O
the O
NMT B-TaskName
field O
, O
consistency O
training O
( O
Bachman O
et O
al O
. O
, O
2014 O
) O
has O
been O
widely O
adopted O
and O
shown O
great O
promise O
to O
improve O
NMT B-TaskName
performance O
. O
It O
simply O
regularizes O
the O
NMT B-TaskName
model O
predictions O
to O
be O
invariant O
to O
either O
small O
perturbations O
applied O
to O
the O
inputs O
( O
Sato O
et O
al O
. O
, O
2019 O
; O
Shen O
et O
al O
. O
, O
2020 O
) O
and O
hidden O
states O
or O
the O
model O
randomness O
and O
variance O
existed O
in O
the O
training O
procedure O
( O
Liang O
et O
al O
. O
, O
2021 O
) O
. O
Specifically O
, O
Shen O
et O
al O
. O
( O
2020 O
) O
introduce O
a O
set O
of O
cutoff O
data O
augmentation O
methods O
and O
utilize O
Jensen O
- O
Shannon O
( O
JS O
) O
divergence O
loss O
to O
force O
the O
consistency O
between O
the O
output O
distributions O
of O
the O
original O
and O
the O
cutoff O
augmented O
samples O
in O
the O
training O
procedure O
. O
Despite O
its O
impressive O
performance O
, O
finding O
the O
proper O
values O
for O
the O
four O
additional O
hyper O
- O
parameters O
introduced O
in O
cutoff O
augmentation O
seems O
to O
be O
tedious O
and O
time O
- O
consuming O
if O
there O
are O
limited O
resources O
available O
, O
which O
hinders O
its O
practical O
value O
in O
the O
NMT B-TaskName
field O
. O

In O
this O
paper O
, O
our O
main O
goal O
is O
to O
provide O
a O
simple O
, O
easy O
- O
to O
- O
reproduce O
, O
but O
tough O
- O
to O
- O
beat O
strategy O
for O
training O
NMT B-TaskName
models O
. O
Inspired O
by O
cutoff O
augmentation O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
and O
virtual O
adversarial O
regularization O
( O
Sato O
et O
al O
. O
, O
2019 O
) O
for O
NMT B-TaskName
, O
we O
firstly O
introduce O
a O
simple O
yet O
effective O
regularization O
method O
named O
SimCut B-MethodName
. O
Technically O
, O
SimCut B-MethodName
is O
not O
a O
new O
method O
and O
can O
be O
viewed O
as O
a O
simplified O
version O
of O
Token B-MethodName
Cutoff I-MethodName
proposed O
in O
Shen O
et O
al O
. O
( O
2020 O
) O
. O
We O
show O
that O
bidirectional O
backpropagation O
in O
Kullback O
- O
Leibler O
( O
KL O
) O
regularization O
plays O
a O
key O
role O
in O
improving O
NMT B-TaskName
performance O
. O
We O
also O
regard O
SimCut B-MethodName
as O
a O
perturbation O
- O
based O
method O
and O
discuss O
its O
robustness O
to O
the O
noisy O
inputs O
. O
At O
last O
, O
motivated O
by O
bidirectional O
training O
( O
Ding O
et O
al O
. O
, O
2021 O
) O
in O
NMT B-TaskName
, O
we O
present O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
, O
a O
two O
- O
stage O
training O
strategy O
consisting O
of O
bidirectional O
pretraining O
and O
unidirectional O
finetuning O
equipped O
with O
SimCut B-MethodName
regularization O
. O

The O
contributions O
of O
this O
paper O
can O
be O
summarized O
as O
follows O
: O

• O
We O
propose O
a O
simple O
but O
effective O
regularization O
method O
, O
SimCut B-MethodName
, O
for O
improving O
the O
generalization O
of O
NMT B-TaskName
models O
. O
SimCut B-MethodName
could O
be O
regarded O
as O
a O
perturbation O
- O
based O
method O
and O
serves O
as O
a O
strong O
baseline O
for O
the O
approaches O
of O
robustness O
. O
We O
also O
show O
the O
compatibility O
of O
SimCut B-MethodName
with O
the O
pretrained O
language O
models O
such O
as O
mBART B-MethodName
. O

• O
We O
propose O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
, O
a O
training O
strategy O
for O
NMT B-TaskName
that O
consists O
of O
bidirectional O
pretraining O
and O
unidirectional O
finetuning O
with O
SimCut B-MethodName
regularization O
. O

• O
Our O
experimental O
results O
show O
that O
NMT B-TaskName
training O
with O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
achieves O
significant O
improvements O
over O
the O
Transformer O
model O
on O
five O
translation O
benchmarks O
( O
data O
sizes O
range O
from O
160 O
K O
to O
20.2 O
M O
) O
, O
and O
outperforms O
the O
current O
SOTA O
method O
BiBERT B-MethodName
( O
Xu O
et O
al O
. O
, O
2021 O
) O
on O
several O
benchmarks O
. O

Background O

Neural B-TaskName
Machine I-TaskName
Translation I-TaskName

The O
NMT B-TaskName
model O
refers O
to O
a O
neural O
network O
with O
an O
encoder O
- O
decoder O
architecture O
, O
which O
receives O
a O
sentence O
as O
input O
and O
returns O
a O
corresponding O
translated O
sentence O
as O
output O
. O
Assume O
x O
= O
x O
1 O
, O
... O
, O
x O
I O
and O
y O
= O
y O
1 O
, O
... O
, O
y O
J O
that O
correspond O
to O
the O
source O
and O
target O
sentences O
with O
lengths O
I O
and O
J O
respectively O
. O
Note O
that O
y O
J O
denotes O
the O
special O
end O
- O
of O
- O
sentence O
symbol O
⟨eos⟩. O
The O
encoder O
first O
maps O
a O
source O
sentence O
x O
into O
a O
sequence O
of O
word O
embeddings O
e O
( O
x O
) O
= O
e O
( O
x O
1 O
) O
, O
... O
, O
e O
( O
x O
I O
) O
, O
where O
e O
( O
x O
) O
∈ O
R O
d×I O
, O
and O
d O
is O
the O
embedding O
dimension O
. O
The O
word O
embeddings O
are O
then O
encoded O
to O
the O
corresponding O
hidden O
representations O
h. O
Similarly O
, O
the O
decoder O
maps O
a O
shifted O
copy O
of O
the O
target O
sentence O
y O
, O
i.e. O
, O
⟨bos⟩ O
, O
y O
1 O
, O
... O
, O
y O
J−1 O
, O
into O
a O
sequence O
of O
word O
embeddings O
e O
( O
y O
) O
= O
e O
( O
⟨bos⟩ O
) O
, O
e O
( O
y O
1 O
) O
, O
... O
, O
e O
( O
y O
J−1 O
) O
, O
where O
⟨bos⟩ O
denotes O
a O
special O
beginning O
- O
of O
- O
sentence O
symbol O
, O
and O
e O
( O
y O
) O
∈ O
R O
d×J O
. O
The O
decoder O
then O
acts O
as O
a O
conditional O
language O
model O
that O
operates O
on O
the O
word O
embeddings O
e O
( O
y O
) O
and O
the O
hidden O
representations O
h O
learned O
by O
the O
encoder O
. O
Given O
a O
parallel O
corpus O
S O
= O
{ O
x O
i O
, O
y O
i O
} O
|S| O
i=1 O
, O
the O
standard O
training O
objective O
is O
to O
minimize O
the O
empirical O
risk O
: O

L O
ce O
( O
θ O
) O
= O
E O
( O
x O
, O
y O
) O
∈S O
[ O
ℓ O
( O
f O
( O
x O
, O
y O
; O
θ O
) O
, O
ÿ O
) O
] O
, O
( O
1 O
) O

where O
ℓ O
denotes O
the O
cross O
- O
entropy O
loss O
, O
θ O
is O
a O
set O
of O
model O
parameters O
, O
f O
( O
x O
, O
y O
; O
θ O
) O
is O
a O
sequence O
of O
probability O
predictions O
, O
i.e. O
, O

f O
j O
( O
x O
, O
y O
; O
θ O
) O
= O
P O
( O
y|x O
, O
y O
< O
j O
; O
θ O
) O
, O
( O
2 O
) O

andÿ O
is O
a O
sequence O
of O
one O
- O
hot O
label O
vectors O
for O
y O
. O

Cutoff O
Augmentation O

Shen O
et O
al O
. O
( O
2020 O
) O
introduce O
a O
set O
of O
cutoff O
methods O
which O
augments O
the O
training O
by O
creating O
the O
partial O
views O
of O
the O
original O
sentence O
pairs O
and O
propose O
Token B-MethodName
Cutoff I-MethodName
for O
the O
machine O
translation O
task O
. O
Given O
a O
sentence O
pair O
( O
x O
, O
y O
) O
, O
N O
cutoff O
samples O
{ O
x O
i O
cut O
, O
y O
i O
cut O
} O
N O
i=1 O
are O
constructed O
by O
randomly O
setting O
the O
word O
embeddings O
of O
x O
1 O
, O
... O
, O
x O
I O
and O
y O
1 O
, O
... O
, O
y O
J O
to O
be O
zero O
with O
a O
cutoff O
probability O
p O
cut O
. O
For O
each O
sentence O
pair O
, O
the O
training O
objective O
of O
Token B-MethodName
Cutoff I-MethodName
is O
then O
defined O
as O
: O

L O
tokcut O
( O
θ O
) O
= O
L O
ce O
( O
θ O
) O
+ O
αL O
cut O
( O
θ O
) O
+ O
βL O
kl O
( O
θ O
) O
, O
( O
3 O
) O

where O

L O
ce O
( O
θ O
) O
= O
ℓ O
( O
f O
( O
x O
, O
y O
; O
θ O
) O
, O
ÿ O
) O
, O
( O
4 O
) O

L O
cut O
( O
θ O
) O
= O
1 O
N O
N O
i=1 O
ℓ O
( O
f O
( O
x O
i O
cut O
, O
y O
i O
cut O
; O
θ O
) O
, O
ÿ O
) O
, O
( O
5 O
) O
L O
kl O
( O
θ O
) O
= O
1 O
N O
+ O
1 O
{ O
N O
i=1 O
KL O
( O
f O
( O
x O
i O
cut O
, O
y O
i O
cut O
; O
θ O
) O
∥p O
avg O
) O
+ O
KL O
( O
f O
( O
x O
, O
y O
; O
θ O
) O
∥p O
avg O
) O
} O
, O
( O
6 O
) O

p O
avg O
= O
1 O
N O
+ O
1 O
{ O
N O
i=1 O
f O
( O
x O
i O
cut O
, O
y O
i O
cut O
; O
θ O
) O
+ O
f O
( O
x O
, O
y O
; O
θ O
) O
} O
, O
( O
7 O
) O

in O
which O
KL O
( O
•∥• O
) O
denotes O
the O
Kullback O
- O
Leibler O
( O
KL O
) O
divergence O
of O
two O
distributions O
, O
and O
α B-HyperparameterName
and O
β B-HyperparameterName
are O
the O
scalar O
hyper O
- O
parameters O
that O
balance O
L O
ce O
( O
θ O
) O
, O
L O
cut O
( O
θ O
) O
and O
L O
kl O
( O
θ O
) O
. O

Datasets O
and O
Baseline O
Settings O

In O
this O
section O
, O
we O
describe O
the O
datasets O
used O
in O
experiments O
as O
well O
as O
the O
model O
configurations O
. O

For O
fair O
comparisons O
, O
we O
keep O
our O
experimental O
settings O
consistent O
with O
previous O
works O
. O
sentence O
pairs O
. O
Following O
the O
common O
practice O
, O
we O
lowercase O
all O
words O
in O
the O
dataset O
. O
We O
build O
a O
shared O
dictionary O
with O
10 O
K O
byte O
- O
pair O
- O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
types O
. O

Settings O
We O
implement O
our O
approach O
on O
top O
of O
the O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
We O
apply O
a O
Transformer B-MethodName
with O
6 B-HyperparameterValue
encoder B-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
4 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
embedding B-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
, O
and O
FFN B-HyperparameterName
layer O
dimension O
1024 B-HyperparameterValue
. O
We O
apply O
cross O
- O
entropy O
loss O
with O
label B-HyperparameterName
smoothing I-HyperparameterName
rate I-HyperparameterName
0.1 B-HyperparameterValue
and O
set O
max B-HyperparameterName
tokens I-HyperparameterName
per I-HyperparameterName
batch I-HyperparameterName
to O
be O
4096 B-HyperparameterValue
. O
We O
use O
Adam B-HyperparameterName
optimizer O
with O
Beta O
( O
0.9 B-HyperparameterValue
, O
0.98 B-HyperparameterValue
) O
, O
4000 B-HyperparameterValue
warmup B-HyperparameterName
updates I-HyperparameterName
, O
and O
inverse O
square O
root O
learning O
rate O
scheduler O
with O
initial O
learning B-HyperparameterName
rates I-HyperparameterName
5e B-HyperparameterValue
−4 I-HyperparameterValue
. O
We O
use O
dropout B-HyperparameterName
rate I-HyperparameterName
0.3 B-HyperparameterValue
and O
beam O
search O
decoding O
with O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
and O
length B-HyperparameterName
penalty I-HyperparameterName
1.0 B-HyperparameterValue
. O
We O
apply O
the O
same O
training O
configurations O
in O
both O
pretraining O
and O
finetuning O
stages O
which O
will O
be O
discussed O
in O
the O
following O
sections O
. O
We O
use O
multi-bleu.pl O
2 O
for O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
evaluation O
. O
We O
train O
all O
models O
until O
convergence O
on O
a O
single O
NVIDIA O
Tesla O
V100 O
GPU O
. O
All O
reported O
BLEU B-MetricName
scores O
are O
from O
a O
single O
model O
. O
For O
all O
the O
experiments O
below O
, O
we O
select O
the O
saved O
model O
state O
with O
the O
best O
validation O
performance O
. O

Bi B-MethodName
- I-MethodName
SimCut I-MethodName

In O
this O
section O
, O
we O
formally O
propose O
Bidirectional B-MethodName
Pretrain I-MethodName
and I-MethodName
Unidirectional I-MethodName
Finetune I-MethodName
with I-MethodName
Simple I-MethodName
Cutoff I-MethodName
Regularization I-MethodName
( O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
) O
, O
a O
simple O
but O
effective O
training O
strategy O
that O
can O
greatly O
enhance O
the O
generalization O
of O
the O
NMT B-TaskName
model O
. O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
consists O
of O
a O
simple O
cutoff O
regularization O
and O
a O
two O
- O
phase O
pretraining O
and O
finetuning O
strategy O
. O
We O
introduce O
the O
details O
of O
each O
part O
below O
. O

SimCut B-MethodName
: O
A O
Simple O
Cutoff O
Regularization O
for O
NMT B-TaskName

Despite O
the O
impressive O
performance O
reported O
in O
Shen O
et O
al O
. O
( O
2020 O
) O
, O
finding O
the O
proper O
hyperparameters O
( O
p B-HyperparameterName
cut I-HyperparameterName
, O
α B-HyperparameterName
, O
β B-HyperparameterName
, O
N B-HyperparameterName
) O
in O
Token B-MethodName
Cutoff I-MethodName
seems O
to O
be O
tedious O
and O
time O
- O
consuming O
if O
there O
are O
limited O
resources O
available O
, O
which O
hinders O
its O
practical O
value O
in O
the O
NMT B-TaskName
community O
. O
To O
reduce O
the O
burden O
in O
hyper O
- O
parameter O
searching O
, O
we O
propose O
SimCut B-MethodName
, O
a O
simple O
regularization O
method O
that O
forces O
the O
consistency O
between O
the O
output O
distributions O
of O
the O
original O
sentence O
pairs O
and O
the O
cutoff O
samples O
. O

Our O
problem O
formulation O
is O
motivated O
by O
Virtual B-MethodName
Adversarial I-MethodName
Training I-MethodName
( O
VAT B-MethodName
) O
, O
where O
Sato O
et O
al O
. O
( O
2019 O
) O
introduces O
a O
KL O
- O
based O
adversarial O
regularization O
that O
forces O
the O
output O
distribution O
of O
the O
samples O
with O
adversarial O
perturbations O
δ O
x O
∈ O
R O
d×I O
and O
δ O
y O
∈ O
R O
d×J O
to O
be O
consistent O
with O
that O
of O
the O
original O
samples O
: O
KL O
( O
f O
( O
e O
( O
x O
) O
, O
e O
( O
y O
) O
; O
θ O
) O
∥f O
( O
e O
( O
x O
) O
+ O
δ O
x O
, O
e O
( O
y O
) O
+ O
δ O
y O
; O
θ O
) O
) O
. O

Instead O
of O
generating O
perturbed O
samples O
by O
gradient O
- O
based O
adversarial O
methods O
, O
for O
each O
sentence O
pair O
( O
x O
, O
y O
) O
, O
we O
only O
generate O
one O
cutoff O
sample O
( O
x O
cut O
, O
y O
cut O
) O
by O
following O
the O
same O
cutoff O
strategy O
used O
in O
Token B-MethodName
Cutoff I-MethodName
. O
For O
each O
sentence O
pair O
, O
the O
training O
objective O
of O
SimCut B-MethodName
is O
defined O
as O
: O

L O
simcut O
( O
θ O
) O
= O
L O
ce O
( O
θ O
) O
+ O
αL O
simkl O
( O
θ O
) O
, O
( O
8 O
) O

where O

L O
simkl O
( O
θ O
) O
= O
KL O
( O
f O
( O
x O
, O
y O
; O
θ O
) O
∥f O
( O
x O
cut O
, O
y O
cut O
; O
θ O
) O
) O
. O

There O
are O
only O
two O
hyper O
- O
parameters O
α B-HyperparameterName
and O
p B-HyperparameterName
cut I-HyperparameterName
in O
SimCut B-MethodName
, O
which O
greatly O
simplifies O
the O
hyperparameter O
searching O
step O
in O
Token O
Cutoff O
. O
Note O
that O
VAT B-MethodName
only O
allows O
the O
gradient O
to O
be O
backpropagated O
through O
the O
right O
- O
hand O
side O
of O
the O
KL O
divergence O
term O
, O
while O
the O
gradient O
is O
designed O
to O
be O
backpropagated O
through O
both O
sides O
of O
the O
KL O
regularization O
in O
SimCut B-MethodName
. O
We O
can O
see O
that O
the O
constraints O
introduced O
by O
L O
cut O
( O
θ O
) O
and O
L O
kl O
( O
θ O
) O
in O
( O
3 O
) O
still O
implicitly O
hold O
in O
( O
8 O
) O
: O

• O
L O
cut O
( O
θ O
) O
in O
Token B-MethodName
Cutoff I-MethodName
is O
designed O
to O
guarantee O
that O
the O
output O
of O
the O
cutoff O
sample O
should O
close O
to O
the O
ground O
- O
truth O
to O
some O
extent O
. O
In O
SimCut B-MethodName
, O
L O
ce O
( O
θ O
) O
requires O
the O
outputs O
of O
the O
original O
sample O
close O
to O
the O
ground O
- O
truth O
, O
and O
L O
simkl O
( O
θ O
) O
requires O
the O
output O
distributions O
of O
the O
cutoff O
sample O
close O
to O
that O
of O
the O
original O
sample O
. O
The O
constraint O
introduced O
by O
L O
cut O
( O
θ O
) O
then O
implicitly O
holds O
. O

• O
L O
kl O
( O
θ O
) O
in O
Token B-MethodName
Cutoff I-MethodName
is O
designed O
to O
guarantee O
that O
the O
output O
distributions O
of O
the O
original O
sample O
and O
N O
different O
cutoff O
samples O
We O
here O
investigate O
whether O
our O
simplification O
on O
Token B-MethodName
Cutoff I-MethodName
hurts O
its O
performance O
on O
machine O
translation O
tasks O
. O
We O
compare O
SimCut B-MethodName
with O
Token B-MethodName
Cutoff I-MethodName
, O
VAT B-MethodName
, O
and O
R B-MethodName
- I-MethodName
Drop I-MethodName
( O
Liang O
et O
al O
. O
, O
2021 O
) O
, O
a O
strong O
regularization O
baseline O
that O
forces O
the O
output O
distributions O
of O
different O
sub O
- O
models O
generated O
by O
dropout O
to O
be O
consistent O
with O
each O
other O
. O
Even O
though O
the O
problem O
formulation O
of O
SimCut B-MethodName
is O
similar O
to O
that O
of O
VAT B-MethodName
, O
one O
key O
difference O
is O
that O
the O
gradients O
are O
allowed O
to O
be O
backpropagated O
bidirectionally O
in O
the O
KL O
regularization O
in O
SimCut B-MethodName
. O

We O
here O
investigate O
the O
impact O
of O
the O
bidirectional O
backpropagation O
in O
the O
regularization O
term O
on O
the O
NMT B-TaskName
performance O
. O
Table O
4 O
shows O
the O
translation O
results O
of O
VAT B-MethodName
and O
SimCut B-MethodName
with O
or O
without O
bidirectional O
backpropagation O
. O
We O
can O
see O
that O
both O
VAT B-MethodName
and O
SimCut B-MethodName
benefit O
from O
the O
bidirectional O
gradient O
backpropagation O
in O
the O
KL O
regularization O
. O

Performance O
on O
Perturbed O
Inputs O

Given O
the O
similar O
problem O
formulations O
of O
VAT B-MethodName
and O
SimCut B-MethodName
, O
it O
is O
natural O
to O
regard O
cutoff O
operation O
as O
a O
special O
perturbation O
and O
consider O
SimCut B-MethodName
as O
a O
perturbation O
- O
based O
method O
. O
We O
here O
investigate O
the O
robustness O
of O
NMT B-TaskName
models O
on O
the O
perturbed O
inputs O
. O
As O
discussed O
in O
Takase O
and O
Kiyono O
( O
2021 O
) O
, O
simple O
techniques O
such O
as O
word O
replacement O
and O
word O
drop O
can O
achieve O
comparable O
performance O
to O
sophisticated O
perturbations O
. O
We O
hence O
include O
them O
as O
baselines O
to O
show O
the O
effectiveness O
of O
our O
method O
as O
follows O
: O

• O
UniRep B-MethodName
: O
Word O
replacement O
approach O
constructs O
a O
new O
sequence O
whose O
tokens O
are O
randomly O
replaced O
with O
sampled O
tokens O
. O
For O
each O
token O
in O
the O
source O
sentence O
x O
, O
we O
samplex O
i O
uniformly O
from O
the O
source O
vocabulary O
, O
and O
use O
it O
for O
the O
new O
sequence O
x O
′ O
with O
probability O
1 O
− O
p O
′ O
: O

x O
′ O
i O
= O
x O
i O
, O
with O
probability O
p O
′ O
, O
x O
i O
, O
with O
probability O
1 O
− O
p O
′ O
. O
( O
9 O

We O
construct O
y O
′ O
from O
the O
target O
sentence O
y O
in O
the O
same O
manner O
. O
Following O
the O
curriculum O
learning O
strategy O
used O
in O
Bengio O
et O
al O
. O
( O
2015 O
) O
, O
we O
adjust O
p O
′ O
with O
the O
inverse O
sigmoid O
decay O
: O

p O
′ O
t O
= O
max O
( O
q O
, O
k O
k O
+ O
exp O
( O
t O
k O
) O

where O
q B-HyperparameterName
and O
k B-HyperparameterName
are O
hyper O
- O
parameters O
. O
ber O
t. O
We O
use O
p O
′ O
t O
as O
p O
′ O
in O
epoch O
t. O
We O
set O
q B-HyperparameterName
and O
k B-HyperparameterName
to O
be O
0.9 B-HyperparameterValue
and O
25 B-HyperparameterValue
respectively O
in O
the O
experiments O
. O

• O
WordDrop B-MethodName
: O
Word B-MethodName
drop I-MethodName
randomly O
applies O
the O
zero O
vector O
instead O
of O
the O
word O
embedding O
e O
( O
x O
i O
) O
or O
e O
( O
y O
i O
) O
for O
the O
input O
token O
x O
i O
or O
y O
i O
( O
Gal O
and O
Ghahramani O
, O
2016 O
) O
. O
For O
each O
token O
in O
both O
source O
and O
target O
sentences O
, O
we O
keep O
the O
original O
embedding O
with O
the O
probability O
β B-HyperparameterName
and O
set O
it O
to O
be O
the O
zero O
vector O
otherwise O
. O
We O
set O
β B-HyperparameterName
to O
be O
0.9 B-HyperparameterValue
in O
the O
experiments O
. O

We O
construct O
noisy O
inputs O
by O
randomly O
replacing O
words O
in O
the O
source O
sentences O
based O
on O
a O
predefined O
probability O
. O
If O
the O
probability O
is O
0.0 O
, O
we O
use O
the O
original O
source O
sentence O
. O
If O
the O
probabil O
- O
ity O
is O
1.0 O
, O
we O
use O
completely O
different O
sentences O
as O
source O
sentences O
. O
We O
set O
the O
probability O
to O
be O
0.00 O
, O
0.01 O
, O
0.05 O
, O
and O
0.10 O
in O
our O
experiments O
. O
We O
randomly O
replace O
each O
word O
in O
the O
source O
sentence O
with O
a O
word O
uniformly O
sampled O
from O
the O
vocabulary O
. O
We O
apply O
this O
procedure O
to O
IWSLT14 B-DatasetName
de→en O
test O
set O
. O
Table O
5 O
shows O
the O
BLEU B-MetricName
scores O
of O
each O
method O
on O
the O
perturbed O
test O
set O
. O
Note O
that O
the O
BLEU B-MetricName
scores O
are O
calculated O
against O
the O
original O
reference O
sentences O
. O
We O
can O
see O
that O
all O
methods O
improve O
the O
robustness O
of O
the O
NMT B-TaskName
model O
, O
and O
SimCut O
achieves O
the O
best O
performance O
among O
all O
the O
methods O
on O
both O
the O
clean O
and O
perturbed O
test O
sets O
. O
The O
performance O
results O
indicate O
that O
SimCut B-MethodName
could O
be O
considered O
as O
a O
strong O
baseline O
for O
the O
perturbation O
- O
based O
method O
for O
the O
NMT B-TaskName
model O
. O

As O
shown O
in O
Table O
6 O
, O
the O
baseline O
model O
completely O
ignores O
the O
translation O
of O
" O
in O
spielen O
( O
in O
games O
) O
" O
due O
to O
the O
replacement O
of O
" O
denken O
( O
think O
) O
" O
with O
" O
festgelegten O
( O
determined O
) O
" O
in O
the O
source O
sentence O
. O
In O
contrast O
, O
our O
model O
successfully O
captures O
the O
translation O
of O
" O
in O
spielen O
" O
under O
the O
noisy O
input O
. O
This O
result O
shows O
that O
our O
model O
is O
more O
robust O
to O
small O
perturbations O
in O
an O
authentic O
context O
. O

Effects O
of O
α B-HyperparameterName
and O
p B-HyperparameterName
cut O

We O
here O
investigate O
the O
impact O
of O
the O
scalar O
hyperparameters O
α B-HyperparameterName
and O
p B-HyperparameterName
cut I-HyperparameterName
in O
SimCut B-MethodName
. O
α B-HyperparameterName
is O
a O
penalty O
parameter O
that O
controls O
the O
regularization O
strength O
in O
our O
optimization O
problem O
. O
p B-HyperparameterName
cut I-HyperparameterName
controls O
the O
percentage O
of O
the O
cutoff O
perturbations O
in O
SimCut B-MethodName
. O
We O
here O
vary O
α B-HyperparameterName
and O
p B-HyperparameterName
cut O
in O
{ O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
} O
and O
{ O
0.00 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.10 B-HyperparameterValue
, O
0.15 B-HyperparameterValue
, O
0.20 B-HyperparameterValue
} O
respectively O
and O
conduct O
the O
experiments O
on O
the O
IWSLT14 B-DatasetName
de→en O
dataset O
. O
Note O
that O
SimCut B-MethodName
is O
simplified O
to O
R B-MethodName
- I-MethodName
Drop I-MethodName
approximately O
when O
p B-HyperparameterName
cut I-HyperparameterName
= O
0.00 B-HyperparameterValue
. O
The O
test O
BLEU B-MetricName
scores O
are O
reported O
in O
Figure O
1 O
. O
By O
checking O
model O
performance O
under O
different O
combinations O
of O
α B-HyperparameterName
and O
p B-HyperparameterName
cut I-HyperparameterName
, O
we O
have O
the O
following O
observations O
: O
1 O
) O
A O
too O
small O
α B-HyperparameterName
( O
e.g. O
, O
1 B-HyperparameterValue
) O
can O
not O
achieve O
as O
good O
performance O
as O
larger O
α B-HyperparameterName
( O
e.g. O
, O
3 B-HyperparameterValue
) O
, O
indicating O
a O
certain O
de O
- O
Input O
wir O
denken O
( O
festgelegten O
) O
, O
dass O
wir O
in O
der O
realität O
nicht O
so O
gut O
sind O
wie O
in O
spielen O
. O
Reference O
we O
feel O
that O
we O
are O
not O
as O
good O
in O
reality O
as O
we O
are O
in O
games O
. O
Vaswani O
et O
al O
. O
( O
2017 O
) O
on O
Input O
we O
think O
we O
're O
not O
as O
good O
in O
reality O
as O
we O
are O
in O
games O
. O

on O
Noisy O
Input O
we O
realized O
that O
we O
were O
n't O
as O
good O
as O
we O
were O
in O
real O
life O
. O
SimCut B-MethodName
on O
Input O
we O
think O
in O
reality O
, O
we O
're O
not O
as O
good O
as O
we O
do O
in O
games O
. O
on O
Noisy O
Input O
we O
realized O
that O
we O
're O
not O
as O
good O
in O
reality O
as O
we O
are O
in O
games O
. O

Table O
6 O
: O
SimCut B-MethodName
is O
more O
robust O
to O
small O
perturbations O
in O
an O
authentic O
context O
. O
SimCut B-MethodName
captures O
the O
translation O
of O
" O
in O
spielen O
" O
under O
the O
noisy O
input O
while O
the O
vanilla O
Transformer O
ignores O
the O
translation O
of O
" O
in O
spielen O
" O
due O
to O
the O
replacement O
of O
" O
denken O
" O
with O
" O
festgelegten O
" O
. O

gree O
of O
regularization O
strength O
during O
NMT B-TaskName
model O
training O
is O
conducive O
to O
generalization O
. O
Meanwhile O
, O
an O
overwhelming O
regularization O
( O
α B-HyperparameterName
= O
5 B-HyperparameterValue
) O
is O
not O
plausible O
for O
learning O
NMT B-TaskName
models O
. O
2 O
) O
When O
α B-HyperparameterName
= O
3 B-HyperparameterValue
, O
the O
best O
performance O
is O
achieved O
when O
p B-HyperparameterName
cut I-HyperparameterName
= O
0.05 B-HyperparameterValue
, O
and O
p B-HyperparameterName
cut I-HyperparameterName
= O
0.00 B-HyperparameterValue
performs O
suboptimal O
among O
all O
selected O
probabilities O
. O
Such O
an O
observation O
demonstrates O
that O
the O
cutoff O
perturbation O
in O
SimCut B-MethodName
can O
effectively O
promote O
the O
generalization O
compared O
with O
R B-MethodName
- I-MethodName
Drop I-MethodName
. O

Is O
SimCut B-MethodName
Compatible O
with O
the O
Pretrained O
Language O
Model O
? O

The O
multilingual O
sequence O
- O
to O
- O
sequence O
pretrained O
language O
models O
( O
Song O
et O
al O
. O
, O
2019 O
; O
Xue O
et O
al O
. O
, O
2021 O
) O
et O
al O
. O
, O
2020 O
) O
as O
the O
backbone O
model O
, O
which O
is O
a O
sequence O
- O
to O
- O
sequence O
denoising O
auto O
- O
encoder O
pretrained O
on O
CC25 B-DatasetName
Corpus I-DatasetName
3 I-DatasetName
. O
We O
conduct O
experiments O
on O
IWSLT14 B-DatasetName
de→en O
dataset O
and O
only O
remove O
the O
duplicated O
sentence O
pairs O
following O
mBART50 B-MethodName
( O
Tang O
et O
al O
. O
, O
2021 O
) O
in O
the O
data O
preprocessing O
step O
. O
The O
source O
and O
target O
sentences O
are O
jointly O
tokenized O
into O
sub O
- O
word O
units O
with O
the O
250 B-DatasetName
K I-DatasetName
Sentence I-DatasetName
- I-DatasetName
Piece I-DatasetName
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
vocabulary O
of O
mBART B-MethodName
. O
We O
use O
case O
- O
sensitive O
sacreBLEU B-MetricName
( O
Post O
, O
2018 O
) O
to O
evaluate O
the O
translation O
quality O
, O
and O
the O
methods O
applied O
in O
the O
experiments O
are O
as O
follows O
: O

• O
Transformer B-MethodName
: O
The O
Transformer O
model O
is O
randomly O
initialized O
and O
trained O
from O
scratch O
. O

We O
utilize O
the O
same O
model O
and O
training O
configurations O
discussed O
in O
Section O
3 O
. O

• O
mBART B-MethodName
: O
The O
Transformer B-MethodName
model O
is O
directly O
finetuned O
from O
mBART B-MethodName
. O
We O
utilize O
the O
default O
training O
configurations O
of O
mBART B-MethodName
. O

• O
mBART B-MethodName
with O
SimCut B-MethodName
: O
The O
Transformer O
model O
is O
finetuned O
from O
mBART B-MethodName
with O
SimCut B-MethodName
regularization O
. O
We O
utilize O
the O
default O
training O
configurations O
of O
mBART B-MethodName
. O

From O
which O
again O
shows O
the O
effectiveness O
and O
universality O
of O
our O
method O
. O

Training O
Strategy O
: O
Bidirectional O
Pretrain O
and O
Unidirectional O
Finetune O

Bidirectional O
pretraining O
is O
shown O
to O
be O
very O
effective O
to O
improve O
the O
translation O
performance O
of O
the O
unidirectional O
NMT B-TaskName
system O
( O
Ding O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2021 O
) O
. O
The O
main O
idea O
is O
to O
pretrain O
a O
bidirectional O
NMT B-TaskName
model O
at O
first O
and O
use O
it O
as O
the O
initialization O
to O
finetune O
a O
unidirectional O
NMT B-TaskName
model O
. O
Assume O
we O
want O
to O
train O
an O
NMT B-TaskName
model O
for O
" O
English→German O
" O
, O
we O
first O
reconstruct O
the O
training O
sentence O
pairs O
to O
" O
English+German→German+English O
" O
, O
where O
the O
training O
dataset O
is O
doubled O
. O
We O
then O
firstly O
train O
a O
bidirectional O
NMT B-TaskName
model O
with O
the O
new O
training O
sentence O
pairs O
: O

E O
( O
x O
, O
y O
) O
∈S O
[ O
ℓ O
( O
f O
( O
x O
, O
y O
; O
θ O
) O
, O
ÿ O
) O
+ O
ℓ O
( O
f O
( O
y O
, O
x O
; O
θ O
) O
, O
ẍ O
) O
] O
, O
( O
11 O
) O

and O
finetune O
the O
model O
with O
" O
English→German O
" O
direction O
. O
We O
follow O
the O
same O
training O
strategy O
in O
Ding O
et O
al O
. O
( O
2021 O
) O
and O
apply O
SimCut B-MethodName
regularization O
to O
both O
pretraining O
and O
finetuning O
procedures O
. O
Table O
8 O
shows O
that O
bidirectional O
pretraining O
and O
unidirectional O
finetuning O
strategy O
with O
SimCut B-MethodName
regularization O
could O
achieve O
superior O
performance O
compared O
with O
strong O
baseline O
such O
as O
R B-MethodName
- I-MethodName
Drop I-MethodName
. O

Comparison O
with O
Existing O
Methods O

We O
summarize O
the O
recent O
results O
of O
several O
existing O
works O
on O
IWSLT14 B-DatasetName
en↔de O
benchmark O
in O
Table O
9 O
. O
The O
existing O
methods O
vary O
from O
different O
aspects O
, O
including O
Virtual B-MethodName
Adversarial I-MethodName
Training I-MethodName
( O
Sato O
et O
al O
. O
, O
2019 O
) O
, O
Mixed O
Tokenization O
for O
NMT B-TaskName
, O
Unified O
Dropout O
for O
the O
Transformer B-MethodName
model O
2021 O
) O
, O
and O
BiBERT B-MethodName
( O
Xu O
et O
al O
. O
, O
2021 O
) O
. O
We O
can O
see O
that O
our O
approach O
achieves O
an O
improvement O
of O
2.92 B-MetricValue
BLEU B-MetricName
score O
over O
Vaswani O
et O
al O
. O
( O
2017 O
) O
and O
surpass O
the O
current O
SOTA O
method O
BiBERT B-MethodName
that O
incorporates O
large O
- O
scale O
pretrained O
model O
, O
stochastic O
layer O
selection O
, O
and O
bidirectional O
pretraining O
. O
Given O
the O
simplicity O
of O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
, O
we O
believe O
it O
could O
be O
considered O
as O
a O
strong O
baseline O
for O
the O
NMT B-TaskName
task O
. O

Standard O
Resource O
Scenario O

We O
here O
investigate O
the O
performance O
of O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
on O
the O
larger O
translation O
benchmark O
compared O
with O
the O
IWSLT14 B-DatasetName
benchmark O
. O

Dataset O
Description O
and O
Model O
Configuration O

For O
the O
standard O
resource O
scenario O
, O
we O
evaluate O
NMT B-TaskName
models O
on O
the O
WMT14 B-DatasetName
English O
- O
German O
dataset O
, O
which O
contains O
4.5 O
M O
parallel O
sentence O
pairs O
. O
We O
combine O
newstest2012 B-DatasetName
and O
newstest2013 B-DatasetName
as O
the O
validation O
set O
and O
use O
newstest2014 B-DatasetName
as O
the O
test O
set O
. O
We O
collect O
the O
pre O
- O
processed O
data O
from O
Xu O
et O
al O
. O
( O
2021 O
) O
's O
release O
4 O
, O
where O
a O
shared O
dictionary O
with O
52 O
K O
BPE O
types O
is O
built O
. O
We O
apply O
a O
standard O
Transformer O
Big O
model O
with O
6 B-HyperparameterValue
encoder B-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
16 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
embedding B-HyperparameterName
size I-HyperparameterName
1024 B-HyperparameterValue
, O
and O
FFN B-HyperparameterName
layer O
dimension O
4096 B-HyperparameterValue
. O
We O
apply O
cross O
- O
entropy O
loss O
with O
label B-HyperparameterName
smoothing I-HyperparameterName
rate I-HyperparameterName
0.1 B-HyperparameterValue
and O
set O
max B-HyperparameterName
tokens I-HyperparameterName
per I-HyperparameterName
batch I-HyperparameterName
to O
be O
4096 B-HyperparameterValue
. O
We O
use O
Adam B-HyperparameterName
optimizer O
with O
Beta O
( O
0.9 B-HyperparameterValue
, O
0.98 B-HyperparameterValue
) O
, O
4000 B-HyperparameterValue
warmup B-HyperparameterName
updates I-HyperparameterName
, O
and O
inverse O
square O
root O
learning O
rate O
scheduler O
with O
initial O
learning B-HyperparameterName
rates I-HyperparameterName
1e B-HyperparameterValue
−3 I-HyperparameterValue
. O
We O
decrease O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
5e B-HyperparameterValue
−4 I-HyperparameterValue
in O
the O
finetuning O
stage O
. O
We O
select O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
from O
0.3 B-HyperparameterValue
, O
0.2 B-HyperparameterValue
, O
and O
0.1 B-HyperparameterValue
based O
on O
the O
validation O
performance O
. O
We O
Method O
en→de O
de→en O
Average B-MethodName
Transformer I-MethodName
+ I-MethodName
Large I-MethodName
Batch I-MethodName
† O
( O
Ott O
et O
al O
. O
, O
2018 O
) O
29.30 O
-- O
Evolved B-MethodName
Transformer I-MethodName
† O
( O
So O
et O
al O
. O
, O
2019 O
) O
29.80 O
-- O
BERT B-MethodName
Initialization I-MethodName
( O
12 O
layers O
) O
† O
( O
Rothe O
et O
al O
. O
, O
2020 O
) O
30.60 O
33.60 O
32.10 O
BERT B-MethodName
- I-MethodName
Fuse I-MethodName
† O
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
30.75 O
-- O
R B-MethodName
- I-MethodName
Drop I-MethodName
( O
Liang O
et O
al O
. O
, O
2021 O
) O
30.13 O
34.54 O
32.34 O
BiBERT B-MethodName
† O
( O
Xu O
et O
al O
. O
, O
2021 O
) O
31 O
use O
beam O
search O
decoding O
with O
beam B-HyperparameterName
size I-HyperparameterName
4 B-HyperparameterValue
and O
length B-HyperparameterName
penalty I-HyperparameterName
0.6 B-HyperparameterValue
. O
We O
train O
all O
models O
until O
convergence O
on O
8 O
NVIDIA O
Tesla O
V100 O
GPUs O
. O
All O
reported O
BLEU B-MetricName
scores O
are O
from O
a O
single O
model O
. O

Results O

We O
report O
test O
BLEU B-MetricName
scores O
of O
all O
comparison O
methods O
and O
our O
approach O
on O
the O
WMT14 B-DatasetName
dataset O
in O
Table O
10 O
. O
With O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
bidirectional O
pretraining O
and O
unidirectional O
finetuning O
procedures O
, O
our O
NMT B-TaskName
model O
achieves O
strong O
or O
SOTA O
BLEU B-MetricName
scores O
on O
en→de O
and O
de→en O
translation O
benchmarks O
. O

During O
the O
NMT B-TaskName
training O
process O
, O
we O
fix O
p B-HyperparameterName
cut I-HyperparameterName
to O
be O
0.05 B-HyperparameterValue
and O
tune O
the O
hyper O
- O
parameter O
α B-HyperparameterName
in O
both O
R B-MethodName
- I-MethodName
Drop I-MethodName
and O
SimCut B-MethodName
based O
on O
the O
performance O
on O
the O
validation O
set O
. O
Note O
that O
the O
BLEU B-MetricName
scores O
of O
R B-MethodName
- I-MethodName
Drop I-MethodName
are O
lower O
than O
that O
reported O
in O
Liang O
et O
al O
. O
( O
2021 O
) O
. O
Such O
gap O
might O
be O
due O
to O
the O
different O
prepossessing O
steps O
used O
in O
Liang O
et O
al O
. O
( O
2021 O
) O
and O
Xu O
et O
al O
. O
( O
2021 O
) O
. O
It O
is O
worth O
mentioning O
that O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
outperforms O
BiBERT B-MethodName
on O
de→en O
direction O
even O
though O
BiBERT B-MethodName
incorporates O
bidirectional O
pretraining O
, O
large O
- O
scale O
pretrained O
contextualized O
embeddings O
, O
and O
stochastic O
layer O
selection O
mechanism O
. O

High O
Resource O
Scenario O

To O
investigate O
the O
performance O
of O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
on O
the O
distant O
language O
pairs O
which O
naturally O
do O
not O
share O
dictionaries O
, O
we O
here O
discuss O
the O
effectiveness O
of O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
on O
the O
Chinese O
- O
English O
translation O
task O
. O

Dataset O
Description O
and O
Model O
Configuration O

For O

Results O

We O
report O
test O
BLEU B-MetricName
scores O
of O
the O
baselines O
and O
our O
approach O
on O
the O
WMT17 B-DatasetName
dataset O
in O
Table O
11 O
. O
Note O
that O
share O
means O
the O
embedding O
matrices O
for O
encoder O
input O
, O
decoder O
input O
and O
decoder O
output O
are O
all O
shared O
. O
The O
NMT B-TaskName
models O
with O
separated O
dictionaries O
perform O
slightly O
better O
than O
those O
with O
the O
shared O
dictionary O
. O
We O
can O
see O
that O
our O
approach O
significantly O
improves O
the O
translation O
performance O
. O
In O
particular O
, O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
achieves O
more O
than O
1.6 B-MetricValue
BLEU B-MetricName
score O
improvement O
over O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
showing O
the O
effectiveness O
and O
univer O
- O
sality O
of O
our O
approach O
on O
the O
distant O
language O
pair O
in O
the O
NMT B-TaskName
task O
. O

Related O
Work O

Adversarial O
Perturbation O
It O
is O
well O
known O
that O
neural O
networks O
are O
sensitive O
to O
noisy O
inputs O
, O
and O
adversarial O
perturbations O
are O
firstly O
discussed O
in O
the O
filed O
of O
image O
processing O
( O
Szegedy O
et O
al O
. O
, O
2014 O
; O
Goodfellow O
et O
al O
. O
, O
2015 O
) O
. O
SimCut B-MethodName
could O
be O
regarded O
as O
a O
perturbation O
- O
based O
method O
for O
the O
robustness O
research O
. O
In O
the O
field O
of O
natural O
language O
processing O
, O
Miyato O
et O
al O
. O
( O
2017 O
) O
Consistency O
Training O
Besides O
perturbationbased O
methods O
, O
our O
approach O
also O
highly O
relates O
to O
a O
few O
works O
of O
model O
- O
level O
and O
data O
- O
level O
consistency O
training O
in O
the O
NMT B-TaskName
field O
. O
Among O
them O
, O
the O
most O
representative O
methods O
are O
R B-MethodName
- I-MethodName
Drop I-MethodName
( O
Liang O
et O
al O
. O
, O
2021 O
) O
and O
Cutoff B-MethodName
( O
Shen O
et O
al O
. O
, O
2020 O
) O
. O
R B-MethodName
- I-MethodName
Drop I-MethodName
studies O
the O
intrinsic O
randomness O
in O
the O
NMT B-TaskName
model O
and O
regularizes O
the O
NMT B-TaskName
model O
by O
utilizing O
the O
output O
consistency O
between O
two O
dropout O
sub O
- O
models O
with O
the O
same O
inputs O
. O
Cutoff B-MethodName
considers O
consistency O
training O
from O
a O
data O
perspective O
by O
regularizing O
the O
inconsistency O
between O
the O
original O
sentence O
pair O
and O
the O
augmented O
samples O
with O
part O
of O
the O
information O
within O
the O
input O
sentence O
pair O
being O
dropped O
. O
Note O
that O
Cutoff B-MethodName
takes O
the O
dropout O
submodels O
into O
account O
during O
the O
training O
procedure O
as O
well O
. O
We O
want O
to O
emphasize O
that O
SimCut B-MethodName
is O
not O
a O
new O
method O
, O
but O
a O
version O
of O
Cutoff B-MethodName
simplified O
and O
adapted O
for O
the O
NMT B-TaskName
tasks O
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
: O
a O
simple O
but O
effective O
two O
- O
stage O
training O
strategy O
to O
improve O
NMT B-TaskName
performance O
. O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
consists O
of O
bidirectional O
pretraining O
and O
unidirectional O
finetuning O
procedures O
equipped O
with O
SimCut B-MethodName
regularization O
for O
improving O
the O
generality O
of O
the O
NMT B-TaskName
model O
. O
Experiments O
on O
low O
( O
IWSLT14 B-DatasetName
en↔de O
) O
, O
standard O
( O
WMT14 B-DatasetName
en↔de O
) O
, O
and O
high O
( O
WMT17 B-DatasetName
zh→en O
) O
resource O
translation O
benchmarks O
demonstrate O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
and O
SimCut B-MethodName
's O
capabilities O
to O
improve O
translation O
performance O
and O
robustness O
. O
Given O
the O
universality O
and O
simplicity O
of O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
and O
Sim B-MethodName
- I-MethodName
Cut I-MethodName
, O
we O
believe O
: O
1 O
) O
SimCut B-MethodName
could O
be O
regarded O
as O
a O
perturbation O
- O
based O
method O
, O
and O
it O
could O
be O
used O
as O
a O
strong O
baseline O
for O
the O
robustness O
research O
. O
2 O
) O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
outperforms O
many O
complicated O
methods O
which O
incorporate O
large O
- O
scaled O
pretrained O
models O
or O
sophisticated O
mechanisms O
, O
and O
it O
could O
be O
used O
as O
a O
strong O
baseline O
for O
future O
NMT B-TaskName
research O
. O
We O
hope O
researchers O
of O
perturbations O
and O
NMT B-TaskName
could O
use O
SimCut B-MethodName
and O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
as O
strong O
baselines O
to O
make O
the O
usefulness O
and O
effectiveness O
of O
their O
proposed O
methods O
clear O
. O
For O
future O
work O
, O
we O
will O
explore O
the O
effectiveness O
of O
SimCut B-MethodName
and O
Bi B-MethodName
- I-MethodName
SimCut I-MethodName
on O
more O
sequence O
learning O
tasks O
, O
such O
as O
multilingual O
machine O
translation O
, O
domain O
adaptation O
, O
text O
classification O
, O
natural O
language O
understanding O
, O
etc O
. O

Acknowledgements O

We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O

Adversarial B-MethodName
Text I-MethodName
Normalization I-MethodName

Text B-TaskName
- I-TaskName
based I-TaskName
adversarial I-TaskName
attacks I-TaskName
are O
becoming O
more O
commonplace O
and O
accessible O
to O
general O
internet O
users O
. O
As O
these O
attacks O
proliferate O
, O
the O
need O
to O
address O
the O
gap O
in O
model O
robustness O
becomes O
imminent O
. O
While O
retraining O
on O
adversarial O
data O
may O
increase O
performance O
, O
there O
remains O
an O
additional O
class O
of O
character O
- O
level O
attacks O
on O
which O
these O
models O
falter O
. O
Additionally O
, O
the O
process O
to O
retrain O
a O
model O
is O
time O
and O
resource O
intensive O
, O
creating O
a O
need O
for O
a O
lightweight O
, O
reusable O
defense O
. O
In O
this O
work O
, O
we O
propose O
the O
Adversarial B-MethodName
Text I-MethodName
Normalizer I-MethodName
, O
a O
novel O
method O
that O
restores O
baseline O
performance O
on O
attacked O
content O
with O
low O
computational O
overhead O
. O
We O
evaluate O
the O
efficacy O
of O
the O
normalizer O
on O
two O
problem O
areas O
prone O
to O
adversarial O
attacks O
, O
i.e. O
Hate O
Speech O
and O
Natural O
Language O
Inference O
. O
We O
find O
that O
text O
normalization O
provides O
a O
task O
- O
agnostic O
defense O
against O
character O
- O
level O
attacks O
that O
can O
be O
implemented O
supplementary O
to O
adversarial O
retraining O
solutions O
, O
which O
are O
more O
suited O
for O
semantic O
alterations O
. O

Introduction O

Natural O
language O
processing O
( O
NLP O
) O
models O
help O
preserve O
the O
integrity O
of O
discourse O
in O
online O
social O
networks O
by O
detecting O
hate O
speech O
, O
misinformation O
, O
and O
other O
content O
that O
violates O
community O
policies O
( O
Halevy O
, O
2020 O
) O
. O
In O
these O
application O
scenarios O
, O
classifiers O
operate O
under O
significantly O
more O
adversarial O
conditions O
than O
in O
the O
standard O
paradigm O
of O
model O
development O
. O
Users O
often O
post O
content O
that O
is O
heavily O
altered O
in O
order O
to O
induce O
worst O
- O
case O
errors O
, O
dramatically O
reducing O
model O
performance O
relative O
to O
a O
standard O
test O
set O
. O
Recent O
research O
in O
machine O
learning O
has O
made O
strides O
towards O
building O
robust O
models O
to O
defend O
against O
sophisticated O
adversaries O
. O
Nevertheless O
, O
our O
experience O
and O
experiments O
show O
that O
models O
remain O
vulnerable O
to O
many O
simple O
and O
intuitive O
attacks O
. O

One O
such O
class O
of O
highly O
- O
effective O
attacks O
are O
The O
Worst O
( O
LFTW B-DatasetName
) O
test O
set O
for O
several O
augmentation O
types O
. O
The O
model O
scores O
in O
this O
graph O
are O
the O
averaged O
scores O
across O
all O
five O
LFTW B-DatasetName
models O
. O
To O
see O
the O
raw O
evaluation O
results O
, O
view O
Table O
4 O
in O
the O
Appendix O
. O

syntactic O
attacks O
, O
which O
include O
adding O
punctuation O
and O
spacing O
, O
replacing O
fonts O
, O
and O
inserting O
zero O
- O
width O
characters O
. O
Attackers O
commonly O
employ O
these O
methods O
with O
little O
or O
no O
expert O
knowledge O
of O
machine O
learning O
algorithms O
, O
and O
constitute O
a O
large O
proportion O
of O
practical O
threats O
to O
natural O
language O
processing O
models O
deployed O
in O
industry O
. O

In O
our O
experiments O
on O
state O
- O
of O
- O
the O
- O
art O
robust O
NLP O
models O
, O
such O
attacks O
can O
decrease O
a O
model O
's O
performance O
by O
more O
than O
half O
. O
Thus O
, O
these O
text O
- O
based O
attacks O
remain O
a O
serious O
and O
unaddressed O
problem O
for O
the O
at O
- O
scale O
deployment O
of O
language O
models O
in O
integrity O
areas O
. O

The O
literature O
has O
proposed O
methods O
to O
improve O
adversarial O
robustness O
by O
retraining O
models O
on O
" O
hard O
" O
and O
adversarial O
examples O
( O
Nie O
et O
al O
. O
, O
2020 O
; O
but O
we O
observe O
several O
open O
challenges O
with O
applying O
these O
approaches O
. O
First O
, O
as O
our O
experiments O
demonstrate O
, O
even O
state O
- O
of O
- O
theart O
models O
trained O
on O
multiple O
iterations O
of O
adversarial O
data O
continue O
to O
lack O
robustness O
to O
easily O
accessible O
syntactic O
attacks O
. O
For O
example O
, O
in O
Figure O
2 O
, O
inserting O
zero O
- O
width O
characters O
reduces O
a O
hate O
speech O
classification O
model O
's O
accuracy O
to O
less O
than O
15 O
% O
. O
Second O
, O
these O
approaches O
are O
not O
amenable O
to O
quick O
iteration O
. O
Whenever O
a O
new O
adversarial O
attack O
becomes O
more O
prevalent O
, O
a O
model O
developer O
would O
need O
to O
collect O
new O
data O
or O
create O
synthetic O
examples O
of O
such O
attacks O
, O
retrain O
the O
model O
, O
and O
redeploy O
it O
. O
This O
costs O
time O
, O
computational O
resources O
and O
has O
an O
environmental O
impact O
. O

To O
address O
these O
outstanding O
issues O
, O
we O
propose O
a O
novel O
lightweight O
and O
easily O
extensible O
method O
for O
recovering O
model O
performance O
in O
the O
face O
of O
adversaries O
who O
perform O
syntactic O
attacks O
. O
Our O
key O
insight O
is O
that O
many O
adversarial O
modifications O
can O
be O
undone O
before O
they O
even O
reach O
the O
model O
with O
little O
computational O
overhead O
. O
From O
a O
machine O
learning O
perspective O
, O
this O
can O
be O
thought O
of O
as O
restoring O
the O
distribution O
of O
inference O
- O
time O
inputs O
to O
the O
original O
distribution O
on O
which O
the O
model O
was O
trained O
. O
From O
a O
computer O
security O
perspective O
, O
this O
method O
is O
analogous O
to O
sanitizing O
the O
inputs O
to O
programs O
so O
as O
to O
make O
the O
input O
safe O
for O
further O
processing O
. O
In O
this O
work O
, O
we O
present O
the O
design O
and O
implementation O
of O
a O
system O
called O
the O
Adversarial B-MethodName
Text I-MethodName
Normalizer I-MethodName
( O
ATN B-MethodName
) O
, O
that O
achieves O
this O
goal O
. O
This O
iterative O
approach O
produces O
a O
defense O
mechanism O
that O
can O
be O
applied O
at O
scale O
in O
a O
lightweight O
fashion O
to O
ensure O
robust O
model O
performance O
. O

An O
important O
principle O
in O
computer O
security O
is O
that O
defense O
mechanisms O
should O
be O
evaluated O
against O
adaptive O
adversaries O
( O
Petitcolas O
, O
2011 O
) O
, O
i.e. O
those O
that O
can O
adjust O
their O
techniques O
to O
actions O
by O
the O
defender O
. O
Therefore O
, O
we O
also O
partner O
with O
red O
teaming O
experts O
skilled O
at O
creating O
novel O
adversarial O
inputs O
to O
classical O
computer O
systems O
to O
conduct O
an O
adaptive O
evaluation O
of O
the O
ATN B-MethodName
. O
Our O
text O
normalizer O
can O
provide O
sufficient O
robustness O
gains O
even O
in O
the O
face O
of O
such O
adaptive O
adversaries O
. O

Our O
contributions O
are O
as O
follows O
: O

• O
We O
design O
and O
implement O
a O
system O
for O
undoing O
syntactic O
attacks O
on O
textual O
models O
called O
the O
Adversarial B-MethodName
Text I-MethodName
Normalizer I-MethodName
. O

• O
We O
conduct O
an O
adaptive O
attacker O
red O
teaming O
exercise O
to O
evaluate O
the O
ATN B-MethodName
's O
performance O
against O
skilled O
human O
adversaries O
. O

• O
Through O
extensive O
experiments O
on O
three O
different O
benchmarks O
, O
we O
evaluate O
the O
performance O
of O
the O
ATN B-MethodName
and O
conclude O
that O
it O
successfully O
recovers O
the O
original O
performance O
of O
a O
model O
when O
faced O
with O
syntactic O
attacks O
. O

Related O
Work O

Several O
papers O
have O
introduced O
benchmarks O
for O
adversarial O
attacks O
on O
NLP O
systems O
. O
Attacks O
focused O
on O
preserving O
semantic O
content O
and O
grammar O
( O
Jin O
et O
al O
. O
, O
2020 O
; O
Alzantot O
et O
al O
. O
, O
2018 O
; O
Iyyer O
et O
al O
. O
, O
2018 O
) O
have O
been O
shown O
to O
be O
effective O
against O
state O
- O
ofthe O
- O
art O
models O
at O
the O
cost O
of O
requiring O
a O
greater O
understanding O
of O
the O
sentence O
structure O
and O
task O
context O
. O
In O
contrast O
, O
Eger O
and O
Benz O
( O
2020 O
) O
propose O
a O
benchmark O
of O
character O
level O
, O
orthographic O
perturbations O
as O
more O
realistic O
attacks O
in O
general O
applications O
, O
attributing O
the O
success O
of O
their O
high O
performance O
attacks O
to O
large O
out O
- O
of O
- O
vocabulary O
rates O
and O
disruption O
to O
tokenization O
. O
Other O
work O
( O
Eger O
et O
al O
. O
, O
2019 O
; O
Boucher O
et O
al O
. O
, O
2021 O
) O
investigates O
the O
replacement O
of O
characters O
with O
visually O
similar O
embedding O
spaces O
and O
the O
insertion O
of O
zero O
- O
width O
characters O
, O
noting O
the O
effectiveness O
of O
those O
methods O
against O
NLP O
models O
but O
marginal O
effect O
on O
human O
legibility O
-especially O
when O
perturbing O
key O
offensive O
words O
. O
For O
such O
targeted O
attacks O
, O
( O
Rodriguez O
and O
Rojas O
- O
Galeano O
, O
2018 O
) O
use O
a O
simple O
string O
matching O
algorithm O
to O
filter O
obfuscated O
and O
negated O
key O
words O
( O
Rojas O
- O
Galeano O
, O
2017 O
) O
, O
focusing O
on O
a O
limited O
list O
of O
target O
vocables O
on O
each O
pass O
. O
Our O
work O
focuses O
on O
the O
implementation O
of O
text O
normalization O
as O
a O
computationally O
inexpensive O
and O
reusable O
solution O
to O
mitigate O
a O
range O
of O
highly O
accessible O
but O
effective O
adversarial O
text O
attacks O
such O
as O
character O
insertions O
, O
replacements O
, O
and O
censorship O
. O
Concurrent O
work O
in O
the O
NLI B-TaskName
domain O
has O
addressed O
the O
bias O
in O
model O
performance O
on O
classic O
test O
sets O
and O
adversarial O
user O
attacks O
through O
iterative O
human O
- O
and O
- O
model O
- O
in O
- O
the O
- O
loop O
( O
Nie O
et O
al O
. O
, O
2020 O
) O
data O
generation O
and O
model O
training O
. O
Similarly O
, O
proposed O
a O
complementary O
approach O
with O
the O
amalgamation O
of O
targeted O
annotator O
samples O
including O
challenging O
perturbations O
to O
generate O
adversarial O
data O
for O
hate O
speech O
classification O
. O
Both O
works O
explored O
leveraging O
domain O
- O
experienced O
annotator O
resources O
to O
progressively O
train O
more O
robust O
models O
with O
each O
successive O
iteration O
. O
Other O
works O
on O
small O
text O
pertur O
- O
bations O
such O
as O
adversarial O
typos O
, O
have O
proposed O
the O
use O
of O
robust O
token O
- O
level O
encodings O
( O
Jones O
et O
al O
. O
, O
2020 O
) O
and O
preceding O
word O
recognition O
models O
( O
Pruthi O
et O
al O
. O
, O
2019 O
) O
as O
reusable O
systems O
that O
are O
trained O
once O
and O
then O
reused O
across O
models O
and O
tasks O
. O
In O
this O
work O
we O
explore O
a O
more O
lightweight O
, O
systematic O
correction O
layer O
that O
does O
not O
require O
training O
to O
create O
model O
and O
task O
- O
agnostic O
defenses O
. O

Methodology O

Models O
and O
Datasets O

We O
identify O
two O
natural O
language O
tasks O
with O
significant O
importance O
to O
industrial O
applications O
and O
adversarial O
pressure O
. O
First O
, O
hate B-TaskName
speech I-TaskName
classification I-TaskName
is O
the O
problem O
of O
detecting O
statements O
that O
are O
likely O
to O
cause O
harm O
and O
inject O
toxicity O
in O
online O
discourse O
. O
It O
has O
now O
become O
standard O
practice O
for O
providers O
of O
services O
where O
people O
can O
post O
comments O
and O
discuss O
content O
to O
employ O
hate O
speech O
classification O
models O
. O
These O
models O
are O
set O
up O
as O
binary O
classification O
models O
that O
output O
a O
score O
for O
the O
" O
hatefulness O
" O
of O
a O
given O
input O
statement O
. O
Second O
, O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( O
NLI B-TaskName
) O
has O
been O
adapted O
for O
the O
detection O
of O
misinformation O
( O
Nie O
et O
al O
. O
, O
2020 O
) O
. O
In O
this O
setting O
, O
NLI B-TaskName
models O
aim O
to O
flag O
statements O
that O
do O
not O
receive O
support O
from O
reputable O
sources O
or O
directly O
contradict O
information O
in O
them O
. O
Thus O
, O
a O
model O
is O
given O
access O
to O
a O
set O
of O
support O
statements O
and O
a O
" O
hypothesis O
" O
and O
it O
outputs O
a O
3 O
- O
way O
classification O
from O
among O
" O
supported O
, O
" O
" O
not O
supported O
, O
" O
and O
" O
not O
enough O
information O
. O
" O
In O
the O
cases O
of O
" O
supported O
" O
or O
" O
not O
supported O
, O
" O
the O
model O
also O
outputs O
the O
statement O
that O
supports O
or O
refutes O
the O
hypothesis O
. O

Since O
both O
tasks O
are O
the O
subject O
of O
adversarial O
pressure O
, O
there O
have O
been O
several O
proposed O
approaches O
to O
robustifying O
models O
trained O
on O
them O
. O
Most O
notably O
, O
the O
Dynalab O
approach O
proactively O
samples O
" O
hard O
" O
examples O
by O
asking O
human B-MethodName
raters I-MethodName
to O
conceive O
inputs O
that O
challenge O
the O
model O
. O
Researchers O
then O
retrain O
the O
models O
and O
repeat O
the O
process O
for O
several O
rounds O
. O
This O
paradigm O
helps O
achieve O
a O
large O
increase O
in O
robustness O
through O
the O
rounds O
, O
so O
we O
evaluate O
our O
approaches O
on O
those O
models O
as O
the O
benchmark O
for O
state O
- O
of O
- O
the O
- O
art O
robust O
performance O
. O

For O
Hate O
Speech O
, O
we O
utilize O
the O
Hate B-DatasetName
- I-DatasetName
Check I-DatasetName
( O
Röttger O
et O
al O
. O
, O
2021 O
) O
trained O
on O
adversarial O
data O
from O
Learning B-DatasetName
from I-DatasetName
the I-DatasetName
Worst I-DatasetName
( O
LFTW B-DatasetName
) O
were O
compared O
with O
and O
without O
the O
addition O
of O
the O
text O
normalizer O
on O
both O
baseline O
and O
augmented O
versions O
of O
the O
dataset O
. O
Additionally O
, O
we O
chose O
to O
evaluate O
NLI B-TaskName
models O
on O
the O
test O
sets O
from O
all O
three O
rounds O
of O
Adversarial B-DatasetName
NLI I-DatasetName
( O
Nie O
et O
al O
. O
, O
2020 O
) O
( O
1,000 O
, O
1,000 O
, O
and O
1,200 O
examples O
respectively O
) O
. O
We O
assessed O
the O
performance O
of O
these O
baseline O
, O
augmented O
, O
and O
normalized O
datasets O
on O
five O
model O
architectures O
trained O
on O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
FEVER B-DatasetName
( O
Thorne O
et O
al O
. O
, O
2018 O
) O
and O
all O
three O
rounds O
of O
Adversarial B-DatasetName
NLI I-DatasetName
. O
To O
see O
which O
models O
we O
evaluate O
on O
, O
please O
review O
Table O
1 O
. O
We O
specifically O
choose O
models O
already O
trained O
on O
adversarial O
datasets O
to O
assess O
opportunities O
for O
improvement O
beyond O
retraining O
. O

Attacks O

To O
attack O
the O
aforementioned O
datasets O
, O
we O
use O
the O
open O
- O
source O
augmentation O
library O
AugLy O
( O
Papakipos O
and O
Bitton O
, O
2022 O
) O
to O
simulate O
various O
textbased O
adversarial O
attacks O
commonly O
used O
on O
social O
media O
platforms O
. O
We O
focus O
on O
using O
character O
- O
level O
attacks O
as O
opposed O
to O
attacks O
that O
can O
potentially O
add O
, O
remove O
, O
or O
change O
full O
words O
in O
the O
text O
, O
to O
avoid O
perturbations O
in O
the O
semantic O
meaning O
. O
The O
specific O
augmentations O
selected O
for O
the O
analysis O
are O
listed O
in O
Table O
2 O
. O

In O
addition O
to O
the O
synthetically O
generated O
attacks O
, O
we O
collected O
98 O
samples O
of O
hate O
speech O
text O
which O
were O
adversarially O
created O
and O
modified O
by O
individuals O
in O
a O
cybersecurity O
Red O
Team O
. O
In O
the O
first O
half O
of O
the O
session O
, O
participants O
were O
tasked O
with O
creating O
their O
own O
attacks O
based O
off O
of O
their O
prior O
knowledge O
of O
text O
- O
based O
attacks O
seen O
online O
. O
In O
the O
second O
half O
of O
the O
session O
, O
they O
were O
given O
direct O
access O
to O
the O
code O
implementation O
for O
the O
text O
nor- O
( O
Papakipos O
and O
Bitton O
, O
2022 O
) O
, O
leveraged O
in O
the O
analysis O
for O
adversarial O
attacks O
on O
the O
text O
datasets O
, O
and O
their O
normalized O
counterparts O
. O
Note O
that O
while O
the O
output O
of O
insert_zero_width_chars O
appears O
visually O
identical O
to O
the O
original O
sentence O
, O
there O
are O
actually O
zero O
- O
width O
unicode O
characters O
embedded O
throughout O
the O
entire O
string O
. O
We O
include O
augmentations O
that O
are O
not O
covered O
by O
the O
normalizer O
( O
such O
as O
merge_words O
) O
in O
our O
evaluations O
to O
showcase O
that O
our O
method O
does O
not O
further O
corrupt O
unknown O
attack O
types O
. O

malizer O
and O
were O
tasked O
to O
bypass O
it O
using O
targeted O
attacks O
. O

Many O
of O
the O
attacks O
created O
were O
similar O
to O
the O
attacks O
of O
insert_punctuation_chars O
, O
replace_fun_fonts O
, O
simulate_typos O
, O
replace_similar_unicode_chars O
, O
and O
replace_similar_chars O
. O
The O
adversaries O
also O
created O
letter O
repetition O
attacks O
, O
i.e. O
" O
hellllooooo O
" O
, O
censored O
violating O
text O
, O
and O
replaced O
words O
with O
emojis O
. O

Adversarial B-MethodName
Text I-MethodName
Normalizer I-MethodName

The O
text O
normalizer O
is O
an O
isolated O
correction O
unit O
that O
can O
be O
placed O
in O
front O
of O
models O
to O
target O
character O
- O
level O
attacks O
for O
various O
NLP O
tasks O
. O
The O
normalizer O
is O
designed O
to O
be O
used O
as O
a O
preprocessing O
step O
prior O
to O
text O
tokenization O
. O
For O
optimal O
computational O
efficiency O
, O
the O
operator O
is O
written O
in O
torchscript O
, O
and O
can O
process O
approximately O
77 O
examples O
per O
second O
on O
a O
server O
with O
an O
Intel O
( O
R O
) O
Xeon O
( O
R O
) O
CPU O
E5 O
- O
2680 O
v4 O
@ O
2.40GHz O
processor O
. O
This O
operator O
was O
incorporated O
into O
the O
PyTorch O
model O
as O
a O
customizable O
data O
transform O
. O
The O
algorithm O
relies O
on O
sophisticated O
string O
manipulation O
as O
a O
targeted O
defense O
against O
known O
adversarial O
attacks O
, O
and O
is O
easily O
scaled O
up O
to O
support O
additional O
attacks O
as O
the O
adversarial O
environment O
progresses O
. O

Currently O
, O
the O
text O
normalizer O
supports O
removing O
three O
overarching O
categories O
of O
text O
attacks O
: O

1 O
. O
Character O
insertion O
: O
addition O
of O
characters O
such O
as O
punctuation O
marks O
, O
whitespaces O
, O
Unicode O
characters O
, O
emojis O
, O
and O
more O
to O
separate O
characters O
in O
a O
word O
with O
the O
intent O
to O
disrupt O
proper O
tokenization O
. O
This O
category O
includes O
augmentation O
methods O
such O
as O
insert_punctuation_chars O
, O
insert_whitespace_chars O
, O
and O
insert_zero_width_chars O
. O

2 O
. O

Character O
replacement O
: O
substitution O
of O
standard O
Latin O
characters O
with O
visually O
similar O
characters O
from O
other O
languages O
or O
Unicode O
characters O
with O
the O
intent O
of O
obfuscation O
. O
This O
category O
includes O
augmentation O
methods O
such O
as O
replace_fun_fonts O
, O
replace_similar_chars O
, O
and O
replace_similar_unicode_chars O
. O

Censorship O
of O
violating O
words O
: O
replacement O

of O
letters O
in O
violating O
words O
with O
punctuation O
characters O
to O
avoid O
explicit O
content O
. O
For O
instance O
" O
kill O
" O
could O
be O
censored O
as O
" O
k O
* O
* O
* O
" O
, O
" O
k O
! O
ll O
" O
, O
" O
k O
# O
* O
! O
" O
and O
more O
. O

To O
undo O
the O
effects O
of O
a O
character O
insertion O
attack O
such O
as O
insert_punctuation_chars O
, O
the O
text O
is O
first O
split O
by O
whitespaces O
to O
identify O
' O
words O
' O
. O
For O
each O
word O
, O
we O
then O
determine O
how O
many O
extraneous O
punctuation O
characters O
have O
been O
inserted O
, O
ignoring O
punctuation O
marks O
at O
the O
beginning O
and O
end O
, O
as O
such O
additions O
do O
not O
segment O
the O
word O
to O
disrupt O
tokenization O
. O
If O
the O
amount O
of O
punctuation O
characters O
is O
below O
a O
set O
threshold O
or O
the O
word O
resembles O
a O
URL O
, O
we O
do O
not O
modify O
the O
word O
and O
add O
it O
to O
our O
normalized O
string O
. O
Otherwise O
, O
we O
replace O
the O
superfluous O
punctuation O
with O
spaces O
, O
strip O
the O
string O
of O
excessive O
whitespace O
, O
and O
concatenate O
consecutive O
single O
character O
entities O
together O
. O

For O
character O
replacement O
attacks O
, O
we O
predefined O
multiple O
mappings O
between O
Unicode O
characters O
and O
their O
keyboard O
character O
pairs O
, O
and O
performed O
a O
string O
search O
method O
to O
reverse O
the O
replacement O
. O

As O
for O
censorship O
attacks O
, O
a O
list O
of O
common O
usercensored O
toxic O
terms O
were O
identified O
prior O
based O
on O
flagged O
user O
content O
. O
For O
each O
toxic O
term O
, O
a O
regex O
for O
the O
censorship O
pattern O
was O
defined O
such O
that O
the O
first O
and O
last O
letters O
of O
the O
word O
remain O
constant O
but O
any O
of O
the O
letters O
in O
between O
can O
be O
replaced O
by O
punctuation O
characters O
-maintaining O
the O
same O
length O
as O
the O
original O
, O
uncensored O
word O
. O
For O
every O
match O
found O
, O
we O
replace O
the O
censored O
string O
with O
its O
uncensored O
pair O
accordingly O
. O

Evaluation O

In O
this O
section O
, O
we O
examine O
the O
performance O
of O
Hate O
Speech O
and O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( O
NLI B-TaskName
) O
models O
with O
respect O
to O
the O
original O
, O
the O
augmented O
, O
and O
normalized O
datasets O
to O
evaluate O
the O
efficacy O
of O
the O
text O
normalizer O
. O
All O
evaluations O
were O
conducted O
using O
Dynabench O
, O
in O
which O
AWS O
ECR O
models O
are O
deployed O
as O
endpoints O
and O
Batch O
Transform O
jobs O
are O
run O
on O
AWS O
Sagemaker O
to O
get O
dataset O
predictions O
. O
We O
roughly O
spent O
38.36 O
CPU O
hours O
on O
model O
inference O
( O
no O
GPUs O
were O
used O
) O
. O

Hate O
Speech O

To O
assess O
performance O
on O
adversarial O
hate O
speech O
data O
, O
we O
evaluated O
on O
five O
models O
from O
the O
Learning B-DatasetName
from I-DatasetName
the I-DatasetName
Worst I-DatasetName
( O
LFTW B-DatasetName
) O
paper O
that O
were O
previously O
retrained O
on O
varying O
amounts O
of O
" O
rounds O
" O
of O
adversarial O
hate O
speech O
data O
collection O
. O

Learning B-DatasetName
from I-DatasetName
the I-DatasetName
Worst I-DatasetName

We O
first O
evaluate O
on O
the O
test O
set O
from O
Learning O
from O
the O
Worst O
. O
Figure O
1 O
showcases O
these O
results O
. O
Overall O
, O
the O
text O
normalizer O
maintains O
or O
improves O
initial O
performance O
on O
all O
augmented O
datasets O
and O
the O
baseline O
. O
Across O
all O
models O
, O
normalizing O
the O
insert_zero_width_chars O
, O
replace_similar_unicode_chars O
, O
and O
replace_fun_fonts O
augmentations O
resulted O
in O
the O
most O
significant O
performance B-MetricName
gains O
, O
with O
a O
maximum O
of O
a O
32.18 B-MetricValue
% I-MetricValue
increase O
. O
In O
between O
, O
normalizing O
insert_punctuation_chars O
and O
insert_whitespace_chars O
had O
increases O
of O
at O
most O
11.95 B-MetricValue
% I-MetricValue
. O
For O
the O
LFTW O
R3 O
model O
, O
normalizing O
the O
whitespace O
text O
resulted O
in O
a O
1.27 B-MetricValue
% I-MetricValue
loss O
in O
performance B-MetricName
. O
This O
may O
be O
due O
to O
the O
fact O
that O
not O
every O
whitespace O
character O
in O
the O
AugLy O
augmentation O
is O
removed O
by O
the O
normalizer O
. O
As O
expected O
, O
other O
augmentations O
that O
are O
n't O
covered O
by O
the O
normalizer O
did O
not O
see O
any O
substantial O
gains O
or O
losses O
in O
performance O
. O
To O
view O
the O
raw O
model O
scores O
, O
see O
Table O
4 O
in O
the O
Appendix O
. O

HateCheck B-DatasetName

In O
addition O
to O
evaluating O
on O
the O
LFTW B-DatasetName
test O
set O
, O
we O
also O
evaluated O
on O
an O
out O
- O
of O
- O
distribution O
dataset O
, O
HateCheck B-DatasetName
. O
Figure O
2 O
displays O
these O
results O
. O
The O
trends O
observed O
in O
the O
LFTW B-DatasetName
test O
set O
overall O
have O
agreement O
with O
the O
HateCheck B-DatasetName
results O
. O
However O
, O
in O
this O
case O
, O
there O
were O
no O
losses O
in O
performance O
for O
normalized O
augmentations O
covered O
by O
our O
method O
. O
The O
largest O
performance B-MetricName
gain O
was O
48.89 B-MetricValue
% I-MetricValue
by O
normalizing O
insert_zero_width_unicode_chars O
, O
and O
the O
smallest O
performance B-MetricName
gain O
was O
6.1 B-MetricValue
% I-MetricValue
by O
normalizing O
insert_punctuation_chars O
. O
To O
see O
the O
raw O
evaluation O
results O
, O
please O
review O
Table O
5 O
in O
the O
Appendix O
. O
6 O
, O
7 O
, O
and O
8 O
in O
the O
Appendix O
. O

Red O
Team O
Attacks O

Lastly O
, O
we O
evaluated O
the O
LFTW B-DatasetName
models O
on O
the O
Red B-DatasetName
Team I-DatasetName
dataset O
tasked O
to O
bypass O
the O
text O
normalizer O
. O
The O
baseline O
scores O
across O
all O
models O
averaged O
at O
39.52 B-MetricValue
% I-MetricValue
and O
the O
normalized O
scores O
averaged O
at O
41.32 B-MetricValue
% I-MetricValue
. O
In O
comparison O
to O
the O
synthetically O
- O
augmented O
text O
, O
applying O
the O
normalizer O
resulted O
in O
a O
less O
drastic O
increase O
in O
performance B-MetricName
. O
This O
difference O
can O
be O
attributed O
to O
the O
fact O
that O
a O
significant O
amount O
of O
the O
data O
was O
attacked O
similarly O
to O
replace_similar_chars O
and O
simulate_typos O
, O
two O
augmentations O
not O
covered O
by O
our O
defenses O
. O
To O
view O
the O
raw O
model O
scores O
, O
see O
Table O
3 O
in O
the O
Appendix O
. O

Natural B-TaskName
Language I-TaskName
Inference I-TaskName

To O
validate O
our O
results O
in O
another O
problem O
space O
, O
we O
evaluated O
on O
five O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( O
NLI B-TaskName
) O
models O
previously O
trained O
on O
a O
collection O
of O
adversarial O
and O
benign O
NLI B-TaskName
datasets O
. O
We O
evaluated O
the O
models O
on O
the O
baseline O
, O
augmented O
, O
and O
normalized O
ANLI B-DatasetName
test O
sets O
from O
rounds O
1 O
- O
3 O
. O

Overall O
, O
the O
results O
from O
the O
ANLI B-DatasetName
experiments O
align O
with O
the O
previous O
insights O
discussed O
in O
the O
Hate B-DatasetName
Speech I-DatasetName
task O
. O
The O
replace_fun_fonts O
, O
insert_zero_width_chars O
, O
and O
replace_similar_unicode_chars O
attacks O
were O
the O
most O
performant O
, O
followed O
by O
insert_punctuation_chars O
, O
insert_whitespace_chars O
, O
and O
finally O
the O
non O
- O
covered O
attack O
types O
. O

The O
largest O
gain O
, O
29.1 B-MetricValue
% I-MetricValue
, O
was O
made O
by O
normalizing O
insert_zero_width_unicode_chars O
in O
the O
ANLI B-DatasetName
R1 I-DatasetName
test O
set O
. O
To O
view O
the O
raw O
model O
scores O
, O
see O
Tables O
6 O
, O
7 O
, O
and O
8 O
in O
the O
Appendix O
. O

Discussions O

Retraining O
models O
on O
adversarial O
data O
has O
been O
proposed O
as O
a O
mitigation O
method O
for O
adversarial O
attacks O
Nie O
et O
al O
. O
, O
2020 O
) O
. O
Its O
benefits O
are O
that O
it O
's O
relatively O
simple O
to O
implement O
, O
shown O
to O
be O
effective O
( O
Goodfellow O
et O
al O
. O
, O
2015 O
) O
, O
and O
does O
n't O
affect O
model O
throughput O
once O
deployed O
. O
However O
, O
if O
a O
new O
adversarial O
attack O
were O
to O
be O
discovered O
on O
a O
system O
, O
the O
turnaround O
time O
for O
deploying O
a O
robust O
model O
can O
be O
quite O
long O
. O
A O
developer O
would O
need O
to O
( O
1 O
) O
gather O
and O
annotate O
or O
systematically O
generate O
the O
attacked O
data O
( O
2 O
) O
retrain O
the O
model O
( O
3 O
) O
redeploy O
the O
model O
. O
Steps O
( O
1 O
) O
and O
( O
2 O
) O
could O
be O
highly O
nontrivial O
depending O
on O
the O
attack O
type O
and O
model O
size O
. O
In O
addition O
, O
there O
can O
also O
be O
a O
significant O
monetary O
and O
environmental O
cost O
in O
retraining O
a O
large O
model O
. O

On O
the O
other O
hand O
, O
text O
normalization O
allows O
a O
developer O
to O
move O
fast O
. O
Instead O
of O
collecting O
data O
, O
an O
engineer O
would O
simply O
need O
to O
write O
one O
additional O
function O
to O
reverse O
said O
attack O
and O
test O
it O
. O
In O
addition O
, O
it O
's O
lightweight O
and O
there O
is O
no O
need O
to O
retrain O
, O
as O
this O
is O
a O
text O
preprocessing O
step O
. O
However O
, O
text O
normalization O
can O
not O
handle O
every O
attack O
type O
. O
Text O
normalization O
is O
best O
suited O
to O
mitigate O
character O
- O
level O
attacks O
that O
do O
not O
change O
the O
semantic O
meaning O
of O
the O
text O
, O
i.e. O
syntactic O
attacks O
. O

Another O
important O
consideration O
is O
that O
text O
normalization O
does O
affect O
real O
- O
time O
performance O
. O
A O
balance O
must O
be O
found O
between O
intelligently O
mitigating O
attacks O
and O
compute O
. O
In O
our O
use O
case O
, O
we O
limited O
the O
normalizer O
to O
sophisticated O
string O
manipulation O
, O
as O
anything O
more O
would O
result O
in O
too O
much O
compute O
. O
Hence O
, O
it O
becomes O
less O
feasible O
to O
build O
defenses O
that O
require O
knowledge O
or O
understanding O
of O
words O
, O
etc O
. O

Thus O
, O
we O
recommend O
the O
best O
way O
to O
mitigate O
adversarial O
attacks O
is O
to O
use O
a O
combination O
of O
text O
normalization O
and O
retraining O
. O
Specifically O
, O
retraining O
should O
be O
used O
for O
semantic O
adversarial O
attacks O
, O
and O
adversarial O
text O
normalization O
for O
syntactic O
ad O
- O
versarial O
attacks O
. O
Despite O
all O
the O
models O
evaluated O
on O
being O
retrained O
on O
adversarial O
data O
, O
they O
were O
still O
vulnerable O
to O
character O
- O
level O
text O
attacks O
. O
However O
, O
together O
with O
the O
text O
normalizer O
, O
we O
were O
able O
to O
increase O
performance O
and O
even O
return O
to O
baseline O
performance O
at O
times O
. O
This O
study O
is O
meant O
to O
show O
that O
text O
normalization O
in O
general O
is O
a O
viable O
approach O
to O
mitigating O
syntactic O
text O
- O
based O
attacks O
. O
This O
is O
certainly O
not O
the O
final O
method O
, O
and O
we O
hope O
researchers O
extend O
this O
work O
to O
support O
multilingual O
text O
. O

Conclusion O

We O
proposed O
a O
new O
method O
to O
mitigate O
text O
- O
based O
adversarial O
attacks O
, O
called O
the O
Adversarial B-MethodName
Text I-MethodName
Normalizer I-MethodName
. O
We O
evaluated O
the O
performance O
of O
models O
retrained O
on O
adversarial O
data O
with O
and O
without O
the O
ATN B-MethodName
. O
Our O
experiments O
show O
that O
text O
normalization O
and O
retraining O
should O
be O
used O
together O
in O
order O
to O
maintain O
baseline O
performance O
against O
a O
broad O
range O
of O
adversaries O
. O

Impact O
Statement O

Risks O
from O
the O
Work O

Our O
work O
is O
meant O
to O
reduce O
risk O
to O
online O
discourse O
by O
enabling O
NLP O
models O
to O
function O
robustly O
in O
an O
adversarial O
setting O
. O
We O
acknowledge O
that O
releasing O
this O
work O
might O
enable O
stronger O
attacks O
against O
some O
systems O
, O
but O
we O
believe O
it O
is O
important O
to O
discuss O
these O
defenses O
publicly O
for O
two O
reasons O
. O
First O
, O
other O
practitioners O
and O
researchers O
can O
benefit O
from O
our O
findings O
in O
building O
stronger O
defenses O
. O
Second O
, O
the O
history O
of O
security O
and O
cryptographic O
research O
clearly O
demonstrates O
that O
robust O
systems O
are O
built O
only O
when O
they O
go O
through O
ongoing O
attack O
/ O
defend O
iteration O
cycles O
. O
To O
that O
end O
, O
we O
hope O
that O
our O
work O
informs O
the O
next O
steps O
in O
building O
robust O
NLP O
systems O
. O
As O
with O
any O
NLP O
system O
and O
computer O
technology O
, O
we O
acknowledge O
robustness O
may O
be O
at O
odds O
with O
safety O
when O
the O
models O
defended O
with O
our O
mechanism O
are O
themselves O
used O
for O
nefarious O
purposes O
. O

Use O
of O
Scientific O
Artifacts O

In O
this O
work O
, O
we O
made O
heavy O
use O
of O
the O
Dynabench O
platform O
and O
the O
models O
trained O
on O
data O
collected O
with O
it O
. O
We O
worked O
closely O
with O
the O
creators O
of O
the O
platform O
and O
the O
models O
, O
and O
they O
were O
always O
fully O
aware O
of O
our O
intentions O
. O
The O
Dynabench O
platform O
has O
been O
released O
under O
the O
MIT O
license O
: O
https O
: O
/ O
/ O
github.com O
/ O
facebookresearch O
/ O
dynabench O
/ O
blob O
/ O
main O
/ O
LICENSE O
. O

We O
also O
used O
the O
Adversarial B-DatasetName
NLI I-DatasetName
, O
HateCheck B-DatasetName
, O
and O
Learning B-DatasetName
from I-DatasetName
the I-DatasetName
Worst I-DatasetName
datasets O
. O
Each O
of O
those O
do O
not O
contain O
identifying O
information O
and O
only O
associate O
a O
pseudonymous O
annotator O
ID O
with O
each O
example O
. O
We O
verified O
this O
manually O
by O
looking O
at O
samples O
from O
the O
datasets O
. O
The O
HateCheck B-DatasetName
and O
Learning B-DatasetName
from I-DatasetName
the I-DatasetName
Worst I-DatasetName
datasets O
contain O
offensive O
language O
as O
part O
of O
the O
nature O
of O
the O
task O
they O
were O
set O
up O
for O
. O
All O
datasets O
are O
in O
English O
and O
were O
created O
by O
English O
- O
speaking O
authors O
and O
annotators O
in O
the O
United O
States O
. O

Humans O
Involved O
in O
the O
Research O

We O
did O
not O
employ O
annotators O
or O
perform O
human O
subject O
experiments O
. O
We O
engaged O
with O
a O
partner O
team O
in O
a O
collegial O
capacity O
to O
expand O
our O
insights O
into O
the O
adversarial O
text O
normalizer O
and O
we O
discuss O
those O
findings O
here O
for O
the O
benefit O
of O
the O
broader O
research O
community O
. O

Acknowledgements O

The O
authors O
would O
like O
to O
thank O
Adina O
Williams O
, O
Tristan O
Thrush O
, O
and O
Kushal O
Tirumala O
for O
their O
support O
in O
running O
experiments O
with O
the O
Dynabench O
platform O
. O
The O
authors O
would O
also O
like O
to O
thank O
Luke O
Zettlemoyer O
, O
Caner O
Hazirbas O
, O
and O
Stefan O
Hermanek O
for O
helpful O
discussions O
on O
the O
work O
in O
this O
paper O
. O
Finally O
, O
the O
authors O
extend O
their O
gratitude O
to O
Aaron O
Grattafiori O
, O
Tom O
Ravenscroft O
, O
Adi O
Ajit O
, O
Aimee O
Couglin O
, O
Athena O
Cheung O
, O
Ben O
Actis O
, O
Christopher O
Korban O
, O
Greg O
Prosser O
, O
Hamza O
Kwisaba O
, O
James O
Burton O
, O
Jayson O
Grace O
, O
Martin O
Vigo O
, O
Nat O
Hirsch O
, O
Ryan O
Hall O
, O
Sasha O
Hanganu O
, O
Tom O
Hebb O
, O
and O
Vlad O
Ionescu O
for O
participating O
in O
the O
evaluation O
of O
the O
Adversarial B-MethodName
Text I-MethodName
Normalizer I-MethodName
. O

Appendix O
A O
AugLy O
Augmentation O
Generation O

To O
augment O
the O
hate O
speech O
and O
natural O
language O
inference O
datasets O
, O
we O
chose O
random O
parameters O
for O
each O
augmentation O
for O
every O
piece O
of O
text O
. O
The O
ranges O
we O
used O
for O
every O
parameter O
are O
listed O
below O
. O
Since O
multiple O
AugLy O
augmentations O
accept O
as O
input O
the O
same O
parameters O
, O
we O
do O
not O
break O
this O
down O
by O
augmentation O
type O
( O
the O
same O
value O
was O
used O
across O
all O
augmentations O
) O
. O
The O
following O
ranges O
were O
inputted O
into O
random.uniform O
( O
) O
and O
lists O
of O
options O
were O
inputted O
random.choice O
( O
) O
: O

• O
aug_p O
: O
( O
0.3 O
, O
1.0 O
) O

• O
aug_word_p O
: O
( O
0.3 O
, O
1.0 O
) O

• O
aug_char_p O
: O
( O
0.1 O
, O
0.4 O
) O

• O
granularity O
: O
[ O
" O
char O
" O
, O
" O
word O
" O
, O
" O
all O
" O
] O

Learning O
Natural O
Language O
Generation O
with O
Truncated B-MethodName
Reinforcement I-MethodName
Learning I-MethodName

Learning O
for O
Language O
( O
TrufLL B-MethodName
) O
, O
an O
original O
approach O
to O
train O
conditional O
language O
models O
without O
a O
supervised O
learning O
phase O
, O
by O
only O
using O
reinforcement O
learning O
( O
RL O
) O
. O
As O
RL O
methods O
unsuccessfully O
scale O
to O
large O
action O
spaces O
, O
we O
dynamically O
truncate O
the O
vocabulary O
space O
using O
a O
generic O
language O
model O
. O
TrufLL B-MethodName
thus O
enables O
to O
train O
a O
language O
agent O
by O
solely O
interacting O
with O
its O
environment O
without O
any O
task O
- O
specific O
prior O
knowledge O
; O
it O
is O
only O
guided O
with O
a O
task O
- O
agnostic O
language O
model O
. O
Interestingly O
, O
this O
approach O
avoids O
the O
dependency O
to O
labelled O
datasets O
and O
inherently O
reduces O
pretrained O
policy O
flaws O
such O
as O
language O
or O
exposure O
biases O
. O
We O
evaluate O
TrufLL B-MethodName
on O
two O
visual O
question O
generation O
tasks O
, O
for O
which O
we O
report O
positive O
results O
over O
performance O
and O
language O
metrics O
, O
which O
we O
then O
corroborate O
with O
a O
human O
evaluation O
. O
To O
our O
knowledge O
, O
it O
is O
the O
first O
approach O
that O
successfully O
learns O
a O
language O
generation O
policy O
without O
pre O
- O
training O
, O
using O
only O
reinforcement O
learning O
. O
1 O

Introduction O

Since O
the O
development O
of O
generic O
language O
models O
trained O
on O
massive O
unlabelled O
text O
corpora O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
state O
- O
of O
- O
the O
art O
language O
processing O
systems O
rely O
on O
sequential O
transfer O
learning O
( O
Ruder O
, O
2019 O
) O
. O
The O
pretrained O
Language O
Model O
( O
LM O
) O
is O
fine O
- O
tuned O
on O
the O
downstream O
task O
using O
a O
standard O
supervised O
learning O
( O
SL O
) O
1 O
Code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
AMDonati O
/ O
RL O
- O
NLP O
VQG O
, O
TrufLL B-MethodName
truncates O
the O
vocabulary O
space O
by O
using O
a O
language O
model O
. O
Here O
, O
' O
run O
, O
' O
and O
' O
the O
' O
are O
syntactically O
incorrect O
and O
thus O
truncated O
. O
Yet O
, O
' O
car O
' O
is O
not O
trimmed O
as O
the O
LM O
is O
not O
visually O
grounded O
. O
( O
right O
) O
In O
a O
VQG O
training O
loop O
, O
the O
agent O
generates O
a O
question O
given O
an O
image O
- O
answer O
pair O
, O
which O
is O
then O
fed O
to O
a O
VQA O
model O
predicting O
an O
expected O
answer O
. O
If O
both O
answers O
match O
, O
the O
agent O
is O
rewarded O
. O

objective O
Peters O
et O
al O
. O
, O
2019 O
) O
. O
Yet O
, O
such O
an O
approach O
suffers O
from O
several O
issues O
: O
( O
i O
) O
catastrophic O
forgetting O
when O
a O
model O
forgets O
previously O
learned O
knowledge O
and O
overfits O
to O
target O
domains O
, O
( O
ii O
) O
computational O
inefficiency O
from O
fine O
- O
tuning O
billion O
- O
parameters O
networks O
, O
and O
( O
iii O
) O
the O
need O
of O
supervised O
datasets O
. O
Moreover O
, O
task O
- O
specific O
language O
models O
learned O
with O
SL O
suffer O
from O
well O
- O
studied O
text O
degeneration O
issues O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
, O
such O
as O
the O
exposure O
bias O
, O
language O
biases O
( O
Saleh O
et O
al O
. O
, O
2020 O
; O
, O
or O
a O
lack O
of O
diversity O
( O
Li O
et O
al O
. O
, O
2015 O
) O
. O

On O
the O
other O
hand O
, O
text O
generation O
can O
be O
naturally O
framed O
as O
a O
sequential O
decision O
making O
problem O
, O
with O
the O
sequence O
of O
words O
seen O
as O
successive O
actions O
over O
a O
vocabulary O
. O
Thus O
, O
some O
researchers O
have O
recently O
focused O
on O
learning O
language O
models O
using O
instead O
Reinforcement O
Learning O
( O
RL O
) O
Das O
et O
al O
. O
, O
2017 O
; O
Narasimhan O
et O
al O
. O
, O
2015 O
) O
. O
RL O
methods O
allow O
acquiring O
language O
through O
interactions O
within O
rich O
and O
diverse O
environments O
( O
Luketina O
et O
al O
. O
, O
2019 O
) O
, O
help O
understanding O
language O
acquisition O
and O
language O
pragmatics O
( O
Lazaridou O
et O
al O
. O
, O
2016 O
; O
Bisk O
et O
al O
. O
, O
2020 O
) O
. O
" O
Reward O
is O
enough O
" O
( O
Silver O
et O
al O
. O
, O
2021 O
) O
highlights O
the O
necessity O
of O
using O
RL O
for O
AI O
systems O
to O
acquire O
language O
in O
its O
full O
richness O
. O
Indeed O
, O
( O
i O
) O
language O
may O
be O
intertwined O
with O
other O
modalities O
of O
action O
and O
observation O
, O
( O
ii O
) O
the O
utility O
of O
language O
varies O
according O
to O
situations O
and O
behaviours O
, O
( O
iii O
) O
it O
is O
consequential O
and O
purposeful O
, O
and O
( O
iv O
) O
some O
linguistic O
problems O
are O
better O
solved O
dynamically O
, O
through O
experience O
( O
such O
as O
using O
a O
diplomatic O
tone O
in O
a O
speech O
. O
) O
In O
addition O
, O
RL O
allows O
optimizing O
a O
non O
- O
differentiable O
learning O
signal O
, O
hence O
handles O
more O
diverse O
objective O
functions O
, O
and O
also O
avoids O
some O
of O
the O
text O
degeneration O
issues O
previously O
mentioned O
. O
So O
far O
, O
RL O
- O
based O
text O
- O
generation O
tasks O
have O
relied O
on O
a O
pre O
- O
training O
phase O
to O
ease O
learning O
: O
the O
policy O
language O
model O
is O
trained O
with O
SL O
on O
the O
task O
dataset O
, O
before O
being O
fine O
- O
tuned O
with O
policy O
gradient O
methods O
( O
Sutton O
et O
al O
. O
, O
1999 O
) O
on O
the O
task O
at O
hand O
. O
Those O
approaches O
often O
require O
human O
- O
labelled O
datasets O
. O
Besides O
, O
combining O
pre O
- O
training O
and O
fine O
- O
tuning O
phases O
either O
barely O
change O
the O
policy O
distribution O
, O
or O
induces O
language O
drift O
( O
Lazaridou O
et O
al O
. O
, O
2020 O
; O
Lu O
et O
al O
. O
, O
2020b O
) O
, O
i.e O
the O
generated O
language O
drifts O
semantically O
or O
syntactically O
from O
natural O
language O
. O

In O
this O
paper O
, O
we O
aim O
at O
learning O
a O
conditional O
language O
model O
using O
RL O
without O
a O
pre O
- O
training O
phase O
, O
so O
that O
( O
i O
) O
we O
get O
free O
from O
datasets O
with O
human O
annotations O
, O
and O
( O
ii O
) O
we O
avoid O
the O
text O
generation O
flaws O
induced O
by O
the O
common O
methods O
. O
While O
appealing O
, O
such O
an O
approach O
requires O
overcoming O
the O
hurdle O
of O
the O
combinatorial O
language O
action O
space O
, O
a O
vocabulary O
usually O
containing O
more O
than O
10,000 O
words O
. O
Yet O
, O
while O
large O
and O
discrete O
, O
a O
language O
action O
space O
contains O
a O
specific O
structure O
, O
made O
of O
all O
the O
syntactical O
and O
semantics O
rules O
of O
a O
given O
language O
. O
TrufLL B-MethodName
leverages O
such O
structure O
to O
drive O
the O
exploration O
of O
the O
RL O
- O
based O
language O
agent O
during O
training O
. O
At O
each O
time O
step O
of O
the O
text O
generation O
process O
, O
TrufLL B-MethodName
truncates O
its O
effective O
action O
space O
to O
a O
small O
subset O
of O
words O
provided O
by O
a O
pretrained O
task O
- O
agnostic O
language O
model O
. O
Such O
an O
approach O
injects O
a O
generic O
prior O
linguistic O
knowledge O
into O
the O
RL O
algorithm O
, O
is O
usable O
on O
tasks O
lacking O
in O
- O
domain O
labeled O
data O
, O
and O
can O
be O
easily O
transferred O
to O
new O
RL O
- O
based O
text O
generation O
tasks O
. O
Thus O
, O
TrufLL B-MethodName
can O
be O
applied O
to O
any O
language O
generation O
task O
given O
a O
generic O
LM O
and O
a O
reward O
. O
We O
here O
evaluate O
it O
on O
two O
Visual B-TaskName
Question I-TaskName
Generation I-TaskName
( O
VQG B-TaskName
) O
tasks O
, O
the O
synthetic O
CLEVR B-DatasetName
dataset O
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
, O
and O
the O
natural O
language O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al O
. O
, O
2017 O
) O
. O
Unlike O
alternative O
RL O
without O
pre O
- O
training O
approaches O
, O
TrufLL B-MethodName
manages O
to O
ask O
meaningful O
and O
valid O
questions O
on O
large O
vocabularies O
, O
exhibiting O
success O
rate O
and O
language O
metrics O
close O
to O
pretrain O
models O
with O
labeled O
data O
, O
while O
producing O
more O
original O
language O
. O

Background O

Language O
Generation O
as O
an O
RL O
Problem O
. O
We O
cast O
the O
word O
- O
based O
text O
generation O
task O
as O
a O
Markov O
Decision O
Process O
to O
apply O
RL O
methods O
( O
Sutton O
et O
al O
. O
, O
1998 O
) O
. O
In O
this O
setting O
, O
a O
language O
model O
agent O
generates O
a O
sequence O
of O
words O
w O
< O
t O
= O
( O
w O
0 O
, O
w O
1 O
, O
... O
, O
w O
t−1 O
) O
drawn O
from O
a O
vocabulary O
V O
, O
given O
an O
initial O
context O
c O
associated O
with O
a O
reward O
r O
t O
. O
Translation O
, O
text O
summarization O
or O
image O
captioning O
are O
examples O
of O
such O
tasks O
respectively O
using O
a O
source O
sentence O
, O
a O
text O
article O
, O
or O
an O
image O
as O
a O
context O
( O
c O
) O
. O
During O
this O
process O
, O
the O
agent O
may O
be O
rewarded O
with O
language O
scores O
( O
Ranzato O
et O
al O
. O
, O
2016 O
) O
, O
human O
preferences O
( O
Stiennon O
et O
al O
. O
, O
2020 O
) O
or O
task O
completion O
scores O
. O

Formally O
, O
a O
language O
generation O
agent O
is O
defined O
by O
a O
policy O
π O
θ O
( O
a O
distribution O
over O
V O
) O
parametrized O
by O
θ O
, O
first O
initialized O
with O
the O
context O
c. O
At O
each O
time O
step O
t O
, O
the O
agent O
samples O
a O
new O
word O
w O
t O
from O
its O
policy O
π O
θ O
( O
w O
t O
|w O
< O
t O
, O
c O
) O
. O
It O
moves O
to O
a O
new O
state O
( O
w O
< O
t+1 O
, O
c O
) O
and O
receives O
a O
reward O
r O
t O
= O
r O
( O
w O
< O
t O
, O
c O
, O
w O
t O
) O
, O
where O
r O
is O
a O
reward O
function O
relative O
to O
the O
language O
task O
. O
The O
RL O
language O
agent O
aims O
to O
learn O
a O
policy O
that O
maximizes O
E O
π O
θ O
[ O
T O
t=0 O
r O
t O
] O
, O
2 O
while O
generating O
the O
sequence O
of O
words O
w O
< O
T O
, O
where O
E O
π O
θ O
is O
the O
expectation O
under O
π O
θ O
, O
and O
T O
the O
maximal O
length O
of O
the O
words O
sequence O
. O

Policy O
Gradient O
This O
optimization O
process O
may O
be O
performed O
through O
Policy O
Gradient O
( O
PG O
) O
algorithms O
( O
Sutton O
et O
al O
. O
, O
1999 O
) O
. O
In O
the O
language O
literature O
, O
REINFORCE O
( O
Williams O
, O
1992 O
) O
has O
been O
used O
as O
a O
simple O
Monte O
Carlo O
approximation O
of O
this O
gradient O
.Yet O
, O
in O
this O
paper O
, O
we O
use O
a O
Proximal O
Policy O
Optimization O
approach O
( O
PPO O
) O
( O
Schulman O
et O
al O
. O
, O
2017 O
) O
to O
have O
a O
lower O
variance O
and O
better O
convergence O
rate O
; O
PPO O
clips O
the O
gradient O
estimate O
to O
have O
smooth O
policy O
updates O
. O
For O
all O
0≤t≤T O
, O
let O
s O
t O
= O
( O
w O
< O
t O
, O
c O
) O
and O
a O
t O
= O
w O
t O
be O
the O
state O
and O
action O
at O
time O
t. O
Policy O
gradient O
methods O
minimize O
the O
objective O
: O

L O
pg O
( O
θ O
) O
=E O
π O
θ O
T O
t=0 O
logπ O
θ O
( O
a O
t O
|s O
t O
) O
Â O
t O
, O

whereÂ O
t O
is O
an O
estimator O
of O
the O
advantage O
function O
, O
here O
defined O
asÂ O
t O
= O
T O
u O
= O
t O
r O
u O
− O
V O
ϕ O
( O
s O
t O
) O
with O
V O
ϕ O
( O
s O
) O
an O
estimator O
of O
the O
value O
function O

V O
π O
θ O
( O
s O
) O
= O
E O
π O
θ O
[ O
T O
u O
= O
t O
r O
( O
s O
u O
, O
a O
u O
) O
|s O
t O
= O
s O
] O
. O

PPO O
then O
keeps O
track O
of O
the O
previous O
policy O
π O
θ O
old O
before O
the O
PG O
update O
to O
compute O
the O
training O
objective O
: O

L O
ppo O
( O
θ O
) O
=E O
π O
θ O
old O
T O
t=0 O
ρ O
θ O
tÂ O
t O
∧clip O
( O
1−ϵ O
, O
ρ O
θ O
t O
, O
1+ϵ O
) O
Â O
t O
, O

where O
for O
all O
real O
numbers O
a O
, O
b O
, O
a O
∧ O
b O
= O
min O
( O
a O
, O
b O
) O
, O
ρ O
θ O
t O
= O
π O
θ O
( O
a O
t O
|s O
t O
) O
/ O
π O
θ O
old O
( O
a O
t O
|s O
t O
) O
, O
ϵ B-HyperparameterName
is O
a O
hyper O
- O
parameter O
controlling O
the O
magnitude O
of O
the O
policy O
updates O
, O
and O

clip O
( O
a O
, O
x O
, O
b O
) O
is O
the O
function O
that O
clips O
x O
in O
interval O
[ O
a O
, O
b O
] O
. O

The O
expectation O
is O
estimated O
in O
practice O
using O
a O
Monte O
Carlo O
approach O
, O
with O
an O
empirical O
average O
over O
a O
finite O
batch O
of O
episodes O
, O
i.e O
a O
succession O
of O
transitions O
s O
t O
, O
a O
t O
∼π O
θ O
old O
( O
.|s O
t O
) O
, O
r O
t O
from O
an O
initial O
state O
s O
0 O
to O
a O
terminal O
state O
s O
T O
. O
Finally O
, O
the O
training O
loss O
is O
completed O
first O
with O
a O
value O
- O
based O
loss O
to O
learn O
the O
baseline O
V O
ϕ O
that O
reduces O
the O
gradient O
variance O
; O
it O
computes O
for O
each O
timestep O
t O
of O
an O
episode O
the O
mean O
squared O
error O
| O
T O
u O
= O
t O
r O
u O
−V O
ϕ O
( O
s O
t O
) O
| O
2 O
. O
3 O
Secondly O
, O
the O
loss O
is O
completed O
with O
an O
entropy O
term O
to O
soften O
the O
policy O
distribution O
, O
which O
computes O
for O
each O
timestep O
t O
of O
an O
episode O
H O
( O
π O
θ O
( O
a O
t O
|s O
t O
) O
) O
, O
where O
H O
is O
the O
entropy O
function O
. O

TrufLL B-MethodName

We O
here O
aim O
at O
making O
RL O
methods O
feasible O
in O
the O
language O
setting O
by O
dynamically O
reducing O
the O
action O
space O
, O
i.e. O
, O
by O
restricting O
the O
language O
agent O
to O
select O
a O
word O
within O
a O
subset O
of O
the O
vocabulary O
at O
each O
time O
step O
. O
We O
detail O
below O
the O
action O
space O
's O
truncation O
model O
and O
the O
associated O
RL O
algorithm O
to O
learn O
the O
language O
agent O
. O

Dynamic O
Vocabulary O
Truncation O

TrufLL B-MethodName
combines O
two O
distinct O
language O
models O
, O
which O
share O
the O
same O
vocabulary O
V O
: O
a O
RL O
language O
agent O
π O
θ O
and O
a O
pretrained O
language O
model O
f O
LM O
. O
At O
each O
timestep O
t O
, O
TrufLL B-MethodName
restricts O
the O
vocabulary O
space O
of O
the O
RL O
language O
agent O
with O
: O

V O
− O
t O
= O
{ O
w|w O
∈V O
, O
g O
trunc O
( O
w|w O
< O
t O
) O
= O
1 O
} O
, O

where O
g O
trunc O
is O
a O
truncation O
function O
based O
on O
f O
LM O
which O
either O
associates O
0 O
or O
1 O
with O
each O
word O
in O
the O
vocabulary O
given O
the O
past O
words O
w O
< O
t O
. O
From O
a O
language O
modelling O
perspective O
, O
the O
vocabulary O
space O
of O
the O
language O
agent O
is O
reduced O
from O
V O
to O
V O
− O
where O
|V O
− O
|≪|V| O
, O
with O
|•| O
the O
cardinal O
of O
a O
finite O
set O
. O
From O
a O
RL O
perspective O
, O
the O
RL O
agent O
follows O
a O
truncated O
policy O
π O
− O
θ O
which O
only O
samples O
actions O
over O
the O
subset O
V O
− O
. O
In O
practice O
, O
such O
a O
policy O
is O
computed O
using O
a O
masked O
softmax O
function O
over O
the O
truncated O
vocabulary O
V O
− O
t O
: O
π O
− O
θ O
( O
.|w O
< O
t O
, O
c O
) O
= O
softmax O
( O
m O
* O
logits O
π O
θ O
( O
w O
< O
t O
, O
c O
) O
) O
where O
m=1 O
when O
g O
trunc O
( O
w|w O
< O
t O
) O
= O
1 O
otherwise O
m=−∞ O
. O

Truncation O
Functions O

We O
here O
list O
the O
different O
truncation O
functions O
g O
trunc O
explored O
through O
the O
paper O
. O

Top O
- O
k O
words O
: O
This O
function O
selects O
the O
k O
words O
with O
the O
highest O
probability O
given O
by O
f O
LM O
( O
.|w O
< O
t O
) O
: O

g O
top O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O
k O
) O
=1 O
wt∈top O
( O
k O
) O
( O
f O
LM O
( O
.|w O
< O
t O
) O
) O
. O

Probability O
threshold O
( O
α O
) O
: O
This O
function O
only O
keeps O
words O
having O
a O
probability O
f O
LM O
( O
.|w O
< O
t O
) O
greater O
than O
α O
: O

g O
p O
th O
( O
α O
) O
( O
w O
t O
|w O
< O
t O
; O
α O
) O
=1 O
f O
LM O
( O
wt|w O
< O
t O
) O
> O
α O
. O

Top O
- O
p O
: O
This O
function O
is O
based O
on O
nucleus O
sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
, O
and O
it O
keeps O
the O
most O
likely O
words O
contained O
in O
a O
probability O
mass O
p O
of O
f O
LM O
( O
.|w O
< O
t O
) O
. O
Formally O
, O
we O
define O
V O
p O
t O
as O
: O

V O
p O
t O
= O

g O
sample O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O
k O
) O
=1 O
wt∈ O
{ O
w O
i O
∼f O
LM O
( O
.|w O
< O
t O
) O
i∈ O
1 O
, O
... O
, O
k O
} O
. O

Only O
top O
( O
k O
) O
provides O
a O
fixed O
number O
of O
words O
at O
each O
time O
step O
. O
p O
th O
( O
α O
) O
, O
top O
( O
p O
) O
, O
and O
sample O
( O
k O
) O
have O
a O
dynamic O
truncation O
, O
whose O
size O
at O
t O
depends O
on O
the O
language O
model O
entropy O
. O

Task O
- O
Specific O
vs. O
Generic O
LM O

We O
benchmark O
two O
types O
of O
language O
models O
for O
truncation O
. O
On O
the O
one O
hand O
, O
we O
use O
an O
external O
language O
model O
pretrained O
on O
a O
large O
task O
- O
agnostic O
language O
corpora O
. O
Such O
a O
model O
provides O
a O
generic O
linguistic O
prior O
to O
the O
RL O
agent O
exploration O
process O
, O
solely O
encoding O
syntactic O
and O
semantic O
information O
. O
On O
the O
other O
hand O
, O
we O
use O
a O
task O
- O
related O
language O
model O
pretrained O
on O
the O
supervised O
dataset O
associated O
with O
the O
task O
. O
Such O
a O
model O
provides O
a O
task O
- O
specific O
linguistic O
prior O
to O
the O
RL O
language O
agent O
, O
and O
captures O
language O
pragmatics O
. O
We O
emphasize O
that O
this O
paper O
aims O
at O
leveraging O
taskagnostic O
language O
models O
as O
they O
discard O
the O
need O
for O
task O
- O
specific O
data O
. O
For O
the O
sake O
of O
completeness O
, O
we O
also O
study O
the O
truncation O
with O
the O
task O
- O
related O
LM O
as O
an O
additional O
benchmark O
to O
assess O
our O
approach O
. O

Experimental O
Setting O

We O
here O
list O
the O
experimental O
setting O
and O
detail O
the O
network O
and O
hyperparameters O
in O
Appendix O
A.4 O
. O

Visual B-TaskName
Question I-TaskName
Generation I-TaskName

We O
showcase O
TrufLL B-MethodName
on O
the O
task O
of O
Visual B-TaskName
Question I-TaskName
Generation I-TaskName
( O
VQG B-TaskName
) O
( O
Mostafazadeh O
et O
al O
. O
, O
2016 O
) O
, O
which O
is O
a O
form O
of O
Visual O
Jeopardy O
! O
™ O
( O
Ferrucci O
, O
2012 O
) O
. O
There O
, O
the O
language O
agent O
observes O
an O
image O
- O
answer O
pair O
and O
has O
to O
generate O
a O
question O
that O
results O
in O
a O
similar O
answer O
, O
as O
illustrated O
in O
Figure O
1 O
. O
Such O
a O
task O
presents O
multiple O
advantages O
. O
First O
, O
by O
combining O
vision O
, O
scene O
understanding O
and O
language O
generation O
, O
it O
requires O
high O
- O
level O
reasoning O
and O
exhibits O
a O
large O
spectrum O
of O
language O
difficulties O
. O
Secondly O
, O
the O
success O
criterion O
is O
naturally O
non O
- O
differentiable O
, O
hence O
a O
natural O
fit O
for O
RL O
methods O
. O
Such O
a O
criterion O
, O
unlike O
metrics O
based O
on O
ground O
- O
truth O
sentences O
, O
allows O
generating O
diverse O
grounded O
questions O
given O
an O
image O
- O
answer O
pair O
. O

Formally O
, O
the O
initial O
context O
c O
is O
composed O
of O
the O
image O
- O
answer O
pair O
( O
I O
, O
A O
) O
. O
The O
RL O
agent O
then O
generates O
a O
sequence O
of O
words O
w O
< O
t O
of O
maximum O
length O
T O
. O
We O
then O
provide O
the O
generated O
question O
to O
a O
pretrained O
VQA B-TaskName
model O
. O
This O
model O
takes O
as O
inputs O
the O
image O
I O
, O
the O
generated O
question O
w O
< O
t O
and O
outputs O
a O
predicted O
answerÂ. O
Finally O
, O
the O
agent O
receives O
a O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
based O
on O
A O
andÂ O
. O

Datasets O

We O
evaluate O
TrufLL B-MethodName
on O
the O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
datasets O
to O
simulate O
large O
- O
scale O
VQG B-TaskName
datasets O
. O
The O
two O
datasets O
have O
been O
originally O
created O
for O
the O
task O
of O
Visual B-TaskName
Question I-TaskName
Answering I-TaskName
( O
VQA B-TaskName
) O
, O
i.e. O
for O
multi O
- O
modal O
classification O
algorithms O
predicting O
an O
answer O
given O
an O
image O
- O
question O
pair O
. O

CLEVR B-DatasetName
The O
CLEVR B-DatasetName
VQA B-TaskName
dataset O
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
is O
made O
of O
template O
questions O
on O
synthetic O
images O
, O
which O
contain O
simple O
objects O
with O
four O
distinct O
properties O
( O
shape O
, O
material O
, O
color O
, O
size O
) O
. O
The O
vocabulary O
contains O
86 O
words O
and O
28 O
potential O
answers O
, O
making O
it O
a O
valuable O
proof O
of O
concept O
for O
assessing O
TrufLL B-MethodName
. O
Both O
language O
models O
are O
single O
- O
layer O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
512 B-HyperparameterValue
units O
, O
and O
512 B-HyperparameterValue
word B-HyperparameterName
embedding I-HyperparameterName
dimension I-HyperparameterName
. O
The O
task O
- O
specific O
LM O
is O
trained O
over O
the O
full O
train O
dataset O
of O
CLEVR B-DatasetName
questions O
. O
The O
external O
language O
model O
is O
trained O
on O
the O
mixture O
of O
CLOSURE B-DatasetName
( O
Bahdanau O
et O
al O
. O
, O
2019 O
) O
and O
CLEVR B-DatasetName
- O
Dialog O
( O
Kottur O
et O
al O
. O
, O
2019 O
) O
datasets O
. O
Although O
those O
two O
datasets O
share O
the O
CLEVR B-DatasetName
vocabulary O
, O
their O
language O
distribution O
differs O
from O
vanilla O
CLEVR B-DatasetName
. O
Finally O
, O
we O
use O
a O
pretrained O
GT B-MethodName
- I-MethodName
Vector I-MethodName
- I-MethodName
NMN I-MethodName
( O
Bahdanau O
et O
al O
. O
, O
2019 O
) O
to O
compute O
the O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O
A O
= O
Â O
, O
t O
= O
T O
−1 O
, O
where O
1 O
is O
the O
indicator O
function O
. O

VQAv2 B-DatasetName
The O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al O
. O
, O
2017 O
) O
is O
made O
of O
natural O
language O
and O
open O
- O
formed O
questions O
on O
images O
from O
the O
MS B-DatasetName
- I-DatasetName
Coco I-DatasetName
Dataset O
( O
Lin O
et O
al O
. O
, O
2014 O
) O
. O
It O
has O
a O
vocabulary O
of O
14,810 O
words O
and O
3,149 O
answers O
. O
The O
task O
- O
specific O
language O
model O
is O
a O
one O
- O
layer O
LSTM B-MethodName
with O
512 B-HyperparameterValue
units B-HyperparameterName
and O
a O
512 B-HyperparameterValue
word B-HyperparameterName
embedding I-HyperparameterName
dimension I-HyperparameterName
, O
pretrained O
over O
the O
full O
training O
dataset O
of O
VQAv2 B-DatasetName
questions O
. O
The O
External O
Language O
Model O
is O
Open O
- O
AI O
's O
GPT-2 O
. O
The O
original O
language O
model O
outputs O
a O
probability O
distribution O
over O
50,257 O
tokens O
, O
but O
we O
use O
a O
masked O
softmax O
function O
to O
restrict O
the O
probability O
distribution O
to O
the O
14,810 O
tokens O
of O
the O
VQAv2 B-DatasetName
dataset O
. O
Unlike O
most O
NLP O
tasks O
relying O
on O
pretrained O
generic O
language O
models O
, O
we O
do O
not O
fine O
- O
tune O
it O
on O
the O
task O
dataset O
. O
Instead O
, O
we O
leverage O
the O
few O
- O
shot O
generalization O
capabilities O
of O
GPT-2 O
, O
by O
feeding O
the O
language O
model O
with O
the O
prompt O
" O
Here O
are O
a O
few O
examples O
: O
" O
followed O
by O
100 O
random O
questions O
q O
< O
100 O
from O
the O
dataset O
. O
The O
truncation O
is O
then O
based O
on O
the O
probability O
distribution O
f O
gpt2 O
LM O
( O
.|q O
< O
100 O
, O
w O
< O
t O
) O
. O
Finally O
, O
we O
used O
a O
pretrained O
Vil B-MethodName
- I-MethodName
BERT I-MethodName
to O
compute O
the O
reward O
( O
Lu O
et O
al O
. O
, O
2020a O
) O
. O
Given O
the O
large O
number O
of O
answers O
, O
we O
use O
as O
reward O
a O
decreasing O
function O
of O
the O
rank O
of O
the O
reference O
answer O
rk O
( O
A O
) O
: O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O
rk O
( O
A O
) O
≤10 O
, O
t O
= O
T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
as O
further O
explained O
in O
Appendix O
A.5 O
. O

In O
these O
two O
settings O
, O
we O
acknowledge O
that O
the O
task O
dataset O
is O
still O
used O
to O
train O
the O
VQA B-TaskName
models O
. O
Please O
note O
that O
the O
VQA B-TaskName
modules O
are O
only O
used O
to O
model O
the O
environment O
, O
i.e. O
to O
provide O
a O
positive O
/ O
negative O
feedback O
to O
the O
agent O
. O
In O
other O
settings O
, O
TrufLL B-MethodName
would O
still O
work O
if O
we O
replace O
the O
VQA B-TaskName
model O
by O
any O
language O
interface O
: O
text O
- O
game O
( O
e.g. O
Zork O
) O
, O
expert O
- O
systems O
, O
or O
humans O
. O
Here O
, O
we O
only O
use O
the O
VQG B-TaskName
framework O
as O
a O
proof O
of O
concept O
that O
natural O
language O
can O
be O
learned O
through O
pure O
interaction O
given O
any O
task O
reward O
. O
Other O
language O
generation O
applications O
are O
discussed O
in O
Section O
5.3 O
. O

Baselines O

In O
this O
paper O
, O
we O
aim O
to O
show O
that O
a O
RL O
language O
agent O
can O
be O
trained O
from O
scratch O
, O
i.e. O
without O
the O
usual O
pre O
- O
training O
phase O
by O
solely O
interacting O
with O
another O
language O
system O
, O
the O
VQA B-TaskName
model O
, O
when O
supported O
by O
truncation O
methods O
. O
The O
truncation O
with O
the O
task O
- O
related O
LM O
is O
referred O
to O
as O
TrufLL B-MethodName
( O
Task B-MethodName
- I-MethodName
LM I-MethodName
) O
, O
while O
the O
one O
with O
the O
External O
LM O
is O
referred O
as O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
. O
We O
first O
emphasize O
the O
difficulty O
of O
training O
an O
RL O
language O
agent O
without O
a O
supervised O
pre O
- O
training O
phase O
through O
two O
baselines O
. O
We O
trained O
a O
simple O
on O
- O
policy O
PPO O
algorithm O
without O
any O
action O
space O
pruning O
, O
and O
refer O
to O
it O
as O
scratch O
. O
Then O
, O
we O
added O
a O
Kullback O
- O
Leibler O
( O
KL O
) O
regularization O
term O
to O
the O
loss O
, O
λ O
KL O
KL O
( O
π O
θ O
||f O
LM O
) O
, O
with O
λ O
KL O
> O
0 O
, O
to O
incorporate O
language O
prior O
to O
the O
agent O
as O
in O
( O
Jaques O
et O
al O
. O
, O
2017 O
( O
Jaques O
et O
al O
. O
, O
, O
2019 O
. O
We O
refer O
to O
it O
as O
scratch O
+ O
KL O
- O
task O
when O
distilling O
the O
task O
- O
specific O
language O
model O
, O
and O
scratch O
+ O
KL O
- O
ext O
with O
the O
external O
language O
model O
. O
Finally O
, O
we O
include O
two O
baselines O
with O
a O
pre O
- O
training O
phase O
. O
We O
trained O
a O
language O
agent O
on O
the O
task O
- O
dataset O
with O
a O
log O
- O
likelihood O
objective O
, O
and O
refer O
to O
it O
as O
pretrain O
. O
Then O
, O
we O
fine O
- O
tune O
the O
pretrained O
language O
agent O
with O
PPO O
without O
truncation O
, O
and O
refer O
to O
it O
as O
pretrain O
+ O
RL O
fine O
- O
tune O
. O
These O
two O
baselines O
should O
be O
viewed O
as O
gold O
standards O
as O
they O
rely O
on O
task O
- O
related O
data O
; O
additionally O
, O
pretrain O
+ O
RL O
fine O
- O
tune O
is O
today O
the O
state O
- O
of O
- O
the O
- O
art O
method O
for O
learning O
RL O
- O
based O
LM O
. O

Metrics O
and O
Evaluation O
Methods O

Evaluating O
text O
generation O
is O
an O
open O
- O
research O
problem O
in O
language O
literature O
. O
We O
decompose O
automatic O
language O
evaluation O
into O
three O
categories O
to O
assess O
different O
facets O
of O
language O
, O
and O
perform O
as O
well O
a O
human O
evaluation O
study O
. O

Performance O
metrics O
. O
We O
measure O
the O
taskcompletion O
score O
or O
recall B-MetricName
@ I-MetricName
1 I-MetricName
which O
states O
whether O
the O
target O
answer O
A O
is O
the O
top O
answer O
of O
the O
VQA B-TaskName
models O
, O
and O
the O
recall B-MetricName
@ I-MetricName
5 I-MetricName
( O
R B-MetricName
@ I-MetricName
5 I-MetricName
) O
, O
which O
assesses O
whether O
A O
is O
in O
the O
5 O
top O
answers O
. O
These O
scores O
measure O
the O
task O
- O
solving O
abilities O
of O
the O
agent O
, O
but O
they O
are O
also O
conditioned O
by O
the O
VQA B-TaskName
model O
abilities O
. O

Language O
Metrics O
. O
First O
, O
we O
used O
n B-MetricName
- I-MetricName
grams I-MetricName
metrics O
, O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
and O
CIDEr B-MetricName
( O
Vedantam O
et O
al O
. O
, O
2015 O
) O
, O
to O
measure O
the O
similarity O
between O
the O
generated O
question O
and O
the O
reference O
questions O
in O
the O
evaluation O
set O
. O
While O
those O
scores O
can O
capture O
syntactic O
and O
semantic O
properties O
of O
language O
, O
they O
also O
fall O
short O
when O
dealing O
with O
open O
- O
form O
language O
, O
e.g. O
an O
identical O
answer O
may O
arise O
from O
two O
non O
- O
overlapping O
but O
syntactically O
correct O
questions O
. O
Thus O
, O
we O
also O
compute O
two O
metrics O
assessing O
the O
quality O
of O
the O
language O
independently O
of O
reference O
questions O
, O
the O
perplexity O
of O
the O
question O
given O
an O
external O
LM O
( O
ppl O
- O
e O
) O
, O
and O
its O
perplexity O
given O
the O
task O
- O
related O
LM O
( O
ppl O
- O
t O
) O
. O

Diversity O
Metrics O
. O
We O
here O
estimate O
a O
self B-MetricName
- I-MetricName
BLEU I-MetricName
( O
sBLEU B-MetricName
) O
score O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
over O
10 O
questions O
generated O
on O
the O
same O
image O
- O
answer O
pair O
. O
Although O
such O
score O
detects O
potential O
mode O
collapse O
, O
i.e. O
, O
when O
the O
language O
utters O
identical O
sequences O
of O
words O
, O
it O
also O
values O
babbling O
, O
i.e. O
, O
outputting O
random O
words O
. O
We O
thus O
also O
measure O
the O
probability O
mass O
of O
the O
ten O
most O
frequent O
words O
( O
Choshen O
et O
al O
. O
, O
2020 O
) O
, O
and O
refer O
to O
it O
as O
peakiness O
( O
peak O
) O
. O

Human O
Evaluation O
. O
On O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
performed O
human O
evaluation O
by O
surveying O
53 O
participants O
on O
the O
first O
50 O
questions O
produced O
by O
some O
of O
the O
models O
at O
test O
time O
. O
The O
study O
( O
further O
detailed O
in O
Appendix O
C O
) O
is O
based O
on O
pairwise O
comparison O
of O
question O
samples O
produced O
by O
the O
concurrent O
algorithms O
according O
to O
four O
criteria O
. O
First O
, O
we O
evaluated O
the O
language O
quality O
of O
the O
question O
samples O
, O
by O
asking O
the O
participants O
to O
select O
the O
most O
syntactically O
and O
semantically O
correct O
question O
among O
the O
two O
samples O
of O
the O
questions O
pair O
. O
Secondly O
, O
we O
evaluated O
language O
grounding O
, O
i.e O
adequacy O
of O
the O
sample O
to O
the O
image O
- O
answer O
pair O
, O
by O
asking O
the O
participants O
to O
select O
the O
question O
most O
suitable O
given O
the O
two O
elements O
. O
Thirdly O
, O
we O
evaluated O
the O
language O
originality O
and O
diversity O
, O
by O
asking O
participants O
to O
select O
the O
question O
the O
most O
different O
from O
the O
dataset O
reference O
question O
. O
Finally O
, O
we O
evaluated O
the O
number O
of O
syntax O
errors O
by O
asking O
participants O
to O
tick O
the O
question O
if O
it O
is O
grammatically O
incorrect O
. O
Examples O
of O
questions O
asked O
during O
the O
study O
are O
included O
in O
the O
Appendix O
C O
. O

Sampling O
methods O
for O
text O
generation O

When O
generating O
text O
from O
a O
trained O
language O
model O
, O
the O
quality O
and O
diversity O
of O
samples O
depend O
on O
the O
decoding O
algorithm O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O
We O
consider O
three O
text O
generation O
methods O
. O
greedy O
uses O
the O
argmax O
of O
the O
policy O
, O
while O
sampling O
uses O
the O
multinomial O
distribution O
. O
Finally O
, O
we O
sampled O
ten O
text O
sequences O
from O
the O
policy O
, O
and O
selected O
the O
one O
with O
the O
lowest O
perplexity O
according O
to O
the O
external O
language O
model O
, O
and O
refer O
to O
it O
as O
lm O
- O
ranking O
. O
This O
process O
has O
been O
used O
recently O
in O
Text O
- O
to O
- O
Image O
Generation O
tasks O
( O
Ramesh O
et O
al O
. O
, O
2021 O
5 O
Results O

CLEVR B-DatasetName
results O

Quantitative O
performance O
: O
In O
Table O
1 O
, O
vanilla O
RL O
from O
scratch O
fails O
to O
have O
a O
decent O
performance O
even O
with O
synthetic O
language O
. O
Besides O
, O
adding O
a O
KL O
regularisation O
term O
does O
kick O
- O
start O
the O
learning O
process O
. O
Yet O
, O
as O
soon O
as O
we O
apply O
the O
dynamic O
truncation O
, O
TrufLL B-MethodName
matches O
the O
pretrained O
baselines O
performance O
when O
using O
the O
external O
LM O
, O
and O
even O
outperforms O
them O
with O
the O
task O
- O
specific O
LM O
. O
In O
this O
synthetic O
VQG B-TaskName
setting O
, O
TrufLL B-MethodName
seems O
to O
be O
a O
viable O
and O
promising O
procedure O
to O
learn O
a O
RL O
language O
agent O
without O
a O
supervised O
training O
phase O
. O
Pretrained O
baselines O
have O
high O
language O
scores O
when O
assessed O
with O
datasetbased O
metrics O
, O
e.g O
BLEU B-MetricName
or O
task O
- O
perplexity O
. O
Yet O
, O
they O
also O
remain O
close O
to O
the O
original O
dataset O
distribution O
with O
a O
medium O
external O
perplexity O
. O
Noticeably O
, O
TrufLL B-MethodName
with O
the O
task O
- O
specific O
LM O
follows O
the O
same O
pattern O
. O
On O
the O
other O
hand O
, O
TrufLL B-MethodName
with O
the O
external O
LM O
reports O
poor O
dataset O
- O
based O
language O
scores O
, O
while O
maintaining O
a O
low O
external O
perplexity O
. O
Therefore O
, O
TrufLL B-MethodName
seems O
to O
correctly O
capture O
the O
language O
distribution O
of O
the O
initial O
LM O
. O
As O
the O
performance O
score O
is O
high O
when O
using O
an O
external O
LM O
, O
it O
suggests O
that O
our O
approach O
can O
learn O
a O
policy O
on O
a O
language O
task O
with O
- O
out O
the O
need O
of O
a O
task O
- O
related O
dataset O
. O
Less O
positively O
, O
TrufLL B-MethodName
diversity O
metrics O
suggest O
potential O
mode O
collapse O
, O
with O
a O
high O
peakiness O
and O
self O
- O
BLEU B-MetricName
score O
. O

Qualitative O
performance O
: O
We O
display O
qualitative O
samples O
in O
Figure O
2 O
and O
Appendix O
D. O
On O
the O
one O
hand O
, O
the O
pretrained O
baselines O
generate O
either O
a O
question O
inconsistent O
with O
the O
visual O
context O
, O
or O
which O
fails O
to O
answer O
the O
expected O
answer O
. O
action O
space O
, O
while O
having O
a O
lower O
performance O
, O
yields O
to O
the O
most O
correct O
and O
diverse O
language O
, O
with O
higher O
language O
scores O
and O
a O
lower O
self O
- O
BLEU B-MetricName
. O
A O
stochastic O
action O
space O
might O
be O
harder O
to O
explore O
efficiently O
for O
reaching O
good O
task O
- O
solving O
abilities O
, O
but O
might O
strengthen O
the O
agent O
language O
generation O
properties O
. O

VQAv2 B-DatasetName
task O

In O
CLEVR B-DatasetName
, O
we O
observe O
that O
TrufLL B-MethodName
seems O
a O
promising O
approach O
to O
learn O
a O
language O
policy O
without O
a O
supervised O
training O
phase O
, O
by O
solely O
interacting O
with O
another O
language O
system O
. O
We O
scale O
our O
approach O
to O
natural O
language O
with O
large O
vocabulary O
( O
15k O
tokens O
) O
through O
the O
VQAv2 B-DatasetName
dataset O
. O

Quantitative O
performance O
: O
Table O
3 O
reports O
the O
VQAv2 B-DatasetName
results O
, O
for O
which O
TrufLL B-MethodName
and O
the O
baselines O
present O
a O
similar O
trend O
than O
on O
CLEVR B-DatasetName
. O
First O
, O
the O
scratch O
baselines O
keep O
failing O
to O
learn O
a O
valuable O
policy O
, O
with O
performance O
scores O
and O
n O
- O
grams O
metrics O
close O
to O
zero O
. O
Although O
TrufLL B-MethodName
does O
not O
outperform O
the O
performance O
of O
the O
pretrained O
baselines O
anymore O
, O
it O
still O
leads O
to O
similar O
performances O
, O
and O
satisfactory O
language O
scores O
. O
The O
similarity O
between O
TrufLL B-MethodName
( O
Task B-MethodName
- I-MethodName
LM I-MethodName
) O
and O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
results O
suggests O
that O
the O
truncation O
approach O
is O
viable O
when O
using O
a O
generic O
LM O
whose O
original O
vocabulary O
distribution O
differs O
from O
the O
task O
. O
Interestingly O
, O
TrufLL B-MethodName
displays O
a O
self O
- O
BLEU B-MetricName
score O
similar O
to O
the O
pretrained O
baselines O
. O
This O
suggests O
that O
the O
poor O
diversity O
behavior O
observed O
on O
CLEVR B-DatasetName
is O
likely O
attributable O
to O
the O
small O
vocabulary O
and O
synthetic O
language O
distribution O
. O

Qualitative O
performance O
: O
In O
Figure O
2 O
and O
Appendix O
D O
, O
we O
display O
question O
samples O
for O
all O
models O
. O

TrufLL B-MethodName
and O
the O
pretrained O
baselines O
successfully O
generate O
a O
question O
giving O
the O
expected O
answer O
( O
" O
Black O
" O
) O
, O
while O
the O
RL O
from O
scratch O
baselines O
fail O
, O
and O
even O
showcase O
degenerated O
language O
. O
Pretrained O
baselines O
tend O
to O
output O
a O
question O
closer O
to O
the O
reference O
question O
whereas O
TrufLL B-MethodName
outputs O
original O
questions O
which O
differs O
from O
the O
VQA B-TaskName
distribution O
, O
yet O
consistent O
with O
the O
context O
. O

Human B-MethodName
Evaluation I-MethodName
: O
Figure O
3 O
details O
the O
Human B-MethodName
Evaluation I-MethodName
results O
. O
Among O
the O
RL O
from O
scratch O
baselines O
, O
we O
selected O
scratch+KL O
- O
task O
as O
the O
only O
model O
producing O
sometimes O
meaningful O
questions O
. O
Yet O
, O
it O
fails O
to O
generate O
correct O
and O
grounded O
language O
; O
it O
is O
thus O
not O
a O
viable O
approach O
despite O
its O
diverse O
output O
. O
In O
line O
with O
the O
automatic O
metrics O
, O
the O
supervised O
baselines O
produce O
the O
best O
language O
, O
while O
being O
accurately O
grounded O
. O
Yet O
, O
they O
exhibit O
significantly O
less O
diversity O
with O
the O
reference O
language O
; O
this O
suggests O
in O
particular O
that O
pretrain+RL O
fails O
to O
go O
beyond O
the O
initial O
task O
- O
data O
distribution O
. O
Finally O
, O
unlike O
TrufLL B-MethodName
( O
Task B-MethodName
- I-MethodName
LM I-MethodName
) O
which O
suffers O
from O
syntactic O
errors O
, O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
produces O
language O
that O
qualitatively O
competes O
with O
pretrain O
models O
( O
53 O
% O
) O
, O
with O
a O
similar O
ratio O
of O
syntactic O
uncorrect O
samples O
. O
Although O
its O
questions O
are O
less O
grounded O
, O
they O
are O
diverse O
, O
which O
suggests O
that O
they O
follow O
a O
different O
distribution O
from O
the O
initial O
VQA B-TaskName
dataset O
. O
It O
confirms O
that O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
could O
be O
an O
alternative O
approach O
as O
it O
has O
an O
excellent O
trade O
- O
off O
between O
language O
quality O
, O
diversity O
, O
and O
grounding O
. O

Decoding O
procedure O
: O
In O
Table O
4 O
, O
we O
evaluate O
the O
text O
sampling O
procedures O
described O
in O
Section O
4.5 O
. O
While O
greedy O
decoding O
produces O
the O
best O
outcome O
for O
pretrained O
models O
, O
lm O
- O
ranking O
provides O
an O
excellent O
trade O
- O
off O
between O
task O
performance O
and O
language O
quality O
with O
RL O
- O
based O
methods O
. O
As O
PG O
solely O
optimizes O
the O
task O
success O
ratio O
, O
this O
may O
reduce O
overall O
language O
quality O
, O
the O
re O
- O
ranking O
thus O
retrieves O
the O
best O
syntactically O
sentences O
a O
posteriori O
. O

Discussion O

Removing O
the O
truncation O
at O
evaluation O
with O
offpolicy O
RL O
. O
So O
far O
, O
TrufLL B-MethodName
directly O
learns O
the O
truncated O
policy O
over O
the O
truncated O
vocabulary O
V O
− O
each O
cell O
displays O
the O
proportion O
of O
questions O
chosen O
for O
the O
models O
in O
the O
row O
( O
bold O
) O
when O
compared O
to O
the O
concurrent O
model O
in O
the O
column O
. O
The O
table O
at O
the O
bottom O
displays O
the O
proportion O
of O
incorrect O
questions O
coming O
from O
each O
model O
among O
all O
incorrect O
samples O
. O
In O
all O
figures O
, O
bracket O
numbers O
indicates O
the O
model O
rank O
per O
criteria O
, O
from O
1= O
" O
best O
" O
to O
5= O
" O
worst O
" O
. O
2012 O
) O
. O
Formally O
, O
the O
off O
- O
policy O
PPO O
loss O
is O
defined O
by O
: O

L O
off O
ppo O
( O
θ O
) O
=E O
π O
− O
θ O
min O
( O
ρ O
θ O
t O
A O
t O
, O
clip O
( O
1−ϵ O
, O
ρ O
θ O
t O
, O
1+ϵ O
) O
A O
t O
) O
, O
whereρ O
θ O
t O
= O
π O
θ O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
− O
θ O
old O

( O
at|st O
) O
is O
the O
new O
ratio O
. O
4 O
Table O
5 O
displays O
the O
on O
- O
policy O
and O
off O
- O
policy O
results O
on O
both O
VQG B-TaskName
tasks O
for O
TrufLL B-MethodName
( O
task O
- O
LM O
) O
, O
and O
is O
further O
detailed O
in O
Appendix O
B.3 O
. O
We O
also O
monitor O
the O
probability O
mass O
of O
the O
policy O
attributed O
to O
the O
truncated O
action O
space O
( O
sumVA O
) O
. O
The O
policy O
only O
samples O
words O
within O
the O
truncated O
action O
space O
when O
sumVA O
= O
1 O
, O
without O
needing O
the O
truncation O
. O
On O
CLEVR B-DatasetName
, O
the O
TrufLL B-MethodName
off O
has O
lower O
-yet O
close O
-performance O
on O
language O
and O
task O
scores O
than O
TrufLL B-MethodName
. O
As O
its O
sumVA O
ratios O
are O
very O
close O
to O
1 O
, O
the O
agent O
has O
learned O
to O
generalize O
over O
the O
full O
vocabulary O
. O
However O
, O
the O
approach O
does O
not O
manage O
to O
sufficiently O
scale O
to O
VQAv2 B-DatasetName
. O
It O
could O
be O
improved O
with O
regularisation O
techniques O
and O
the O
use O
of O
TruFLL B-MethodName
within O
state O
- O
of O
- O
the O
- O
art O
off O
- O
policy O
RL O
algorithms O
. O
We O
leave O
such O
possibilities O
to O
future O
works O
. O
Additional O
experiments O
. O
We O
sweep O
over O
truncation O
hyper O
- O
parameters O
in O
Table O
6 O
of O
Appendix O
B. O
In O
Table O
8 O
, O
we O
observe O
that O
rewarding O
an O
agent O
with O
a O
BLEU B-MetricName
score O
is O
sub O
- O
optimal O
in O
both O
language O
and O
task O
scores O
on O
CLEVR B-DatasetName
. O
In O
VQA B-TaskName
, O
we O
apply O
temperature O
scheduling O
on O
the O
LM O
to O
perform O
fine O
- O
grained O
truncations O
in O
Table O
9 O
of O
B.2 O
. O
Finally O
, O
we O
explore O
TrufLL B-MethodName
with O
a O
pre O
- O
training O
phase O
in O
Table O
10 O
. O

Generalization O
of O
the O
approach O
. O
TrufLL B-MethodName
learns O
conditional O
language O
models O
able O
to O
solve O
specific O
Natural B-TaskName
Language I-TaskName
Generation I-TaskName
tasks O
given O
a O
context O
c. O
For O
solving O
such O
tasks O
, O
it O
only O
requires O
the O
context O
, O
a O
reward O
function O
that O
scores O
the O
language O
generated O
by O
the O
RL O
agent O
with O
respect O
to O
the O
task O
, O
and O
eventually O
a O
few O
natural O
language O
demonstrations O
fed O
as O
input O
prompt O
to O
the O
generic O
language O
model O
used O
in O
the O
truncation O
algorithm O
. O
Hence O
, O
the O
method O
is O
transferable O
to O
a O
wide O
variety O
of O
NLG B-TaskName
tasks O
, O
without O
requiring O
upfront O
large O
- O
scale O
labelled O
datasets O
. O
Additionally O
, O
the O
RL O
framework O
allows O
to O
optimize O
non O
- O
differentiable O
objectives O
, O
making O
TrufLL B-MethodName
a O
natural O
choice O
to O
learn O
end O
- O
to O
- O
end O
task O
- O
oriented O
dialogs O
, O
such O
as O
Das O
et O
al O
. O
, O
2017 O
) O
. O
Other O
interesting O
tasks O
for O
TrufLL B-MethodName
include O
the O
ones O
typically O
found O
in O
Vision O
and O
Language O
Representation O
Learning O
( O
Lu O
et O
al O
. O
, O
2020a O
) O
, O
such O
as O
Image O
Captioning O
, O
Grounding O
Referring O
Expressions O
( O
generation O
of O
a O
referring O
expression O
over O
a O
specific O
bounding O
box O
of O
an O
image O
) O
, O
Captionbased O
Image O
Retrieval O
( O
generation O
of O
a O
caption O
that O
discriminates O
an O
image O
between O
a O
set O
of O
images O
) O
. O

Reward O
functions O
for O
such O
tasks O
can O
be O
based O
on O
similarity O
scores O
between O
the O
generated O
language O
and O
the O
associated O
image O
or O
image O
region O
, O
which O
can O
be O
computed O
using O
pretrained O
language O
representations O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
or O
multi O
- O
modal O
pretrained O
systems O
such O
as O
ViLBERT B-MethodName
( O
Lu O
et O
al O
. O
, O
2019 O
) O
. O
The O
context O
can O
be O
any O
kind O
of O
data O
structure O
( O
natural O
language O
, O
database O
, O
video O
, O
etc O
) O
: O
if O
it O
is O
a O
linguistic O
input O
, O
TrufLL O
can O
be O
applied O
for O
instance O
to O
text O
summarization O
, O
paraphrase O
generation O
( O
with O
reward O
functions O
based O
on O
similarity O
scores O
between O
the O
context O
and O
the O
generated O
language O
) O
or O
text O
- O
based O
games O
( O
Ammanabrolu O
and O
Riedl O
, O
2018 O
) O
. O

6 O
Related O
work O
RL O
and O
NLP O
Tasks O
. O
Following O
( O
Singh O
et O
al O
. O
, O
2002 O
; O
Lemon O
and O
Pietquin O
, O
2007 O
) O
, O
recent O
RL O
- O
based O
taskoriented O
dialogues O
Das O
et O
al O
. O
, O
2017 O
; O
Lewis O
et O
al O
. O
, O
2017 O
; O
Narasimhan O
et O
al O
. O
, O
2015 O
) O
have O
been O
developed O
, O
where O
the O
policy O
language O
model O
is O
generally O
pretrained O
with O
SL O
followed O
RL O
( O
Yao O
et O
al O
. O
, O
2020 O
) O
combines O
a O
pretrained O
language O
model O
to O
prune O
the O
action O
space O
with O
a O
Deep O
- O
Q O
network O
, O
aka O
DRNN O
) O
. O
Yet O
, O
its O
truncation O
language O
model O
remains O
fine O
- O
tuned O
on O
the O
RL O
dataset O
. O
Besides O
, O
CALM B-MethodName
is O
only O
evaluated O
on O
a O
vocabulary O
of O
697 O
tokens O
, O
and O
on O
4 O
- O
words O
action O
sequences O
. O

Learning O
Language O
Models O
from O
scratch O
. O
( O
Ziegler O
et O
al O
. O
, O
2019 O
; O
Garg O
et O
al O
. O
, O
2021 O
) O
finetune O
pretrained O
GPT-2 B-MethodName
models O
with O
RL O
for O
language O
generation O
tasks O
without O
task O
- O
related O
data O
, O
only O
using O
reward O
signals O
. O
Yet O
, O
they O
still O
face O
optimization O
and O
computational O
challenges O
( O
Parisotto O
et O
al O
. O
, O
2020 O
) O
. O

Conclusion O

We O
proposed O
TrufLL B-MethodName
, O
an O
original O
approach O
to O
learn O
a O
natural B-TaskName
language I-TaskName
generation I-TaskName
( O
NLG B-TaskName
) O
task O
using O
RL O
, O
without O
the O
usual O
pre O
- O
training O
phase O
requiring O
supervised O
datasets O
. O
To O
our O
knowledge O
, O
this O
is O
the O
first O
RL O
- O
based O
algorithm O
dedicated O
to O
learning O
a O
word O
- O
based O
text O
- O
generation O
task O
, O
which O
does O
not O
rely O
on O
a O
pre O
- O
training O
phase O
while O
scaling O
to O
large O
vocabularies O
. O
Although O
it O
comes O
with O
its O
limitations O
, O
the O
truncated O
RL O
algorithm O
provided O
by O
TrufLL B-MethodName
gets O
free O
from O
labelled O
data O
in O
task O
- O
oriented O
language O
models O
, O
presents O
interesting O
language O
generation O
properties O
, O
and O
provides O
a O
generic O
and O
transferable O
method O
to O
learn O
any O
NLG B-TaskName
problem O
. O

A O
Dataset O
and O
training O
details O
A.1 O
Evaluation O
Metrics O

For O
the O
BLEU B-MetricName
and O
METEOR B-MetricName
scores O
, O
we O
used O
the O
NLTK O
5 O
implementations O
with O
the O
smoothing O
function O
number O
2 O
for O
the O
BLEU B-MetricName
score O
. O
For O
the O
CIDEr B-MetricName
score O
, O
we O
used O
the O
nlg O
- O
eval O
implementation O
6 O
. O

A.2 O
Answer O
filtering O

For O
each O
dataset O
, O
we O
remove O
yes O
and O
no O
question O
- O
answer O
pairs O
which O
frequency O
largely O
exceeds O
other O
answers O
, O
to O
avoid O
any O
bias O
in O
the O
question O
generation O
process O
, O
as O
usually O
done O
in O
the O
VQG B-TaskName
litterature O
( O
Mostafazadeh O
et O
al O
. O
, O
2016 O
) O
. O

A.3 O
Dataset O
split O

For O
CLEVR B-DatasetName
( O
resp O
. O
VQAv2 B-DatasetName
) O
, O
the O
RL O
language O
agent O
is O
trained O
for O
50k O
( O
resp O
. O
100k O
) O
episodes O
over O
the O
first O
20k O
images O
( O
resp O
. O
all O
the O
images O
) O
of O
the O
training O
dataset O
, O
and O
is O
then O
evaluated O
on O
the O
first O
5k O
( O
resp O
. O
20k O
) O
images O
of O
the O
validation O
set O
. O
Besides O
, O
we O
uniformly O
sample O
the O
answer O
in O
the O
set O
of O
reference O
answers O
for O
each O
image O
to O
reduce O
the O
bias O
in O
the O
distribution O
of O
answers O
. O
Finally O
, O
questions O
are O
limited O
to O
20 O
( O
resp O
. O
10 O
) O
words O
. O

A.4 O
Language O
Agent O
Networks O
and O
Training O

For O
CLEVR B-DatasetName
( O
resp O
. O
VQAv2 B-DatasetName
) O
, O
we O
used O
a O
single O
- O
layer O
LSTM B-MethodName
with O
64 B-HyperparameterValue
( O
resp O
. O
256 B-HyperparameterValue
) O
units O
for O
the O
policy O
network O
. O
At O
every O
time O
step O
, O
the O
LSTM O
input O
is O
then O
the O
concatenation O
of O
the O
word B-HyperparameterName
embedding I-HyperparameterName
of O
dimension O
32 B-HyperparameterValue
( O
resp O
. O
128 B-HyperparameterValue
) O
, O
the O
answer B-HyperparameterName
embedding I-HyperparameterName
of O
dimension O
32 B-HyperparameterValue
( O
resp O
. O
128 B-HyperparameterValue
) O
, O
and O
the O
image O
representation O
. O
For O
CLEVR B-DatasetName
, O
the O
image O
representation O
is O
extracted O
from O
a O
pretrained O
ResNet50 B-MethodName
and O
projected O
into O
a O
tensor O
of O
size O
( O
32,7,7 O
) O
before O
being O
flattened O
. O
For O
VQAv2 B-DatasetName
, O
the O
image O
representation O
is O
the O
average O
of O
200 O
bounding O
box O
features O
of O
dimension O
1048 O
, O
extracted O
from O
a O
faster O
R- B-MethodName
CNN I-MethodName
( O
Ren O
et O
al O
. O
, O
2015 O
) O
. O
We O
optimize O
the O
full O
loss O
L O
= O
L O
P O
P O
O O
+ O
αL O
V O
F O
+ O
βL O
E O
with O
α=0.5 B-HyperparameterName
, O
β B-HyperparameterName
= O
0.01 B-HyperparameterValue
and O
a O
PPO O
clipping O
ratio O
ϵ=0.02 B-HyperparameterName
( O
resp O
. O
0.01 B-HyperparameterValue
) O
for O
CLEVR B-DatasetName
( O
resp O
. O
VQAv2 B-DatasetName
) O
. O
We O
use O
Adam B-HyperparameterName
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
) O
of O
10 B-HyperparameterValue
−3 I-HyperparameterValue
for O
TrufLL B-MethodName
and O
the O
scratch O
baseline O
, O
10 B-HyperparameterValue
−5 I-HyperparameterValue
( O
resp O
. O
10 B-HyperparameterValue
−6 I-HyperparameterValue
) O
for O
RL O
algorithms O
with O
a O
pre O
- O
training O
phase O
on O
CLEVR B-DatasetName
( O
resp O
. O
VQAv2 B-DatasetName
) O
, O
and O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
for O
models O
including O
a O
KL O
regularization O
term O
. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
( O
bs B-HyperparameterName
) O
of O
128 B-HyperparameterValue
for O
all O
models O
except O
the O
ones O
with O
KL O
regularization O
, O
for O
which O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
. O
Finally O
, O
for O
the O
RL O
from O
scratch O
baselines O
, O
we O
perform O
gradient B-HyperparameterName
clipping I-HyperparameterName
( O
gladclip B-HyperparameterName
) O
of O
1 B-HyperparameterValue
( O
resp O
. O
5 B-HyperparameterValue
) O
for O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
. O

Such O
hyper O
- O
parameters O
were O
selected O
, O
after O
conducting O
an O
extensive O
hyper O
- O
parameter O
search O
. O
The O
following O
values O
were O
tested O
: O
β B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, O
0.02 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
} O
, O
ϵ B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, O
0.02 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
0.9 B-HyperparameterValue
} O
, O
lr B-HyperparameterName
∈ O
{ O
10 B-HyperparameterValue
−6 I-HyperparameterValue
, O
10 B-HyperparameterValue
−5 I-HyperparameterValue
, O
10 B-HyperparameterValue
−4 I-HyperparameterValue
, O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, O
10 B-HyperparameterValue
−3 I-HyperparameterValue
, O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−3 I-HyperparameterValue
, O
10 B-HyperparameterValue
−2 I-HyperparameterValue
, O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−2 I-HyperparameterValue
} O
, O
gradclip B-HyperparameterName
∈ O
{ O
None,1,5,10,100 B-HyperparameterValue
} O
, O
bs B-HyperparameterName
∈ O
{ O
32,64,128 B-HyperparameterValue
} O
. O

Additionally O
, O
we O
also O
tested O
for O
VQAv2 B-DatasetName
policy O
networks O
with O
64 B-HyperparameterValue
, O
256 B-HyperparameterValue
and O
1024 B-HyperparameterValue
units B-HyperparameterName
, O
with O
respectively O
32 B-HyperparameterValue
, O
128 B-HyperparameterValue
and O
512 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimensions I-HyperparameterName
. O
We O
kept O
the O
network O
size O
giving O
the O
best O
performances O
, O
i.e. O
policy O
network O
of O
256 B-HyperparameterValue
units B-HyperparameterName
and O
128 B-HyperparameterValue
word B-HyperparameterName
embedding I-HyperparameterName
dimension O
. O

A.5 O
Reward O
formula O
for O
VQAv2 B-DatasetName

In O
this O
section O
, O
we O
detail O
the O
reward O
function O
used O
for O
the O
VQAv2 B-DatasetName
task O
. O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
=1 O
rk O
( O
A O
) O
≤10 O
, O
t O
= O
T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
with O
rk O
( O
A O
) O
the O
rank O
of O
the O
ground O
- O
truth O
answer O
given O
by O
the O
VQA B-TaskName
model O
, O
when O
predicting O
the O
actual O
answer O
from O
the O
terminal O
state O
( O
c O
, O
w O
< O
T O
) O
. O
Formally O
, O
it O
is O
defined O
as O
: O

rk O
( O
A O
) O
=rank O
( O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
[ O
A O
] O
) O
, O

with O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
the O
probability O
distribution O
given O
by O
the O
VQA B-TaskName
model O
over O
the O
set O
of O
answers O
, O
and O
rank O
the O
function O
which O
ranks O
the O
probability O
of O
answer O
A O
within O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
probability O
distribution O
. O

B O
Additional O
experiments O
B.1 O
CLEVR B-DatasetName

Table O
6 O
displays O
the O
complete O
ablation O
on O
the O
truncation O
functions O
with O
parameters O
sweep O
. O
The O
' O
sizeVA O
' O
variable O
indicates O
the O
average O
size O
of O
the O
truncated O
action O
space O
for O
each O
truncation O
function O
. O
Table O
7 O
displays O
the O
ablation O
over O
the O
three O
decoding O
procedures O
defined O
in O
Section O
4.5 O
. O
Such O
an O
ablation O
presents O
a O
similar O
pattern O
than O
VQAv2 B-DatasetName
results O
described O
in O
section O
5.2 O
. O

Finally O
, O
Table O
8 O
reports O
CLEVR B-DatasetName
metrics O
when O
using O
the O
BLEU B-MetricName
score O
as O
the O
reward O
. O
While O
on O
such O
a O
task O
TrufLL B-MethodName
still O
exhibits O
promising O
language O
scores O
, O
the O
n B-MetricName
- I-MetricName
grams I-MetricName
metrics O
remain O
lower O
than O
the O
pretrained O
baselines O
. O
This O
illustrates O
that O
using O
a O
language O
similarity O
score O
as O
a O
reward O
signal O
is O
much O
less O
interesting O
than O
a O
reward O
based O
on O
a O
task O
completion O
score O
. O

B.2 O
VQAv2 B-DatasetName

Temperature O
scheduling O
: O
On O
the O
CLEVR B-DatasetName
task O
, O
we O
observed O
that O
dynamic O
truncations O
outperform O
static O
ones O
such O
as O
top O
( O
k O
) O
: O
indeed O
, O
they O
better O
take O
into O
account O
the O
inherent O
variability O
of O
the O
language O
structure O
at O
the O
sentence O
- O
level O
. O
When O
scaling O
up O
to O
the O
15k O
words O
of O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
dynamically O
decrease O
the O
truncation O
size O
through O
training O
, O
by O
applying O
a O
decreasing O
temperature O
schedule O
on O
the O
language O
model O
. O
While O
temperature O
scaling O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
is O
usually O
used O
at O
test O
time O
to O
control O
the O
smoothness O
of O
the O
language O
model O
distribution O
, O
temperature O
schedules O
during O
training O
of O
language O
models O
have O
been O
used O
in O
several O
settings O
( O
Jang O
et O
al O
. O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2020 O
) O
. O
Formally O
, O
f O
LM O
( O
w O
i O
|w O
< O
t O
) O
distribution O
is O
computed O
as O
softmax B-HyperparameterName
( O
x O
i O
) O
= O
e O
−x O
i O
/ O
τ O
/ O
j O
e O
−x O
j O
/ O
τ O
, O
with O
x O
j O
the O
LM O
logits O
and O
τ O
the O
temperature O
, O
which O
decreases O
from O
τ O
max O
to O
τ O
min O
by O
a O
factor O
T O
F O
every O
T O
u O
training O
step O
. O
In O
Table O
9 O
, O
both O
TrufLL B-MethodName
( O
Task B-MethodName
- I-MethodName
LM I-MethodName
) O
and O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
benefit O
slightly O
from O
truncation O
with O
a O
temperature O
schedule O
compared O
to O
a O
vanilla O
truncation O
. O
The O
former O
displays O
the O
best O
performance O
/ O
language O
scores O
trade O
- O
off O
for O
the O
schedule O
" O
τ O
: O
3 O
> O
1 O
. O
& O
T O
u O
= O
5,000 O
" O
, O
while O
the O
latter O
has O
the O
best O
metrics O
trade O
- O
off O
for O
" O
τ O
: O
1.5 O
> O
1 O
. O
& O
T O
u O
= O
5,000 O
" O
. O
Finally O
, O
Figure O
4 O
displays O
the O
evolution O
of O
the O
training O
return O
for O
TrufLL B-MethodName
and O
the O
baselines O
. O
As O
expected O
, O
the O
pretrain+RL O
fine O
- O
tune O
baseline O
return O
does O
not O
evolve O
much O
, O
confirming O
that O
the O
policy O
distribution O
almost O
does O
not O
shift O
through O
the O
fine O
- O
tuning O
phase O
. O
The O
training O
curves O
of O
TrufLL O
present O
a O
steady O
increase O
in O
the O
return O
until O
reaching O
convergence O
, O
confirming O
that O
our O
approach O
, O
by O
guiding O
the O
exploration O
of O
the O
action O
space O
, O
provides O
a O
sufficient O
learning O
signal O
. O
On O
the O
other O
hand O
, O
the O
scratch+KL O
baselines O
stay O
stuck O
to O
a O
low O
training O
return O
. O
This O
suggests O
that O
the O
KL O
regularization O
term O
, O
while O
encouraging O
the O
policy O
distribution O
to O
resemble O
the O
language O
model O
distribution O
, O
fails O
to O
capture O
the O
task O
pragmatics O
, O
which O
requires O
generating O
a O
language O
that O
is O
visually O
grounded O
. O

B.3 O
Additional O
discussion O

TrufLL B-MethodName
with O
a O
pre O
- O
training O
phase O
. O
Although O
TrufLL B-MethodName
aims O
at O
providing O
a O
robust O
method O
to O
learn O
a O
language O
model O
( O
almost O
) O
from O
scratch O
, O
we O
investigate O
whether O
such O
algorithm O
can O
be O
complementary O
to O
RL O
algorithms O
with O
a O
pre O
- O
training O
phase O
. O
Therefore O
, O
when O
using O
the O
task O
- O
related O
dataset O
, O
we O
evaluate O
TrufLL B-MethodName
from O
a O
pretrained O
policy O
, O
and O
we O
refer O
to O
it O
as O
TrufLL B-MethodName
pretrain O
. O

In O
table O
10 O
, O
while O
on O
CLEVR B-DatasetName
, O
TrufLL B-MethodName
pretrain O
marginally O
improves O
the O
results O
of O
the O
pretrain+RL O
fine O
- O
tune O
baseline O
, O
the O
combination O
of O
TrufLL B-MethodName
with O
a O
pre O
- O
training O
phase O
leads O
to O
performance O
degradation O
on O
VQAv2 B-DatasetName
. O
This O
suggests O
that O
on O
a O
large O
vocabulary O
task O
, O
the O
language O
distribution O
learned O
by O
the O
SL O
pretrained O
policy O
is O
significantly O
different O
from O
the O
one O
learned O
with O
TrufLL B-MethodName
. O

On O
- O
policy O
TrufLL B-MethodName
versus O
off O
- O
policy O
TrufLL B-MethodName
. O
To O
ease O
off O
- O
policy O
learning O
, O
we O
propose O
to O
add O
a O
KLregularization O
term O
in O
the O
RL O
loss O
( O
Jaques O
et O
al O
. O
, O
2017 O
( O
Jaques O
et O
al O
. O
, O
, O
2019 O
, O
and O
refer O
to O
it O
as O
TrufLL B-MethodName
off O
, O
KL O
. O
Intuitively O
, O
it O
encourages O
the O
policy O
to O
stay O
close O
to O
the O
language O
model O
's O
distribution O
, O
with O
a O
distribution O
support O
attributing O
negligible O
probabilities O
to O
words O
outside O
the O
truncated O
action O
space O
. O
Table O
11 O
displays O
the O
full O
results O
of O
on O
- O
policy O
versus O
off O
- O
policy O
scores O
for O
TrufLL B-MethodName
( O
Task B-MethodName
- I-MethodName
LM I-MethodName
) O
and O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
on O
the O
two O
tasks O
. O
The O
full O
results O
emphasize O
the O
challenges O
of O
the O
approach O
for O
the O
large O
vocabulary O
of O
VQAv2 B-DatasetName
. O
Indeed O
, O
on O
the O
off O
- O
policy O
setting O
for O
such O
a O
task O
, O
the O
exploding O
values O
for O
e O
- O
ppl O
suggest O
that O
the O
optimized O
language O
agent O
samples O
incoherent O
words O
taken O
outside O
the O
truncated O
action O
space O
, O
as O
corroborated O
by O
the O
low O
values O
of O
the O
sumVA O
ratio O
. O
Interestingly O
, O
while O
on O
CLEVR B-DatasetName
, O
TrufLL B-MethodName
off O
, O
KL O
trades O
off O
task O
performance O
for O
language O
quality O
when O
compared O
to O
TrufLL B-MethodName
off O
, O
on O
VQAv2 B-DatasetName
, O
it O
mainly O
provides O
a O
better O
learning O
signal O
for O
the O
complete O
( O
large O
) O
vocabulary O
. O
In O
such O
a O
setting O
, O
it O
hence O
improves O
the O
global O
scores O
of O
the O
off O
- O
policy O
version O
of O
TrufLL O
, O
and O
enables O
a O
much O
better O
generalization O
at O
test O
time O
of O
the O
global O
policy O
over O
the O
full O
vocabulary O
. O
Yet O
, O
keeping O
truncation O
at O
test O
time O
remains O
crucial O
with O
large O
vocabulary O
. O
Note O
that O
for O
VQAv2 B-DatasetName
, O
the O
poor O
performances O
of O
TrufLL B-MethodName
off O
, O
KL O
on O
the O
external O
LM O
is O
mainly O
due O
to O
numerical O
instability O
challenges O
when O
using O
GPT-2 B-MethodName
as O
the O
target O
policy O
of O
the O
KL O
regularization O
term O
. O

Additionally O
, O
on O
- O
policy O
versus O
off O
- O
policy O
scores O
split O
per O
sampling O
procedure O
are O
displayed O
in O
table O
12 O
: O
unsurprisingly O
, O
greedy O
decoding O
for O
TrufLL B-MethodName
off O
outperforms O
the O
two O
sampling O
- O
based O
methods O
, O
that O
are O
more O
penalized O
by O
the O
imperfect O
generalization O
of O
the O
optimized O
policy O
over O
the O
full O
vocabulary O
. O

C O
Human O
Evaluation O
details O

For O
the O
Human O
Evaluation O
study O
, O
we O
designed O
one O
form O
per O
participant O
, O
with O
three O
sections O
evaluating O
respectively O
the O
language O
quality O
, O
language O
grounding O
and O
diversity O
criteria O
. O
Given O
the O
five O
evaluated O
models O
, O
there O
are O
ten O
different O
model O
pairs O
: O
each O
section O
of O
the O
form O
contains O
10 O
pairwise O
comparison O
covering O
all O
the O
possible O
model O
pairs O
for O
the O
criteria O
. O
Each O
pairwise O
comparison O
is O
sampled O
uniformly O
over O
the O
50 O
first O
question O
samples O
generated O
by O
the O
algorithms O
at O
test O
time O
. O
The O
evaluation O
of O
syntax O
errors O
was O
made O
within O
the O
diversity O
section O
: O
for O
each O
questions O
pair O
, O
we O
asked O
participants O
to O
tick O
the O
questions O
if O
they O
are O
grammatically O
incorrect O
. O
Figure O
5 O
displays O
one O
pairwise O
comparison O
example O
for O
the O
three O
sections O
, O
and O
a O
full O
form O
example O
is O
available O
at O
the O
following O
url O
: O
https O
: O
/ O
/ O
forms.gle O
/ O
kkL38x31wF7A9YKx5 O
. O
TrufLL B-MethodName
( O
Task B-MethodName
- I-MethodName
LM I-MethodName
) O
how O
many O
other O
things O
are O
the O
same O
material O
as O
the O
small O
cyan O
cylinder O
? O
TrufLL B-MethodName
( O
Ext B-MethodName
- I-MethodName
LM I-MethodName
) O
how O
many O
other O
things O
in O
the O
material O
of O
the O
small O
thing O
that O
is O
the O
same O
material O
as O
green O
thing O
? O

Figure O
7 O
: O
Samples O
on O
Clevr O
. O

Acknowledgements O

This O
action O
benefited O
from O
the O
support O
of O
the O
Chair O
New O
Gen O
RetAIl O
led O
by O
l'X O
-École O
Polytechnique O
and O
the O
Fondation O
de O
l'Ecole O
Polytechnique O
, O
sponsored O
by O
CARREFOUR O
. O

MCSE B-MethodName
: O
Multimodal B-MethodName
Contrastive I-MethodName
Learning I-MethodName
of I-MethodName
Sentence I-MethodName
Embeddings I-MethodName

Learning O
semantically O
meaningful O
sentence O
embeddings O
is O
an O
open O
problem O
in O
natural O
language O
processing O
. O
In O
this O
work O
, O
we O
propose O
a O
sentence O
embedding O
learning O
approach O
that O
exploits O
both O
visual O
and O
textual O
information O
via O
a O
multimodal O
contrastive O
objective O
. O
Through O
experiments O
on O
a O
variety O
of O
semantic O
textual O
similarity O
tasks O
, O
we O
demonstrate O
that O
our O
approach O
consistently O
improves O
the O
performance O
across O
various O
datasets O
and O
pre O
- O
trained O
encoders O
. O
In O
particular O
, O
combining O
a O
small O
amount O
of O
multimodal O
data O
with O
a O
large O
text O
- O
only O
corpus O
, O
we O
improve O
the O
state O
- O
of O
- O
the O
- O
art O
average O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
by O
1.7 B-MetricValue
% I-MetricValue
. O
By O
analyzing O
the O
properties O
of O
the O
textual O
embedding O
space O
, O
we O
show O
that O
our O
model O
excels O
in O
aligning O
semantically O
similar O
sentences O
, O
providing O
an O
explanation O
for O
its O
improved O
performance O
. O

Introduction O

Sentence O
embedding O
learning O
, O
i.e. O
, O
encoding O
sentences O
into O
fixed O
- O
length O
vectors O
that O
faithfully O
reflect O
the O
semantic O
relatedness O
among O
sentences O
, O
is O
a O
fundamental O
challenge O
in O
natural O
language O
processing O
( O
NLP O
) O
. O
Despite O
the O
tremendous O
success O
of O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
it O
has O
been O
shown O
that O
the O
off O
- O
theshelf O
sentence O
embeddings O
of O
PLMs O
without O
finetuning O
are O
even O
inferior O
to O
averaging O
Glove O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
in O
terms O
of O
semantic O
similarity O
measure O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
. O
Hence O
, O
recent O
research O
( O
Li O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Su O
et O
al O
. O
, O
2021 O
) O
focuses O
on O
adjusting O
the O
original O
sentence O
embeddings O
derived O
from O
PLMs O
in O
an O
unsupervised O
manner O
. O
In O
particular O
, O
there O
has O
been O
growing O
interest O
in O
adopting O
contrastive O
learning O
objectives O
to O
achieve O
this O
goal O
( O
Carlsson O
et O
al O
. O
, O
2020 O
; O
Gao O
et O
al O
. O
, O
2021 O
) O
. O

Although O
purely O
text O
- O
based O
models O
have O
led O
to O
impressive O
progress O
, O
it O
remains O
an O
open O
question O
to O
what O
extent O
they O
capture O
the O
deeper O
notion O
of O
sentence O
meaning O
beyond O
the O
statistical O
distribution O
of O
texts O
, O
which O
lies O
outside O
of O
the O
text O
and O
is O
grounded O
in O
the O
real O
- O
world O
( O
Bender O
and O
Koller O
, O
2020 O
; O
Bisk O
et O
al O
. O
, O
2020 O
) O
. O
As O
a O
central O
part O
of O
the O
human O
perceptual O
experience O
, O
vision O
has O
been O
shown O
to O
be O
effective O
in O
grounding O
language O
models O
and O
improving O
performance O
on O
various O
NLP O
tasks O
( O
Zhang O
et O
al O
. O
, O
2019 O
; O
Bordes O
et O
al O
. O
, O
2019 O
; O
Zhao O
and O
Titov O
, O
2020 O
) O
. O
We O
hypothesize O
that O
using O
vision O
as O
supplementary O
semantic O
information O
can O
further O
promote O
sentence O
representation O
learning O
. O

In O
this O
work O
, O
we O
propose O
MCSE B-MethodName
, O
an O
approach O
for O
multimodal O
contrastive O
learning O
of O
sentence O
embeddings O
. O
To O
exploit O
both O
visual O
and O
textual O
information O
, O
we O
adopt O
the O
state O
- O
of O
- O
the O
- O
art O
contrastive O
sentence O
embedding O
framework O
SimCSE B-MethodName
( O
Gao O
et O
al O
. O
, O
2021 O
) O
and O
extend O
it O
with O
a O
multimodal O
contrastive O
objective O
. O
In O
addition O
to O
the O
textual O
objective O
in O
SimCSE B-MethodName
that O
maximizes O
agreement O
between O
positive O
sentence O
pairs O
, O
the O
multimodal O
objective O
maximizes O
agreement O
between O
sentences O
and O
corresponding O
images O
in O
a O
shared O
space O
. O
We O
conduct O
extensive O
experiments O
on O
standard O
Semantic B-TaskName
Textual I-TaskName
Similarity I-TaskName
( O
STS B-TaskName
) O
benchmarks O
and O
show O
the O
effectiveness O
of O
MCSE B-MethodName
across O
various O
datasets O
and O
pre O
- O
trained O
encoders O
. O
We O
find O
that O
, O
using O
a O
small O
amount O
of O
multimodal O
data O
in O
addition O
to O
a O
textonly O
corpus O
yields O
significant O
improvements O
on O
STS B-TaskName
tasks O
. O
By O
analyzing O
the O
alignment O
and O
uniformity O
properties O
of O
the O
embedding O
space O
( O
Wang O
and O
Isola O
, O
2020 O
) O
, O
we O
show O
that O
MCSE B-MethodName
better O
aligns O
the O
semantically O
similar O
sentences O
while O
maintaining O
uniformity O
, O
providing O
an O
explanation O
for O
its O
superior O
performance O
. O
1 O
categorized O
into O
supervised O
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Cer O
et O
al O
. O
, O
2018 O
; O
Reimers O
and O
Gurevych O
, O
2019 O
; O
Wieting O
et O
al O
. O
, O
2020 O
) O
and O
unsupervised O
approaches O
( O
Li O
et O
al O
. O
, O
2020 O
; O
Carlsson O
et O
al O
. O
, O
2020 O
; O
Su O
et O
al O
. O
, O
2021 O
; O
Gao O
et O
al O
. O
, O
2021 O
; O
Yan O
et O
al O
. O
, O
2021 O
) O
. O
Supervised O
approaches O
mostly O
utilize O
supervision O
from O
annotated O
natural O
language O
inference O
data O
or O
parallel O
data O
. O
Unsupervised O
approaches O
are O
able O
to O
make O
use O
of O
the O
intrinsic O
semantic O
information O
embedded O
in O
the O
natural O
language O
text O
corpus O
by O
adjusting O
the O
training O
objective O
to O
STS B-TaskName
tasks O
, O
thereby O
eliminating O
the O
need O
for O
a O
costly O
annotation O
process O
. O
In O
particular O
, O
contrastive O
learning O
objective O
( O
Carlsson O
et O
al O
. O
, O
2020 O
; O
Gao O
et O
al O
. O
, O
2021 O
; O
Yan O
et O
al O
. O
, O
2021 O
) O
regularizes O
the O
embedding O
space O
by O
pulling O
positive O
( O
i.e. O
, O
semantically O
similar O
) O
sentences O
closer O
and O
pushing O
apart O
negatives O
, O
showcasing O
great O
effectiveness O
in O
capturing O
the O
semantic O
similarity O
among O
sentences O
. O
Our O
approach O
adopts O
the O
contrastive O
learning O
framework O
and O
is O
built O
on O
top O
of O
the O
current O
state O
- O
of O
- O
the O
- O
art O
approach O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
, O
further O
pushing O
the O
frontier O
of O
STS B-TaskName
by O
leveraging O
multimodal O
semantic O
information O
. O

Visually O
Grounded O
Representation O
Learning O
. O

There O
are O
various O
works O
showing O
that O
grounding O
NLP O
models O
to O
the O
visual O
world O
can O
improve O
textual O
representation O
learning O
. O
Lazaridou O
et O
al O
. O
( O
2015 O
) O
and O
Zablocki O
et O
al O
. O
( O
2018 O
) O
learn O
word O
embeddings O
by O
aligning O
words O
to O
the O
visual O
entity O
or O
visual O
context O
. O
Kiela O
et O
al O
. O
( O
2018 O
) O
ground O
sentence O
embeddings O
by O
predicting O
both O
images O
and O
alternative O
captions O
related O
to O
the O
same O
image O
. O
Bordes O
et O
al O
. O
( O
2019 O
) O
enhance O
the O
Skip O
- O
Thought O
model O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
by O
learning O
a O
grounded O
space O
that O
preserves O
the O
structure O
of O
visual O
and O
textual O
spaces O
. O

Recently O
, O
Tan O
and O
Bansal O
( O
2020 O
) O
and O
Tang O
et O
al O
. O
( O
2021 O
) O
train O
large O
scale O
language O
models O
with O
multimodal O
supervision O
from O
scratch O
with O
the O
goal O
of O
improving O
general O
language O
understanding O
. O
Different O
from O
the O
aforementioned O
works O
, O
we O
focus O
on O
learning O
visually O
grounded O
sentence O
embeddings O
by O
fine O
- O
tuning O
pre O
- O
trained O
models O
in O
a O
contrastive O
learning O
framework O
. O

Method O

To O
exploit O
both O
visual O
and O
textual O
information O
, O
we O
adopt O
SimCSE B-MethodName
( O
Gao O
et O
al O
. O
, O
2021 O
) O
as O
the O
textual O
baseline O
and O
extend O
it O
with O
a O
multimodal O
contrastive O
learning O
objective O
. O

Background O
: O
Unsupervised O
SimCSE B-MethodName

Data O
augmentation O
plays O
a O
critical O
role O
in O
contrastive O
self O
- O
supervised O
representation O
learning O
( O
Chen O
et O
al O
. O
, O
2020 O
) O
. O
The O
idea O
of O
unsupervised O
SimCSE B-MethodName
is O
to O
use O
dropout O
noise O
as O
a O
simple O
yet O
effective O
data O
augmentation O
strategy O
. O
Given O
a O
collection O
of O
sentences O
{ O
x O
i O
} O
m O
i=1 O
, O
we O
construct O
a O
positive O
pair O
for O
each O
input O
x O
i O
by O
encoding O
it O
twice O
using O
different O
dropout O
masks O
: O

h O
z O
i O
= O
g O
ϕ O
( O
f O
θ O
( O
x O
i O
, O
z O
) O
) O
and O
h O
z O
′ O
i O
= O
g O
ϕ O
( O
f O
θ O
( O
x O
i O
, O
z O
′ O
) O
) O

, O
where O
z O
and O
z O
′ O
denote O
different O
dropout O
masks O
2 O
, O
f O
θ O
( O
• O
) O
is O
a O
pre O
- O
trained O
language O
encoder O
such O
as O
BERT B-MethodName
, O
and O
g O
ϕ O
( O
• O
) O
is O
a O
projection O
head O
3 O
on O
top O
of O
the O
[ O
CLS O
] O
token O
. O
The O
training O
objective O
is O
: O
Gao O
et O
al O
. O
( O
2021 O
) O
. O
All O
other O
results O
are O
from O
our O
implementation O
. O
Models O
are O
trained O
with O
5 O
random O
seeds O
and O
we O
report O
the O
means O
and O
standard O
deviations O
. O

ℓ O
S O
i O
= O
− O
log O
e O
sim O
( O
h O
z O
i O
i O
, O
h O
z O
′ O
i O
i O
) O
/ O
τ O
N O
j=1 O
e O
sim O
( O
h O
z O
i O
i O
, O
h O
z O
′ O
j O
j O
) O
/ O
τ O
, O
( O
1 O
) O

where O
N B-HyperparameterName
is O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
mini I-HyperparameterName
- I-HyperparameterName
batch I-HyperparameterName
, O
τ B-HyperparameterName
is O
a O
temperature B-HyperparameterName
parameter O
and O
sim O
( O
h O
1 O
, O
h O
2 O
) O
is O
the O
cosine O

similarity O
h O
T O
1 O
h O
2 O
∥h O
1 O
∥•∥h O
2 O
∥ O
. O

After O
training O
, O
the O
[ O
CLS O
] O
token O
outputs O
of O
the O
language O
encoder O
are O
taken O
as O
the O
sentence O
embeddings O
. O

Multimodal O
Contrastive O
Learning O

Beyond O
the O
textual O
objective O
in O
SimCSE B-MethodName
, O
we O
introduce O
a O
multimodal O
objective O
within O
the O
contrastive O
learning O
framework O
. O
The O
overview O
of O
our O
MCSE B-MethodName
model O
is O
shown O
in O
Figure O
1 O
. O
Given O
a O
collection O
of O
sentence O
- O
image O
pairs O
D O
= O
{ O
x O
i O
, O
y O
i O
} O
m O
i=1 O
, O
firstly O
we O
map O
sentence O
x O
i O
and O
image O
y O
i O
into O
a O
shared O
space O
: O

s O
z O
i O
= O
g O
ϕ O
1 O
( O
f O
θ O
( O
x O
i O
, O
z O
) O
) O
, O
v O
i O
= O
g O
ϕ O
2 O
( O
f O
v O
( O
y O
i O
) O
) O
, O
( O
2 O

where O
f O
v O
( O
• O
) O
is O
a O
pre O
- O
trained O
image O
encoder O
such O
as O
ResNet B-MethodName
( O
He O
et O
al O
. O
, O
2016 O
) O
, O
which O
is O
fixed O
during O
training O
. O
g O
ϕ O
1 O
( O
• O
) O
and O
g O
ϕ O
2 O
( O
• O
) O
are O
distinct O
projection O
heads O
for O
text O
and O
image O
modality O
respectively O
. O
To O
pull O
semantically O
close O
image O
- O
sentence O
pairs O
together O
and O
push O
away O
non O
- O
related O
pairs O
, O
we O
define O
the O
multimodal O
contrastive O
learning O
objective O
as O
: O

ℓ O
M O
i O
= O
− O
z∈ O
{ O
z O
i O
, O
z O
′ O
i O
} O
log O
e O
sim O
( O
s O
z O
i O
, O
v O
i O
) O
/ O
τ O
′ O
N O
j=1 O
e O
sim O
( O
s O
z O
i O
, O
v O
j O
) O
/ O
τ O
′ O
, O
( O
3 O
) O

where O
τ B-HyperparameterName
′ I-HyperparameterName
is O
a O
temperature O
parameter O
. O
Let O
λ B-HyperparameterName
denote O
the O
trade O
- O
off O
hyperparameter O
between O
two O
objectives O
, O
we O
formulate O
the O
final O
loss O
as O
: O

ℓ O
i O
= O
ℓ O
S O
i O
+ O
λℓ B-HyperparameterName
M O
i O
. O
( O
4 O
) O

Our O
method O
further O
regularizes O
the O
sentence O
representation O
in O
a O
way O
that O
aligns O
with O
the O
image O
representation O
in O
the O
grounded O
space O
. O
( O
Agirre O
et O
al O
. O
, O
2012 O
( O
Agirre O
et O
al O
. O
, O
, O
2013 O
( O
Agirre O
et O
al O
. O
, O
, O
2014 O
( O
Agirre O
et O
al O
. O
, O
, O
2015 O
( O
Agirre O
et O
al O
. O
, O
, O
2016 O
, O
STS O
Benchmark O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
and O
SICK O
- O
Relatedness O
( O
Marelli O
et O
al O
. O
, O
2014 O
) O
. O
Each O
of O
these O
datasets O
consists O
of O
a O
collection O
of O
sentence O
pairs O
and O
the O
goal O
is O
to O
predict O
a O
similarity O
score O
for O
each O
sentence O
pair O
. O
Following O
Gao O
et O
al O
. O
( O
2021 O
) O
, O
we O
report O
the O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
( O
×100 O
) O
between O
gold O
annotations O
and O
predicted O
scores O
in O
the O
" O
all O
" O
setting O
, O
i.e. O
, O
for O
each O
task O
, O
we O
concatenate O
all O
the O
subsets O
and O
report O
the O
overall O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
. O

Main O
Results O

Augmenting O
text O
- O
only O
corpus O
with O
small O
scale O
multimodal O
data O
yields O
significant O
improvements O
. O
To O
further O
investigate O
the O
impact O
of O
different O
datasets O
, O
we O
train O
models O
solely O
on O
multimodal O
data O
and O
report O
results O
in O
Table O
2 O
. O
We O
observe O
that O
, O
without O
the O
large O
text O
- O
only O
corpus O
, O
the O
performances O
decrease O
considerably O
compared O
to O
results O
in O
Table O
1 O
. O
Still O
, O
MCSE B-MethodName
models O
consistently O
surpass O
SimCSE B-MethodName
models O
( O
0.9 B-MetricValue
-3.8 B-MetricValue
points O
improvement O
) O
. O
Moreover O
, O
replacing O
the O
paired O
images O
with O
shuffled O
images O
before O
training O
MCSE B-MethodName
leads O
to O
0.8 B-MetricValue
-5.0 B-MetricValue
points O
reduction O
in O
terms O
of O
average O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
, O
further O
validating O
the O
efficacy O
of O
visual O
semantics O
. O
We O
also O
replace O
the O
ResNet B-MethodName
encoder O
with O
CLIP B-MethodName
( O
Radford O
et O
al O
. O
, O
2021 O
) O
The O
alignment O
loss O
prefers O
encoders O
that O
assign O
similar O
features O
to O
semantically O
similar O
instances O
( O
assuming O
features O
have O
been O
normalized O
) O
: O

L O
align O
≜ O
E O
( O
x O
, O
x O
+ O
) O
∼ O
ppos O
f O
( O
x O
) O
− O
f O
( O
x O
+ O
) O
2 O
2 O
. O
( O
5 O
) O

And O
the O
uniformity O
loss O
prefers O
a O
uniform O
distribution O
in O
the O
hypersphere O
: O
and O
results O
are O
presented O
in O
Figure O
2 O
. O
It O
shows O
that O
MCSE B-MethodName
models O
achieve O
better O
alignment O
scores O
compared O
to O
SimCSE B-MethodName
while O
maintaining O
uniformity O
. O
This O
analysis O
provides O
further O
support O
that O
visually O
grounding O
can O
enhance O
sentence O
representation O
learning O
by O
improving O
the O
alignment O
property O
of O
the O
textual O
embedding O
space O
. O

L O
unif O
orm O
≜ O
log O
E O
x O
, O
y O
i.i.d O
. O
∼ O
p O
data O
e O
−2∥f O
( O
x O
) O
−f O
( O
y O
) O
∥ O
2 O
2 O
. O
( O
6 O

Analysis O

For O
brevity O
, O
we O
take O
BERT B-MethodName
- O
based O
models O
trained O
merely O
on O
caption O
datasets O
and O
investigate O
the O
impact O
of O
training O
data O
scales O
. O
More O
analysis O
results O
( O
sentence O
retrieval O
, O
cross O
- O
modal O
retrieval O
) O
are O
provided O
in O
Appendix O
B.3 O
. O
We O
limit O
the O
number O
of O
training O
samples O
to O
100 O
, O
500 O
, O
1000 O
, O
5000 O
and O
10000 O
, O
and O
compare O
their O
performance O
with O
the O
full O
set O
performance O
. O
In O
all O
of O
these O
settings O
, O
we O
optimize O
the O
models O
for O
same O
number O
of O
training O
steps O
as O
the O
full O
set O
setting O
. O
The O
results O
are O
shown O
in O
Figure O
3 O
. O
SimCSE B-MethodName
achieves O
better O
performance O
than O
MCSE B-MethodName
with O
limited O
samples O
, O
while O
MCSE B-MethodName
starts O
to O
outperform O
SimCSE B-MethodName
with O
the O
increasing O
data O
scale O
. O
We O
conjecture O
that O
this O
phenomenon O
can O
be O
ascribed O
to O
the O
progressive O
training O
of O
weights O
in O
multimodal O
projection O
heads O
. O

Limitations O

Despite O
showing O
performance O
improvements O
on O
STS B-TaskName
benchmarks O
, O
MCSE B-MethodName
has O
its O
limitations O
as O
well O
. O
We O
take O
caption O
datasets O
as O
the O
source O
of O
multimodal O
information O
, O
while O
these O
datasets O
are O
collected O
and O
curated O
with O
non O
- O
negligible O
human O
efforts O
. O
It O
will O
have O
great O
practical O
value O
if O
we O
can O
properly O
leverage O
noisy O
image O
- O
sentence O
pairs O
or O
even O
get O
rid O
of O
the O
explicit O
alignments O
between O
imrelease O
the O
code O
for O
calculating O
these O
two O
losses O
, O
the O
absolute O
values O
we O
obtained O
might O
be O
different O
from O
theirs O
. O
We O
make O
sure O
our O
calculation O
across O
different O
models O
is O
consistent O
. O
ages O
and O
sentences O
. O
Furthermore O
, O
we O
find O
that O
only O
subsets O
from O
related O
domains O
can O
get O
significant O
improvements O
while O
others O
suffer O
from O
distribution O
shifts O
. O
It O
is O
critical O
to O
mitigate O
domain O
gaps O
for O
learning O
general O
- O
purpose O
sentence O
embeddings O
. O
In O
addition O
, O
the O
definition O
of O
" O
semantic O
similarity O
" O
is O
highly O
task O
- O
dependent O
. O
Besides O
STS B-TaskName
benchmarks O
, O
it O
is O
worth O
exploring O
the O
performance O
gap O
between O
text O
- O
only O
models O
and O
multimodal O
models O
on O
other O
benchmarks O
that O
can O
also O
assess O
the O
quality O
of O
sentence O
representations O
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
MCSE B-MethodName
, O
a O
novel O
approach O
for O
sentence O
embedding O
learning O
that O
applies O
a O
multimodal O
contrastive O
objective O
to O
align O
sentences O
and O
corresponding O
images O
in O
a O
grounded O
space O
. O
Experiments O
show O
that O
MCSE B-MethodName
consistently O
improves O
the O
performance O
on O
STS B-TaskName
tasks O
. O
We O
also O
highlight O
the O
superiority O
of O
our O
method O
by O
analyzing O
the O
alignment O
and O
uniformity O
properties O
of O
the O
embedding O
space O
. O
The O
multimodal O
objective O
is O
generic O
and O
can O
be O
potentially O
incorporated O
into O
other O
sentence O
embedding O
methods O
to O
boost O
their O
performance O
. O

A O
Implementation O
Details O

Language O
Encoder O
Our O
implementation O
is O
based O
on O
the O
Hugging O
Face O
Transformers O
library O
6 O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
We O
start O
from O
the O
checkpoints O
of O
bert O
- O
base O
- O
uncased O
and O
roberta O
- O
base O
, O
and O
fine O
- O
tune O
the O
pre O
- O
trained O
models O
using O
a O
contrastive O
objective O
function O
. O
We O
use O
the O
768dimensional O
[ O
CLS O
] O
token O
outputs O
before O
the O
MLP O
pooler O
layer O
as O
sentence O
embeddings O
for O
evaluation O
. O

Image O
Encoder O
We O
use O
ResNet-50 B-MethodName
and O
extract O
2048 B-HyperparameterValue
- O
dimensional B-HyperparameterName
feature I-HyperparameterName
vectors I-HyperparameterName
at I-HyperparameterName
the I-HyperparameterName
last I-HyperparameterName
layer I-HyperparameterName
. O
The O
image O
encoder O
is O
not O
fine O
- O
tuned O
. O
7 B-HyperparameterValue
Projection B-HyperparameterName
Heads I-HyperparameterName
We O
use O
distinct O
projection O
heads O
for O
different O
modalities O
and O
objectives O
. O
All O
of O
them O
are O
implemented O
by O
single O
- O
layer O
MLPs O
with O
Tanh B-HyperparameterName
activation O
. O
We O
map O
sentence O
embeddings B-HyperparameterName
to O
a O
768dimensional B-HyperparameterValue
space O
before O
calculating O
the O
textual O
objective O
. O
We O
map O
both O
sentence B-HyperparameterName
embeddings I-HyperparameterName
and O
image B-HyperparameterName
feature I-HyperparameterName
vectors I-HyperparameterName
to O
a O
256 B-HyperparameterValue
- O
dimensional O
shared O
space O
, O
and O
normalize O
them O
before O
calculating O
the O
multimodal O
objective O
. O
Parameter O
Settings O
We O
explore O
5 O
training O
settings O
in O
the O
paper O
: O
{ O
wiki O
, O
wiki+flickr O
, O
wiki+coco O
, O
flickr O
, O
coco O
} O
. O
For O
wiki+flickr O
and O
wiki+coco O
, O
we O
sample O
mini O
- O
batches O
from O
either O
Wiki1 O
M O
or O
the O
caption O
dataset O
in O
proportion O
to O
their O
data O
size O
. O
We O
adopt O
most O
of O
the O
parameter O
settings O
suggested O
by O
Gao O
et O
al O
. O
( O
2021 O
) O
. O
Moreover O
, O
temperature B-HyperparameterName
parameters I-HyperparameterName
τ B-HyperparameterName
and O
τ B-HyperparameterName
′ I-HyperparameterName
are O
set O
to O
0.05 B-HyperparameterValue
, O
and O
other O
hyperparameters O
are O
reported O
in O
Table O
3 O
. O
We O
use O
the O
dev O
set O
of O
STS B-TaskName
- I-TaskName
B I-TaskName
to O
tune O
the O
trade B-HyperparameterName
- I-HyperparameterName
off I-HyperparameterName
parameter I-HyperparameterName
λ B-HyperparameterName
and O
ablation O
studies O
are O
shown O
in O

B O
More O
Results O

B.1 O
Improvements O
on O
Different O
Subsets O

To O
delve O
into O
the O
performance O
gap O
between O
MCSE B-MethodName
- I-MethodName
BERT I-MethodName
and O
SimCSE B-MethodName
- I-MethodName
BERT I-MethodName
, O
we O
calculate O
the O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
for O
different O
subsets O
of O
each O
year O
's O
STS B-TaskName
challenge O
separately O
. O
The O
improvements O
of O
MCSE B-MethodName
over O
SimCSE B-MethodName
are O
shown O
in O
Figure O
4 O
. O
In O
STS12 B-TaskName
, O
" O
MSRvid O
" O
subset O
achieves O
the O
largest O
improvement O
, O
which O
is O
a O
corpus O
of O
video O
descriptions O
. O
" O
Image O
" O
subsets O
in O
STS14 B-TaskName
and O
STS15 B-TaskName
also O
get O
considerable O
improvements O
. O
Meanwhile O
, O
the O
performance O
of O
" O
answers O
- O
students O
" O
subset O
in O
STS15 B-TaskName
drops O
extensively O
, O
and O
none O
of O
the O
subsets O
in O
STS16 B-TaskName
get O
noticeable O
improvement O
by O
MCSE B-MethodName
. O
The O
results O
indicate O
that O
the O
subsets O
benefit O
to O
different O
degrees O
from O
the O
visually O
grounding O
because O
of O
domain O
discrepancy O
. O

B.2 O
Ablation O
Study O

CLIP B-MethodName
as O
Image O
Encoder O
We O
use O
CLIP B-MethodName
( O
Radford O
et O
al O
. O
, O
2021 O
) O
as O
an O
alternative O
image O
encoder O
. O
The O
implementation O
is O
based O
on O
the O
Sentence O
Transformer O
library O
8 O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
and O
we O
use O
the O
checkpoint O
clip B-MethodName
- I-MethodName
ViT I-MethodName
- I-MethodName
B-32 I-MethodName
to O
extract O
512 B-HyperparameterValue
- O
dimensional B-HyperparameterName
feature I-HyperparameterName
vectors I-HyperparameterName
. O
As O
shown O
in O
Table O
7 O
, O
different O
image O
encoders O
lead O
to O
very O
similar O
results O
, O
thus O
we O
use O
ResNet B-MethodName
as O
the O
default O
image O
encoder O
. O

Combining O
Wiki1 B-DatasetName
M I-DatasetName
, O
Flickr30k B-DatasetName
and O
MS B-DatasetName
- I-DatasetName
COCO I-DatasetName

We O
adopt O
the O
same O
parameter O
setting O
as O
wiki+flickr O
and O
wiki+coco O
, O
and O
train O
models O
on O
the O
combination O
of O
Wiki1 B-DatasetName
M I-DatasetName
, O
Flickr30k B-DatasetName
, O
and O
MS B-DatasetName
- I-DatasetName
COCO I-DatasetName
. O
As O
shown O
in O
Table O
5 O
, O
MCSE B-MethodName
models O
achieve O
1.9 B-MetricValue
point O
and O
2.6 B-MetricValue
point O
improvements O
when O
using O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
respectively O
. O

B.3 O
Analysis O

Sentence O
Retrieval O
We O
take O
BERT B-MethodName
- O
based O
models O
trained O
on O
the O
Flickr30k B-DatasetName
train O
set O
( O
same O
seed O
) O
and O
conduct O
a O
sentence O
retrieval O
experiment O
on O
Flickr30k B-DatasetName
test O
set O
. O
Given O
an O
input O
sentence O
, O
the O
nearest O
neighbor O
will O
be O
retrieved O
based O
on O
cosine O

Model O
Result O

Query O
1 O
: O
A O
young O
girl O
is O
washing O
her O
teddy O
bear O
in O
the O
kitchen O
sink O
. O

SimCSE B-MethodName
: O
A O
middle O
- O
aged O
woman O
is O
vacuuming O
her O
kitchen O
floor O
with O
a O
canister O
vac O
. O

MCSE B-MethodName
: O

A O
young O
girl O
, O
blond O
and O
wearing O
a O
polka O
- O
dot O
shirt O
, O
washes O
a O
stuffed O
animal O
in O
a O
vanity O
sink O
. O

Query O
2 O
: O
Three O
chefs O
, O
wearing O
white O
hats O
and O
black O
aprons O
, O
are O
preparing O
food O
in O
a O
crowded O
kitchen O
. O

SimCSE B-MethodName
: O
Numerous O
workers O
with O
blue O
shirts O
and O
white O
aprons O
are O
preparing O
fish O
for O
sale O
. O
MCSE B-MethodName
: O
Three O
men O
are O
preparing O
food O
in O
a O
kitchen O
setting O
. O

Query O
3 O
: O
A O
couple O
kisses O
in O
a O
shady O
walkway O
. O

SimCSE B-MethodName
: O
A O
couple O
strolls O
down O
a O
path O
near O
benches O
and O
water O
. O
MCSE B-MethodName
: O
Couple O
kissing O
outside O
on O
street O
. O

Query O
4 O
: O
A O
man O
is O
standing O
on O
the O
streets O
taking O
photographs O
. O

SimCSE B-MethodName
: O
People O
run O
a O
marathon O
on O
a O
city O
street O
with O
a O
crowd O
watching O
. O

MCSE B-MethodName
: O

A O
guy O
wearing O
a O
white O
shirt O
is O
taking O
a O
picture O
. O

Query O
5 O
: O
Two O
boys O
are O
playing O
in O
pool O
filled O
with O
sparkling O
blue O
water O
. O

SimCSE B-MethodName
: O
A O
little O
girl O
is O
swimming O
under O
the O
crystal O
blue O
water O
. O
MCSE B-MethodName
: O
Two O
children O
are O
swimming O
in O
a O
pool O
. O

Query O
6 O
: O
An O
old O
man O
sitting O
on O
a O
bench O
staring O
at O
the O
ocean O
. O
SimCSE B-MethodName
: O
A O
man O
sitting O
on O
a O
bench O
by O
the O
ocean O
. O
MCSE B-MethodName
: O
An O
old O
man O
sits O
on O
a O
bench O
overlooking O
the O
water O
. O

Acknowledgements O

We O
thank O
Dingfan O
Chen O
, O
Fangzhou O
Zhai O
and O
Xiaoyu O
Shen O
for O
their O
helpful O
comments O
on O
the O
paper O
draft O
. O
We O
would O
also O
like O
to O
thank O
the O
reviewers O
for O
their O
valuable O
feedback O
. O
This O
work O
was O
funded O
by O
the O
Deutsche O
Forschungsgemeinschaft O
( O
DFG O
, O
German O
Research O
Foundation O
) O
-project O
- O
id O
232722074 O
-SFB O
1102 O
. O
This O
work O
was O
also O
partially O
funded O
by O
the O
EU O
Horizon O
2020 O
project O
ROXANNE O
under O
grant O
number O
833635 O
and O
COMPRISE O
grant O
number O
3081705 O
. O

Unsupervised B-MethodName
Paraphrasability I-MethodName
Prediction I-MethodName
for O
Compound O
Nominalizations O

Commonly O
found O
in O
academic O
and O
formal O
texts O
, O
a O
nominalization O
uses O
a O
deverbal O
noun O
to O
describe O
an O
event O
associated O
with O
its O
corresponding O
verb O
. O
Nominalizations O
can O
be O
difficult O
to O
interpret O
because O
of O
ambiguous O
semantic O
relations O
between O
the O
deverbal O
noun O
and O
its O
arguments O
. O
Automatic O
generation O
of O
clausal O
paraphrases O
for O
nominalizations O
can O
help O
disambiguate O
their O
meaning O
. O
However O
, O
previous O
work O
has O
not O
identified O
cases O
where O
it O
is O
awkward O
or O
impossible O
to O
paraphrase B-TaskName
a I-TaskName
compound I-TaskName
nominalization I-TaskName
. O
This O
paper O
investigates O
unsupervised O
prediction O
of O
paraphrasability O
, O
which O
determines O
whether O
the O
prenominal O
modifier O
of O
a O
nominalization O
can O
be O
re O
- O
written O
as O
a O
noun O
or O
adverb O
in O
a O
clausal O
paraphrase O
. O
We O
adopt O
the O
approach O
of O
overgenerating O
candidate O
paraphrases O
followed O
by O
candidate O
ranking O
with O
a O
neural O
language O
model O
. O
In O
experiments O
on O
an O
English O
dataset O
, O
we O
show O
that O
features O
from O
an O
Abstract O
Meaning O
Representation O
graph O
lead O
to O
statistically O
significant O
improvement O
in O
both O
paraphrasability O
prediction O
and O
paraphrase O
generation O
. O

Introduction O

A O
nominalization O
is O
a O
noun O
( O
e.g. O
, O
" O
response O
" O
) O
that O
is O
morphologically O
derived O
from O
a O
verb O
( O
" O
respond O
" O
) O
, O
and O
that O
designates O
some O
aspects O
of O
the O
event O
referred O
to O
by O
the O
verb O
( O
Quirk O
et O
al O
. O
, O
1985 O
) O
. O
In O
a O
compound O
nominalization O
, O
this O
deverbal O
noun O
may O
have O
both O
prenominal O
and O
postnominal O
modifiers O
. O
The O
prenominal O
modifier O
can O
be O
a O
noun O
( O
e.g. O
, O
" O
police O
response O
to O
the O
rioting O
" O
) O
or O
an O
adjective O
( O
" O
bodily O
injury O
to O
a O
friend O
" O
) O
, O
while O
postnominal O
modifiers O
are O
prepositional O
phrases O
( O
" O
presidential O
nomination O
of O
Harrison O
" O
) O
. O

Academic O
and O
other O
formal O
texts O
utilize O
nominalization O
extensively O
to O
produce O
a O
compact O
and O
abstract O
writing O
style O
. O
The O
meaning O
of O
compound O
nominalizations O
can O
however O
be O
difficult O
to O
interpret O
because O
of O
ambiguous O
semantic O
relations O
be O
- O
tween O
the O
deverbal O
noun O
and O
its O
modifiers O
. O
In O
particular O
, O
the O
prenominal O
modifier O
can O
play O
multiple O
semantic O
roles O
in O
the O
corresponding O
predicate O
or O
clausal O
paraphrase O
: O
as O
a O
subject O
( O
e.g. O
, O
" O
the O
police O
response O
" O
→ O
" O
the O
police O
responds O
" O
) O
; O
as O
an O
object O
( O
" O
bodily O
injury O
" O
→ O
" O
injure O
the O
body O
" O
) O
; O
as O
an O
oblique O
( O
" O
presidential O
nomination O
" O
→ O
" O
nominate O
as O
president O
" O
; O
as O
an O
adverb O
( O
" O
symbolic O
admission O
" O
→ O
" O
admit O
symbolically O
" O
; O
or O
none O
of O
the O
above O
( O
" O
stellar O
performance O
" O
→ O
" O
a O
star O
performs O
" O
) O
. O

The O
paraphrasability O
of O
the O
prenominal O
modifier O
-whether O
it O
describes O
an O
entity O
, O
the O
manner O
of O
an O
action O
, O
or O
neither O
-therefore O
has O
direct O
impact O
on O
NLP O
tasks O
that O
require O
interpretation O
of O
compound O
nominalizations O
. O
This O
ambiguity O
affects O
accuracy O
in O
relation O
extraction O
, O
which O
is O
important O
for O
information O
retrieval O
and O
question O
answering O
( O
Greenwood O
, O
2004 O
; O
Klein O
et O
al O
. O
, O
2020 O
) O
. O
A O
machine O
translation O
system O
must O
also O
be O
able O
to O
render O
the O
deverbal O
noun O
and O
its O
prenominal O
modifier O
properly O
when O
there O
is O
no O
equivalent O
nominalization O
in O
the O
target O
language O
. O
Further O
, O
paraphrasability O
prediction O
could O
benefit O
nominal O
semantic O
role O
labeling O
, O
which O
needs O
to O
identify O
the O
role O
played O
by O
the O
prenominal O
modifier O
( O
Lapata O
, O
2002 O
; O
Padó O
et O
al O
. O
, O
2008 O
; O
Kilicoglu O
et O
al O
. O
, O
2010 O
) O
. O
Finally O
, O
it O
is O
critical O
for O
nominalization B-TaskName
paraphrasing I-TaskName
. O
When O
a O
clausal O
paraphrase O
is O
not O
available O
for O
the O
input O
nominalization O
, O
approaches O
that O
do O
not O
consider O
paraphrasability O
may O
produce O
an O
invalid O
or O
misleading O
output O
( O
Lee O
et O
al O
. O
, O
2021 O
) O
. O

This O
study O
focuses O
on O
English O
, O
the O
dominant O
language O
for O
academic O
texts O
. O
It O
aims O
to O
make O
two O
contributions O
. O
First O
, O
we O
enlarge O
an O
existing O
dataset O
to O
cover O
three O
paraphrasability O
categories O
for O
prenominal O
modifiers O
in O
a O
compound O
nominalization O
( O
paraphrased O
as O
noun O
, O
as O
adverb O
or O
nonparaphrasable O
) O
. O
Second O
, O
we O
extend O
an O
algorithm O
to O
take O
paraphrasability O
into O
account O
, O
and O
show O
that O
features O
from O
Abstract O
Meaning O
Representation O
graphs O
improve O
performance O
in O
both O
paraphrasabil O
- O
ity O
prediction O
and O
paraphrase O
generation O
. O

The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O
After O
defining O
our O
task O
( O
Section O
2 O
) O
, O
we O
summarize O
previous O
research O
( O
Section O
3 O
) O
. O
We O
then O
describe O
our O
dataset O
( O
Section O
4 O
) O
and O
present O
our O
approach O
( O
Sections O
5 O
- O
6 O
) O
. O
Then O
, O
we O
discuss O
experimental O
results O
( O
Section O
7 O
- O
8 O
) O
and O
conclude O
( O
Section O
9 O
) O
. O

Paraphrasability B-TaskName
of I-TaskName
nominalizations I-TaskName
2.1 O
Motivation O

Our O
goal O
is O
to O
paraphrase B-TaskName
a I-TaskName
nominalization I-TaskName
into O
a O
" O
clausal O
paraphrase O
" O
, O
which O
we O
define O
as O
a O
clause O
headed O
by O
a O
verb O
whose O
syntactic O
arguments O
( O
e.g. O
, O
subject O
, O
object O
, O
and O
prepositional O
object O
) O
are O
transformed O
from O
the O
nominal O
arguments O
in O
the O
input O
nominalization O
. O
We O
focus O
on O
compound O
nominalizations O
in O
which O
the O
head O
noun O
has O
a O
prenominal O
modifier O
and O
a O
prepositional O
object O
, O
following O
the O
syntactic O
form O
targeted O
by O
the O
only O
publicly O
available O
dataset O
for O
nominalization O
paraphrasing O
( O
Lee O
et O
al O
. O
, O
2021 O
) O
. O
Some O
example O
inputs O
and O
outputs O
are O
shown O
in O
Table O
1 O
. O

A O
clausal O
paraphrase O
would O
not O
be O
possible O
for O
a O
compound O
nominalization O
if O
there O
is O
no O
suitable O
verb O
equivalent O
for O
its O
head O
noun O
. O
Large O
- O
scale O
language O
resources O
( O
Meyers O
et O
al O
. O
, O
1998 O
) O
already O
exist O
to O
help O
determine O
whether O
such O
a O
verb O
exists O
, O
and O
the O
task O
has O
been O
tackled O
in O
the O
context O
of O
QA O
semantic O
role O
labeling O
for O
nominalization O
( O
Klein O
et O
al O
. O
, O
2020 O
) O
. O
Less O
attention O
has O
been O
paid O
to O
another O
factor O
, O
namely O
, O
whether O
the O
prenominal O
modifier O
can O
be O
expressed O
as O
a O
subject O
, O
object O
or O
prepositional O
object O
in O
the O
paraphrase O
. O
We O
are O
not O
aware O
of O
previous O
data O
- O
driven O
research O
on O
this O
task O
, O
which O
is O
the O
focus O
of O
this O
paper O
. O
We O
will O
not O
consider O
the O
prepositional O
object O
in O
the O
input O
nominalization O
, O
since O
it O
can O
be O
incorporated O
into O
a O
clausal O
paraphrase O
in O
most O
cases O
. O

Task O
definition O

The O
term O
" O
paraphrasability O
" O
has O
been O
used O
for O
the O
degree O
of O
semantic O
equivalence O
between O
syntactic O
variants O
of O
predicate O
phrases O
( O
Fujita O
and O
Sato O
, O
2008 O
) O
. O
We O
will O
use O
this O
term O
to O
refer O
to O
the O
three O
categories O
of O
paraphrasing O
behavior O
of O
prenominal O
modifiers O
in O
compound O
nominalizations O
: O

Noun O
The O
prenominal O
modifier O
is O
a O
noun O
, O
or O
is O
an O
adjective O
that O
pertains O
to O
a O
noun O
, O
that O
can O
serve O
as O
the O
subject O
, O
object O
or O
prepositional O
object O
in O
a O
clausal O
paraphrase O
. O
In O
other O
words O
, O
either O
the O
prenominal O
modifier O
itself O
( O
e.g. O
, O
" O
police O
" O
) O
or O
its O
pertainym O
( O
" O
president O
" O
for O
" O
presidential O
" O
) O
literally O
refers O
to O
the O
entity O
that O
participates O
in O
the O
event O
denoted O
by O
the O
deverbal O
noun O
( O
" O
police O
response O
" O
, O
" O
presidential O
nomination O
" O
) O
. O

Adverb O
The O
prenominal O
modifier O
is O
an O
adjective O
that O
can O
appear O
in O
the O
clausal O
paraphrase O
in O
its O
adverbial O
form O
( O
e.g. O
, O
" O
frontal O
opposition O
" O
→ O
" O
oppose O
frontally O
" O
) O
, O
but O
not O
as O
pertainym O
( O
" O
frontal O
opposition O
" O
→ O
" O
the O
front O
opposes O
" O
) O
. O

Nil O
The O
prenominal O
modifier O
can O
not O
be O
paraphrased O
with O
either O
method O
above O
( O
e.g. O
, O
" O
stellar O
performance O
" O
→ O
" O
a O
star O
performs O
" O
; O
" O
brain O
drain O
" O
→ O
" O
drain O
a O
brain O
" O
) O
. O

As O
shown O
in O
Table O
1 O
, O
the O
input O
is O
a O
nominalization O
that O
consists O
of O
a O
deverbal O
noun O
( O
derived O
from O
the O
verb O
V O
) O
; O
its O
prenominal O
modifier O
( O
bolded O
) O
; O
and O
a O
prepositional O
phrase O
. O
The O
output O
of O
the O
Paraphrasability O
Prediction O
task O
is O
the O
part O
- O
of O
- O
speech O
label O
of O
the O
word O
to O
which O
the O
prenominal O
modifier O
is O
paraphrased O
( O
bolded O
) O
. O
The O
label O
can O
be O
Noun O
, O
Adverb O
, O
or O
Nil O
when O
it O
is O
not O
paraphrasable O
. O

The O
output O
of O
the O
Paraphrase O
Generation O
task O
is O
a O
clausal O
paraphrase O
of O
the O
input O
. O
It O
incorporates O
the O
verb O
V O
, O
the O
prepositional O
object O
from O
the O
input O
( O
marked O
with O
O O
) O
; O
and O
either O
a O
noun O
( O
marked O
with O
M O
) O
or O
an O
adverb O
( O
marked O
with O
B O
) O
corresponding O
to O
the O
prenominal O
modifier O
. O
The O
gold O
paraphrase O
of O
the O
Nil O
type O
input O
is O
defined O
as O
null O
. O
The O
only O
way O
to O
render O
such O
an O
input O
as O
a O
clause O
is O
with O
a O
support O
verb O
or O
light O
verb O
( O
e.g. O
, O
" O
stellar O
performance O
" O
→ O
" O
give O
a O
stellar O
performance O
" O
) O
. O
Since O
the O
paraphrase O
retains O
the O
original O
nominalization O
, O
it O
does O
not O
serve O
our O
goal O
of O
unpacking O
its O
meaning O
. O

3 O
Previous O
work O

Noun O
literality O
prediction O

There O
has O
been O
extensive O
research O
on O
compositionality O
analysis O
on O
noun O
compounds O
( O
Reddy O
et O
al O
. O
, O
2011 O
) O
, O
adjective O
- O
noun O
combinations O
and O
other O
types O
of O
multiword O
expressions O
( O
MWEs O
) O
( O
Biemann O
and O
Giesbrecht O
, O
2011 O
; O
Ramisch O
et O
al O
. O
, O
2016 O
; O
Cordeiro O
et O
al O
. O
, O
2019 O
; O
Jana O
et O
al O
. O
, O
2019 O
) O
. O
Compositionality O
refers O
to O
the O
extent O
to O
which O
the O
meaning O
of O
the O
MWE O
can O
be O
expressed O
in O
terms O
of O
the O
meaning O
of O
its O
constituents O
. O
It O
therefore O
has O
considerable O
overlap O
with O
literality O
prediction O
, O
which O
would O
identify O
, O
for O
example O
, O
the O
noun O
" O
rat O
" O
in O
" O
rat O
race O
" O
as O
non O
- O
literal O
( O
Reddy O
et O
al O
. O
, O
2011 O
Our O
task O
is O
closely O
related O
to O
literality O
prediction O
since O
compound O
nominalizations O
are O
a O
subset O
of O
noun O
- O
noun O
compounds O
; O
in O
particular O
, O
a O
prenominal O
modifier O
that O
is O
" O
literal O
" O
would O
likely O
be O
of O
paraphrasability O
type O
Noun O
( O
Section O
2.2 O
) O
. O
We O
will O
therefore O
evaluate O
the O
performance O
of O
a O
state O
- O
ofthe O
- O
art O
noun O
literality O
prediction O
model O
( O
Shwartz O
and O
Dagan O
, O
2019 O
) O
in O
our O
experiment O
. O

Our O
task O
is O
nonetheless O
distinct O
from O
literality O
prediction O
since O
it O
focuses O
on O
paraphrasability O
rather O
than O
literalness O
. O
For O
example O
, O
even O
when O
a O
prenominal O
modifier O
is O
used O
metaphorically O
and O
is O
non O
- O
literal O
( O
e.g. O
" O
circular O
argument O
" O
) O
, O
it O
would O
be O
labeled O
Noun O
in O
terms O
of O
paraphrasability O
if O
it O
can O
appear O
in O
a O
clausal O
paraphrase O
( O
" O
argue O
in O
a O
circle O
" O
) O
. O

Noun O
compound O
interpretation O

A O
noun O
compound O
can O
be O
disambiguated O
with O
a O
free O
- O
form O
paraphrase O
( O
Hendrickx O
et O
al O
. O
, O
2013 O
) O
, O
or O
with O
verbs O
and O
prepositions O
linking O
the O
two O
nouns O
, O
e.g. O
, O
" O
apple O
pie O
" O
is O
a O
" O
pie O
with O
apples O
" O
( O
Butnariu O
et O
al O
. O
, O
2010 O
; O
Nakov O
and O
Hearst O
, O
2013 O
) O
. O
Unsupervised O
approaches O
have O
been O
found O
to O
be O
effective O
for O
noun O
compound O
interpretation O
. O
Paraphrase O
templates O
with O
slots O
for O
prepositions O
and O
predicates O
, O
for O
example O
, O
can O
be O
filled O
using O
pre O
- O
trained O
masked O
language O
models O
( O
Ponkiya O
et O
al O
. O
, O
2020 O
) O
. O
We O
will O
likewise O
investigate O
unsupervised O
approaches O
in O
this O
work O
. O
Even O
though O
compound O
nominalizations O
are O
a O
subset O
of O
noun O
- O
noun O
compounds O
, O
our O
task O
is O
different O
since O
paraphrases O
in O
noun O
compound O
interpretation O
do O
not O
transform O
the O
head O
noun O
into O
a O
verb O
. O

Paraphrasing B-TaskName
nominalizations I-TaskName

Research O
on O
nominalization O
interpretation O
has O
mostly O
focused O
on O
nominal O
semantic O
role O
labeling O
, O
which O
assigns O
abstract O
labels O
( O
e.g. O
, O
agent O
, O
patient O
, O
manner O
) O
to O
arguments O
of O
nominalizations O
( O
Lapata O
, O
2002 O
; O
Nicholson O
and O
Baldwin O
, O
2008 O
; O
Padó O
et O
al O
. O
, O
2008 O
; O
Kilicoglu O
et O
al O
. O
, O
2010 O
) O
. O
Given O
the O
systematic O
correspondences O
between O
nominalization O
and O
clause O
structure O
, O
there O
have O
also O
been O
efforts O
to O
paraphrase B-TaskName
nominalizations I-TaskName
as O
clauses O
. O
Algorithms O
have O
been O
proposed O
for O
automatic O
acquisition O
of O
paraphrase O
templates O
, O
which O
can O
cover O
nominalization O
inputs O
( O
Shinyama O
et O
al O
. O
, O
2002 O
) O
. O
The O
paraphrasing O
task O
has O
also O
been O
indirectly O
addressed O
in O
a O
model O
for O
question O
and O
answer O
generation O
from O
nominalizations O
( O
Klein O
et O
al O
. O
, O
2020 O
) O
. O

The O
most O
closely O
related O
work O
to O
this O
paper O
was O
reported O
in O
Lee O
et O
al O
. O
( O
2021 O
) O
. O
Their O
proposed O
model O
first O
overgenerates O
paraphrase O
candidates O
, O
and O
then O
uses O
textual O
entailment O
to O
identify O
the O
optimal O
candidate O
. O
However O
, O
since O
all O
nominalizations O
in O
their O
dataset O
have O
paraphrases O
, O
their O
algorithm O
makes O
no O
judgment O
on O
paraphrasability O
. O
We O
extend O
their O
dataset O
and O
investigate O
paraphrasability O
prediction O
to O
fill O
in O
this O
research O
gap O
. O

Dataset O

The O
only O
publicly O
available O
dataset O
of O
clausal O
paraphrases O
, O
developed O
by O
Lee O
et O
al O
. O
( O
2021 O
) O

Data O
source O

In O
the O
interest O
of O
consistency O
with O
the O
existing O
dataset O
, O
we O
focus O
on O
nominalizations O
with O
the O
same O
syntactic O
pattern O
. O
Specifically O
, O
we O
collected O
sentences O
from O
English O
Wikipedia O
that O
contain O
a O
noun O
phrase O
headed O
by O
a O
deverbal O
noun O
with O
one O
prenominal O
modifier O
and O
one O
postnominal O
modifier O
. O

As O
shown O
in O
Table O
1 O
, O
the O
postnominal O
modifier O
is O
a O
prepositional O
phrase O
with O
prepositional O
object O
O. O
The O
prenominal O
modifier O
can O
be O
a O
noun O
or O
an O
adjective O
. O
To O
create O
a O
challenging O
dataset O
, O
the O
adjective O
must O
have O
a O
pertainym O
, O
or O
can O
itself O
also O
serve O
as O
noun O
( O
e.g. O
, O
" O
light O
" O
) O
, O
such O
that O
multiple O
paraphrasability O
labels O
are O
plausible O
. O

Annotation O

Two O
annotators O
, O
a O
native O
speaker O
and O
a O
near O
- O
native O
speaker O
of O
English O
, O
independently O
classified O
the O
nominalizations O
into O
one O
of O
three O
paraphrasability O
labels O
( O
Section O
2.2 O
) O
. O
For O
those O
labeled O
as O
Adverb O
, O
the O
annotator O
composed O
a O
paraphrase O
with O
an O
adverb O
that O
is O
derivationally O
related O
to O
the O
adjective O
. O
For O
those O
labeled O
as O
Nil O
, O
no O
paraphrase O
was O
required O
. O
Examples O
for O
each O
label O
are O
provided O
in O
Table O
1 O
. O

A O
professor O
of O
linguistics O
who O
is O
a O
native O
speaker O
of O
English O
reviewed O
the O
annotation O
, O
either O
keeping O
both O
or O
selecting O
one O
of O
them O
. O
A O
total O
of O
184 O
non O
- O
Noun O
instances O
were O
collected O
and O
added O
to O
the O
dataset O
, O
resulting O
in O
an O
expanded O
dataset O
with O
634 O
paraphrases O
( O
Table O
2 O
) O
. O
1 O

Paraphrase O
candidates O

Our O
approach O
is O
to O
first O
overgenerate O
paraphrase O
candidates O
for O
each O
input O
, O
and O
then O
identify O
the O
op-1 O
Accessible O
at O
https O
: O
/ O
/ O
github.com O
/ O
NominalizationParaphrase O
timal O
candidate O
. O
This O
section O
presents O
the O
candidate O
types O
, O
and O
the O
next O
section O
describes O
the O
candidate O
selection O
algorithm O
. O

Table O
3 O
shows O
the O
paraphrase O
candidates O
for O
the O
input O
" O
frontal O
opposition O
of O
the O
employers O
" O
. O
The O
prenominal O
modifier O
, O
" O
frontal O
" O
, O
is O
transformed O
into O
various O
parts O
- O
of O
- O
speech O
and O
placed O
at O
different O
positions O
in O
the O
candidates O
. O
Each O
candidate O
is O
associated O
with O
one O
of O
the O
three O
paraphrasability O
types O
: O

Noun O

The O
Noun O
type O
candidates O
are O
the O
five O
paraphrases O
defined O
in O
Lee O
et O
al O
. O
( O
2021 O
) O
. O
The O
prenominal O
modifier O
is O
paraphrased O
as O
a O
noun O
in O
the O
subject O
( O
MVO O
, O
MOV O
) O
, O
object O
( O
OVM O
, O
VMO O
) O
, O
and O
oblique O
( O
VOM O
) O
positions O
. O

Adverb O

There O
are O
four O
paraphrase O
candidates O
for O
the O
Adverb O
type O
. O
The O
prenominal O
modifier O
must O
be O
an O
adjective O
. O
It O
is O
paraphrased O
as O
an O
adverb O
that O
pertains O
to O
itself O
, O
according O
to O
WordNet O
( O
Fellbaum O
, O
2010 O
) O
; O
or O
an O
adverb O
that O
is O
derivationally O
related O
to O
itself O
, O
according O
to O
CatVar O
( O
Habash O
and O
Dorr O
, O
2003 O
) O
. O
The O
adverb O
( O
B O
) O
is O
placed O
either O
before O
the O
verb O
( O
BVO O
, O
OBV O
) O
or O
at O
the O
end O
of O
the O
clause O
( O
VOB O
, O
OVB O
) O
. O

Nil O

There O
are O
, O
by O
definition O
, O
no O
obvious O
paraphrase O
candidates O
to O
represent O
inputs O
of O
the O
Nil O
type O
. O
We O
implemented O
the O
following O
alternatives O
: O

Identity O
The O
nominalization O
input O
itself O
. O

Light O
verb O

The O
paraphrase O
retains O
the O
nominalization O
as O
the O
object O
of O
a O
light O
verb O
or O
support O
verb O
( O
Grefenstette O
and O
Teufel O
, O
1995 O
) O
. O
One O
paraphrase O
candidate O
prepends O
the O
light O
verb O
; O
e.g. O
, O
" O
home O
run O
against O
Arizona O
" O
→ O
" O
hit O
a O
home O
run O
against O
Arizona O
" O
. O
The O
other O
candidate O
uses O
the O
prepositional O
object O
( O
O O
) O
as O
subject O
; O
e.g. O
, O
" O
stellar O
performance O
of O
the O
rookies O
" O
→ O
" O
the O
rookies O
give O
a O
stellar O
performance O
" O
. O

Predicative O
adjective O
Limited O
to O
prenominal O
modifiers O
that O
are O
adjectives O
, O
this O
paraphrase O
uses O
the O
adjective O
predicatively O
to O
form O
a O
clause O
. O
The O
paraphrase O
is O
designed O
, O
on O
the O
one O
hand O
, O
to O
be O
acceptable O
for O
Nil O
type O
inputs O
, O
e.g. O
, O
" O
stellar O
performance O
of O
the O
rookies O
" O
→ O
" O
the O
performance O
of O
the O
rookies O
is O
stellar O
" O
; O
and O
on O
the O
other O
hand O
, O
to O
be O
unacceptable O
for O
( O
Coates O
, O
1971 O
) O
, O
e.g. O
, O
" O
presidential O
nomination O
of O
Harrison O
" O
→ O
" O
the O
nomination O
of O
Harrison O
is O
presidential O
" O
. O

We O
used O
a O
masked O
language O
model O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
to O
generate O
the O
most O
likely O
determiners O
, O
prepositions O
and O
light O
verbs O
for O
the O
paraphrase O
candidates O
above O
. O

Approach O

As O
discussed O
in O
Section O
4.1 O
, O
the O
input O
is O
a O
sentence O
that O
contains O
a O
nominalization O
, O
headed O
by O
a O
deverbal O
noun O
that O
has O
a O
prenominal O
modifier O
and O
a O
prepositional O
phrase O
( O
Table O
1 O
) O
. O
The O
prenominal O
modifier O
may O
be O
a O
noun O
or O
an O
adjective O
. O

We O
first O
overgenerate O
paraphrases O
to O
construct O
a O
candidate O
pool O
( O
Section O
6.1 O
) O
, O
and O
then O
filter O
the O
pool O
by O
considering O
paraphrasability O
( O
Section O
6.2 O
) O
. O
For O
the O
Paraphrase O
Generation O
task O
, O
the O
output O
is O
the O
best O
candidate O
selected O
by O
the O
textual O
entailment O
and O
language O
models O
( O
Section O
6.3 O
) O
. O
For O
the O
Paraphrarasability O
Prediction O
task O
, O
the O
output O
is O
the O
paraphrasability O
label O
associated O
with O
the O
selected O
candidate O
( O
Table O
3 O
) O
. O

Candidate O
pool O
construction O

This O
step O
constructs O
a O
pool O
of O
paraphrase O
candidates O
. O
We O
evaluated O
the O
following O
methods O
: O

All O
Include O
all O
paraphrase O
candidates O
in O
Table O
3 O
. O

Gold O
Include O
only O
those O
paraphrase O
candidates O
associated O
with O
the O
gold O
paraphrasability O
label O
. O

Majority O
baseline O
Include O
only O
those O
paraphrase O
candidates O
associated O
with O
the O
majority O
paraphrasability O
label O
, O
which O
is O
Noun O
in O
our O
dataset O
. O
This O
baseline O
replicates O
the O
algorithm O
proposed O
by O
Lee O
et O
al O
. O
( O
2021 O
) O
, O
which O
considers O
only O
the O
MVO O
, O
OVM O
, O
VMO O
, O
VOM O
, O
and O
MOV O
paraphrases O
in O
Table O
3 O
. O

Word O
frequency O
baseline O
Include O
the O
Noun O
type O
( O
Adverb O
type O
) O
paraphrase O
candidates O
only O
when O
the O
noun O
( O
the O
adverb O
) O
corresponding O
to O
the O
prenominal O
modifier O
has O
high O
frequency O
. O

The O
frequency O
threshold O
is O
optimized O
on O
our O
dataset O
based O
on O
frequency O
statistics O
in O
the O
Google O
Web O
1 O
T O
N O
- O
gram O
Corpus O
( O
Brants O
and O
Franz O
, O
2006 O
) O
. O

Candidate O
pool O
filtering O

This O
step O
filters O
the O
candidate O
pool O
constructed O
by O
the O
All O
model O
. O
We O
evaluated O
two O
methods O
that O
consider O
paraphrasability O
through O
semantic O
parsing O
and O
literality O
prediction O
, O
respectively O
. O

Filtering O
with O
AMR O

Abstract O
Meaning O
Representation O
( O
AMR O
) O
abstracts O
away O
from O
the O
syntactic O
realization O
of O
a O
sentence O
and O
expresses O
its O
meaning O
with O
a O
directed O
acyclic O
graph O
, O
where O
nodes O
represent O
events O
and O
concepts O
, O
and O
edges O
represent O
relationships O
between O
the O
nodes O
( O
Banarescu O
et O
al O
. O
, O
2013 O
) O
. O
In O
an O
AMR O
graph O
, O
deverbal O
nouns O
are O
annotated O
as O
verbs O
, O
and O
adjectives O
pertaining O
to O
nouns O
are O
annotated O
in O
their O
nominal O
form O
whenever O
possible O
. O
We O
use O
PERIN O
( O
Samuel O
and O
Straka O
, O
2020 O
) O
to O
construct O
an O
AMR O
graph O
for O
the O
input O
sentence O
, O
and O
align O
the O
nodes O
with O
the O
words O
in O
the O
input O
. O
We O
will O
refer O
to O
the O
node O
aligned O
to O
the O
deverbal O
noun O
as O
the O
" O
deverbal O
noun O
node O
" O
; O
and O
the O
node O
aligned O
to O
the O
prenominal O
modifier O
as O
the O
" O
prenominal O
modifier O
node O
" O
. O
In O
Figure O
1 O
, O
the O
" O
constitution O
" O
node O
is O
the O
prenominal O
modifier O
node O
( O
aligned O
to O
" O
constitutional O
" O
in O
the O
input O
) O
; O
and O
the O
" O
interpret-01 O
" O
node O
is O
the O
deverbal O
noun O
node O
( O
aligned O
to O
" O
interpretation O
" O
) O
. O

The O
All+AMR O
model O
predicts O
Noun O
as O
paraphrasability O
label O
and O
removes O
all O
non O
- O
Noun O
type O
candidates O
from O
the O
pool O
if O
the O
prenominal O
modifier O
node O
: O

• O
is O
an O
argument O
of O
the O
deverbal O
noun O
node O
; O
or O
• O
is O
the O
domain O
of O
the O
deverbal O
noun O
node O
, O
and O
is O
annotated O
as O
a O
noun O
. O

Otherwise O
, O
it O
predicts O
paraphrasability O
to O
be O
non O
- O
Noun O
and O
removes O
all O
Noun O
type O
candidates O
from O
the O
pool O
. O

For O
example O
, O
the O
model O
predicts O
Noun O
as O
paraphrasability O
label O
for O
the O
sentence O
in O
Figure O
1 O
, O
since O
the O
prenominal O
modifier O
node O
( O
" O
constitution O
" O
) O
serves O
as O
arg0 O
to O
the O
deverbal O
noun O
node O
( O
" O
interpret-01 O
" O
) O
. O
The O
model O
rejects O
Noun O
as O
paraphrasability O
Figure O
2 O
: O
AMR O
graph O
of O
the O
sentence O
" O
... O
the O
secular O
celebration O
of O
Christmas O
" O
, O
which O
is O
predicted O
as O
non O
- O
Noun O
by O
the O
All+AMR O
model O
( O
Section O
6.2.1 O
) O
label O
for O
the O
sentence O
in O
Figure O
2 O
. O
Even O
though O
the O
prenominal O
modifier O
node O
( O
" O
secular O
" O
) O
is O
the O
domain O
of O
the O
deverbal O
noun O
node O
( O
" O
celebrate-02 O
" O
) O
, O
it O
is O
annotated O
with O
the O
original O
adjective O
rather O
than O
its O
nominal O
form O
. O

Filtering O
with O
noun O
literality O
prediction O

Noun O
literality O
prediction O
is O
closely O
related O
to O
paraphrasability O
prediction O
( O
Section O
3.1 O
) O
. O
We O
use O
Lex O
- O
Comp O
( O
Shwartz O
and O
Dagan O
, O
2019 O
) O
, O
a O
state O
- O
of O
- O
theart O
model O
in O
noun O
literality O
prediction O
, O
which O
is O
trained O
on O
datasets O
from O
Reddy O
et O
al O
. O
( O
2011 O
) O
and O
Tratz O
et O
al O
. O
( O
2010 O
) O
using O
contextualized O
word O
embeddings O
. O
2 O
Given O
the O
prenominal O
modifier O
and O
the O
deverbal O
noun O
in O
the O
input O
, O
the O
All+LexComp O
model O
predicts O
Noun O
as O
paraphrasability O
label O
if O
LexComp O
predicts O
" O
literal O
" O
, O
and O
removes O
all O
non O
- O
Noun O
type O
candidates O
. O
Otherwise O
, O
it O
predicts O
Nil O
and O
keeps O
only O
the O
Nil O
type O
paraphrases O
. O
This O
model O
does O
not O
perform O
filtering O
on O
an O
input O
with O
an O
adjectival O
modifier O
. O

Candidate O
selection O

A O
textual O
entailment O
model O
( O
TE O
) O
, O
enhanced O
with O
re O
- O
ranking O
by O
language O
model O
scores O
, O
was O
found O
to O
yield O
the O
strongest O
performance O
in O
paraphrase O
generation O
for O
compound O
nominalizations O
( O
Lee O
et O
al O
. O
, O
2021 O
) O
. O
Taking O
the O
nominalization O
input O
as O
the O
premise O
and O
a O
paraphrase O
candidate O
as O
the O
hypothesis O
, O
the O
TE O
model O
predicts O
whether O
the O
facts O
in O
the O
former O
imply O
those O
in O
the O
latter O
. O
Among O
the O
three O
candidate O
paraphrases O
that O
yield O
the O
highest O
TE O
scores O
, O
the O
candidate O
with O
the O
highest O
language O
model O
( O
LM O
) O
score O
is O
selected O
. O

We O
replicate O
this O
algorithm O
and O
apply O
it O
on O
the O
filtered O
candidate O
pool O
. O
We O
use O
the O
AllenNLP O
textual O
entailment O
model O
3 O
, O
and O
the O
log O
- O
probability O
score O
based O
on O
GPT-2 O
( O
117 O
M O
) O
as O
the O
LM O
score O
( O
Salazar O
et O
al O
. O
, O
2020 O
) O
. O
4 O
7 O
Experimental O
set O
- O
up O
and O
metrics O

The O
entire O
dataset O
was O
used O
for O
evaluation O
since O
our O
approach O
is O
unsupervised O
. O
We O
used O
SpaCy O
( O
Honnibal O
and O
Johnson O
, O
2015 O
) O
for O
POS O
tagging O
to O
determine O
the O
POS O
of O
the O
prenominal O
modifier O
. O

Paraphrasability O
prediction O
. O
We O
report O
precision O
, O
recall O
and O
F O
1 O
. O
Precision O
is O
defined O
as O
the O
number O
of O
actual O
Noun O
instances O
, O
out O
of O
all O
instances O
predicted O
by O
the O
system O
as O
Noun O
. O
Recall O
is O
defined O
as O
the O
proportion O
of O
gold O
Noun O
instances O
that O
are O
correctly O
identified O
by O
the O
system O
as O
Noun O
. O

Paraphrase O
generation O
. O
We O
report O
" O
paraphrase B-MetricName
accuracy I-MetricName
" O
and O
" O
word B-MetricName
order I-MetricName
accuracy I-MetricName
" O
as O
defined O
in O
Lee O
et O
al O
. O
( O
2021 O
) O
. O
For O
the O
former O
, O
the O
determiners O
are O
removed O
from O
all O
paraphrases O
. O
The O
system O
is O
considered O
correct O
if O
the O
lemmatized O
form O
of O
all O
words O
in O
the O
predicted O
paraphrase O
are O
identical O
with O
those O
in O
the O
gold O
paraphrase O
. O
The O
latter O
is O
defined O
likewise O
, O
except O
that O
prepositions O
are O
not O
taken O
into O
consideration O
. O
It O
thus O
essentially O
measures O
the O
system O
's O
ability O
to O
predict O
the O
verb O
and O
arguments O
and O
to O
put O
them O
into O
the O
correct O
word O
order O
. O
The O
word O
orders O
VOB O
/ O
BVO O
and O
OVB O
/ O
OBV O
are O
considered O
interchangeable O
. O

Results O

Table O
4 O
shows O
system O
performance O
on O
the O
paraphrasability O
prediction O
( O
Section O
8.1 O
) O
and O
its O
effect O
on O
paraphrase O
generation O
( O
Section O
8.2 O
) O
. O

Paraphrasability O
prediction O

Given O
the O
preponderance O
of O
the O
Noun O
label O
in O
our O
dataset O
, O
the O
Majority B-MethodName
baseline I-MethodName
produced O
a O
strong O
performance O
at O
0.673 B-MetricValue
precision B-MetricName
and O
perfect O
recall O
. O
It O
outperforms O
the O
Word O
Frequency O
baseline O
, O
which O
has O
slightly O
higher O
precision B-MetricName
( O
0.686 B-MetricValue
) O
but O
lower O
recall B-MetricName
( O
0.911 B-MetricValue
) O
, O
both O
in O
terms O
of O
F O
1 O
and O
accuracy O
. O

Using O
all O
paraphrase O
candidates O
resulted O
in O
an O
improvement O
in O
binary B-MetricName
classification I-MetricName
accuracy I-MetricName
( O
0.711 B-MetricValue
vs. O
0.673 B-MetricValue
) O
over O
the O
Majority B-MethodName
baseline I-MethodName
, O
demonstrating O
the O
effectiveness O
of O
the O
Adverb O
and O
Nil O
paraphrases O
( O
Section O
5.2 O
- O
5.3 O
) O
. O
In O
terms O
of O
three O
- O
way O
classification O
, O
however O
, O
it O
offered O
no O
improvement O
over O
the O
Majority B-MethodName
baseline I-MethodName
( O
0.671 B-MetricValue
vs. O
0.673 B-MetricValue
) O
. O
This O
indicates O
that O
while O
the O
candidate O
selection O
method O
( O
Section O
6.3 O
) O
can O
correctly O
detect O
some O
Noun O
type O
candidates O
as O
inappropriate O
, O
it O
is O
less O
competent O
in O
judging O
between O
Adverb O
vs. O
Nil O
paraphrases O
. O

The O
All+LexComp B-MethodName
model O
raised O
the O
accuracy B-MetricName
by O
only O
0.4 B-MetricValue
% I-MetricValue
in O
comparison O
to O
the O
All B-MethodName
model O
. O
This O
result O
suggests O
that O
noun O
literality O
prediction O
is O
only O
slightly O
helpful O
as O
a O
proxy O
for O
paraphrasability O
. O

The O
All+AMR B-MethodName
model O
achieved O
the O
highest O
F B-MetricName
1 I-MetricName
( O
0.852 B-MetricValue
) O
by O
raising O
both O
the O
precision B-MetricName
and O
recall B-MetricName
of O
the O
All B-MethodName
model O
. O
The O
improvement O
is O
statistically O
significant O
in O
terms O
of O
both O
binary O
classification O
( O
0.782 B-MetricValue
) O
5 O
and O
three O
- O
way O
classification O
( O
0.744 B-MetricValue
) O
6 O
. O
These O
results O
show O
that O
AMR O
is O
useful O
for O
predicting O
paraphrasability O
, O
which O
may O
be O
due O
to O
the O
more O
fine O
- O
grained O
semantic O
information O
in O
the O
AMR O
graphs O
that O
could O
not O
be O
inferred O
by O
the O
LM O
and O
TE O
models O
in O
the O
candidate O
selection O
step O
. O
The O
improvement O
of O
the O
All+AMR B-MethodName
model O
over O
the O
All+LexComp B-MethodName
model O
is O
also O
significant O
7 O
, O
likely O
because O
the O
semantic O
features O
in O
the O
AMR O
graphs O
are O
more O
relevant O
to O
paraphrasability O
than O
literality O
. O

Table O
5 O
shows O
the O
paraphrasability O
labels O
predicted O
by O
the O
All+AMR B-MethodName
model O
. O
While O
it O
was O
able O
to O
identify O
most O
of O
the O
Noun O
inputs O
, O
it O
did O
so O
for O
only O
half O
of O
the O
Adverb O
ones O
. O
The O
most O
challenging O
turned O
out O
to O
be O
the O
Nil O
inputs O
, O
which O
the O
model O
succeeded O
in O
detecting O
less O
than O
one O
- O
third O
of O
the O
time O
. O

Paraphrase O
generation O

Despite O
its O
higher O
accuracy B-MetricName
in I-MetricName
paraphrasability I-MetricName
prediction I-MetricName
, O
the O
Majority O
baseline O
( O
0.264 B-MetricValue
paraphrase B-MetricName
accuracy I-MetricName
) O
performed O
worse O
than O
the O
Word O
Frequency O
baseline O
( O
0.275 B-MetricValue
) O
in O
paraphrase O
generation O
. O
This O
likely O
reflects O
the O
greater O
challenge O
in O
identifying O
the O
correct O
Noun O
type O
paraphrases O
than O
the O
Advice O
and O
Nil O
types O
. O

For O
the O
other O
models O
, O
performance O
in O
para- O
phrasability O
prediction O
is O
largely O
correlated O
to O
paraphrase O
generation O
. O
The O
All O
model O
improved O
over O
the O
Majority B-MethodName
baseline I-MethodName
both O
in O
terms O
of O
word O
order O
( O
0.416 B-MetricValue
) O
and O
paraphrase B-MetricName
accuracy I-MetricName
( O
0.318 B-MetricValue
) O
. O
Consider O
the O
input O
" O
... O
the O
apocalyptic O
destruction O
of O
the O
town O
and O
the O
cult O
" O
. O
The O
All O
model O
correctly O
declined O
to O
paraphrase O
( O
null O
output O
) O
on O
the O
basis O
of O
the O
high O
score O
secured O
by O
the O
light O
- O
verb O
paraphrase O
" O
the O
town O
and O
the O
cult O
suffer O
an O
apocalyptic O
destruction O
" O
. O
In O
contrast O
, O
the O
Majority B-MethodName
baseline I-MethodName
produced O
the O
inappropriate O
paraphrase O
" O
the O
apocalypses O
destroy O
the O
town O
and O
the O
cult O
" O
. O
The O
All+AMR B-MethodName
model O
again O
offered O
the O
best O
performance O
, O
at O
0.462 B-MetricValue
word B-MetricName
order I-MetricName
accuracy I-MetricName
and O
0.347 B-MetricValue
paraphrase B-MetricName
accuracy I-MetricName
. O
8 O
For O
the O
sentence O
in O
Figure O
1 O
, O
the O
All B-MethodName
model O
generated O
the O
predicative O
adjective O
paraphrase O
" O
interpretation O
of O
law O
is O
constitutional O
" O
due O
to O
the O
high O
LM O
score O
, O
even O
though O
the O
word O
" O
constitutional O
" O
yields O
a O
different O
meaning O
in O
this O
paraphrase O
. O
The O
All+AMR B-MethodName
model O
was O
able O
to O
reject O
this O
paraphrase O
since O
the O
word O
" O
constitution O
" O
was O
inferred O
to O
play O
the O
subject O
role O
. O
Conversely O
, O
in O
Figure O
2 O
, O
the O
model O
was O
able O
to O
reject O
paraphrases O
involving O
the O
noun O
" O
secularism O
" O
, O
since O
the O
AMR O
parser O
annotated O
with O
the O
original O
adjective O
" O
secu- O
8 O
The O
improvement O
in O
word O
order O
accuracy O
is O
statistically O
significant O
over O
the O
All O
and O
All+LexComp B-MethodName
models O
at O
p O
= O
0.0211 O
and O
p O
= O
0.0482 O
, O
respectively O
. O
The O
improvement O
in O
paraphrase O
accuracy O
is O
not O
significant O
, O
however O
, O
at O
p O
= O
0.0970 O
and O
p O
= O
0.428 O
against O
the O
All O
and O
All+LexComp B-MethodName
models O
, O
respectively O
. O

lar O
" O
. O
The O
considerable O
performance O
gap O
from O
the O
Gold B-MethodName
model I-MethodName
( O
paraphrase B-MetricName
accuracy I-MetricName
0.591 B-MetricValue
) O
, O
however O
, O
indicates O
there O
is O
still O
much O
room O
for O
improvement O
in O
interpreting O
nominalizations O
. O

Conclusion O

A O
clausal O
paraphrase O
can O
help O
disambiguate O
a O
nominalization O
semantically O
, O
especially O
when O
the O
prenominal O
modifier O
is O
difficult O
to O
interpret O
. O
This O
paper O
has O
presented O
the O
first O
study O
on O
determining O
the O
paraphrasability O
of O
the O
prenominal O
modifier O
in O
a O
compound O
nominalization O
. O
We O
have O
expanded O
an O
existing O
dataset O
to O
cover O
cases O
when O
the O
prenominal O
modifier O
can O
appear O
as O
a O
noun O
in O
the O
paraphrase O
, O
as O
an O
adverb O
, O
or O
not O
at O
all O
. O

Our O
experiments O
suggest O
that O
overgeneration O
of O
paraphrase O
candidates O
, O
followed O
by O
ranking O
with O
a O
textual O
entailment O
model O
and O
language O
model O
, O
can O
yield O
competitive O
results O
. O
Further O
, O
AMR O
- O
based O
features O
lead O
to O
statistically O
significant O
improvement O
in O
performance O
. O

A O
limitation O
of O
our O
study O
is O
the O
restricted O
syntactic O
form O
of O
the O
input O
nominalizations O
. O
To O
facilitate O
more O
comprehensive O
evaluation O
, O
future O
research O
should O
consider O
expanding O
the O
dataset O
further O
to O
cover O
a O
wider O
range O
of O
nominalizations O
, O
and O
richer O
variations O
in O
their O
clausal O
paraphrases O
. O

Acknowledgments O

This O
project O
was O
supported O
by O
an O
HKSAR O
UGC O
Teaching O
Learning O
Grant O
( O
Meeting O
the O
Challenge O
of O
Teaching O
and O
Learning O
Language O
in O
the O
University O
: O
Enhancing O
Linguistic O
Competence O
and O
Performance O
in O
English O
and O
Chinese O
, O
2016 O
- O
19 O
Triennium O
) O
. O

Prompt O
Tuning O
Pushes O
Farther O
, O
Contrastive O
Learning O
Pulls O
Closer O
: O
A O
Two O
- O
Stage O
Approach O
to O
Mitigate B-TaskName
Social I-TaskName
Biases I-TaskName

As O
the O
representation O
capability O
of O
Pre O
- O
trained O
Language O
Models O
( O
PLMs O
) O
improve O
, O
there O
is O
growing O
concern O
that O
they O
will O
inherit O
social O
biases O
from O
unprocessed O
corpora O
. O
Most O
previous O
debiasing O
techniques O
used O
Counterfactual B-MethodName
Data I-MethodName
Augmentation I-MethodName
( O
CDA B-MethodName
) O
to O
balance O
the O
training O
corpus O
. O
However O
, O
CDA B-MethodName
slightly O
modifies O
the O
original O
corpus O
, O
limiting O
the O
representation O
distance O
between O
different O
demographic O
groups O
to O
a O
narrow O
range O
. O
As O
a O
result O
, O
the O
debiasing O
model O
easily O
fits O
the O
differences O
between O
counterfactual O
pairs O
, O
which O
affects O
its O
debiasing O
performance O
with O
limited O
text O
resources O
. O
In O
this O
paper O
, O
we O
propose O
an O
adversarial O
training O
- O
inspired O
two O
- O
stage O
debiasing O
model O
using O
Contrastive B-MethodName
learning I-MethodName
with I-MethodName
Continuous I-MethodName
Prompt I-MethodName
Augmentation I-MethodName
( O
named O
CCPA B-MethodName
) O
to O
mitigate O
social O
biases O
in O
PLMs O
' O
encoding O
. O
In O
the O
first O
stage O
, O
we O
propose O
a O
data O
augmentation O
method O
based O
on O
continuous O
prompt O
tuning O
to O
push O
farther O
the O
representation O
distance O
between O
sample O
pairs O
along O
different O
demographic O
groups O
. O
In O
the O
second O
stage O
, O
we O
utilize O
contrastive O
learning O
to O
pull O
closer O
the O
representation O
distance O
between O
the O
augmented O
sample O
pairs O
and O
then O
fine O
- O
tune O
PLMs O
' O
parameters O
to O
get O
debiased O
encoding O
. O
Our O
approach O
guides O
the O
model O
to O
achieve O
stronger O
debiasing O
performance O
by O
adding O
difficulty O
to O
the O
training O
process O
. O
Extensive O
experiments O
show O
that O
CCPA B-MethodName
outperforms O
baselines O
in O
terms O
of O
debiasing O
performance O
. O
Meanwhile O
, O
experimental O
results O
on O
the O
GLUE B-DatasetName
benchmark O
show O
that O
CCPA B-MethodName
retains O
the O
language O
modeling O
capability O
of O
PLMs O
. O

Introduction O

Pre O
- O
trained O
Language O
Models O
( O
PLMs O
) O
have O
demonstrated O
outstanding O
performance O
in O
recent O
years O
and O
have O
been O
widely O
used O
in O
natural O
language O
understanding O
tasks O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Delobelle O
et O
al O
. O
, O
2022 O
) O
. O
However O
, O
the O
powerful O
language O
modeling O
capability O
enables O
PLMs O
to O
learn O
good O
representations O
from O
large O
- O
scale O
training O
corpora O
while O
capturing O
human O
- O
like O
social O
biases O
. O
Recent O
studies O
have O
demonstrated O
that O
the O
representations O
encoded O
by O
PLMs O
learn O
social O
biases O
specific O
to O
demographic O
groups O
( O
e.g. O
, O
gender O
, O
race O
, O
religion O
) O
and O
can O
be O
amplified O
to O
downstream O
tasks O
, O
leading O
to O
unfair O
outcomes O
and O
adverse O
social O
effects O
( O
Zhao O
et O
al O
. O
, O
2019 O
; O
Webster O
et O
al O
. O
, O
2020 O
) O
. O
As O
a O
result O
, O
mitigating B-TaskName
social I-TaskName
biases I-TaskName
in I-TaskName
PLMs I-TaskName
' I-TaskName
encoding I-TaskName
can O
improve O
the O
fairness O
of O
NLP O
systems O
significantly O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Bender O
and O
Friedman O
, O
2018 O
) O
. O

Most O
existing O
debiasing O
techniques O
first O
need O
to O
construct O
sample O
pairs O
using O
Counterfactual B-MethodName
Data I-MethodName
Augmentation I-MethodName
( O
CDA B-MethodName
) O
( O
Zmigrod O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2022 O
) O
to O
balance O
the O
training O
corpora O
. O
The O
general O
approach O
of O
CDA B-MethodName
is O
to O
replace O
the O
original O
corpus O
with O
attribute O
words O
( O
e.g. O
, O
he O
/ O
she O
, O
man O
/ O
woman O
) O
specific O
to O
different O
demographic O
groups O
. O
For O
example O
, O
RCDA O
uses O
a O
generator O
to O
generate O
a O
large O
number O
of O
antisense O
sentences O
and O
then O
uses O
a O
discriminator O
to O
evaluate O
the O
quality O
of O
the O
original O
and O
antisense O
samples O
jointly O
. O
FairFil B-MethodName
( O
Cheng O
et O
al O
. O
, O
2021 O
) O
obtains O
a O
pair O
of O
positive O
sample O
sentences O
by O
replacing O
the O
attribute O
words O
in O
the O
training O
corpora O
with O
the O
antonyms O
and O
then O
uses O
contrastive O
learning O
to O
train O
a O
filter O
for O
debiasing O
. O
Auto B-MethodName
- I-MethodName
Debias I-MethodName
( O
Guo O
et O
al O
. O
, O
2022 O
) O
uses O
pairs O
of O
attribute O
words O
as O
training O
corpora O
, O
amplifies O
the O
bias O
between O
sample O
pairs O
by O
searching O
biased O
prompt O
texts O
in O
the O
Wikipedia O
vocabulary O
, O
and O
then O
performs O
semantic O
alignment O
using O
Jensen O
- O
Shannon O
divergence O
. O
These O
methods O
aim O
to O
mitigate B-TaskName
social I-TaskName
biases I-TaskName
between O
different O
demographic O
groups O
by O
narrowing O
the O
representation O
distance O
between O
sample O
pairs O
. O
However O
, O
CDA B-MethodName
slightly O
modifies O
the O
original O
corpus O
, O
limiting O
the O
representation O
distance O
between O
different O
demographic O
groups O
to O
a O
narrow O
range O
. O
As O
a O
result O
, O
the O
debiasing O
model O
is O
easy O
to O
overfit O
the O
difference O
between O
counterfactual O
pairs O
, O
which O
affects O
its O
learning O
ability O
with O
limited O
text O
resources O
. O
As O
shown O
in O
Figure O
1 O
, O
it O
is O
difficult O
for O
PLMs O
to O
achieve O
the O
ideal O
debiasing O
performance O
for O
newly O
input O
samples O
with O
greater O
difficulty O
. O

In O
this O
work O
, O
we O
propose O
a O
two O
- O
stage O
debiasing O
method O
using O
Contrastive B-MethodName
learning I-MethodName
with I-MethodName
Continuous I-MethodName
Prompt I-MethodName
Augmentation I-MethodName
( O
named O
CCPA B-MethodName
) O
to O
mitigate B-TaskName
social I-TaskName
biases I-TaskName
in I-TaskName
PLMs I-TaskName
' I-TaskName
encoding I-TaskName
. O
Inspired O
by O
adversarial O
training O
, O
our O
approach O
improves O
the O
debiasing O
ability O
of O
PLMs O
by O
first O
amplifying O
and O
then O
attenuating O
the O
bias O
between O
different O
demographic O
groups O
. O
Specifically O
, O
we O
first O
use O
CDA B-MethodName
to O
replace O
attribute O
words O
in O
the O
original O
training O
corpus O
to O
construct O
counterfactual O
pairs O
corresponding O
to O
different O
demographic O
groups O
. O
In O
the O
first O
stage O
, O
we O
augment O
the O
positive O
sample O
pairs O
with O
continuous O
prompt O
tuning O
to O
increase O
the O
distance O
between O
them O
to O
amplify O
the O
biases O
between O
different O
demographic O
groups O
. O
In O
the O
second O
stage O
, O
we O
utilize O
contrastive O
learning O
to O
pull O
the O
distance O
between O
the O
positive O
sample O
pairs O
to O
attenuate O
the O
biases O
between O
different O
demographic O
groups O
. O
CCPA B-MethodName
increases O
the O
difficulty O
of O
model O
fitting O
by O
expanding O
the O
representation O
space O
between O
sample O
pairs O
. O
We O
believe O
that O
difficult O
learning O
experiences O
make O
the O
model O
more O
powerful O
, O
thus O
improving O
the O
debiasing O
ability O
of O
PLMs O
training O
in O
corpora O
with O
limited O
resources O
. O
Our O
main O
contributions O
are O
as O
follows O
: O

• O
We O
propose O
the O
CCPA B-MethodName
debiasing O
framework O
that O
combines O
prompt O
tuning O
and O
contrastive O
learning O
to O
learn B-TaskName
a I-TaskName
debiased I-TaskName
PLM I-TaskName
representation I-TaskName
. O
The O
PLM O
's O
parameters O
are O
fixed O
in O
the O
first O
stage O
, O
and O
a O
generator O
encoding O
continuous O
prompts O
is O
trained O
. O
In O
the O
second O
stage O
, O
the O
prompts O
are O
fixed O
, O
and O
the O
PLM O
's O
parameters O
are O
fine O
- O
tuned O
using O
contrastive O
learning O
. O

• O
We O
propose O
data O
augmentation O
using O
continuous O
prompts O
to O
achieve O
excellent O
debiasing O
performance O
using O
small O
training O
data O
rather O
than O
relying O
on O
a O
large O
external O
corpus O
. O
Given O
that O
continuous O
prompts O
may O
cause O
the O
representation O
distance O
between O
sample O
pairs O
to O
be O
too O
far O
apart O
, O
causing O
the O
semantic O
space O
to O
degrade O
, O
we O
propose O
constraining O
the O
prompt O
tuning O
using O
the O
Mahalanobis O
Distance O
to O
keep O
the O
semantic O
space O
as O
stable O
as O
possible O
. O

• O
We O
train O
CCPA B-MethodName
on O
several O
real O
- O
world O
corpora O
and O
mitigate O
bias O
on O
the O
most O
common O
gender O
bias O
. O
The O
results O
on O
BERT O
and O
DistilBERT O
show O
that O
CCPA B-MethodName
is O
superior O
to O
state O
- O
of O
- O
the O
- O
art O
models O
. O
In O
addition O
, O
we O
test O
the O
downstream O
tasks O
on O
the O
GLUE B-DatasetName
benchmark O
, O
and O
show O
that O
CCPA B-MethodName
retains O
the O
language O
modeling O
capability O
while O
improving O
the O
PLMs O
' O
fairness O
. O

Methodology O

In O
this O
section O
, O
we O
propose O
the O
Contrastive B-MethodName
learning I-MethodName
with I-MethodName
Continuous I-MethodName
Prompt I-MethodName
Augmentation I-MethodName
( O
CCPA B-MethodName
) O
framework O
to O
mitigate O
the O
social O
bias O
in O
the O
encoding O
of O
PLMs O
specific O
to O
the O
most O
common O
gender O
bias O
. O
Our O
proposed O
CCPA B-MethodName
consists O
of O
two O
stages O
: O
1 O
) O
Continuous O
Prompt O
Tuning O
and O
2 O
) O
Fine O
- O
Tuning O
with O
Contrastive O
Learning O
. O
The O
framework O
of O
CCPA B-MethodName
is O
shown O
in O
Figure O
2 O
. O

Pre O
- O
Processing O
based O
on O
CDA B-MethodName

First O
, O
we O
pre O
- O
process O
the O
training O
corpus O
with O
imbalanced O
samples O
using O
Counterfactual B-MethodName
Data I-MethodName
Augmentation I-MethodName
( O
CDA B-MethodName
) O
. O
Given O
a O
list O
of O
attribute O
words O
specific O
to O
gender O
bias O
, O
1 O
for O
each O
attribute O
word O
( O
e.g. O
, O
male O
/ O
female O
) O
, O
we O
match O
sentences O
containing O
an O
attribute O
word O
in O
the O
training O
corpus O
. O
The O
attribute O
word O
is O
then O
replaced O
with O
the O
opposite O
word O
in O
a O
different O
gender O
direction O
( O
e.g. O
, O
male O
is O
replaced O
by O
female O
) O
, O
leaving O
the O
other O
words O
unchanged O
. O
Then O
, O
we O
get O
the O
pre O
- O
processed O
train- O

ing O
corpus O
S O
= O
{ O
( O
s O
1 O
, O
s O
′ O
1 O
) O
, O
( O
s O
2 O
, O
s O
′ O
2 O
) O
, O
• O
• O
• O
, O
( O
s O
N O
, O
s O
′ O
N O
) O
} O
consists O
of O
N O
counterfactual O
pairs O
( O
s O
i O
, O
s O
′ O
i O
) O
along O
different O
gender O
directions O
. O
[ O
1 O
] O
[ O
2 O
] O
⋯ O
[ O
] O
⊕ O
Women O
always O
get O
into O
situations O
. O
[ O
1 O
] O
[ O
2 O
] O
⋯ O
[ O
] O
⊕ O
Men O
always O
get O
into O
situations O
. O
Prompt O
Generator O
G O
( O
• O
) O
⋯ O
⋯ O
[ O
1 O
] O
[ O
2 O
] O
⋯ O
[ O
] O
= O
+ O
× O
[ O
1 O
] O
[ O
2 O
] O
⋯ O
[ O
] O
⊕ O

Women O
always O
get O
into O
situations O
. O

1 O
] O
[ O
2 O
] O
⋯ O
[ O
] O
⊕ O
Men O
always O
get O
into O
situations O
. O
Prompt O
Generator O
G O
( O
• O
) O
⋯ O
⋯ O
[ O
1 O
] O
[ O
2 O
] O
⋯ O
[ O
] O
= O
+ O
× O
Gradient O
Descent O
Gradient O
Descent O

Continuous O
Prompt O
Tuning O

Prompt O
- O
based O
learning O
is O
similar O
to O
giving O
instructions O
to O
the O
model O
task O
to O
guide O
the O
model O
learning O
knowledge O
more O
directly O
( O
Petroni O
et O
al O
. O
, O
2019 O
) O
. O
A O
lot O
of O
work O
utilize O
manually O
constructed O
prompts O
Schütze O
, O
2020 O
, O
2021 O
) O
or O
automatically O
searched O
discrete O
prompts O
( O
Shin O
et O
al O
. O
, O
2020 O
) O
to O
assist O
language O
models O
. O
However O
, O
manually O
constructed O
templates O
are O
heavily O
based O
on O
the O
designers O
' O
experience O
and O
automatically O
searched O
prompts O
are O
limited O
by O
the O
search O
space O
( O
Liu O
et O
al O
. O
, O
2021a O
) O
. O
Instead O
of O
limiting O
the O
prompts O
to O
human O
interpretable O
natural O
language O
, O
the O
continuous O
prompts O
( O
Li O
and O
Liang O
, O
2021 O
; O
Zhong O
et O
al O
. O
, O
2021 O
) O
guide O
directly O
within O
the O
embedding O
space O
of O
the O
model O
. O
Meanwhile O
, O
continuous O
prompts O
tune O
their O
parameters O
, O
removing O
the O
constraint O
of O
templates O
being O
parameterized O
by O
PLMs O
' O
parameters O
. O

Inspired O
by O
adversarial O
training O
, O
we O
believe O
that O
increasing O
the O
difficulty O
of O
the O
training O
process O
can O
guide O
the O
model O
in O
acquiring O
a O
stronger O
learning O
ability O
. O
To O
achieve O
this O
goal O
, O
we O
propose O
a O
data O
augmentation O
method O
based O
on O
continuous O
prompt O
tuning O
to O
further O
push O
the O
differences O
between O
counterfactual O
pairs O
. O
Data O
augmentation O
method O
based O
on O
continuous O
prompt O
tuning O
adds O
difficult O
information O
to O
the O
model O
by O
concatenating O
embeddings O
that O
amplify O
bias O
across O
different O
demographic O
groups O
over O
counterfactual O
pairs O
. O

Given O
a O
template O
T O
= O
{ O
[ O
p O
1 O
] O
, O
[ O
p O
2 O
] O
, O
• O
• O
• O
, O
[ O
p O
m O
] O
, O

s O
} O
, O
where O
s O
denotes O
a O
sentence O
, O
[ O
p O
j O
] O
is O
a O
virtual O
token O
represented O
as O
[ O
P O
ROM O
P O
T O
] O
and O
m O
virtual O
tokens O
form O
a O
prompt O
sequence O
P. O
For O
each O
counterfactual O
pair O
( O
s O
i O
, O
s O
′ O
i O
) O
∈ O
S O
obtained O
by O
data O
preprocessing O
, O
we O
concatenate O
the O
same O
prompt O
sequence O
P O
at O
the O
head O
of O
each O
sentence O
( O
see O
Figure O
2 O
) O
. O
The O
augmented O
sample O
pair O
is O
denoted O
by O
( O
ŝ O
i O
, O
ŝ O
i O
′ O
) O
and O
is O
fed O
into O
a O
PLM O
to O
obtain O
the O
sentence O
representation O
. O
Formally O
, O
let O
M O
denote O
a O
PLM O
whose O
encoder O
E O
( O
• O
) O
encodes O
an O
input O
sentenceŝ O
i O
and O
outputs O
a O
sentence O
embedding O

z O
i O
= O
E O
( O
ŝ O
i O
) O
. O
Similarly O
, O
z O
′ O
i O
= O
E O
( O
ŝ O
i O
′ O
) O
. O

In O
order O
to O
obtain O
continuous O
prompt O
embeddings O
, O
we O
train O
a O
generator O
G O
( O
• O
) O
to O
encode O
the O
prompt O
sequence O
P. O
Following O
P O
- O
Tuning O
( O
Liu O
et O
al O
. O
, O
2021b O
) O
, O
we O
choose O
a O
bidirectional O
long O
- O
short O
- O
term O
memory O
network O
( O
LSTM O
) O
, O
which O
consists O
of O
a O
two O
- O
layer O
multilayer O
perceptron O
( O
MLP O
) O
and O
a O
ReLU O
activation O
layer O
. O
The O
embedding O
h O
j O
of O
each O
virtual O
token O
[ O
p O
j O
] O
in O
the O
prompts O
sequence O
is O
encoded O
by O
G O
( O
• O
) O
as O
follows O
: O

h O
j O
= O
G O
( O
[ O
− O
→ O
h O
j O
: O
← O
− O
h O
j O
] O
) O
= O
G O
( O
[ O
LST O
M O
( O
h O
1 O
: O
j O
) O
: O
LST O
M O
( O
h O
j O
: O
m+1 O
) O
] O
) O
. O
( O
1 O
) O

Afterwards O
, O
we O
replace O
the O
continuous O
prompt O
embeddings O
{ O
h O
1 O
, O
h O
2 O
, O
• O
• O
• O
, O
h O
m O
} O
to O
the O
corresponding O
positions O
of O
the O
sentence O
embeddings O
z O
i O
to O
obtain O
the O
sentence O
representations O
pairs O
( O
z O
i O
, O
z O
′ O
i O
) O
. O
In O
this O
stage O
, O
our O
training O
objective O
is O
to O
push O
away O
the O
distance O
of O
representation O
( O
z O
i O
, O
z O
′ O
i O
) O
between O
sample O
pairs O
( O
ŝ O
i O
, O
ŝ O
i O
′ O
) O
. O
Briefly O
, O
we O
take O
the O
Cosine O
Similarity O
between O
sentence O
representations O
as O
the O
loss O
function O
, O
defined O
as O
follows O
: O

L O
cos O
= O
z O
• O
z O
′ O
∥z∥∥z O
′ O
∥ O
= O
n O
i=1 O
z O
i O
• O
z O
′ O
i O
n O
i=1 O
z O
2 O
i O
n O
i=1 O
z O
′2 O
i O
, O
( O
2 O
) O

where O
z O
and O
z O
′ O
denote O
sentence O
representations O
with O
different O
sensitive O
attributes O
within O
a O
batch O
of O
size O
n O
, O
respectively O
. O
The O
representation O
distance O
between O
the O
sample O
pairs O
is O
enlarged O
with O
the O
gradient O
of O
similarity O
decreasing O
, O
thus O
amplifying O
the O
bias O
information O
between O
different O
genders O
. O

Considering O
that O
the O
sentence O
representation O
with O
high O
- O
dimensional O
linear O
distribution O
is O
not O
independently O
and O
equally O
distributed O
among O
the O
dimensions O
, O
only O
relying O
on O
Euclidean O
distance O
training O
may O
cause O
the O
sentence O
representation O
to O
deviate O
from O
the O
original O
distribution O
and O
thus O
destroy O
the O
semantic O
information O
. O
To O
constrain O
the O
change O
of O
sentence O
representation O
within O
the O
original O
distribution O
, O
Mahalanobis O
distance O
is O
taken O
as O
the O
regularization O
term O
of O
the O
loss O
function O
: O

L O
mahal O
= O
( O
z O
− O
S O
) O
⊤ O
Σ O
−1 O
( O
z O
− O
S O
) O
, O
( O
3 O

where O
z O
is O
the O
representation O
of O
a O
batch O
size O
of O
samples O
with O
concatenated O
prompt O
embeddings O
, O
S O
is O
the O
representation O
of O
the O
entire O
pre O
- O
processed O
training O
samples O
without O
concatenated O
prompt O
embeddings O
, O
and O
Σ O
is O
the O
covariance O
matrix O
of O
S. O
Mahalanobis O
distance O
is O
a O
correction O
of O
the O
Euclidean O
distance O
, O
which O
corrects O
the O
assumption O
that O
the O
Euclidean O
distance O
is O
independent O
and O
equally O
distributed O
among O
all O
dimensions O
. O
With O
the O
constraint O
of O
Mahalanobis O
distance O
, O
the O
augmented O
samples O
of O
each O
batch O
can O
vary O
within O
the O
distribution O
range O
of O
the O
original O
training O
data O
to O
maintain O
the O
semantics O
. O

The O
overall O
loss O
function O
of O
the O
continuous O
prompt O
tuning O
stage O
is O
defined O
as O
: O

L O
P O
T O
= O
L O
cos O
+ O
α B-HyperparameterName
× O
L O
mahal O
, O
( O
4 O

where O
α B-HyperparameterName
is O
a O
hyperparameter O
that O
adjusts O
the O
weight O
of O
L O
mahal O
. O
In O
the O
gradient O
descent O
process O
of O
L O
P O
T O
, O
we O
only O
adjust O
the O
parameters O
of O
the O
generator O
G O
( O
• O
) O
and O
fix O
the O
PLMs O
' O
parameters O
to O
obtain O
the O
continuous O
prompt O
embeddings O
that O
further O
amplifies O
the O
bias O
between O
different O
sensitive O
attributes O
. O

Algorithm O
1 O
: O
Proposed O
CCPA B-MethodName
framework O
. O
( O
Gutmann O
and O
Hyvärinen O
, O
2010 O
) O
is O
usually O
used O
as O
a O
contrastive O
loss O
function O
, O
given O
an O
augmented O
sample O
pair O
of O
a O
batch O
{ O
( O
ŝ O
i O
, O
ŝ O
′ O
i O
) O
} O
n O
i=1 O
, O
which O
is O
defined O
as O
follows O
: O

Input O
: O
Pre O
- O
processed O
training O
corpus O
S O
, O
PLM O
encoder O
E O
( O
• O
) O
, O
Initial O
prompt O
generator O
G O
( O
• O
) O
, O
Prompt O
template O
T O
, O
Hyperparameter O
α B-HyperparameterName
, O
β B-HyperparameterName
, O
τ B-HyperparameterName
. O
1 O
while O
stage O
1 O
do O
2 O
Apply O
T O
to O
∀ O
( O
si O
, O
s O
′ O
i O
) O
∈ O
S O
to O
obtain O
( O
ŝi O
, O
ŝ O
′ O
i O
) O
; O
3 O
Obtain O
( O
zi O
, O
z O
′ O
i O
) O
= O
( O
E O
( O
ŝi O
) O
, O
E O
( O
ŝ O
′ O
i O
) O
) O
; O
4 O
Replace O
{ O
h1 O
, O
h2 O
, O
• O
• O
• O
, O
hm O
} O
encoded O
by O
G O
( O
• O
) O

L O
nce O
= O
1 O
n O
n O
i=1 O
log O
e O
sim O
( O
z O
i O
, O
z O
′ O
i O
) O
/ O
τ B-HyperparameterName
1 O
n O
n O
j=1 O
e O
sim O
( O
z O
i O
, O
z O
j O
) O
/ O
τ B-HyperparameterName
, O
( O
5 O
) O
where O
( O
z O
i O
, O
z O
′ O
i O
) O
= O
( O
E O
( O
ŝ O
i O
) O
, O
E O
( O
ŝ O
′ O
i O
) O
) O

, O
τ B-HyperparameterName
is O
a O
temperature O
hyperparameter O
and O
sim O
( O
• O
, O
• O
) O
denotes O
the O
similarity O
function O
usually O
using O
cosine O
similarity O
. O
During O
training O
, O
we O
only O
fine O
- O
tune O
the O
PLMs O
' O
parameters O
and O
fix O
the O
embedding O
of O
continuous O
prompts O
. O
By O
maximizing O
L O
nce O
, O
differences O
in O
the O
encoding O
of O
PLM O
outputs O
specific O
to O
different O
demographic O
groups O
are O
eliminated O
, O
resulting O
in O
representations O
independent O
of O
sensitive O
attributes O
. O

Considering O
that O
the O
attenuation O
of O
biases O
towards O
encoding O
may O
affect O
PLMs O
' O
language O
modeling O
capability O
, O
we O
add O
a O
Masking O
Language O
Modeling O
( O
MLM O
) O
loss O
during O
the O
fine O
- O
tuning O
stage O
to O
aid O
PLM O
training O
( O
He O
et O
al O
. O
, O
2022 O
) O
. O
Following O
previous O
work O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
we O
randomly O
mask O
tokens O
in O
training O
texts O
with O
a O
15 O
% O
probability O
. O
2 O
Our O
objective O
is O
to O
train O
the O
encoder O
to O
predict O
the O
masked O
tokens O
through O
contextual O
semantics O
, O
thereby O
preserving O
the O
language O
modeling O
capability O
of O
PLMs O
. O
The O
overall O
loss O
function O
in O
the O
fine O
- O
tuning O
stage O
is O
defined O
as O
follows O
: O

L O
F O
T O
= O
L O
nce O
+ O
β B-HyperparameterName
× O
L O
mlm O
, O
( O
6 O

where O
β B-HyperparameterName
is O
a O
hyperparameter O
that O
controls O
the O
weight O
of O
L O
mlm O
. O
Our O
overall O
algorithm O
is O
given O
in O
Algorithm O
1 O
. O

Experiments O

In O
this O
section O
, O
we O
conduct O
experiments O
to O
evaluate O
the O
performance O
of O
CCPA B-MethodName
, O
in O
order O
to O
answer O
the O
following O
three O
research O
questions O
. O
Q1 O
. O
How O
effective O
is O
CCPA B-MethodName
in O
mitigating B-TaskName
social I-TaskName
biases I-TaskName
in I-TaskName
PLMs I-TaskName
' I-TaskName
encoding I-TaskName
? O

Q2 O
. O
How O
does O
each O
component O
affect O
CCPA B-MethodName
? O
Q3 O
. O
Will O
CCPA B-MethodName
preserve O
the O
language O
modeling O
capability O
of O
PLMs O
? O

Experimental O
Setup O

Attribute O
Word O
List O
& O
Datasets O

Following O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Liang O
et O
al O
. O
, O
2020 O
; O
Cheng O
et O
al O
. O
, O
2021 O
; O
He O
et O
al O
. O
, O
2022 O
) O
, O
our O
gender O
attribute O
word O
list O
is O
set O
to O
: O
{ O
MALE O
, O
FEMALE O
} O
= O
{ O
( O
man O
, O
woman O
) O
, O
( O
boy O
, O
girl O
) O
, O
( O
he O
, O
she O
) O
, O
( O
father O
, O
mother O
) O
, O
( O
son O
, O
daughter O
) O
, O
( O
guy O
, O
gal O
) O
, O
( O
male O
, O
female O
) O
, O
( O
his O
, O
her O
) O
, O
( O
himself O
, O
herself O
) O
, O
( O
John O
, O
Mary O
) O
} O
. O

Following O
( O
Liang O
et O
al O
. O
, O
2020 O
; O
Cheng O
et O
al O
. O
, O
2021 O
) O
, O
we O
select O
five O
real O
- O
world O
datasets O
as O
the O
initial O
training O
corpus O
, O
which O
are O
Stanford B-DatasetName
Sentiment I-DatasetName
Treebank I-DatasetName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
POM B-DatasetName
( O
Park O
et O
al O
. O
, O
2014 O
) O
, O
WikiText-2 B-DatasetName
( O
Merity O
et O
al O
. O
, O
2017 O
) O
, O
Reddit B-DatasetName
( O
Völske O
et O
al O
. O
, O
2017 O
) O
and O
MELD B-DatasetName
( O
Poria O
et O
al O
. O
, O
2019 O
) O
respectively O
. O
We O
set O
the O
maximum B-HyperparameterName
sentence I-HyperparameterName
length I-HyperparameterName
to O
100 B-HyperparameterValue
, O
and O
the O
pre O
- O
processed O
training O
corpus O
contained O
10,510 O
sentences O
. O

Baselines O
& O
Implementation O
Details O

We O
select O
seven O
recent O
task O
- O
agnostic O
debiasing O
models O
as O
baselines O
. O
CDA B-MethodName
( O
Zmigrod O
et O
al O
. O
We O
perform O
the O
main O
experiments O
on O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
compare O
CCPA B-MethodName
to O
all O
baseline O
models O
. O
We O
also O
test O
debiasing O
performance O
on O
DistilBERT O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
and O
ELEATRA O
( O
Clark O
et O
al O
. O
, O
2020 O
) O
. O
All O
checkpoints O
use O
bert O
- O
base O
- O
uncased O
, O
distilbert O
- O
base O
- O
uncased O
, O
and O
google O
/ O
electra O
- O
base O
- O
generator O
implemented O
by O
Huggingface O
Transformers O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
In O
the O
continuous O
prompt O
tuning O
stage O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
1e B-HyperparameterValue
−5 I-HyperparameterValue
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
64 B-HyperparameterValue
and O
α B-HyperparameterName
= O
0.005 B-HyperparameterValue
. O
Following O
P O
- O
Tuning O
( O
Liu O
et O
al O
. O
, O
2021b O
) O
, O
the O
virtual O
tokens O
template O
of O
continuous O
prompts O
is O
denoted O
as O
a O
triplet O
with O
the O
length O
of O
each O
element O
selected O
on O
{ O
1 O
, O
2 O
, O
3 O
} O
. O
In O
the O
fine O
- O
tuning O
stage O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
1e B-HyperparameterValue
−4 I-HyperparameterValue
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
, O
β B-HyperparameterName
= O
1 B-HyperparameterValue
and O
τ B-HyperparameterName
= O
1 B-HyperparameterValue
. O
We O
report O
the O
average O
of O
the O
results O
of O
three B-HyperparameterValue
runs B-HyperparameterName
over O
20 B-HyperparameterValue
epochs B-HyperparameterName
. O

To O
compare O
the O
baseline O
models O
more O
fairly O
, O
we O
apply O
the O
same O
attribute O
word O
lists O
and O
training O
datasets O
to O
CDA B-MethodName
and O
Dropout B-MethodName
as O
CCPA B-MethodName
. O
The O
implementation O
codes O
for O
CDA B-MethodName
, O
Dropout B-MethodName
, O
Sent B-MethodName
- I-MethodName
Debias I-MethodName
, O
and O
INLP B-MethodName
are O
provided O
by O
( O
Meade O
et O
al O
. O
, O
2022 O
) O
, O
and O
the O
implementation O
codes O
for O
FairFil B-MethodName
and O
Auto B-MethodName
- I-MethodName
Debias I-MethodName
are O
provided O
by O
the O
authors O
. O
For O
MABEL B-MethodName
, O
we O
report O
the O
results O
from O
its O
original O
paper O
. O

Evaluation O
Metrics O

We O
measure O
debiasing O
performance O
using O
the O
common O
three O
internal O
bias O
evaluation O
metrics O
and O
two O
external O
bias O
evaluation O
metrics O
. O

Internal O
Bias O
Evaluation O
Metrics O

Sentence B-MetricName
Encoder I-MetricName
Association I-MetricName
Test I-MetricName
( O
SEAT B-MetricName
) O
( O
May O
et O
al O
. O
, O
2019 O
) O
uses O
sentence O
templates O
to O
evaluate O
the O
association O
between O
different O
sensitive O
attribute O
demographic O
and O
target O
concepts O
. O
Given O
the O
attribute O
word O
lists O
A O
and O
B O
, O
the O
target O
words O
lists O
X O
, O
Y. O
The O
results O
are O
presented O
by O
effect O
size O
, O
defined O
as O
: O

d B-MetricName
= O
µ O
( O
{ O
s O
( O
x O
, O
A O
, O
B O
) O
} O
) O
− O
µ O
( O
{ O
s O
( O
y O
, O
A O
, O
B O
) O
} O
) O
σ O
( O
{ O
s O
( O
t O
, O
X O
, O
Y O
) O
} O
t∈A∪B O
) O
, O
( O
7 O
) O

where O
x O
∈ O
X O
and O
y O
∈ O
Y O
, O
µ O
( O
• O
) O
is O
the O
mean O
function O
and O
σ O
( O
• O
) O
is O
the O
standard O
deviation O
. O
And O
s O
( O
w O
, O
A O
, O
B O
) O
Table O
1 O
: O
Gender O
debiasing O
results O
of O
SEAT B-MetricName
, O
StereoSet B-DatasetName
, O
and O
CrowSPairs B-DatasetName
on O
BERT O
, O
DistilBERT O
, O
and O
ELEATRA O
. O
The O
best O
result O
is O
indicated O
in O
bold O
. O
We O
report O
CCPA B-MethodName
results O
with O
a O
continuous O
prompt O
template O
( O
1 O
, O
1 O
, O
1 O
) O
. O
The O
closer O
the O
effect O
size O
is O
to O
0 O
and O
the O
closer O
SS B-MetricName
is O
to O
50 O
% O
, O
the O
higher O
the O
fairness O
; O
the O
higher O
the O
LM B-MetricName
and O
ICAT B-MetricName
, O
the O
better O
. O

is O
the O
bias O
degree O
defined O
as O
: O
s O
( O
w O
, O
A O
, O
B O
) O
= O
µ O
( O
cos O
( O
w O
, O
a O
) O
) O
− O
µ O
( O
cos O
( O
w O
, O
b O
) O
) O
. O

The O
gender O
- O
specific O
subsets O
of O
SEAT B-MetricName
are O
6 O
, O
6b O
, O
7 O
, O
7b O
, O
8 O
, O
and O
8b O
. O
We O
report O
the O
effect O
size O
of O
debiasing O
models O
on O
each O
subset O
and O
the O
average O
value O
of O
the O
absolute O
value O
of O
the O
six O
subsets O
, O
respectively O
. O
StereoSet B-DatasetName
( O
Nadeem O
et O
al O
. O
, O
2021 O
) O
uses O
the O
fill O
- O
in O
- O
theblank O
template O
to O
investigate O
the O
stereotype O
association O
of O
PLM O
. O
The O
Language B-MetricName
Modeling I-MetricName
Score I-MetricName
( O
LM B-MetricName
) O
is O
the O
percentage O
of O
stereotype O
or O
anti O
- O
stereotype O
words O
selected O
by O
the O
model O
based O
on O
incomplete O
contextual O
sentences O
. O
The O
Stereotype B-MetricName
Score I-MetricName
( O
SS B-MetricName
) O
is O
the O
percentage O
of O
models O
that O
choose O
stereotypes O
over O
anti O
- O
stereotypes O
. O
The O
Idealized B-MetricName
Context I-MetricName
Association I-MetricName
Test I-MetricName
( O
ICAT B-MetricName
) O
is O
a O
comprehensive O
evaluation O
index O
of O
LM B-MetricName
and O
SS B-MetricName
. O
Crowdsourced B-DatasetName
Stereotype I-DatasetName
Pairs I-DatasetName
( O
CrowS B-DatasetName
- I-DatasetName
Pairs I-DatasetName
) O
( O
Nangia O
et O
al O
. O
, O
2020 O
) O
is O
a O
dataset O
containing O
pairs O
of O
stereotype O
sentences O
and O
anti O
- O
stereotype O
sentences O
. O
We O
report O
the O
ratio O
of O
mask O
token O
probabilities O
assigned O
to O
stereotype O
sentences O
rather O
than O
anti O
- O
stereotype O
sentences O
, O
denoted O
using O
CrowS B-MetricName
. O

External O
Bias O
Evaluation O
Metrics O

Bias B-DatasetName
- I-DatasetName
in I-DatasetName
- I-DatasetName
Bios I-DatasetName
( O
De O
- O
Arteaga O
et O
al O
. O
, O
2019 O
) O
is O
a O
biography O
dataset O
in O
which O
each O
sample O
is O
labeled O
with O
gender O
( O
male O
or O
female O
) O
and O
occupation O
( O
28 O
categories O
) O
. O
We O
fine O
- O
tune O
the O
debiased O
model O
on O
the O
training O
set O
with O
the O
goal O
of O
predicting O
occupations O
. O
Overall B-MetricName
Accuracy I-MetricName
result O
is O
used O
to O
measure O
task O
precision O
, O
and O
individual B-MetricName
Accuracy I-MetricName
results O
for O
male O
and O
female O
are O
used O
to O
measure O
gender O
fairness O
. O
Furthermore O
, O
we O
report O
the O
gap O
between O
the O
true O
positive O
rates O
of O
the O
male O
prediction O
results O
and O
the O
female O
prediction O
results O
denotes O
as O
GAP B-MetricName
T I-MetricName
P I-MetricName
R I-MetricName
, O
as O
well O
as O
the O
root O
mean O
square O
of O
the O
true O
positive O
rates O
difference O
for O
each O
category O
denotes O
as O
GAP B-MetricName
RM I-MetricName
S I-MetricName
. O
The O
closer O
their O
score O
is O
to O
0 O
, O
the O
better O
. O
They O
are O
defined O
as O
follows O
: O

GAP B-MetricName
T I-MetricName
P I-MetricName
R I-MetricName
= O
|T O
P O
R O
M O
− O
T O
P O
R O
F O
| O
, O
( O
8 O
) O

GAP B-MetricName
RM I-MetricName
S I-MetricName
= O
1 O
|C| O
y∈C O
( O
GAP O
T O
P O
R O
, O
y O
) O
2 O
. O
( O
9 O
) O

Bias O
- O
NLI O
( O
Dev O
et O
al O
. O
, O
2020 O
) O
fills O
gender O
words O
and O
occupation O
words O
with O
stereotypes O
into O
sentence O
templates O
to O
form O
sentence O
pairs O
, O
and O
the O
training O
goal O
is O
to O
inference O
whether O
the O
sentence O
pair O
is O
neutral O
or O
not O
. O
It O
defines O
three O
metrics O
to O
reflect O
the O
fairness O
of O
the O
model O
: O
1 O
) O
Net B-MetricName
Neutral I-MetricName
( O
NN B-MetricName
) O
, O
the O
average O
probability O
of O
neutral O
labels O
across O
all O
sentence O
pairs O
; O
2 O
) O
Fraction B-MetricName
Neutral I-MetricName
( O
FN B-MetricName
) O
, O
the O
proportion O
of O
sentence O
pairs O
marked O
as O
neutral O
; O
3 O
) O
Threshold B-MetricName
: I-MetricName
τ I-MetricName
( O
T B-MetricName
: I-MetricName
τ I-MetricName
) O
, O
The O
fraction O
of O
samples O
with O
neutral O
probability O
above O
τ O
is O
reported O
. O

Debiasing O
Performance O
Analysis O

Internal O
Debiasing O
Results O

Table O
1 O
shows O
the O
experimental O
results O
of O
three O
bias O
evaluation O
metrics O
for O
CCPA B-MethodName
and O
baseline O
models O
on O
BERT O
, O
DistilBERT O
, O
and O
ELEATRA O
. O
We O
also O
report O
results O
for O
biased O
BERT O
, O
DistilBERT O
, O
and O
ELEATRA O
as O
references O
. O
The O
results O
show O
that O
CCPA B-MethodName
achieves O
a O
better O
balance O
between O
PLMs O
' O
fairness O
and O
language O
modeling O
capability O
than O
the O
baseline O
models O
. O

For O
BERT O
, O
CCPA B-MethodName
reduces O
the O
average B-MetricName
effect I-MetricName
size I-MetricName
from O
0.621 B-MetricValue
to O
0.249 B-MetricValue
, O
increases O
ICAT B-MetricName
from O
66.86 B-MetricValue
to O
73.28 B-MetricValue
, O
and O
reduces O
CrowS B-MetricName
from O
57.86 B-MetricValue
to O
51.57 B-MetricValue
. O
Our O
method O
has O
achieved O
optimal O
results O
in O
the O
three O
test O
subsets O
of O
SEAT B-MetricName
6 O
, O
7 O
, O
8b O
and O
the O
average B-MetricName
effect I-MetricName
size I-MetricName
, O
and O
has O
also O
been O
greatly O
improved O
in O
the O
other O
test O
subsets O
. O
The O
results O
on O
StereoSet B-DatasetName
show O
that O
CCPA B-MethodName
does O
not O
weaken O
BERT O
's O
language O
modeling O
ability O
but O
slightly O
improves O
it O
. O
Although O
LM B-MetricName
and O
SS B-MetricName
do O
not O
achieve O
optimal O
results O
, O
our O
comprehensive O
index O
ICAT B-MetricName
is O
better O
than O
other O
models O
. O
Both O
FairFil B-MethodName
and O
MABEL B-MethodName
are O
biased O
by O
contrastive O
learning O
, O
but O
their O
overall O
performance O
is O
not O
ideal O
. O
Although O
FairFil B-MethodName
is O
outstanding O
in O
terms O
of O
SS B-MetricName
performance O
, O
it O
seriously O
damages O
BERT O
's O
language O
modeling O
ability O
, O
possibly O
because O
it O
only O
considers O
sentence O
- O
level O
representation O
and O
does O
not O
retain O
token O
- O
level O
encoding O
ability O
. O
MABEL B-MethodName
achieves O
promising O
results O
on O
StereoSet B-DatasetName
and O
CrowS B-DatasetName
- I-DatasetName
Pairs I-DatasetName
, O
but O
its O
SEAT B-MetricName
results O
must O
be O
improved O
. O
Regarding O
overall O
performance O
, O
CCPA B-MethodName
outperforms O
other O
contrastive O
learning O
frameworks O
, O
demonstrating O
that O
our O
adversarial O
training O
inspired O
approach O
can O
improve O
the O
model O
's O
learning O
ability O
by O
increasing O
the O
complex O
information O
in O
the O
model O
. O

For O
DistilBERT O
, O
CCPA B-MethodName
decreases O
the O
average B-MetricName
effect I-MetricName
size I-MetricName
from O
0.883 B-MetricValue
to O
0.152 B-MetricValue
and O
improves O
ICAT B-MetricName
from O
66.93 B-MetricValue
to O
71.30 B-MetricValue
. O
Our O
model O
gets O
excellent O
experimental O
results O
on O
most O
test O
subsets O
of O
SEAT B-MetricName
and O
reaches O
an O
almost O
ideal O
50.31 B-MetricValue
% I-MetricValue
result O
on O
CrowS B-DatasetName
- I-DatasetName
Pairs I-DatasetName
. O
LM B-MetricName
score O
decreases O
, O
and O
we O
analyze O
that O
the O
semantic O
information O
of O
the O
original O
representation O
is O
affected O
by O
too O
much O
debiasing O
. O

For O
ELEATRA O
, O
which O
does O
not O
belong O
to O
the O
bert O
- O
series O
PLM O
, O
the O
debiasing O
effect O
of O
CCPA B-MethodName
is O
equally O
significant O
, O
and O
the O
experimental O
results O
are O
fairer O
than O
the O
original O
ELEATRA O
on O
all O
three O
intrinsic O
metrics O
. O
In O
detail O
, O
CCPA B-MethodName
reduced O
the O
average B-MetricName
effect I-MetricName
size I-MetricName
from O
0.797 B-MetricValue
to O
0.421 B-MetricValue
, O
increases O
ICAT B-MetricName
by O
8.37 B-MetricValue
% I-MetricValue
without O
significantly O
decreasing O
LM B-MetricName
score O
, O
and O
reduces O
CrowS B-MetricName
score O
by O
1.89 B-MetricValue
% I-MetricValue
. O

We O
also O
perform O
a O
small O
qualitative O
study O
by O
vi- O
sualizing O
t O
- O
SNE O
plots O
of O
sentence O
embedding O
. O
As O
can O
be O
seen O
from O
Figure O
3 O
, O
in O
BERT O
, O
male O
attribute O
words O
are O
more O
inclined O
to O
target O
words O
in O
the O
technical O
field O
( O
such O
as O
career O
or O
science O
) O
in O
the O
embedded O
space O
, O
while O
female O
attribute O
words O
are O
more O
inclined O
to O
target O
words O
in O
the O
humanities O
( O
such O
as O
family O
or O
poetry O
) O
. O
After O
using O
CCPA B-MethodName
to O
debias O
, O
it O
is O
observed O
that O
gender O
- O
attribute O
words O
are O
pulled O
closer O
together O
and O
away O
from O
neutral O
words O
in O
the O
representational O
space O
. O

External O
Debiasing O
Results O

We O
fine O
- O
tune O
the O
debiased O
BERT O
on O
two O
downstream O
tasks O
Bias B-TaskName
- I-TaskName
in I-TaskName
- I-TaskName
Bios I-TaskName
and O
Bias B-TaskName
- I-TaskName
NLI I-TaskName
to O
verify O
the O
effect O
of O
CCPA B-MethodName
on O
external O
debiasing O
, O
and O
the O
results O
are O
shown O
in O
Tables O
2 O
and O
3 O
. O
All O
our O
experimental O
setups O
are O
consistent O
with O
MABEL B-MethodName
, O
and O
all O
the O
results O
reported O
in O
the O
table O
for O
the O
baseline O
models O
are O
from O
MABEL B-MethodName
. O

On O
the O
Bias B-TaskName
- I-TaskName
in I-TaskName
- I-TaskName
Bios I-TaskName
task O
as O
shown O
in O
Table O
2 O
, O
CCPA B-MethodName
not O
only O
achieves O
the O
optimal O
results O
on O
task O
accuracy B-MetricName
, O
but O
also O
performs O
the O
best O
on O
all O
gender O
fairness O
metrics O
except O
GAP B-MetricName
RM I-MetricName
S I-MetricName
. O
Although O
INLP B-MethodName
obtains O
the O
best O
score O
on O
the O
GAP B-MetricName
RM I-MetricName
S I-MetricName
metric O
, O
its O
task O
accuracy B-MetricName
is O
clearly O
impaired O
from O
the O
reported O
results O
. O
Compared O
to O
all O
baselines O
, O
CCPA B-MethodName
achieves O
the O
best O
overall O
debiasing O
performance O
while O
preserving O
the O
model O
's O
prediction O
performance O
on O
downstream O
tasks O
. O

On O
the O
Bias B-TaskName
- I-TaskName
NLI I-TaskName
task O
as O
shown O
in O
The O
closer O
the O
effect B-MetricName
size I-MetricName
is O
to O
0 B-MetricValue
and O
the O
closer O
SS B-MetricName
is O
to O
50 B-MetricValue
% I-MetricValue
, O
the O
better O
; O
the O
higher O
the O
LM B-MetricName
and O
ICAT B-MetricName
, O
the O
better O
. O

achieves O
sub O
- O
optimal O
results O
on O
all O
the O
metrics O
. O
It O
is O
worth O
stating O
that O
MABEL B-MethodName
is O
a O
debiasing O
method O
trained O
on O
the O
NLI B-TaskName
task O
, O
which O
we O
analyze O
as O
the O
main O
reason O
for O
its O
most O
outstanding O
performance O
. O
Even O
so O
, O
the O
strong O
debiasing O
effect O
shown O
by O
CCPA B-MethodName
on O
task O
Bias B-TaskName
- I-TaskName
NLI I-TaskName
is O
heartening O
. O

The O
results O
of O
the O
internal O
debiasing O
experiment O
and O
the O
external O
debiasing O
experiment O
show O
that O
our O
proposed O
CCPA B-MethodName
has O
outstanding O
performance O
in O
mitigating B-TaskName
gender I-TaskName
bias I-TaskName
in I-TaskName
PLMs I-TaskName
' I-TaskName
encoding I-TaskName
. O
CCPA B-MethodName
has O
an O
efficient O
debiasing O
performance O
, O
which O
answers O
the O
first O
question O
( O
Q1 O
) O
proposed O
at O
the O
beginning O
of O
this O
section O
. O

Ablation O
Analysis O

We O
conduct O
ablation O
experiments O
on O
BERT O
to O
investigate O
how O
each O
component O
affects O
CCPA B-MethodName
performance O
. O
The O
results O
are O
shown O
in O
Table O
4 O
. O

T O
( O
1,1,1 O
) O
indicates O
that O
the O
continuous O
prompt O
template O
is O
a O
triplet O
with O
one O
virtual O
token O
for O
each O
element O
, O
i.e. O
, O
the O
length O
of O
prompts O
is O
3 O
. O
By O
analogy O
, O
T O
( O
2,2,2 O
) O
and O
T O
( O
3,3,3 O
) O
represent O
prompt O
templates O
of O
lengths O
6 O
and O
9 O
. O
The O
purpose O
of O
this O
setting O
is O
to O
make O
it O
easier O
to O
observe O
the O
effect O
of O
the O
prompts O
' O
length O
on O
the O
model O
. O
In O
the O
experimental O
group O
of O
each O
template O
, O
we O
compare O
three O
versions O
of O
CCPA B-MethodName
: O
the O
original O
CCPA B-MethodName
, O
the O
version O
without O
L O
mlm O
represented O
as O
CCPA B-MethodName
− I-MethodName
and O
the O
version O
without O
L O
mahal O
represented O
as O
CCPA B-MethodName
* I-MethodName
. O
In O
addition O
, O
we O
have O
experimented O
with O
both O
CCPA O
without O
prompts O
and O
CCPA O
without O
prompts O
and O
L O
mlm O
. O

It O
is O
observed O
from O
the O
experimental O
results O
that O
the O
debiasing O
ability O
of O
CCPA B-MethodName
increases O
with O
the O
rise O
of O
the O
template O
's O
length O
. O
This O
indicates O
that O
longer O
continuous O
prompt O
embeddings O
bring O
more O
difficult O
information O
to O
the O
model O
, O
thus O
increasing O
the O
debiasing O
effort O
. O
However O
, O
more O
extended O
templates O
can O
cause O
the O
original O
sentence O
semantics O
to O
be O
broken O
and O
thus O
weaken O
PLM O
's O
language O
modeling O
capability O
. O
In O
each O
experimental O
group O
, O
both O
CCPA B-MethodName
− I-MethodName
and O
CCPA B-MethodName
* I-MethodName
show O
a O
decrease O
in O
the O
results O
of O
the O
three O
evaluation O
metrics O
compared O
to O
CCPA B-MethodName
. O
This O
phenomenon O
verifies O
that O
both O
MLM O
- O
assisted O
loss O
and O
Mahalanobis O
distance O
constraint O
benefit O
CCPA B-MethodName
. O
Overall O
, O
MLM O
has O
a O
greater O
influence O
, O
especially O
on O
SS B-MetricName
and O
CrowS B-MetricName
, O
which O
may O
be O
because O
random O
mask O
tokens O
train O
encoders O
to O
retain O
tokenlevel O
semantic O
information O
. O

In O
addition O
, O
the O
results O
of O
NO O
prompt O
verify O
that O
continuous O
prompts O
play O
an O
essential O
role O
in O
CCPA B-MethodName
. O
NO O
prompt+mask O
tests O
the O
effect O
of O
finetuning O
PLMs O
based O
solely O
on O
contrastive O
learning O
. O
Unsurprisingly O
, O
the O
performance O
on O
all O
indexes O
could O
be O
better O
. O
The O
results O
of O
NO O
prompt O
and O
NO O
prompt+mask O
again O
reflect O
our O
method O
's O
effectiveness O
. O
The O
ablation O
studies O
answer O
our O
second O
question O
( O
Q2 O
) O
by O
exploring O
the O
role O
played O
by O
each O
component O
of O
the O
CCPA B-MethodName
. O

Language O
Modeling O
Capability O
Analysis O

We O
perform O
experiments O
on O
nine O
natural O
language O
understanding O
tasks O
of O
the O
GLUE B-DatasetName
benchmark O
to O
verify O
the O
language O
modeling O
capability O
of O
CCPA B-MethodName
on O
downstream O
tasks O
. O
In O
task O
- O
specific O
fine O
- O
tuning O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
2e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
32 B-HyperparameterValue
for O
all O
models O
. O

As O
in O

Related O
Work O

We O
divide O
debiasing O
methods O
into O
two O
categories O
based O
on O
the O
debiasing O
strategy O
: O
task O
- O
specific O
methods O
and O
task O
- O
agnostic O
methods O
. O

Task O
- O
Specific O
Methods O

Task O
- O
specific O
methods O
adopt O
the O
strategy O
of O
debiasing O
in O
the O
fine O
- O
tuning O
stage O
of O
the O
downstream O
task O
, O
of O
which O
the O
downstream O
task O
is O
known O
Chi O
et O
al O
. O
, O
2022 O
) O
. O
One O
representative O
work O
is O
INLP B-MethodName
( O
Ravfogel O
et O
al O
. O
, O
2020 O
( O
Ravfogel O
et O
al O
. O
, O
, O
2022 O
, O
which O
repeatedly O
trains O
a O
linear O
classifier O
that O
predicts O
the O
target O
concept O
, O
and O
then O
projects O
the O
representation O
into O
the O
null O
space O
of O
the O
classifier O
's O
weight O
matrix O
to O
remove O
the O
representation O
bias O
. O
Contrastive O
learning O
is O
proposed O
to O
mitigate O
bias O
in O
classifier O
training O
( O
Shen O
et O
al O
. O
, O
2021 O
) O
. O
It O
encourages O
instances O
sharing O
the O
same O
class O
labels O
to O
have O
similar O
representations O
while O
ensuring O
that O
protected O
attributes O
have O
different O
distributions O
. O
These O
methods O
use O
attribute O
words O
to O
label O
training O
data O
without O
CDA B-MethodName
. O
However O
, O
they O
are O
biased O
towards O
specific O
downstream O
tasks O
and O
can O
not O
be O
applied O
to O
other O
tasks O
in O
general O
. O
When O
training O
data O
change O
, O
task O
- O
specific O
methods O
are O
difficult O
to O
transfer O
to O
new O
tasks O
. O

Task O
- O
Agnostic O
Methods O

Task O
- O
agnostic O
methods O
adopt O
the O
strategy O
of O
debiasing O
representation O
or O
processing O
unbalanced O
data O
before O
the O
downstream O
task O
, O
and O
they O
can O
be O
applied O
to O
any O
downstream O
task O
( O
Dev O
et O
al O
. O
, O
2020 O
( O
Dev O
et O
al O
. O
, O
, O
2021 O
. O
Most O
of O
these O
methods O
apply O
counterfactual O
data O
augmentation O
to O
augment O
the O
unbalanced O
corpus O
and O
then O
debias O
the O
augmented O
text O
information O
. O
Counterfactual O
data O
augmentation O
( O
Lu O
et O
al O
. O
, O
2020 O
) O
is O
a O
general O
approach O
to O
augment O
corpora O
through O
causal O
intervention O
and O
has O
since O
been O
widely O
used O
to O
mitigate O
social O
biases O
. O
Different O
variants O
of O
counterfactual O
data O
augmentation O
have O
been O
proposed O
, O
such O
as O
Sent B-MethodName
- I-MethodName
Debias I-MethodName
( O
Liang O
et O
al O
. O
, O
2020 O
) O
, O
FairFil B-MethodName
( O
Cheng O
et O
al O
. O
, O
2021 O
) O
, O
MABEL B-MethodName
( O
He O
et O
al O
. O
, O
2022 O
) O
, O
to O
name O
a O
few O
examples O
. O
Task O
- O
agnostic O
methods O
primarily O
use O
the O
CDA B-MethodName
to O
balance O
the O
training O
corpus O
by O
constructing O
counterfactual O
pairs O
specific O
to O
different O
demographic O
groups O
. O
However O
, O
simply O
applying O
CDA B-MethodName
to O
the O
original O
corpus O
makes O
minor O
changes O
, O
constraining O
the O
representation O
space O
to O
a O
narrow O
range O
. O
This O
makes O
the O
model O
easily O
fit O
the O
differences O
between O
counterfactual O
pairs O
, O
weakening O
the O
debiasing O
ability O
. O
Unlike O
existing O
CDA B-MethodName
methods O
, O
we O
train O
a O
generator O
that O
encodes O
continuous O
prompts O
before O
fine O
- O
tuning O
PLM O
. O
The O
goal O
is O
to O
widen O
the O
representation O
distance O
between O
different O
groups O
to O
increase O
the O
difficulty O
of O
the O
model O
- O
learning O
process O
. O

Conclusions O

Inspired O
by O
adversarial O
training O
, O
we O
propose O
CCPA B-MethodName
, O
a O
two O
- O
stage O
debiasing O
model O
that O
combines O
contrastive O
learning O
with O
continuous O
prompts O
. O
In O
the O
continuous O
prompt O
tuning O
stage O
, O
we O
train O
a O
generator O
encoding O
continuous O
prompt O
embeddings O
to O
increase O
the O
representative O
distance O
between O
counterfactual O
pairs O
. O
In O
the O
fine O
- O
tuning O
stage O
, O
we O
use O
contrastive O
learning O
to O
reduce O
the O
representation O
distance O
between O
the O
augmented O
sample O
pairs O
. O
By O
increasing O
the O
difficulty O
of O
the O
training O
process O
, O
CCPA B-MethodName
enables O
PLMs O
to O
learn O
a O
stronger O
debiasing O
ability O
. O
Extensive O
experiments O
on O
BERT O
and O
DistilBERT O
show O
that O
CCPA B-MethodName
effectively O
reduces O
social O
bias O
in O
PLM O
representation O
while O
retaining O
language O
modeling O
capability O
. O

Limitations O

In O
this O
work O
, O
we O
focus O
on O
debiasing O
the O
gender O
bias O
for O
PLMs O
. O
In O
the O
future O
, O
we O
will O
try O
to O
mitigate O
social O
biases O
other O
than O
gender O
, O
such O
as O
race O
and O
religion O
. O
In O
addition O
, O
we O
also O
plan O
to O
extend O
our O
debiasing O
method O
to O
more O
language O
models O
, O
such O
as O
Natural O
Language O
Generation O
( O
NLG O
) O
models O
. O

Ethics O
Statement O

This O
paper O
has O
been O
thoroughly O
reviewed O
for O
ethical O
considerations O
and O
has O
been O
found O
to O
be O
in O
compliance O
with O
all O
relevant O
ethical O
guidelines O
. O
The O
paper O
does O
not O
raise O
any O
ethical O
concerns O
and O
is O
a O
valuable O
contribution O
to O
the O
field O
. O

Acknowledgments O

We O
express O
gratitude O
to O
the O
anonymous O
reviewers O
for O
their O
hard O
work O
and O
kind O
com- O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
No O
response O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
No O
response O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
No O
response O
. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
No O
response O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

3 O
Experiments O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Our O
model O
uses O
the O
standard O
pre O
- O
trained O
language O
model O
as O
the O
backbone O
network O
, O
and O
the O
number O
of O
parameters O
and O
consumption O
are O
equivalent O
to O
that O
of O
the O
pre O
- O
trained O
language O
model O
. O
This O
part O
is O
not O
the O
focus O
of O
our O
discussion O
and O
is O
not O
specifically O
mentioned O
. O

Multimodal B-TaskName
Dialogue I-TaskName
State I-TaskName
Tracking I-TaskName

Designed O
for O
tracking O
user O
goals O
in O
dialogues O
, O
a O
dialogue O
state O
tracker O
is O
an O
essential O
component O
in O
a O
dialogue O
system O
. O
However O
, O
the O
research O
of O
dialogue O
state O
tracking O
has O
largely O
been O
limited O
to O
unimodality O
, O
in O
which O
slots O
and O
slot O
values O
are O
limited O
by O
knowledge O
domains O
( O
e.g. O
restaurant O
domain O
with O
slots O
of O
restaurant O
name O
and O
price O
range O
) O
and O
are O
defined O
by O
specific O
database O
schema O
. O
In O
this O
paper O
, O
we O
propose O
to O
extend O
the O
definition O
of O
dialogue O
state O
tracking O
to O
multimodality O
. O
Specifically O
, O
we O
introduce O
a O
novel O
dialogue O
state O
tracking O
task O
to O
track O
the O
information O
of O
visual O
objects O
that O
are O
mentioned O
in O
video O
- O
grounded O
dialogues O
. O
Each O
new O
dialogue O
utterance O
may O
introduce O
a O
new O
video O
segment O
, O
new O
visual O
objects O
, O
or O
new O
object O
attributes O
and O
a O
state O
tracker O
is O
required O
to O
update O
these O
information O
slots O
accordingly O
. O
We O
created O
a O
new O
synthetic O
benchmark O
and O
designed O
a O
novel O
baseline O
, O
Video B-MethodName
- I-MethodName
Dialogue I-MethodName
Transformer I-MethodName
Network I-MethodName
( O
VDTN B-MethodName
) O
, O
for O
this O
task O
. O
VDTN B-MethodName
combines O
both O
object O
- O
level O
features O
and O
segment O
- O
level O
features O
and O
learns O
contextual O
dependencies O
between O
videos O
and O
dialogues O
to O
generate O
multimodal O
dialogue O
states O
. O
We O
optimized O
VDTN B-MethodName
for O
a O
state O
generation O
task O
as O
well O
as O
a O
self O
- O
supervised O
video O
understanding O
task O
which O
recovers O
video O
segment O
or O
object O
representations O
. O
Finally O
, O
we O
trained O
VDTN B-MethodName
to O
use O
the O
decoded O
states O
in O
a O
response O
prediction O
task O
. O
Together O
with O
comprehensive O
ablation O
and O
qualitative O
analysis O
, O
we O
discovered O
interesting O
insights O
towards O
building O
more O
capable O
multimodal O
dialogue O
systems O
. O
Dialogue O
HUMAN O
: O
I O
am O
looking O
for O
a O
cheap O
restaurant O
in O
the O
centre O
of O
the O
city O
. O

Introduction O

The O
main O
goal O
of O
dialogue O
research O
is O
to O
develop O
intelligent O
agents O
that O
can O
assist O
humans O
through O
conversations O
. O
For O
example O
, O
a O
dialogue O
agent O
can O
be O
tasked O
to O
help O
users O
to O
find O
a O
restaurant O
based O
on O
their O
preferences O
of O
price O
ranges O
and O
food O
choices O
. O
A O
crucial O
part O
of O
a O
dialogue O
system O
is O
Dialogue O
State O
Tracking O
( O
DST O
) O
, O
which O
is O
responsible O
for O
tracking O
and O
updating O
user O
goals O
in O
the O
form O
of O
dialogue O
states O
, O
including O
a O
set O
of O
( O
slot O
, O
value O
) O
pairs O
such O
as O
( O
price O
, O
" O
moderate O
" O
) O
and O
( O
food O
, O
" O
japanese O
" O
) O
. O
Numerous O
machine O
learning O
approaches O
have O
been O
proposed O
to O
tackle O
DST O
, O
including O
fixed O
- O
vocabulary O
models O
and O
open O
- O
vocabulary O
models O
( O
Lei O
et O
al O
. O
, O
2018b O
; O
Wu O
et O
al O
. O
, O
2019 O
; O
Le O
et O
al O
. O
, O
2020c O
) O
, O
for O
either O
single O
- O
domain O
or O
multi O
- O
domain O
dialogues O
( O
Eric O
et O
al O
. O
, O
2017 O
; O
. O

However O
, O
the O
research O
of O
DST O
has O
largely O
limited O
the O
scope O
of O
dialogue O
agents O
to O
unimodality O
. O
In O
this O
setting O
, O
the O
slots O
and O
slot O
values O
are O
defined O
by O
the O
knowledge O
domains O
( O
e.g. O
restaurant O
domain O
) O
and O
database O
schema O
( O
e.g. O
data O
tables O
for O
restaurant O
entities O
) O
. O
The O
ultimate O
goal O
of O
dialogue O
research O
towards O
building O
artificial O
intelligent O
assistants O
necessitates O
DST O
going O
beyond O
unimodal O
systems O
. O
In O
this O
paper O
, O
we O
propose O
Multimodal B-TaskName
Dialogue I-TaskName
State I-TaskName
Tracking I-TaskName
( O
MM B-TaskName
- I-TaskName
DST I-TaskName
) O
that O
extends O
the O
DST O
task O
in O
a O
multimodal O
world O
. O
Specifically O
, O
MM B-TaskName
- I-TaskName
DST I-TaskName
extends O
the O
scope O
of O
dialogue O
states O
by O
defining O
slots O
and O
slot O
values O
for O
visual O
objects O
that O
are O
mentioned O
in O
visually O
- O
grounded O
dialogues O
. O
For O
research O
purposes O
, O
following O
( O
Alamri O
et O
al O
. O
, O
2019 O
) O
, O
we O
limited O
visually O
- O
grounded O
dialogues O
as O
ones O
with O
a O
grounding O
video O
input O
and O
the O
dialogues O
contain O
multiple O
turns O
of O
( O
question O
, O
answer O
) O
pairs O
about O
this O
video O
. O
Each O
new O
utterance O
in O
such O
dialogues O
may O
focus O
on O
a O
new O
video O
segment O
, O
new O
visual O
objects O
, O
or O
new O
object O
attributes O
, O
and O
the O
tracker O
is O
required O
to O
update O
the O
dialogue O
state O
accordingly O
at O
each O
turn O
. O
An O
example O
of O
MM B-TaskName
- I-TaskName
DST I-TaskName
can O
be O
seen O
in O
Figure O
1 O
. O

Toward O
MM B-TaskName
- I-TaskName
DST I-TaskName
, O
we O
developed O
a O
synthetic O
benchmark O
based O
on O
the O
CATER B-DatasetName
universe O
( O
Girdhar O
and O
Ramanan O
, O
2020 O
) O
. O
We O
also O
introduced O
Video B-MethodName
- I-MethodName
Dialogue I-MethodName
Transformer I-MethodName
Network I-MethodName
( O
VDTN B-MethodName
) O
, O
a O
neural O
network O
architecture O
that O
combines O
both O
objectlevel O
features O
and O
segment O
- O
level O
features O
in O
video O
and O
learns O
contextual O
dependencies O
between O
videos O
and O
dialogues O
. O
Specifically O
, O
we O
maintained O
the O
information O
granularity O
of O
visual O
objects O
, O
embedded O
by O
object O
classes O
and O
their O
bounding O
boxes O
and O
injected O
with O
segment O
- O
level O
visual O
context O
. O
VDTN B-MethodName
enables O
interactions O
between O
each O
visual O
object O
representation O
and O
word O
- O
level O
representation O
in O
dialogues O
to O
decode O
dialogue O
states O
. O
To O
decode O
multimodal O
dialogue O
states O
, O
we O
adopted O
a O
decoding O
strategy O
inspired O
by O
the O
Markov O
decision O
process O
in O
traditional O
DST O
. O
In O
this O
strategy O
, O
a O
model O
learns O
to O
decode O
the O
state O
at O
a O
dialogue O
turn O
based O
on O
the O
predicted O
/ O
observed O
dialogue O
state O
available O
from O
the O
last O
dialogue O
turn O
. O
Compared O
to O
the O
conventional O
DST O
, O
MM B-TaskName
- I-TaskName
DST I-TaskName
involves O
the O
new O
modality O
from O
visual O
inputs O
. O
Our O
experiments O
show O
that O
simply O
combining O
visual O
and O
language O
representations O
in O
traditional O
DST O
models O
results O
in O
poor O
performance O
. O
Towards O
this O
challenge O
, O
we O
enhanced O
VDTN B-MethodName
with O
selfsupervised O
video O
understanding O
tasks O
which O
recovers O
object O
- O
based O
or O
segment O
- O
based O
representa O
- O
tions O
. O
Benchmarked O
against O
strong O
unimodal O
DST O
models O
, O
we O
observed O
significant O
performance O
gains O
from O
VDTN B-MethodName
. O
We O
provided O
comprehensive O
ablation O
analysis O
to O
study O
the O
efficacy O
of O
VDTN B-MethodName
models O
. O
Interestingly O
, O
we O
also O
showed O
that O
using O
decoded O
states O
brought O
performance O
gains O
in O
a O
dialogue O
response O
prediction O
task O
, O
supporting O
our O
motivation O
for O
introducing O
multimodality O
into O
DST O
research O
. O

Multimodal B-TaskName
Dialogue I-TaskName
State I-TaskName
Tracking I-TaskName

Traditional O
DST O
. O
As O
defined O
by O
, O
the O
traditional O
DST O
includes O
an O
input O
of O
dialogue O
D O
and O
a O
set O
of O
slots O
S O
to O
be O
tracked O
from O
turn O
to O
turn O
. O
At O
each O
dialogue O
turn O
t O
, O
we O
denote O
the O
dialogue O
context O
as O
D O
t O
, O
containing O
all O
utterances O
up O
to O
the O
current O
turn O
. O
The O
objective O
of O
DST O
is O
for O
each O
turn O
t O
, O
predict O
a O
value O
v O
t O
i O
of O
each O
slot O
s O
i O
from O
a O
predefined O
set O
S O
, O
conditioned O
by O
the O
dialogue O
context O
D O
t O
. O
We O
denote O
the O
dialogue O
state O
at O
turn O
of O
traditional O
DST O
models O
assume O
slots O
are O
conditionally O
independent O
, O
given O
the O
dialogue O
context O
( O
Zhong O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2019 O
; O
Gao O
et O
al O
. O
, O
2019 O
) O
. O
The O
learning O
objective O
is O
defined O
as O
: O

B O
t O
= O
arg O
max O
Bt O
P O
( O
B O
t O
|D O
t O
, O
θ O
) O
= O
arg O
max O
Bt O
|S| O
i O
P O
( O
v O
t O
i O
|s O
i O
, O
D O
t O
, O
θ O
) O
( O
1 O
) O

Motivation O
to O
Multimodality O
. O
Yet O
, O
the O
above O
definition O
of O
DST O
are O
still O
limited O
to O
unimodality O
and O
our O
ultimate O
goal O
of O
building O
intelligent O
dialogue O
agents O
, O
ideally O
with O
similar O
level O
of O
intelligence O
as O
humans O
, O
inspires O
us O
to O
explore O
mulitmodality O
. O
In O
neuroscience O
literature O
, O
several O
studies O
have O
analyzed O
how O
humans O
can O
perceive O
the O
world O
in O
visual O
context O
. O
( O
Bar O
, O
2004 O
; O
Xu O
and O
Chun O
, O
2009 O
) O
found O
that O
humans O
can O
recognize O
multiple O
visual O
objects O
and O
how O
their O
contexts O
, O
often O
embedded O
with O
other O
related O
objects O
, O
facilitate O
this O
capacity O
. O

Our O
work O
is O
more O
related O
to O
the O
recent O
study O
( O
Fischer O
et O
al O
. O
, O
2020 O
) O
which O
focuses O
on O
human O
capacity O
to O
create O
temporal O
stability O
across O
multiple O
objects O
. O
The O
multimodal B-TaskName
DST I-TaskName
task O
is O
designed O
to O
develop O
multimodal O
dialogue O
systems O
that O
are O
capable O
of O
maintaining O
discriminative O
representations O
of O
visual O
objects O
over O
a O
period O
of O
time O
, O
segmented O
by O
dialogue O
turns O
. O
While O
computer O
science O
literature O
has O
focused O
on O
related O
human O
capacities O
in O
intelligent O
systems O
, O
they O
are O
mostly O
limited O
to O
vision O
- O
only O
tasks O
e.g. O
( O
He O
et O
al O
. O
, O
2016 O
; O
Ren O
et O
al O
. O
, O
2015 O
) O
or O
QA O
tasks O
e.g. O
( O
Antol O
et O
al O
. O
, O
2015 O
; O
Jang O
et O
al O
. O
, O
2017 O
) O
but O
not O
in O
a O
dialogue O
task O
. O

Most O
related O
work O
in O
the O
dialogue O
domain O
is O
( O
Pang O
and O
Wang O
, O
2020 O
) O
and O
almost O
concurrent O
to O
our O
work O
is O
( O
Kottur O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
( O
Kottur O
et O
al O
. O
, O
2021 O
) O
is O
limited O
to O
a O
single O
object O
per O
dialogue O
, O
and O
( O
Pang O
and O
Wang O
, O
2020 O
) O
extends O
to O
multiple O
objects O
but O
does O
not O
require O
to O
maintain O
an O
information O
state O
with O
component O
slots O
for O
each O
object O
. O
Our O
work O
aims O
to O
complement O
these O
directions O
and O
address O
their O
limitations O
with O
a O
novel O
definition O
of O
multimodal O
dialogue O
state O
. O

Multimodal B-TaskName
DST I-TaskName
( O
MM B-TaskName
- I-TaskName
DST I-TaskName
) O
. O
To O
this O
end O
, O
we O
proposed O
to O
extend O
conventional O
dialogue O
states O
. O
First O
, O
we O
use O
visual O
object O
identities O
themselves O
as O
a O
component O
of O
the O
dialogue O
state O
to O
enable O
the O
perception O
of O
multiple O
objects O
. O
A O
dialogue O
state O
might O
have O
one O
or O
more O
objects O
and O
a O
dialogue O
system O
needs O
to O
update O
the O
object O
set O
as O
the O
dialogue O
carries O
on O
. O
Secondly O
, O
for O
each O
object O
, O
we O
define O
slots O
that O
represent O
the O
information O
state O
of O
objects O
in O
dialogues O
( O
as O
denoted O
by O
( O
Fischer O
et O
al O
. O
, O
2020 O
) O
as O
" O
content O
" O
features O
of O
objects O
memorized O
by O
humans O
) O
. O
The O
value O
of O
each O
slot O
is O
subject O
- O
specific O
and O
updated O
based O
on O
the O
dialogue O
context O
of O
the O
corresponding O
object O
. O
This O
definition O
of O
DST O
is O
closely O
based O
on O
the O
above O
well O
- O
studied O
human O
capacities O
while O
complementing O
the O
conventional O
dialogue O
research O
, O
and O
more O
lately O
multimodal O
dialogue O
research O
( O
Pang O
and O
Wang O
, O
2020 O
; O
Kottur O
et O
al O
. O
, O
2021 O
) O
. O

We O
denote O
a O
grounding O
visual O
input O
in O
the O
form O
of O
a O
video O
V O
with O
one O
or O
more O
visual O
objects O
o O
j O
. O
We O
assume O
these O
objects O
are O
semantically O
different O
enough O
( O
by O
appearance O
, O
by O
characters O
, O
etc O
. O
) O
such O
that O
each O
object O
can O
be O
uniquely O
identified O
( O
e.g. O
by O
an O
object O
detection O
module O
ω O
) O
. O
The O
objective O
of O
MM B-TaskName
- I-TaskName
DST I-TaskName
is O
for O
each O
dialogue O
turn O
t O
, O
predict O
a O
value O
v O
t O
i O
of O
each O
slot O
s O
i O
∈ O
S O
for O
each O
object O
o O
j O
∈ O
O. O
We O
denote O
the O
dialogue O
state O
at O
turn O

t O
as O
B O
t O
= O
| O
{ O
( O
o O
j O
, O
s O
i O
, O
v O
t O
i O
, O
j O
) O
} O
| O
i=|S| O
, O
j=|O| O
i=1 O
, O
j=1 O

. O
Assuming O
all O
slots O
are O
conditionally O
independent O
given O
dialogue O
and O
video O
context O
, O
the O
learning O
objective O
is O
extended O
from O
Eq O
. O
( O
1 O
) O
: O

Bt O
= O
arg O
max O
B O
t O
P O
( O
Bt|Dt O
, O
V O
, O
θ O
) O
= O
arg O
max O
B O
t O
|O| O
j O
|S| O
i O
P O
( O
v O
t O
i O
, O
j O
|si O
, O
oj O
, O
Dt O
, O
V O
, O
θ O
) O
P O
( O
oj|V O
, O
ω O
) O

One O
limitation O
of O
the O
current O
representation O
is O
the O
absence O
of O
temporal O
placement O
of O
objects O
in O
time O
. O

Naturally O
humans O
are O
able O
to O
associate O
objects O
and O
their O
temporal O
occurrence O
over O
a O
certain O
period O
. O
Therefore O
, O
we O
defined O
two O
temporal O
- O
based O
slots O
: O
s O
start O
and O
s O
end O
, O
denoting O
the O
start O
time O
and O
end O
time O
of O
the O
video O
segment O
that O
an O
object O
can O
be O
located O
by O
each O
dialogue O
turn O
. O
In O
this O
work O
, O
we O
assume O
that O
a O
dialogue O
turn O
is O
limited O
to O
a O
single O
continuous O
time O
span O
, O
and O
hence O
, O
s O
start O
and O
s O
end O
can O
be O
defined O
turn O
- O
wise O
, O
identically O
for O
all O
objects O
. O
While O
this O
is O
a O
strong O
assumption O
, O
we O
believe O
it O
covers O
a O
large O
portion O
of O
natural O
conversational O
interactions O
. O
An O
example O
of O
multimodal O
dialogue O
state O
can O
be O
seen O
in O
Figure O
1 O
. O

Visual O
Perception O
and O
Encoder O

Visual O
Perception O
. O
This O
module O
encodes O
videos O
at O
both O
frame O
- O
level O
and O
segment O
- O
level O
representations O
. O
Specifically O
, O
we O
used O
a O
pretrained O
Faster B-MethodName
R I-MethodName
- I-MethodName
CNN I-MethodName
model O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
to O
extract O
object O
representations O
. O
We O
used O
this O
model O
to O
output O
the O
bounding O
boxes O
and O
object O
identifiers O
( O
object O
classes O
) O
in O
each O
video O
frame O
of O
the O
video O
. O
For O
an O
object O
o O
j O
, O
we O
denoted O
the O
four O
values O
of O
its O
bounding O
boxes O
as O
bb O
j O
= O
( O
x O
1 O
j O
, O
y O
1 O
j O
, O
x O
2 O
j O
, O
y O
2 O
j O
) O
and O
o O
j O
as O
the O
object O
class O
itself O
. O
We O
standardized O
the O
video O
features O
by O
extracting O
features O
of O
up O
to O
N O
obj O
= O
10 O
objects O
per O
frame O
and O
normalizing O
all O
bounding O
box O
coordinates O
by O
the O
frame O
size O
. O
Secondly O
, O
we O
used O
a O
pretrained O
ResNeXt B-MethodName
model O
( O
Xie O
et O
al O
. O
, O
2017 O
) O
to O
extract O
the O
segment O
- O
level O
representations O
of O
videos O
, O
denoted O
as O
z O
m O
∈ O
R O
2048 O
for O
a O
segment O
m. O
Practically O
, O
we O
followed O
the O
best O
practice O
in O
computer O
vision O
by O
using O
a O
temporal O
sliding O
window O
with O
strides O
to O
sample O
video O
segments O
and O
passed O
segments O
to O
ResNeXt O
model O
to O
extract O
features O
. O
To O
standardize O
visual O
features O
, O
we O
use O
the O
same O
strid O
- O
ing O
configuration O
N O
stride O
to O
sub O
- O
sample O
segments O
for O
ResNeXt B-MethodName
and O
frames O
for O
Faster B-MethodName
R I-MethodName
- I-MethodName
CNN I-MethodName
models O
. O

Visual O
Representation O
. O
Note O
that O
we O
do O
not O
finetune O
the O
visual O
feature O
extractors O
in O
VDTN B-MethodName
and O
keep O
the O
extracted O
features O
fixed O
. O
To O
transform O
these O
features O
into O
VDTN B-MethodName
embedding O
space O
, O
we O
first O
concatenated O
all O
object O
identity O
tokens O
OBJ O
< O
class O
> O
of O
all O
frames O
. O
An O
object O
identity O
token O
OBJ O
< O
class O
> O
is O
the O
code O
name O
of O
the O
object O
class O
( O
e.g. O
a O
class O
of O
small O
blue O
metal O
cones O
) O
that O
a O
visual O
object O
can O
be O
unique O
identified O
( O
See O
Figure O
2 O
) O
. O
Frames O
are O
separated O
by O
a O
special O
token O
FRAME O
< O
number O
> O
, O
where O
< O
number O
> O
is O
the O
temporal O
order O
of O
the O
frame O
. O
This O
results O
in O
a O
sequence O
of O
tokens O
X O
obj O
of O
length O
L O
obj O
= O
( O
N O
obj O
+ O
1 O
) O
× O
( O
|V| O
/ O
N O
stride O
) O
where O
|V| O
is O
the O
number O
of O
video O
frames O
. O
Correspondingly O
, O
we O
concatenated O
bounding O
boxes O
of O
all O
objects O
, O
and O
used O
a O
zero O
vector O
in O
positions O
of O
FRAME O
< O
number O
> O
tokens O
. O
We O
denoted O
this O
sequence O
as O
X O
bb O
∈ O
R O
L O
obj O
×4 O
where O
the O
dimension O
of O
4 O
is O
for O
the O
bounding O
box O
coordinates O
( O
x O
1 O
, O
y O
1 O
, O
x O
2 O
, O
y O
2 O
) O
. O
Similarly O
, O
we O
stacked O
each O
ResNeXt B-MethodName
feature O
vector O
by O
( O
N O
obj O
+ O
1 O
) O
for O
each O
segment O
, O
and O
obtained O
a O
sequence O
X O
cnn O
∈ O
R O
L O
obj O
×2048 O
. O

Visual O
Encoding O
. O
We O
passed O
each O
of O
X O
bb O
and O
X O
cnn O
to O
a O
linear O
layer O
with O
ReLU B-HyperparameterName
activation O
to O
map O
their O
feature O
dimension O
to O
a O
uniform O
dimension O
d O
. O

We O
used O
a O
learnable O
embedding O
matrix O
to O
embed O
each O
object O
identity O
in O
X O
obj O
, O
resulting O
in O
embedding O
features O
of O
dimensions O
d. O
The O
final O
video O
input O
representation O
is O
the O
summation O
of O
above O
vectors O
, O
denoted O
as O
Z O
V O
= O
Z O
obj O
+ O
Z O
bb O
+ O
Z O
cnn O
∈ O
R O
L O
obj O
×d O
. O

Dialogue O
and O
State O
Encoder O

Dialogue O
Encoding O
. O
Another O
encoder O
encodes O
dialogue O
into O
continuous O
representations O
. O
Given O
a O
dialogue O
context O
D O
t O
, O
we O
tokenized O
all O
dialogue O
utterances O
into O
sequences O
of O
words O
, O
separated O
by O
special O
tokens O
USR O
for O
human O
utterance O
and O
SYS O
for O
system O
utterance O
. O
We O
used O
a O
trainable O
embedding O
matrix O
and O
sinusoidal O
positional O
embeddings O
to O
embed O
this O
sequence O
into O
d O
- O
dimensional O
vectors O
. O

Flattening O
State O
into O
Sequence O
. O
Similar O
to O
the O
recent O
work O
in O
traditional O
DST O
( O
Lei O
et O
al O
. O
, O
2018b O
; O
Le O
et O
al O
. O
, O
2020b O
; O
, O
we O
are O
motivated O
by O
the O
DST O
decoding O
strategy O
following O
a O
Markov O
principle O
and O
used O
the O
dialogue O
state O
of O
the O
last O
dialogue O
turn O
B O
t−1 O
as O
an O
input O
to O
generate O
the O
current O
state O
B O
t O
. O
Using O
the O
same O
notations O
from O
Section O
2 O
, O
we O
can O
represent O
B O
t O
into O
a O
sequence O
of O
o O
j O
, O
s O
i O
, O
and O
v O
t O
i O
, O
j O
tokens O
, O
such O
as O
" O
OBJ4 O
shape O
cube O
OBJ24 O
size O
small O
color O
red O
" O
. O
This O
sequence O
is O
then O
concatenated O
with O
utterances O
from O
D O
t O
, O
separated O
by O
a O
special O
token O
PRIOR_STATE O
. O
We O
denoted O
the O
resulting O
sequence O
as O
X O
ctx O
which O
is O
passed O
to O
the O
embedding O
matrix O
and O
positional O
encoding O
as O
described O
above O
. O
As O
we O
showed O
in O
our O
experiments O
, O
to O
encode O
dialogue O
context O
, O
this O
strategy O
needs O
only O
a O
few O
dialogue O
utterances O
( O
that O
are O
closer O
to O
the O
current O
turn O
t O
) O
and O
B O
t−1 O
, O
rather O
than O
the O
full O
dialogue O
history O
from O
turn O
1 O
. O
Therefore O
, O
dialogue O
representations O
Z O
ctx O
have O
more O
compressed O
dimensions O
of O

|X O
ctx O
| O
× O
d O
where O
|X O
ctx O
| O
< O
|D O
t O
| O
. O

Multimodal O
Transformer O
Network O

We O
concatenated O
both O
video O
and O
dialogue O
representations O
, O
denoted O
as O

Z O
V O
D O
= O
[ O
Z O
V O
; O
Z O
D O
] O
. O
Z O
V O
D O

has O
a O
length O
of O
L O
obj O
+ O
L O
ctx O
and O
embedding O
dimension O
d. O
We O
pased O
Z O
V O
D O
to O
a O
vanilla O
Transformer O
network O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
through O
multiple O
multi O
- O
head O
attention O
layers O
with O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
and O
residual O
connections O
( O
He O
et O
al O
. O
, O
2016 O
) O
. O
Each O
layer O
allows O
multimodal O
interactions O
between O
object O
- O
level O
representations O
from O
videos O
and O
word O
- O
level O
representations O
from O
dialogues O
. O

Dialogue O
State O
Decoder O
and O

Self O
- O
supervised O
Video O
Denoising O
Decoder O
State O
Decoding O
. O
This O
module O
decodes O
dialogue O
state O
sequence O
auto O
- O
regressively O
, O
i.e. O
each O
token O
is O
conditioned O
on O
all O
dialogue O
and O
video O
representations O
as O
well O
as O
all O
tokens O
previously O
decoded O
. O
At O
the O
first O
decoding O
position O
, O
a O
special O
token O
STATE O
is O
embedded O
into O
dimension O
d O
( O
by O
a O
learned O
embedding O
layer O
and O
sinusoidal O
positional O
encoding O
) O
and O
concatenated O
to O
Z O
V O
D O
. O
The O
resulting O
sequence O
is O
passed O
to O
the O
Transformer O
network O
and O
the O
output O
representations O
of O
STATE O
are O
passed O
to O
a O
linear O
network O
layer O
that O
transforms O
representations O
to O
state O
vocabulary O
embedding O
space O
. O
The O
decoder O
applies O
the O
same O
procedure O
for O
the O
subsequent O
positions O
to O
decode O
dialogue O
states O
auto O
- O
regressively O
. O
Denoting O
b O
k O
, O
t O
as O
the O
k O
th O
token O
in O
B O
t O
, O
i.e. O
token O
of O
slot O
, O
object O
identity O
, O
or O
slot O
value O
, O
we O
defined O
the O
DST O
loss O
function O
as O
the O
negative O
log O
- O
likelihood O
: O

L O
dst O
= O
− O
log O
P O
( O
b O
k O
, O
t O
|b O
< O
k O
, O
t O
, O
X O
ctx O
, O
X O
obj O
) O

Note O
that O
this O
decoder O
design O
partially O
avoids O
the O
assumption O
of O
conditionally O
independent O
slots O
. O
During O
test O
time O
, O
we O
applied O
beam O
search O
to O
decode O
states O
with O
the O
maximum O
length O
of O
25 O
tokens O
in O
all O
models O
and O
a O
beam O
size O
5 O
. O
An O
END_STATE O
token O
is O
used O
to O
mark O
the O
end O
of O
each O
sequence O
. O

Visual O
Denoising O
Decoding O
. O
Finally O
, O
moving O
away O
from O
conventional O
unimodal O
DST O
, O
we O
proposed O
to O
enhance O
our O
DST O
model O
with O
a O
Visual O
Decoder O
that O
learns O
to O
recover O
visual O
representations O
in O
a O
self O
- O
supervised O
learning O
task O
to O
improve O
video O
representation O
learning O
. O
Specifically O
, O
during O
training O
time O
, O
we O
randomly O
sampled O
visual O
representations O
and O
masked O
each O
of O
them O
with O
a O
zero O
vector O
. O
At O
the O
object O
level O
, O
in O
the O
m O
th O
video O
frame O
, O
we O
randomly O
masked O
a O
row O
from O
X O
bb O
( O
m O
) O
∈ O
R O
N O
obj O
×4 O
. O
Since O
each O
row O
represents O
an O
object O
, O
we O
selected O
a O
row O
to O
mask O
by O
a O
random O
object O
index O
j O
∈ O
[ O
1 O
, O
N O
obj O
] O
such O
that O
the O
same O
object O
has O
not O
been O
masked O
in O
the O
preceding O
frame O
or O
following O
frame O
. O
We O
denote O
the O
Transformer O
output O
representations O
from O
video O
inputs O
as O
Z O
′ O
V O
∈ O
R O
L O
obj O
×d O
. O
This O
vector O
is O
passed O
to O
a O
linear O
mapping O
f O
bb O
to O
bounding O
box O
features O
R O
4 O
. O
We O
defined O
the O
learning O
objective O
as O
: O

L O
obj O
= O
o O
1 O
masked O
× O
l O
( O
f O
bb O
( O
Z O
′ O
V O
, O
o O
) O
, O
X O
bb O
, O
o O
) O

where O
l O
is O
a O
loss O
function O
and O
1 O
masked O
= O
{ O
0 O
, O
1 O
} O
is O
a O
masking O
indicator O
. O
We O
experimented O
with O
both O
L1 O
and O
L2 O
loss O
and O
reported O
the O
results O
. O
Similarly O
, O
at O
the O
segment O
level O
, O
we O
randomly O
selected O
a O
segment O
to O
mask O
such O
that O
the O
preceding O
or O
following O
segments O
have O
not O
been O
chosen O
for O
masking O
: O

L O
seg O
= O
s O
1 O
masked O
× O
l O
( O
f O
cnn O
( O
Z O
′ O
V O
, O
s O
) O
, O
X O
cnn O
, O
s O
) O
4 O
Experiments O
4.1 O
Experimental O
Setup O
Dataset O
. O

In O
existing O
benchmarks O
of O
multimodal O
dialogues O
such O
as O
VisDial B-DatasetName
( O
Das O
et O
al O
. O
, O
2017a O
) O
and O
AVSD B-DatasetName
( O
Alamri O
et O
al O
. O
, O
2019 O
) O
, O
we O
observed O
that O
a O
large O
number O
of O
data O
samples O
contain O
strong O
distribution O
bias O
in O
dialogue O
context O
, O
in O
which O
dialogue O
agents O
can O
simply O
ignore O
the O
whole O
dialogue O
and O
rely O
on O
image O
- O
only O
features O
( O
Kim O
et O
al O
. O
, O
2020 O
) O
. O
Another O
observed O
bias O
is O
the O
annotator O
bias O
that O
makes O
a O
causal O
link O
between O
dialogue O
context O
and O
output O
response O
actually O
harmful O
( O
Qi O
et O
al O
. O
, O
2020 O
) O
( O
as O
annotator O
's O
preferences O
are O
treated O
as O
a O
confounding O
factor O
) O
. O
The O
above O
biases O
would O
obviate O
the O
need O
for O
a O
DST O
task O
. O
To O
address O
the O
above O
biases O
, O
Le O
et O
al O
. O
( O
2021b O
) O
developed O
a O
Diagnostic B-DatasetName
Benchmark I-DatasetName
for I-DatasetName
Videogrounded I-DatasetName
Dialogues I-DatasetName
( O
" O
DVD B-DatasetName
" O
) O
, O
by O
synthetically O
creating O
dialogues O
that O
are O
grounded O
on O
videos O
from O
CATER B-DatasetName
videos O
( O
Shamsian O
et O
al O
. O
, O
2020 O
) O
. O
The O
videos O
contain O
visually O
simple O
yet O
highly O
varied O
objects O
. O
The O
dialogues O
are O
synthetically O
designed O
with O
both O
short O
- O
term O
and O
long O
- O
term O
object O
references O
. O
These O
specifications O
remove O
the O
annotation O
bias O
in O
terms O
of O
object O
appearances O
in O
visual O
context O
and O
crossturn O
dependencies O
in O
dialogue O
context O
. O
Extension O
from O
DVD B-DatasetName
( O
Le O
et O
al O
. O
, O
2021b O
) O
. O
We O
generated O
new O
dialogues O
following O
Le O
et O
al O
. O
( O
2021b O
) O
's O
procedures O
but O
based O
on O
an O
extended O
CATER B-DatasetName
video O
split O
( O
Shamsian O
et O
al O
. O
, O
2020 O
) O
rather O
than O
the O
original O
CATER B-DatasetName
video O
data O
( O
Girdhar O
and O
Ramanan O
, O
2020 O
) O
. O
We O
chose O
the O
extended O
CATER B-DatasetName
split O
( O
Shamsian O
et O
al O
. O
, O
2020 O
) O
as O
it O
includes O
additional O
annotations O
of O
ground O
- O
truth O
bounding O
box O
boundaries O
of O
visual O
objects O
in O
video O
frames O
. O
This O
annotation O
facilitates O
experiments O
with O
Faster B-TaskName
- I-TaskName
RCNN I-TaskName
finetuned O
on O
CATER B-DatasetName
objects O
and O
experiments O
with O
models O
of O
perfect O
visual O
perception O
, O
i.e. O
P O
( O
o O
j O
|V O
, O
ω O
) O
≈ O
1 O
. O
As O
shown O
in O
( O
Le O
et O
al O
. O
, O
2021b O
) O
, O
objects O
can O
be O
uniquely O
referred O
in O
utterances O
based O
on O
their O
appearance O
by O
one O
or O
more O
following O
aspects O
: O
" O
size O
" O
, O
" O
color O
" O
, O
" O
material O
" O
, O
and O
" O
shape O
" O
. O
We O
directly O
reuse O
these O
and O
define O
them O
as O
slots O
in O
our O
dialogue O
states O
, O
in O
addition O
to O
2 O
temporal O
slots O
for O
s O
start O
and O
s O
end O
. O
We O
denote O
the O
new O
benchmark O
as O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
and O
summarize O
the O
dataset O
in O
Table O
1 O
( O
for O
more O
detail O
, O
please O
refer O
to O
Appendix O
B O
) O
. O

Baselines O
. O
To O
benchmark O
VDTN B-MethodName
, O
we O
compared O
the O
model O
with O
following O
baseline O
models O
, O
including O
both O
rule O
- O
based O
models O
and O
trainable O
models O
: O

• O
Q O
- O
retrieval O
( O
tf O
- O
idf O
) O
, O
for O
each O
test O
sample O
, O
directly O
retrieves O
the O
training O
sample O
with O
the O
most O
similar O
question O
utterance O
and O
use O
its O
state O
as O
the O
predicted O
state O
; O

• O
State O
prior O
selects O
the O
most O
common O
tuple O
of O
( O
object O
, O
slot O
, O
value O
) O
in O
training O
split O
and O
uses O
it O
as O
predicted O
states O
; O

• O
Object O
( O
random O
) O
, O
for O
each O
test O
sample O
, O
randomly O
selects O
one O
object O
predicted O
by O
the O
visual O
perception O
model O
and O
a O
random O
( O
slot O
, O
value O
) O
tuple O
( O
with O
slots O
and O
values O
inferred O
from O
object O
classes O
) O
as O
the O
predicted O
state O
; O

• O
Object O
( O
all O
) O
is O
similar O
to O
the O
prior O
baseline O
but O
selects O
all O
possible O
objects O
and O
all O
possible O
( O
slot O
, O
value O
) O
tuples O
as O
the O
predicted O
state O
; O

• O
RNN B-MethodName
( I-MethodName
+Att I-MethodName
) I-MethodName
uses O
RNN O
as O
encoder O
and O
an O
MLP O
network O
as O
decoder O
. O
Another O
variant O
of O
the O
model O
is O
enhanced O
with O
a O
vanilla O
dot O
- O
product O
attention O
at O
each O
decoding O
step O
; O

• O
We O
adapted O
and O
experimented O
with O
strong O
unimodal O
DST O
baselines O
, O
including O
: O
TRADE B-MethodName
( O
Wu O
et O
al O
. O
, O
2019 O
) O
, O
UniConv B-MethodName
( O
Le O
et O
al O
. O
, O
2020b O
) O
and O
NADST B-MethodName
( O
Le O
et O
al O
. O
, O
2020c O
) O
. O

We O
implemented O
these O
baselines O
and O
tested O
them O
on O
dialogues O
with O
or O
without O
videos O
. O
When O
video O
inputs O
are O
applied O
, O
we O
embedded O
both O
object O
and O
segment O
- O
level O
features O
( O
See O
Section O
3.1 O
) O
. O
The O
video O
context O
features O
are O
integrated O
into O
baselines O
in O
the O
same O
techniques O
in O
which O
the O
original O
models O
treat O
dialogue O
context O
features O
. O

Training O
. O
We O
trained O
VDTN B-MethodName
by O
jointly O
minimizing O
L O
dst O
and O
L O
bb O
/ O
seg O
. O
We O
trained O
all O
models O
using O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
a O
warm O
- O
up O
learning O
rate O
period O
of O
1 O
epoch O
and O
the O
learning O
rate O
decays O
up O
to O
160 O
epochs O
. O
Models O
are O
selected O
based O
on O
the O
average O
L O
dst O
on O
the O
validation O
set O
. O
To O
standardize O
model O
sizes O
, O
we O
selected O
embedding B-HyperparameterName
dimension I-HyperparameterName
d B-HyperparameterName
= O
128 B-HyperparameterValue
for O
all O
models O
, O
and O
experimented O
with O
both O
shallow O
( O
N O
= O
1 O
) O
and O
deep O
networks O
( O
N O
= O
3 O
) O
( O
by O
stacking O
attention O
or O
RNN O
blocks O
) O
, O
and O
8 O
attention O
heads O
in O
Transformer O
backbones O
. O
We O
implemented O
models O
in O
Pytorch O
and O
released O
the O
code O
and O
model O
checkpoints O
1 O
. O
Refer O
to O
Appendix O
C O
for O
more O
training O
details O
. O

Evaluation O
. O
We O
followed O
the O
unimodal O
DST O
task O
Henderson O
et O
al O
. O
, O
2014a O
) O
and O
used O
the O
state O
accuracy O
metric O
. O
The O
prediction O
is O
counted O
as O
correct O
only O
when O
all O
the O
component O
values O
exactly O
match O
the O
oracle O
values O
. O
In O
multimodal O
states O
, O
there O
are O
both O
discrete O
slots O
( O
object O
attributes O
) O
as O
well O
as O
continuous O
slots O
( O
temporal O
start O
and O
end O
time O
) O
. O
For O
continuous O
slots O
, O
we O
followed O
Gao O
et O
al O
. O
, O
2017 O
) O
by O
using O
Intersection B-MetricName
- I-MetricName
over I-MetricName
- I-MetricName
Union I-MetricName
( O
IoU B-MetricName
) O
between O
predicted O
temporal O
segment O
and O
ground O
- O
truth O
segment O
. O
The O
predicted O
segment O
is O
counted O
as O
correct O
if O
its O
IoU B-MetricName
with O
the O
oracle O
is O
more O
than O
p O
, O
where O
we O
chose O
p O
= O
{ O
0.5 O
, O
0.7 O
} O
. O
We O
reported O
the O
joint B-MetricName
state I-MetricName
accuracy I-MetricName
of O
discrete O
slots O
only O
( O
" O
Joint B-MetricName
Acc I-MetricName
" O
) O
as O
well O
as O
all O
slot O
values O
( O
" O
Joint B-MetricName
Acc I-MetricName
IoU I-MetricName
@ I-MetricName
p I-MetricName
" O
) O
. O
We O
also O
reported O
the O
performance O
of O
component O
state O
predictions O
, O
including O
predictions O
of O
object O
identities O
o O
j O
, O
object O
slot O
tuples O
( O
o O
j O
, O
s O
i O
, O
v O
i O
, O
j O
) O
, O
and O
object O
state O
tuples O
( O
o O
j O
, O
s O
i O
, O
v O
i O
, O
j O
) O
∀s O
i O
∈ O
S. O
Since O
a O
model O
may O
simply O
output O
all O
possible O
object O
identities O
and O
slot O
values O
and O
achieve O
100 O
% O
component O
accuracies O
, O
we O
reported O
the O
F1 B-MetricName
metric O
for O
each O
of O
these O
component O
predictions O
. O

Results O

Overall O
results O
. O
From O
Table O
2 O
, O
we O
have O
the O
following O
observations O
: O

• O
we O
noted O
that O
simply O
using O
naive O
retrieval O
models O
such O
as O
Q O
- O
retrieval O
achieved O
zero O
joint O
state O
accuracy O
only O
. O
State O
prior O
achieved O
only O
about O
15 B-MetricValue
% I-MetricValue
and O
8 B-MetricValue
% I-MetricValue
F1 B-MetricName
on O
object O
identities O
and O
object O
slots O
, O
showing O
that O
a O
model O
can O
not O
simply O
rely O
on O
distribution O
bias O
of O
dialogue O
states O
. O
• O
The O
results O
of O
Object O
( O
random O
/ O
all O
) O
show O
that O
in O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
, O
dialogues O
often O
focus O
on O
a O
subset O
of O
visual O
objects O
and O
an O
object O
perception O
model O
alone O
can O
not O
perform O
well O
. O

• O
The O
performance O
gains O
of O
RNN O
models O
show O
the O
benefits O
of O
neural O
network O
models O
compared O
to O
retrieval O
models O
. O
The O
higher O
results O
of O
RNN B-MethodName
( I-MethodName
D I-MethodName
) I-MethodName
against O
RNN B-MethodName
( I-MethodName
V I-MethodName
) I-MethodName
showed O
the O
dialogue O
context O
is O
essential O
and O
reinforced O
the O
above O
observation O
. O

• O
Comparing O
TRADE B-DatasetName
and O
UniConv B-MethodName
, O
we O
noted O
that O
TRADE B-DatasetName
performed O
slightly O
better O
in O
component O
predictions O
, O
but O
was O
outperformed O
in O
joint O
state O
prediction O
metrics O
. O
This O
showed O
the O
benefits O
of O
UniConv B-MethodName
which O
avoids O
the O
assumptions O
of O
conditionally O
independent O
slots O
and O
learns O
to O
extract O
the O
dependencies O
between O
slot O
values O
. O

• O
Results O
of O
TRADE B-DatasetName
, O
UniConv B-MethodName
, O
and O
NADST B-MethodName
all O
displayed O
minor O
improvement O
when O
adding O
video O
inputs O
to O
dialogue O
inputs O
, O
displaying O
their O
weakness O
when O
exposed O
to O
crossmodality O
learning O
. O

• O
VDTN B-MethodName
achieves O
significant O
performance O
gains O
and O
achieves O
the O
SOTA O
results O
in O
all O
component O
or O
joint O
prediction O
metrics O
. O

We O
also O
experimented O
with O
a O
version O
of O
VDTN B-MethodName
in O
which O
the O
transformer O
network O
( O
Section O
3 O
. O
( O
Lu O
et O
al O
. O
, O
2019 O
; O
, O
to O
MM B-TaskName
- I-TaskName
DST I-TaskName
task O
. O

Impacts O
of O
self O
- O
supervised O
video O
representation O
learning O
. O
From O
Table O
3 O
, O
we O
noted O
that O
compared O
to O
a O
model O
trained O
only O
with O
the O
DST O
objective O
L O
dst O
, O
models O
enhanced O
with O
self O
- O
supervised O
video O
understanding O
objectives O
can O
improve O
the O
results O
. O
However O
, O
we O
observe O
that O
L1 O
loss O
works O
more O
consistently O
than O
L2 O
loss O
in O
most O
cases O
. O
Since O
L2 O
loss O
minimizes O
the O
squared O
differences O
between O
predicted O
and O
ground O
- O
truth O
values O
, O
it O
may O
be O
susceptible O
to O
outliers O
( O
of O
segment O
features O
or O
bounding O
boxes O
) O
in O
the O
dataset O
. O
Since O
we O
could O
not O
control O
these O
outliers O
, O
an O
L1 O
loss O
is O
more O
suitable O
. O
We O
also O
tested O
with O
L O
obj O
( O
tracking O
) O
, O
in O
which O
we O
used O
oracle O
bounding O
box O
labels O
during O
training O
, O
and O
simply O
passed O
the O
features O
of O
all O
objects O
to O
VDTN B-MethodName
. O
This O
modification O
treats O
the O
self O
- O
supervised O
learning O
task O
as O
an O
object O
tracking O
task O
in O
which O
all O
output O
representations O
are O
used O
to O
predict O
the O
ground O
- O
truth O
bounding O
box O
coordinates O
of O
all O
objects O
. O
Interestingly O
, O
we O
found O
L O
obj O
( O
tracking O
) O
only O
improves O
the O
results O
insignificantly O
, O
as O
compared O
to O
the O
self O
- O
supervised O
learning O
objective O
L O
obj O
. O
This O
indicates O
that O
our O
self O
- O
supervised O
learning O
tasks O
do O
not O
strongly O
depend O
on O
the O
availability O
of O
object O
boundary O
labels O
. O
Finally O
, O
we O
found O
combining O
both O
segment O
- O
level O
and O
object O
- O
level O
self O
- O
supervision O
is O
not O
useful O
. O
This O
is O
possibly O
due O
to O
our O
current O
masking O
strategy O
that O
masks O
object O
and O
segment O
features O
independently O
. O
Therefore O
, O
the O
resulting O
context O
features O
might O
not O
be O
sufficient O
for O
recovering O
masked O
representations O
. O
Future O
work O
can O
be O
extended O
by O
studying O
a O
codependent O
masking O
technique O
to O
combine O
segmentbased O
and O
object O
- O
based O
representation O
learning O
. O

Impacts O
of O
video O
features O
and O
time O
- O
based O
slots O
. O

Table O
4 O
shows O
the O
results O
of O
different O
variants O
of O
VDTN B-MethodName
models O
. O
We O
observed O
that O
: O

• O
Segment O
- O
based O
learning O
is O
marginally O
more O
powerful O
than O
object O
- O
based O
learning O
. O

• O
By O
considering O
the O
temporal O
placement O
of O
objects O
and O
defining O
time O
- O
based O
slots O
, O
we O
noted O
the O
performance O
gains O
by O
" O
Joint B-MetricName
Obj I-MetricName
State I-MetricName
Acc I-MetricName
" O
( O
B O
vs. O
B\time O
) O
. O
The O
performance O
gains O
show O
the O
interesting O
relationships O
between O
temporal O
slots O
and O
discrete O
- O
only O
slots O
and O
the O
benefits O
of O
modelling O
both O
in O
dialogue O
states O
. O

• O
Finally O
, O
even O
with O
only O
object O
- O
level O
features O
X O
bb O
, O
we O
still O
observed O
performance O
gains O
from O
using O
self O
- O
supervised O
loss O
L O
obj O
, O
confirming O
the O
benefits O
of O
better O
visual O
representation O
learning O
. O
Ablation O
analysis O
by O
turn O
positions O
. O
Figure O
3 O
reported O
the O
results O
of O
VDTN B-MethodName
predictions O
of O
states O
that O
are O
separated O
by O
the O
corresponding O
dialogue O
positions O
. O
The O
results O
are O
from O
the O
VDTN B-MethodName
model O
trained O
with O
both O
L O
dst O
and O
L O
seg O
. O
As O
expected O
, O
we O
observed O
a O
downward O
trend O
of O
results O
as O
the O
turn O
position O
increases O
. O
We O
noted O
that O
state O
accuracy O
reduces O
more O
dramatically O
( O
as O
shown O
by O
" O
Joint O
Acc O
" O
) O
than O
the O
F1 O
metrics O
of O
component O
predictions O
. O
For O
instance O
, O
" O
Object O
Identity O
F1 O
" O
shows O
almost O
stable O
performance O
lines O
through O
dialogue O
turns O
. O
Interestingly O
, O
we O
noted O
that O
the O
prediction O
performance O
of O
dialogue O
states O
with O
temporal O
slots O
only O
deteriorates O
dramatically O
after O
turn O
2 O
onward O
. O
We O
expected O
that O
VDTN B-MethodName
is O
able O
to O
learn O
short O
- O
term O
dependencies O
( O
1 O
- O
turn O
distance O
) O
between O
temporal O
slots O
, O
but O
failed O
to O
deal O
with O
long O
- O
term O
dependencies O
( O
> O
1 O
- O
turn O
distance O
) O
between O
temporal O
slots O
. O
In O
all O
metrics O
, O
we O
observed O
VDTN B-MethodName
outperforms O
both O
RNN B-MethodName
baseline O
and O
UniConv B-MethodName
( O
Le O
et O
al O
. O
, O
2020b O
) O
, O
across O
all O
turn O
positions O
. O
However O
, O
future O
work O
is O
needed O
to O
close O
the O
performance O
gaps O
between O
lower O
and O
higher O
turn O
positions O
. O

Impacts O
on O
downstream O
response O
prediction O
task O
. O
Finally O
, O
we O
tested O
the O
benefits O
of O
studying O
multimodal O
DST O
for O
a O
response O
prediction O
task O
. O
Specifically O
, O
we O
used O
the O
best O
VDTN B-MethodName
model O
to O
predict O
dialogue O
states O
across O
all O
samples O
in O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
. O
We O
then O
used O
the O
predicted O
slots O
, O
including O
object O
identities O
and O
temporal O
slots O
, O
to O
select O
the O
video O
features O
. O
The O
features O
are O
the O
visual O
objects O
and O
segments O
that O
are O
parts O
of O
the O
predicted O
dialogue O
states O
. O
We O
then O
used O
these O
selected O
features O
as O
input O
to O
train O
new O
Transformer O
decoder O
models O
which O
are O
added O
with O
an O
MLP O
as O
the O
response O
prediction O
layer O
. O
Note O
that O
these O
models O
are O
trained O
only O
with O
a O
cross O
- O
entropy O
loss O
to O
predict O
answer O
candidates O
. O
From O
Table O
5 O
, O
we O
observed O
the O
benefits O
of O
filtering O
visual O
inputs O
by O
predicted O
states O
, O
with O
up O
to O
5.9 B-MetricValue
% I-MetricValue
accuracy B-MetricName
score O
improvement O
3 O
. O
Note O
that O
there O
are O
more O
sophisticated O
approaches O
such O
as O
neural O
module O
networks O
and O
symbolic O
reasoning O
( O
Chen O
et O
al O
. O
, O
2020 O
) O
to O
fully O
exploit O
the O
decoded O
dialogue O
states O
. O
We O
leave O
these O
extensions O
for O
future O
research O
. O

For O
more O
experiment O
results O
, O
analysis O
, O
and O
qualitative O
examples O
, O
please O
refer O
to O
Appendix O
D O
. O

Discussion O
and O
Conclusion O

Compared O
to O
conventional O
DST O
Lei O
et O
al O
. O
, O
2018b O
; O
Gao O
et O
al O
. O
, O
2019 O
; O
Le O
et O
al O
. O
, O
2020c O
) O
, O
we O
show O
that O
the O
scope O
of O
DST O
can O
be O
further O
extended O
to O
a O
multimodal O
world O
. O
Compared O
to O
prior O
work O
in O
multimodal O
dialogues O
( O
Das O
et O
al O
. O
, O
2017a O
; O
Hori O
et O
al O
. O
, O
2019 O
; O
Thomason O
et O
al O
. O
, O
2019 O
) O
which O
focuses O
more O
on O
vision O
- O
language O
interactions O
, O
our O
work O
was O
inspired O
from O
a O
dialogue O
- O
based O
strategy O
with O
a O
formulation O
of O
a O
dialogue O
state O
tracking O
task O
. O
For O
more O
comparison O
to O
related O
work O
, O
please O
refer O
to O
Appendix O
A O
. O

We O
noted O
the O
current O
work O
are O
limited O
to O
a O
synthetic O
benchmark O
with O
a O
limited O
video O
domain O
( O
3D O
objects O
) O
. O
However O
, O
we O
expect O
that O
MM B-TaskName
- I-TaskName
DST I-TaskName
task O
is O
still O
applicable O
and O
can O
be O
extended O
to O
other O
video O
domains O
( O
e.g. O
videos O
of O
humans O
) O
. O
We O
expect O
that O
MM B-TaskName
- I-TaskName
DST I-TaskName
is O
useful O
in O
dialogues O
centered O
around O
a O
" O
focus O
group O
" O
of O
objects O
. O
For O
further O
discussion O
of O
limitations O
, O
please O
refer O
to O
Appendix O
E O
. O

In O
summary O
, O
in O
this O
work O
, O
we O
introduced O
a O
novel O
MM B-TaskName
- I-TaskName
DST I-TaskName
task O
that O
tracks O
visual O
objects O
and O
their O
attributes O
mentioned O
in O
dialogues O
. O
For O
this O
task O
, O
we O
experimented O
on O
a O
synthetic O
benchmark O
with O
videos O
simulated O
in O
a O
3D O
environment O
and O
dialogues O
grounded O
on O
these O
objects O
. O
Finally O
we O
proposed O
VDTN B-MethodName
, O
a O
Transformer O
- O
based O
model O
with O
self O
- O
supervised O
learning O
objectives O
on O
object O
and O
segment O
- O
level O
visual O
representations O
. O

Broader O
Impacts O

During O
the O
research O
of O
this O
work O
, O
there O
is O
no O
human O
subject O
involved O
and O
hence O
, O
no O
ethical O
concerns O
regarding O
the O
experimental O
procedures O
and O
results O
. O
The O
data O
is O
used O
from O
a O
synthetically O
developed O
dataset O
, O
in O
which O
all O
videos O
are O
simulated O
in O
a O
3D O
environment O
with O
synthetic O
non O
- O
human O
visual O
objects O
. O
We O
intentionally O
chose O
this O
dataset O
to O
minimize O
any O
distribution O
bias O
and O
make O
fair O
comparisons O
between O
all O
baseline O
models O
. O

However O
, O
we O
wanted O
to O
emphasize O
on O
ethical O
usage O
of O
any O
potential O
adaptation O
of O
our O
methods O
in O
real O
applications O
. O
Considering O
the O
development O
of O
AI O
in O
various O
industries O
, O
the O
technology O
introduced O
in O
this O
paper O
may O
be O
used O
in O
practical O
applications O
, O
such O
as O
dialogue O
agents O
with O
human O
users O
. O
In O
these O
cases O
, O
the O
adoption O
of O
the O
MM B-TaskName
- I-TaskName
DST I-TaskName
task O
or O
VDTN B-MethodName
should O
be O
strictly O
used O
to O
improve O
the O
model O
performance O
and O
only O
for O
legitimate O
and O
authorized O
purposes O
. O
It O
is O
crucial O
that O
any O
plan O
to O
apply O
or O
extend O
MM B-TaskName
- I-TaskName
DST I-TaskName
in O
real O
systems O
should O
consider O
carefully O
all O
potential O
stakeholders O
as O
well O
as O
the O
risk O
profiles O
of O
application O
domains O
. O
For O
instance O
, O
in O
case O
a O
dialogue O
state O
is O
extended O
to O
human O
subjects O
, O
any O
information O
used O
as O
slots O
should O
be O
clearly O
informed O
and O
approved O
by O
the O
human O
subjects O
before O
the O
slots O
are O
tracked O
. O

A O
Details O
of O
Related O
Work O

Our O
work O
is O
related O
to O
the O
following O
domains O
: O

A.1 O
Dialogue O
State O
Tracking O

Dialogue O
State O
Tracking O
( O
DST O
) O
research O
aims O
to O
develop O
models O
that O
can O
track O
essential O
information O
conveyed O
in O
dialogues O
between O
a O
dialogue O
agent O
and O
human O
( O
defined O
as O
hidden O
information O
state O
by O
or O
belief O
state O
by O
) O
. O
DST O
research O
has O
evolved O
largely O
within O
the O
domain O
of O
task O
- O
oriented O
dialogue O
systems O
. O
DST O
is O
conventionally O
designed O
in O
a O
modular O
dialogue O
system O
Le O
et O
al O
. O
, O
2020b O
) O
and O
preceded O
by O
a O
Natural O
Language O
Understanding O
( O
NLU O
) O
component O
. O
NLU O
learns O
to O
label O
sequences O
of O
dialogue O
utterances O
and O
provides O
a O
tag O
for O
each O
word O
token O
( O
often O
in O
the O
form O
of O
In O
- O
Out O
- O
Begin O
representations O
) O
( O
Kurata O
et O
al O
. O
, O
2016 O
; O
Shi O
et O
al O
. O
, O
2016 O
; O
Rastogi O
et O
al O
. O
, O
2017 O
) O
. O

To O
avoid O
credit O
assignment O
problems O
and O
streamline O
the O
modular O
designs O
, O
NLU O
and O
DST O
have O
been O
integrated O
into O
a O
single O
module O
Xu O
and O
Hu O
, O
2018 O
; O
Zhong O
et O
al O
. O
, O
2018 O
) O
. O
These O
DST O
approaches O
can O
be O
roughly O
categorized O
into O
two O
types O
: O
fixed O
- O
vocabulary O
or O
open O
- O
vocabulary O
. O
Fixedvocabulary O
approaches O
are O
designed O
for O
classification O
tasks O
which O
assume O
a O
fixed O
set O
of O
( O
slot O
, O
value O
) O
candidates O
and O
directly O
retrieve O
items O
from O
this O
set O
to O
form O
dialogue O
states O
during O
test O
time O
( O
Henderson O
et O
al O
. O
, O
2014b O
; O
. O
More O
recently O
, O
we O
saw O
more O
approaches O
toward O
open O
- O
vocabulary O
strategies O
which O
learn O
to O
generate O
candidates O
based O
on O
input O
dialogue O
context O
( O
Lei O
et O
al O
. O
, O
2018b O
; O
Gao O
et O
al O
. O
, O
2019 O
; O
Wu O
et O
al O
. O
, O
2019 O
; O
Le O
et O
al O
. O
, O
2020c O
) O
. O
Our O
work O
is O
more O
related O
to O
open O
- O
vocabulary O
DST O
, O
but O
we O
essentially O
redefined O
the O
DST O
task O
with O
multimodality O
. O
Based O
on O
our O
literature O
review O
, O
we O
are O
the O
first O
to O
formally O
extend O
DST O
and O
bridge O
the O
gap O
between O
traditional O
task O
- O
oriented O
dialogues O
and O
multimodal O
dialogues O
. O

A.2 O
Visually O
- O
grounded O
Dialogues O

A O
novel O
challenge O
to O
machine O
intelligence O
, O
the O
intersection O
of O
vision O
and O
language O
research O
has O
expanded O
considerably O
in O
the O
past O
few O
years O
. O
Earlier O
benchmarks O
test O
machines O
to O
perceive O
visual O
inputs O
, O
and O
learn O
to O
generate O
captions O
( O
Farhadi O
et O
al O
. O
, O
2010 O
; O
Lin O
et O
al O
. O
, O
2014 O
; O
Rohrbach O
et O
al O
. O
, O
2015 O
) O
, O
ground O
text O
phrases O
and O
objects O
( O
Kazemzadeh O
et O
al O
. O
, O
2014 O
; O
Plummer O
et O
al O
. O
, O
2015 O
) O
, O
and O
answer O
questions O
about O
the O
visual O
contents O
( O
Antol O
et O
al O
. O
, O
2015 O
; O
Zhu O
et O
al O
. O
, O
2016 O
; O
Jang O
et O
al O
. O
, O
2017 O
; O
Lei O
et O
al O
. O
, O
2018a O
) O
. O
As O
an O
orthogonal O
development O
from O
Visual O
Question O
Answering O
problems O
, O
we O
noted O
recent O
work O
that O
targets O
vision O
- O
language O
in O
dialogue O
context O
, O
in O
which O
an O
image O
or O
video O
is O
given O
and O
the O
dialogue O
utterances O
are O
centered O
around O
its O
visual O
contents O
( O
De O
Vries O
et O
al O
. O
, O
2017 O
; O
Das O
et O
al O
. O
, O
2017a O
; O
Chattopadhyay O
et O
al O
. O
, O
2017 O
; O
Hori O
et O
al O
. O
, O
2019 O
; O
Thomason O
et O
al O
. O
, O
2019 O
; O
Le O
et O
al O
. O
, O
2021b O
) O
. O
Recent O
work O
has O
addressed O
different O
challenges O
in O
visually O
- O
grounded O
dialogues O
, O
including O
multimodal O
integration O
( O
Hori O
et O
al O
. O
, O
2019 O
; O
Le O
et O
al O
. O
, O
2019 O
; O
, O
crossturn O
dependencies O
( O
Das O
et O
al O
. O
, O
2017b O
; O
Schwartz O
et O
al O
. O
, O
2019 O
; O
Le O
et O
al O
. O
, O
2021a O
) O
, O
visual O
understanding O
( O
Le O
et O
al O
. O
, O
2020a O
) O
, O
and O
data O
distribution O
bias O
( O
Qi O
et O
al O
. O
, O
2020 O
) O
. O
Our O
work O
is O
more O
related O
to O
the O
challenge O
of O
visual O
object O
reasoning O
( O
Seo O
et O
al O
. O
, O
2017 O
; O
Kottur O
et O
al O
. O
, O
2018 O
) O
, O
but O
focused O
on O
a O
multi O
- O
turn O
tracking O
task O
over O
multiple O
turns O
of O
dialogue O
context O
. O
The O
prior O
approaches O
are O
not O
well O
designed O
to O
track O
objects O
and O
maintain O
a O
recurring O
memory O
or O
state O
of O
these O
objects O
from O
turn O
to O
turn O
. O
This O
challenge O
becomes O
more O
obvious O
when O
a O
dialogue O
involves O
multiple O
objects O
of O
similar O
characters O
or O
appearance O
. O
We O
directly O
tackles O
this O
challenge O
as O
we O
formulated O
a O
novel O
multimodal O
state O
tracking O
task O
and O
leveraged O
the O
research O
development O
from O
DST O
in O
task O
- O
oriented O
dialogue O
systems O
. O
As O
shown O
in O
our O
experiments O
, O
baseline O
models O
that O
use O
attention O
strategies O
similar O
to O
( O
Seo O
et O
al O
. O
, O
2017 O
; O
Kottur O
et O
al O
. O
, O
2018 O
) O
did O
not O
perform O
well O
in O
MM B-TaskName
- I-TaskName
DST I-TaskName
. O

A.3 O
Multimodal B-TaskName
DST I-TaskName

We O
noted O
a O
few O
studies O
have O
attempted O
to O
integrate O
some O
forms O
of O
state O
tracking O
in O
multimodal O
dialogues O
. O
In O
( O
Mou O
et O
al O
. O
, O
2020 O
) O
, O
however O
, O
we O
are O
not O
convinced O
that O
a O
dialogue O
state O
tracking O
task O
is O
a O
major O
focus O
, O
or O
correctly O
defined O
. O
In O
( O
Pang O
and O
Wang O
, O
2020 O
) O
, O
we O
noted O
that O
some O
form O
of O
object O
tracking O
is O
introduced O
throughout O
dialogue O
turns O
. O
The O
tracking O
module O
is O
used O
to O
decide O
which O
object O
the O
dialogue O
centers O
around O
. O
This O
method O
extends O
to O
multi O
- O
object O
tracking O
but O
the O
objects O
are O
only O
limited O
within O
static O
images O
, O
and O
there O
is O
no O
recurring O
information O
state O
( O
object O
attributes O
) O
maintained O
at O
each O
turn O
. O
Compared O
to O
our O
work O
, O
their O
tracking O
module O
only O
requires O
object O
identity O
as O
a O
single O
- O
slot O
state O
from O
turn O
to O
turn O
. O
Almost O
concurrent O
to O
our O
work O
, O
we O
noted O
( O
Kottur O
et O
al O
. O
, O
2021 O
) O
which O
formally O
, O
though O
very O
briefly O
, O
focuses O
on O
multimodal B-TaskName
DST I-TaskName
. O
However O
, O
the O
work O
is O
limited O
to O
the O
task O
- O
oriented O
domain O
, O
and O
each O
dialogue O
is O
only O
limited O
to O
a O
single O
goal O
- O
driven O
object O
in O
a O
synthetic O
image O
. O
While O
this O
definition O
is O
useful O
in O
the O
task O
- O
oriented O
dialogue O
domain O
, O
it O
does O
not O
account O
for O
the O
DST O
of O
multiple O
visual O
objects O
as O
defined O
in O
our O
work O
. O

B O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
Dataset O
Details O

For O
each O
of O
CATER B-DatasetName
videos O
from O
the O
extended O
split O
( O
Shamsian O
et O
al O
. O
, O
2020 O
) O
, O
we O
generated O
up O
to O
10 O
turns O
for O
each O
CATER B-DatasetName
video O
. O
In O
total O
, O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
contains O
more O
than O
13k O
dialogues O
, O
resulting O
in O
more O
130k O
( O
human O
, O
system O
) O
utterance O
pairs O
and O
corresponding O
dialogue O
states O
. O
A O
comparison O
of O
statistics O
of O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
and O
prior O
DST O
benchmarks O
can O
be O
seen O
in O
Table O
6 O
. O
We O
observed O
that O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
contains O
a O
larger O
scale O
data O
than O
the O
related O
DST O
benchmark O
. O
Even O
though O
the O
number O
of O
slots O
in O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
is O
only O
6 O
, O
lower O
than O
prior O
state O
tracking O
datasets O
, O
our O
experiments O
indicate O
that O
most O
current O
conventional O
DST O
models O
perform O
poorly O
on O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
. O
CATER B-DatasetName
universe O
. O
Figure O
4 O
displays O
the O
configuration O
of O
visual O
objects O
in O
the O
CATER B-DatasetName
universe O
. O
In O
total O
, O
there O
are O
3 O
object O
sizes O
, O
9 O
colors O
, O
2 O
materials O
, O
and O
5 O
shapes O
. O
These O
attributes O
are O
combined O
randomly O
to O
synthesize O
objects O
in O
each O
CATER B-DatasetName
video O
. O
We O
directly O
adopted O
these O
attributes O
as O
slots O
in O
dialogue O
states O
, O
and O
each O
dialogue O
utterance O
frequently O
refers O
to O
these O
objects O
by O
one O
or O
more O
attributes O
. O
In O
total O
, O
there O
are O
193 O
( O
size O
, O
color O
, O
material O
, O
shape O
) O
valid O
combinations O
, O
each O
of O
which O
corresponds O
to O
an O
object O
class O
in O
our O
models O
. O

Sample O
dialogues O
. O
Please O
refer O
to O
Figure O
5 O
, O
Table O
14 O
and O
Table O
15 O
. O
Usage O
. O
We O
want O
to O
highlight O
that O
the O
DVD B-DatasetName
- I-DatasetName
DST I-DatasetName
dataset O
should O
only O
be O
used O
for O
its O
intended O
purpose O
, O
i.e. O
to O
diagnose O
dialogue O
systems O
on O
their O
tracking O
abilities O
. O
Any O
derivatives O
of O
the O
data O
should O
be O
limited O
within O
the O
research O
contexts O
of O
MM B-TaskName
- I-TaskName
DST I-TaskName
. O

C O
Additional O
Training O
Details O

In O
practice O
, O
we O
applied O
label O
smoothing O
( O
Szegedy O
et O
al O
. O
, O
2016 O
) O
on O
state O
sequence O
labels O
to O
regularize O
the O
training O
. O
As O
the O
segment O
- O
level O
representations O
are O
stacked O
by O
the O
number O
of O
objects O
, O
we O
randomly O
selected O
only O
one O
vector O
per O
masked O
segment O
to O
apply O
L O
seg O
. O
We O
tested O
both O
L1 O
and O
L2 O
losses O
on O
L O
bb O
/ O
seg O
. O
All O
model O
parameters O
, O
except O
pretrained O
visual O
perception O
models O
, O
are O
initialized O
by O
a O
uniform O
distribution O
( O
Glorot O
and O
Bengio O
, O
2010 O
For O
fair O
comparison O
among O
baselines O
, O
all O
models O
use O
both O
object O
- O
level O
and O
segment O
- O
level O
feature O
representations O
, O
encoded O
by O
the O
same O
method O
as O
Describe O
in O
Section O
3.1 O
. O
In O
TRADE B-DatasetName
, O
the O
video O
representations O
are O
passed O
to O
an O
RNN O
encoder O
, O
and O
the O
output O
hidden O
states O
are O
concatenated O
to O
the O
dialogue O
hidden O
states O
. O
Both O
are O
passed O
to O
the O
original O
pointer O
- O
based O
decoder O
. O
In O
UniConv B-MethodName
and O
NADAST B-MethodName
, O
we O
stacked O
another O
Transformer O
attention O
layer O
to O
attend O
on O
video O
representations O
before O
the O
original O
state O
- O
to O
- O
dialogue O
attention O
layer O
. O
We O
all O
baseline O
models O
, O
we O
replaced O
the O
original O
( O
domain O
, O
slot O
) O
embeddings O
as O
( O
object O
class O
, O
slot O
) O
embeddings O
and O
kept O
the O
original O
model O
designs O
. O

Note O
that O
in O
our O
visual O
perception O
model O
, O
we O
adopted O
the O
finetuned O
Faster B-MethodName
R I-MethodName
- I-MethodName
CNN I-MethodName
model O
used O
by O
( O
Shamsian O
et O
al O
. O
, O
2020 O
) O
. O
The O
model O
was O
finetuned O
to O
predict O
object O
bounding O
boxes O
and O
object O
classes O
. O
The O
object O
classes O
are O
derived O
based O
on O
object O
appearance O
, O
based O
on O
the O
four O
attributes O
of O
size O
, O
color O
, O
material O
, O
and O
shape O
. O
In O
total O
, O
there O
are O
193 O
object O
classes O
. O
For O
segment O
embeddings O
, O
we O
adopted O
the O
ResNeXt-101 B-MethodName
model O
( O
Xie O
et O
al O
. O
, O
2017 O
) O
finetuned O
on O
Kinetics O
dataset O
( O
Kay O
et O
al O
. O
, O
2017 O
) O
. O
For O
all O
models O
( O
except O
for O
VDTN B-MethodName
ablation O
analysis O
) O
, O
we O
standardized O
N B-HyperparameterName
obj I-HyperparameterName
= O
10 B-HyperparameterValue
and O
N B-HyperparameterName
stride I-HyperparameterName
= O
12 B-HyperparameterValue
to O
sub O
- O
sample O
object O
and O
segment O
- O
level O
embeddings O
. O

Resources O
. O
Note O
that O
all O
experiments O
did O
not O
require O
particularly O
large O
computing O
resources O
as O
we O
limited O
all O
model O
training O
to O
a O
single O
GPU O
, O
specifically O
on O
a O
Tesla O
V100 O
GPU O
of O
16 O
G O
configuration O
. O

D O
Additional O
Results O

Greedy O
vs. O
Beam O
Search O
Decoding O
. O
Table O
7 O
shows O
the O
results O
of O
different O
variants O
of O
VDTN B-MethodName
models O
. O
We O
observed O
that O
compared O
to O
greedy O
decoding O
, O
beam O
search O
decoding O
improves O
the O
performance O
in O
all O
models O
. O
As O
beam O
search O
decoding O
selects O
the O
best O
decoded O
state O
by O
the O
joint O
probabilities O
of O
tokens O
, O
this O
observation O
indicates O
the O
benefits O
of O
considering O
slot O
values O
to O
be O
co O
- O
dependent O
and O
their O
relationships O
should O
be O
modelled O
. O
This O
is O
consistent O
with O
similar O
observations O
in O
later O
work O
of O
unimodal O
DST O
( O
Lei O
et O
al O
. O
, O
2018b O
; O
Le O
et O
al O
. O
, O
2020c O
) O
. O

Ablation O
analysis O
by O
component O
predictions O
. O

From O
Table O
8 O
, O
we O
have O
the O
following O
observations O
: O

( O
1 O
) O
In O
ablation O
results O
by O
component O
predictions O
, O
we O
noted O
that O
models O
can O
generally O
detect O
object O
identities O
well O
with O
F1 B-MetricName
about O
80 B-MetricValue
% I-MetricValue
. O
However O
, O
when O
considering O
object O
and O
slot O
tuples O
, O
F1 B-MetricName
reduces O
to O
48 B-MetricValue
− O
60 B-MetricValue
% I-MetricValue
, O
indicating O
the O
gaps O
are O
caused O
by O
slot O
value O
predictions O
. O
( O
2 O
) O
By O
individual O
slots O
, O
we O
noted O
" O
color O
" O
and O
" O
shape O
" O
slots O
are O
easier O
to O
track O
than O
" O
size O
" O
and O
" O
material O
" O
slots O
. O
We O
noted O
that O
in O
the O
CATER B-DatasetName
universe O
, O
the O
latter O
two O
slots O
have O
lower O
visual O
variances O
( O
less O
possible O
values O
) O
than O
the O
others O
. O
As O
a O
result O
, O
objects O
are O
more O
likely O
to O
share O
the O
same O
size O
or O
material O
and O
hence O
, O
discerning O
objects O
by O
those O
slots O
and O
tracking O
them O
in O
dialogues O
become O
more O
challenging O
. O
Table O
9 O
and O
10 O
display O
the O
ablation O
results O
by O
component O
predictions O
, O
using O
precision O
and O
recall O
metrics O
. O
We O
still O
noted O
consistent O
observations O
as O
described O
in O
Section O
4 O
. O
Notably O
, O
we O
found O
that O
current O
VDTN B-MethodName
models O
are O
better O
in O
tuning O
the O
correct O
predictions O
( O
as O
shown O
by O
high O
precision O
metrics O
) O
but O
still O
fail O
to O
select O
all O
components O
as O
a O
set O
( O
as O
shown O
by O
low O
recall O
metrics O
) O
. O
This O
might O
be O
caused O
by O
the O
upstream O
errors O
coming O
from O
the O
visual O
perception O
models O
, O
which O
may O
fail O
to O
visually O
perceive O
all O
objects O
and O
their O
attributes O
. O

Results O
by O
turn O
positions O
. O
Table O
11 O
reported O
the O
results O
of O
VDTN B-MethodName
predictions O
of O
states O
that O
are O
separated O
by O
the O
corresponding O
dialogue O
positions O
. O
The O
results O
are O
from O
the O
VDTN B-MethodName
model O
trained O
with O
both O
L O
dst O
and O
L O
seg O
. O
As O
expected O
, O
we O
observed O
a O
downward O
trend O
of O
results O
as O
the O
turn O
position O
increases O
. O

Impacts O
of O
dialogue O
context O
encoder O
. O
In O
Table O
12a O
, O
we O
observed O
the O
benefits O
of O
using O
the O
Markov O
process O
to O
decode O
dialogue O
states O
based O
on O
the O
dialogue O
states O
of O
the O
last O
turn O
B O
t−1 O
. O
This O
strategy O
allow O
us O
to O
discard O
parts O
of O
dialogue O
history O
that O
is O
already O
represented O
by O
the O
state O
. O
We O
noted O
that O
the O
optimal O
design O
is O
to O
use O
at O
least O
1 O
last O
dialogue O
turn O
as O
the O
dialogue O
history O
. O
In O
a O
hypothetical O
scenario O
, O
we O
applied O
the O
oracle O
B O
t−1 O
during O
test O
time O
, O
and O
noted O
the O
performance O
is O
improved O
significantly O
. O
This O
observation O
indicates O
the O
sensitivity O
of O
VDTN B-MethodName
to O
a O
turn O
- O
wise O
auto O
- O
regressive O
decoding O
process O
. O

Impacts O
of O
frame O
- O
level O
and O
segment O
- O
level O
sampling O
. O
As O
expected O
, O
Table O
12b O
displays O
higher O
performance O
with O
higher O
object O
limits O
N O
obj O
, O
which O
increases O
the O
chance O
of O
detecting O
the O
right O
visual O
objects O
in O
videos O
. O
We O
noted O
performance O
gains O
when O
sampling O
strides O
increase O
up O
to O
24 O
frames O
. O
However O
, O
in O
the O
extreme O
case O
, O
when O
sampling O
stride O
is O
300 O
frames O
, O
the O
performance O
on O
temporal O
slots O
reduce O
( O
as O
shown O
by O
" O
Joint O
State O
IoU O
@ O
p O
" O
) O
. O
This O
raises O
the O
issue O
to O
sample O
data O
more O
efficiently O
by O
balancing O
between O
temporal O
sparsity O
in O
videos O
and O
state O
prediction O
performance O
. O
We O
also O
observed O
that O
in O
a O
hypothetical O
scenario O
with O
a O
perfect O
object O
perception O
model O
, O
the O
performance O
improves O
significantly O
, O
especially O
on O
the O
predictions O
of O
discrete O
slots O
, O
although O
less O
effect O
on O
temporal O
slots O
. O

Impacts O
of O
object O
- O
level O
representation O
. O
Table O
13 O
reported O
the O
results O
when O
only O
segment O
- O
level O
features O
are O
used O
. O
We O
observed O
that O
both O
VDTN B-MethodName
and O
RNN B-MethodName
( I-MethodName
V+D I-MethodName
) I-MethodName
are O
affected O
significantly O
, O
specifically O
by O
24 B-MetricValue
% I-MetricValue
and O
3.1 B-MetricValue
% I-MetricValue
" O
Joint B-MetricName
Obj I-MetricName
State I-MetricName
Acc I-MetricName
" O
score O
respectively O
. O
Interestingly O
, O
we O
noted O
that O
RNN B-MethodName
( I-MethodName
V I-MethodName
) I-MethodName
, O
using O
only O
video O
inputs O
, O
are O
not O
affected O
by O
the O
removal O
of O
object O
- O
level O
features O
. O
These O
observations O
indicate O
that O
current O
MM B-TaskName
- I-TaskName
DST I-TaskName
requires O
object O
- O
level O
information O
. O
We O
expected O
that O
existing O
3DCNN O
models O
such O
as O
ResNeXt B-MethodName
still O
fail O
to O
capture O
such O
level O
of O
granularity O
. O

Qualitative O
analysis O
. O
Table O
14 O
and O
15 O
display O
2 O
sample O
dialogues O
and O
state O
predictions O
. O
We O
displayed O
the O
corresponding O
video O
screenshots O
for O
these O
dialogues O
in O
Figure O
5 O
. O
To O
cross O
- O
reference O
between O
videos O
and O
dialogues O
, O
we O
displayed O
the O
bounding O
boxes O
and O
their O
object O
classes O
in O
video O
screenshots O
. O
These O
object O
classes O
are O
indicated O
in O
ground O
- O
truth O
and O
decoded O
dialogue O
states O
in O
dialogues O
. O
Overall O
, O
we O
noted O
that O
VDTN B-MethodName
generated O
temporal O
slots O
of O
start O
and O
end O
time O
such O
that O
the O
resulting O
periods O
better O
match O
the O
ground O
- O
truth O
temporal O
segments O
. O
VDTN B-MethodName
also O
showed O
to O
maintain O
the O
dialogue O
states O
better O
from O
turn O
to O
turn O
. O

E O
Further O
Discussion O

Synthetic O
datasets O
result O
in O
overestimation O
of O
real O
performance O
and O
do O
n't O
translate O
to O
realworld O
usability O
. O
We O
agree O
that O
the O
current O
state B-MetricName
accuracy I-MetricName
seems O
to O
be O
quite O
low O
at O
about O
28 B-MetricValue
% I-MetricValue
. O
However O
, O
we O
want O
to O
highlight O
that O
state O
accuracy O
used O
in O
this O
paper O
is O
a O
very O
strict O
metric O
, O
which O
only O
considers O
a O
prediction O
as O
correct O
if O
it O
completely O
matches O
the O
ground O
truth O
. O
In O
DVD O
, O
assuming O
the O
average O
10 O
objects O
per O
video O
with O
the O
set O
of O
attributes O
as O
in O
Figure O
4 O
( O
+ O
' O
none O
' O
value O
in O
each O
slot O
) O
, O
we O
can O
roughly O
equate O
the O
multimodal O
DST O
as O
a O
7200 O
- O
class O
classification O
task O
, O
each O
class O
is O
a O
distinct O
set O
of O
objects O
, O
each O
with O
all O
possible O
attribute O
combinations O
. O
Combined O
with O
the O
cascading O
error O
from O
object O
perception O
models O
, O
we O
think O
the O
current O
reported O
results O
are O
reasonable O
. O

Moreover O
, O
we O
want O
to O
highlight O
that O
the O
reported O
performance O
of O
baselines O
reasonably O
matches O
their O
own O
capacities O
in O
unimodal O
DST O
. O
We O
can O
consider O
Object B-MetricName
State I-MetricName
F1 I-MetricName
as O
the O
performance O
on O
single O
- O
object O
state O
and O
it O
can O
closely O
correlate O
with O
the O
joint O
state O
accuracy O
in O
unimodal O
DST O
( O
remember O
that O
unimodal O
DST O
such O
as O
MultiWOZ B-DatasetName
is O
only O
limited O
to O
a O
single O
object O
/ O
entity O
per O
dialogue O
) O
. O
As O
seen O
in O
Table O
2 O
, O
the O
Object B-MetricName
State I-MetricName
F1 I-MetricName
results O
of O
TRADE B-MethodName
( O
Wu O
et O
al O
. O
, O
2019 O
) O
, O
UniConv O
( O
Le O
et O
al O
. O
, O
2020b O
) O
, O
and O
NADST B-MethodName
( O
Le O
et O
al O
. O
, O
2020c O
) O
are O
between O
46 B-MetricValue
- O
50 B-MetricValue
% I-MetricValue
. O
This O
performance O
range O
is O
indeed O
not O
very O
far O
off O
from O
the O
performance O
of O
these O
baseline O
models O
in O
unimodal O
DST O
in O
the O
Mul B-DatasetName
- I-DatasetName
tiWOZ I-DatasetName
benchmark O
. O

Finally O
, O
we O
also O
want O
to O
highlight O
that O
like O
other O
synthetic O
benchmarks O
such O
as O
CLEVR B-DatasetName
( O
Johnson O
MM B-TaskName
- I-TaskName
DST I-TaskName
in O
practical O
applications O
e.g. O
with O
videos O
of O
humans O
. O
While O
we O
introduced O
MM B-TaskName
- I-TaskName
DST I-TaskName
task O
and O
VDTN B-MethodName
as O
a O
new O
baseline O
, O
we O
noted O
that O
the O
existing O
results O
are O
limited O
to O
the O
synthetic O
benchmark O
. O
For O
instance O
, O
in O
the O
real O
world O
, O
there O
would O
be O
many O
identical O
objects O
with O
the O
same O
( O
size O
, O
color O
, O
material O
, O
shape O
) O
tuples O
, O
which O
would O
make O
the O
current O
formulation O
of O
dialogue O
states O
difficult O
. O
In O
such O
object O
- O
driven O
conversations O
, O
we O
would O
recommend O
a O
dialogue O
agent O
not O
focus O
on O
all O
possible O
objects O
in O
each O
video O
frame O
, O
but O
only O
on O
a O
" O
focus O
group O
" O
of O
objects O
. O
These O
objects O
, O
required O
to O
be O
semantically O
different O
, O
are O
topical O
subjects O
of O
the O
conversations O
. O
Say O
we O
want O
to O
scale O
to O
a O
new O
domain O
e.g. O
videos O
of O
humans O
, O
the O
first O
challenge O
from O
the O
current O
study O
is O
the O
recognition O
of O
human O
objects O
, O
which O
often O
have O
higher O
visual O
complexity O
than O
moving O
objects O
as O
in O
DVD O
. O
We O
also O
noted O
that O
it O
is O
impossible O
to O
define O
all O
human O
object O
classes O
as O
in O
CATER B-DatasetName
object O
classes O
, O
each O
of O
which O
is O
unique O
by O
its O
own O
appearance O
. O
To O
overcome O
this O
limitation O
, O
we O
would O
want O
to O
explore O
multimodal B-TaskName
DST I-TaskName
with O
the O
research O
of O
human O
object O
tracking O
, O
e.g. O
( O
Fernando O
et O
al O
. O
, O
2018 O
) O
, O
and O
consider O
human O
object O
identities O
uniquely O
defined O
per O
video O
. O
Another O
limitation O
is O
the O
definition O
of O
slots O
to O
track O
in O
each O
human O
object O
. O
While O
this O
requires O
careful O
considerations O
, O
for O
both O
practical O
and O
ethical O
reasons O
, O
we O
noted O
several O
potential O
papers O
that O
investigate O
human O
attributes O
in O
dialogues O
such O
as O
human O
emotions O
( O
Wang O
et O
al O
. O
, O
2021 O
) O
. O
Along O
these O
lines O
, O
we O
are O
excited O
to O
see O
interesting O
adaptations O
of O
multimodal O
dialogue O
states O
grounded O
on O
videos O
of O
humans O
. O
14 O
( O
Video O
# O
002660 O
) O
and O
15 O
( O
Video O
# O
001441 O
) O
. O

We O
showed O
example O
bounding O
boxes O
and O
their O
object O
classes O
in O
each O
video O
. O

WeCheck B-MethodName
: O
Strong O
Factual O
Consistency O
Checker O
via O
Weakly O
Supervised O
Learning O

A O
crucial O
issue O
of O
current O
text O
generation O
models O
is O
that O
they O
often O
uncontrollably O
generate O
text O
that O
is O
factually O
inconsistent O
with O
inputs O
. O
Due O
to O
lack O
of O
annotated O
data O
, O
existing O
factual O
consistency O
metrics O
usually O
train O
evaluation O
models O
on O
synthetic O
texts O
or O
directly O
transfer O
from O
other O
related O
tasks O
, O
such O
as O
question O
answering O
( O
QA O
) O
and O
natural O
language O
inference O
( O
NLI O
) O
. O
Bias O
in O
synthetic O
text O
or O
upstream O
tasks O
makes O
them O
perform O
poorly O
on O
text O
actually O
generated O
by O
language O
models O
, O
especially O
for O
general O
evaluation O
for O
various O
tasks O
. O
To O
alleviate O
this O
problem O
, O
we O
propose O
a O
weakly O
supervised O
framework O
named O
WeCheck B-MethodName
that O
is O
directly O
trained O
on O
actual O
generated O
samples O
from O
language O
models O
with O
weakly O
annotated O
labels O
. O
WeCheck B-MethodName
first O
utilizes O
a O
generative O
model O
to O
infer O
the O
factual O
labels O
of O
generated O
samples O
by O
aggregating O
weak O
labels O
from O
multiple O
resources O
. O
Next O
, O
we O
train O
a O
simple O
noise O
- O
aware O
classification O
model O
as O
the O
target O
metric O
using O
the O
inferred O
weakly O
supervised O
information O
. O
Comprehensive O
experiments O
on O
various O
tasks O
demonstrate O
the O
strong O
performance O
of O
WeCheck B-MethodName
, O
achieving O
an O
average B-MetricName
absolute I-MetricName
improvement I-MetricName
of O
3.3 B-MetricValue
% I-MetricValue
on O
the O
TRUE B-DatasetName
benchmark O
over O
11B O
state O
- O
of O
- O
the O
- O
art O
methods O
using O
only O
435 O
M O
parameters O
. O
Furthermore O
, O
it O
is O
up O
to O
30× B-MetricValue
faster O
than O
previous O
evaluation O
methods O
, O
greatly O
improving O
the O
accuracy B-MetricName
and O
efficiency B-MetricName
of O
factual B-TaskName
consistency I-TaskName
evaluation I-TaskName
. O
1 O
* O
Work O
is O
done O
during O
an O
internship O
at O
Baidu O
Inc O
. O

Introduction O

The O
research O
of O
text O
generation O
has O
achieved O
significant O
progress O
in O
recent O
years O
, O
but O
it O
still O
suffers O
the O
main O
issue O
of O
generating O
output O
which O
is O
factually O
inconsistent O
with O
the O
given O
inputs O
( O
Maynez O
et O
al O
. O
, O
2020 O
) O
. O
To O
tackle O
this O
issue O
, O
various O
metrics O
have O
been O
designed O
to O
check O
the O
consistency O
between O
generated O
text O
and O
the O
given O
inputs O
( O
Kryscinski O
et O
al O
. O
, O
2020 O
; O
Scialom O
et O
al O
. O
, O
2021 O
) O
. O
As O
we O
know O
, O
how O
to O
construct O
such O
a O
metric O
has O
attracted O
increasing O
attention O
in O
a O
variety O
of O
fields O
( O
Wu O
et O
al O
. O
, O
2022b O
) O
, O
including O
text O
summarization O
( O
Kryscinski O
et O
al O
. O
, O
2020 O
; O
Wu O
et O
al O
. O
, O
2022a O
) O
, O
dialogue O
generation O
( O
Welleck O
et O
al O
. O
, O
2019 O
) O
, O
and O
text O
simplification O
( O
Devaraj O
et O
al O
. O
, O
2022 O
) O
. O

Existing O
factual O
metrics O
can O
be O
classified O
into O
two O
types O
: O
one O
based O
on O
synthetic O
data O
and O
the O
other O
based O
on O
task O
transfer O
. O
Synthetic O
- O
data O
based O
metrics O
( O
Kryscinski O
et O
al O
. O
, O
2020 O
; O
Mishra O
et O
al O
. O
, O
2021 O
) O
apply O
data O
augmentation O
techniques O
to O
construct O
factual O
and O
non O
- O
factual O
texts O
as O
positive O
and O
negative O
samples O
, O
respectively O
. O
Metrics O
trained O
from O
these O
synthetic O
samples O
often O
perform O
poorly O
due O
to O
the O
significant O
mismatch O
between O
features O
of O
actual O
generated O
and O
synthetic O
text O
( O
e.g. O
distribution O
of O
factual O
errors O
) O
( O
Goyal O
and O
Durrett O
, O
2021 O
) O
. O
Task O
- O
transfer O
based O
metrics O
utilize O
the O
reasoning O
ability O
of O
models O
trained O
on O
relevant O
upstream O
tasks O
, O
such O
as O
natural O
language O
inference O
( O
NLI O
) O
( O
Falke O
et O
al O
. O
, O
2019 O
; O
Laban O
et O
al O
. O
, O
2022 O
) O
and O
question O
answering O
( O
QA O
) O
( O
Wang O
et O
al O
. O
, O
2020 O
; O
Fabbri O
et O
al O
. O
, O
2022 O
) O
and O
directly O
apply O
them O
to O
evaluate O
factual O
consistency O
without O
any O
adaption O
. O

As O
described O
above O
, O
previous O
metrics O
are O
learned O
indirectly O
from O
other O
related O
resources O
but O
without O
seeing O
the O
actual O
generated O
text O
. O
In O
such O
cases O
, O
they O
may O
overfit O
to O
their O
upstream O
tasks O
and O
fail O
to O
generalize O
to O
actual O
generated O
samples O
that O
have O
significantly O
different O
data O
features O
. O
Figure O
1 O
illustrates O
the O
probability O
density O
of O
three O
metrics O
, O
where O
the O
horizontal O
axis O
is O
metric O
scores O
and O
the O
vertical O
axis O
is O
the O
score O
density O
. O
Though O
these O
metrics O
are O
comparable O
in O
performance O
, O
they O
vary O
significantly O
in O
probability O
distributions O
, O
especially O
in O
the O
XSUM O
dataset O
, O
where O
sample O
features O
are O
greatly O
different O
from O
upstream O
tasks O
of O
these O
metrics O
2 O
, O
NLI O
- O
warmup O
is O
extremely O
confident O
in O
predicting O
both O
very O
high O
and O
low O
scores O
while O
SUMMAC O
and O
QAFact O
are O
only O
confident O
in O
predicting O
low O
scores O
3 O
. O
Furthermore O
, O
during O
testing O
, O
ensembling O
different O
metric O
scores O
by O
simply O
averaging O
will O
further O
improve O
their O
performance O
( O
Honovich O
et O
al O
. O
, O
2022 O
) O
. O
This O
also O
implies O
that O
the O
evaluation O
metrics O
learned O
from O
different O
resources O
are O
also O
complementary O
. O

To O
bridge O
the O
gap O
between O
training O
and O
testing O
and O
mitigate O
the O
scarcity O
of O
labeled O
data O
, O
in O
this O
paper O
, O
we O
propose O
WeCheck B-MethodName
, O
a O
factual O
consistency O
Checking O
framework O
based O
on O
Weakly O
supervised O
learning O
. O
Specifically O
, O
WeCheck B-MethodName
is O
based O
on O
a O
learning O
paradigm O
that O
provides O
weak O
supervision O
via O
modeling O
multiple O
label O
sources O
without O
access O
to O
ground O
truth O
. O
Different O
from O
previous O
metrics O
, O
WeCheck B-MethodName
directly O
utilizes O
the O
abundant O
actual O
generated O
samples O
bootstrapped O
from O
models O
trained O
on O
target O
downstream O
tasks O
, O
e.g. O
BART O
on O
text O
summarization O
. O
Then O
, O
WeCheck B-MethodName
follows O
a O
two O
- O
step O
pipeline O
consisting O
of O
weak O
annotation O
and O
noiseaware O
fine O
- O
tuning O
to O
get O
the O
target O
metric O
model O
. O

In O
the O
weak O
annotation O
step O
, O
by O
aggregating O
multiple O
weak O
supervision O
resources O
, O
we O
infer O
the O
unknown O
ground O
truth O
label O
of O
a O
sample O
. O
To O
reach O
this O
goal O
, O
we O
first O
provide O
each O
sample O
with O
a O
set O
of O
weak O
supervision O
signals O
calculated O
from O
various O
other O
metrics O
. O
These O
metrics O
are O
learned O
from O
various O
resources O
or O
tasks O
such O
as O
QA O
- O
based O
metrics O
and O
NLI O
- O
based O
metrics O
. O
After O
unifying O
and O
filtering O
these O
signals O
, O
we O
train O
a O
generative O
labeling O
model O
that O
models O
agreements O
and O
disagreements O
between O
them O
to O
infer O
the O
likelihood O
of O
their O
latent O
ground O
truth O
label O
. O
The O
inferred O
ground O
truth O
likelihood O
is O
then O
treated O
as O
a O
probabilistic O
label O
to O
provide O
weak O
supervision O
. O
In O
the O
second O
step O
, O
we O
apply O
noise O
- O
aware O
fine O
- O
tuning O
to O
train O
the O
target O
metric O
model O
. O
It O
is O
noted O
here O
, O
the O
weak O
annotation O
also O
brings O
noises O
to O
the O
supervision O
signal O
and O
brings O
new O
challenges O
to O
the O
model O
optimization O
process O
. O
As O
a O
solution O
, O
we O
first O
warmup O
our O
target O
metric O
model O
with O
NLI O
data O
for O
a O
better O
initialization O
before O
weakly O
supervised O
training O
. O
Then O
, O
after O
filtering O
out O
samples O
that O
are O
likely O
to O
be O
noisy O
, O
we O
finetune O
our O
target O
metric O
model O
with O
weak O
annotations O
. O
In O
summary O
, O
WeCheck B-MethodName
could O
learn O
how O
to O
utilize O
multiple O
resources O
for O
weak O
annotation O
while O
recognizing O
and O
filtering O
the O
potential O
noises O
accompanied O
by O
weak O
supervision O
. O

Experimental O
results O
show O
that O
WeCheck B-MethodName
not O
only O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
but O
also O
is O
computationally O
efficient O
. O
On O
the O
TRUE B-DatasetName
benchmark O
( O
Honovich O
et O
al O
. O
, O
2022 O
) O
, O
which O
is O
the O
current O
most O
comprehensive O
benchmark O
for O
factual O
consistency O
evaluation O
, O
WeCheck B-MethodName
obtains O
an O
average O
ROC B-MetricName
AUC I-MetricName
of O
84.8 B-MetricValue
, O
3.3 B-MetricValue
% I-MetricValue
absolute B-MetricName
improvement I-MetricName
over O
previous O
11B O
pre O
- O
trained O
task O
transferred O
metrics O
with O
only O
a O
size O
of O
435 O
M O
parameters O
. O
Moreover O
, O
it O
's O
much O
more O
stable O
for O
various O
generation O
tasks O
, O
with O
much O
lower O
variance O
on O
different O
tasks O
. O
Thus O
, O
WeCheck B-MethodName
is O
a O
simple O
but O
more O
effective O
and O
efficient O
metric O
for O
factual O
consistency O
evaluation O
. O

We O
summarize O
our O
contributions O
as O
follows O
: O

• O
We O
propose O
a O
novel O
factual B-TaskName
consistency I-TaskName
evaluation I-TaskName
metric O
based O
on O
weakly O
supervised O
learning O
, O
namely O
WeCheck B-MethodName
, O
which O
is O
directly O
trained O
on O
actual O
generated O
samples O
from O
language O
models O
with O
weakly O
annotated O
labels O
. O

• O
WeCheck O
is O
both O
effective O
and O
efficient O
achieving O
3.3 B-MetricValue
% I-MetricValue
absolute B-MetricName
improvement I-MetricName
and O
up O
to O
30 B-MetricValue
times I-MetricValue
faster O
comparing O
with O
previous O
state O
- O
ofart O
metrics O
. O

• O
WeCheck B-MethodName
is O
a O
general O
metric O
which O
is O
also O
more O
stable O
on O
various O
generation O
tasks O
and O
datasets O
than O
previous O
methods O
. O

WeCheck B-MethodName
Framework O

Figure O
2 O
illustrates O
the O
two O
- O
step O
pipeline O
of O
WeCheck B-MethodName
framework O
. O
In O
the O
upper O
part O
of O
the O
figure O
, O
during O
the O
weak O
annotation O
step O
, O
we O
first O
calculate O
a O
set O
of O
weak O
supervision O
signals O
for O
each O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
p O
c O
m O
T O
Y O
4 O
n O
3 O
q O
a O
J O
V O
+ O
S O
I O
l O
z O
W O
R O
D O
o O
c O
H O
p O
i O
s O
0 O
= O
" O
> O
A O

Target O
Checker O

Then O
, O
we O
use O
a O
mapping O
function O
to O
unify O
the O
weak O
supervision O
signals O
and O
infer O
the O
likelihood O
of O
the O
ground O
truth O
label O
of O
each O
sample O
. O
After O
annotation O
, O
we O
apply O
noise O
- O
aware O
fine O
- O
tuning O
to O
train O
our O
target O
metric O
model O
, O
shown O
in O
the O
lower O
part O
of O
the O
figure O
. O

Noise O
- O
aware O
fine O
- O
tuning O
first O
warmup O
target O
metric O
model O
with O
NLI O
data O
and O
training O
it O
with O
filtered O
probabilistic O
labels O
. O
In O
the O
following O
, O
we O
introduce O
our O
problem O
definition O
and O
detailed O
method O
. O

Problem O
Definition O

Factual B-TaskName
Consistency I-TaskName
Evaluation I-TaskName
Given O
a O
textual O
sequence O
as O
a O
premise O
, O
and O
another O
textual O
sequence O
as O
a O
hypothesis O
, O
which O
may O
be O
a O
generated O
summary O
or O
dialogue O
, O
the O
goal O
of O
a O
factual O
consistency O
metric O
f O
θ O
is O
to O
predict O
whether O
the O
hypothesis O
is O
factual O
consistent O
given O
the O
premise O
. O
For O
simplicity O
, O
we O
follow O
the O
previous O
textual O
entailment O
based O
framework O
( O
Kryscinski O
et O
al O
. O
, O
2019 O
) O
, O
which O
takes O
x O
, O
the O
concatenation O
of O
hypothesis O
and O
premise O
, O
as O
the O
input O
format O
and O
unifies O
the O
evaluation O
as O
a O
binary O
classification O
problem O
: O
f O
θ O
( O
x O
) O
∈ O
[ O
0 O
, O
1 O
] O
, O
where O
the O
predicted O
logit O
indicates O
the O
probability O
of O
x O
being O
factually O
consistent O
. O
Another O
advantage O
of O
using O
the O
entailment O
- O
based O
framework O
is O
that O
it O
is O
effective O
in O
terms O
of O
time O
complexity O
compared O
with O
other O
methods O
( O
Laban O
et O
al O
. O
, O
2022 O
) O
. O
Taking O
f O
θ O
as O
the O
target O
metric O
model O
, O
the O
goal O
of O
WeCheck B-MethodName
is O
to O
train O
f O
θ O
into O
an O
efficient O
factual O
consistency O
metric O
. O

Weakly O
Supervised O
Training O
In O
our O
weakly O
supervised O
settings O
, O
we O
first O
bootstrap O
a O
set O
of O
sam O
- O
ples O
from O
the O
generation O
tasks O
, O
e.g. O
text O
summarization O
, O
and O
dialogue O
generation O
. O
Using O
various O
factual O
metrics O
trained O
from O
multiple O
resources O
, O
we O
provide O
each O
sample O
x O
with O
a O
set O
of O
weak O
signals O
λ O
= O
( O
λ O
1 O
, O
. O
. O
. O
, O
λ O
k O
) O
, O
where O
each O
λ O
i O
is O
a O
logit O
separately O
calculated O
by O
a O
metric O
. O
We O
treat O
the O
ground O
truth O
label O
y O
of O
x O
as O
a O
hidden O
variable O
that O
can O
be O
estimated O
by O
aggregating O
λ O
. O
To O
reach O
this O
goal O
, O
we O
train O
a O
labeling O
model O
p O
ϕ O
to O
model O
agreements O
and O
disagreements O
relations O
between O
weak O
signals O
in O
λ O
and O
estimate O
the O
probability O
distribution O
of O
the O
truth O
label O
, O
p O
ϕ O
( O
y|λ O
) O
. O
Then O
, O
we O
apply O
p O
ϕ O
( O
y|λ O
) O
to O
supervise O
the O
metric O
model O
f O
θ O
. O

Weak O
Annotation O

To O
provide O
weak O
supervision O
for O
training O
, O
we O
follow O
data O
programming O
, O
a O
weakly O
supervised O
learning O
paradigm O
based O
on O
modeling O
multiple O
label O
sources O
. O
However O
, O
in O
data O
programming O
, O
weak O
supervision O
signals O
are O
often O
produced O
by O
various O
checking O
clauses O
, O
e.g. O
whether O
word O
" O
causes O
" O
appears O
in O
the O
sentence O
? O
and O
produce O
a O
discrete O
weak O
signal O
λ O
i O
∈ O
{ O
0 O
, O
1 O
, O
−1 O
} O
, O
where O
0 O
/ O
1 O
stands O
for O
a O
vote O
for O
positive O
/ O
negative O
label O
and O
−1 O
stands O
for O
a O
abstain O
vote O
. O
However O
, O
in O
our O
scenario O
, O
due O
to O
the O
diversity O
of O
metric O
frameworks O
, O
outputs O
of O
different O
metrics O
often O
do O
not O
share O
a O
unified O
output O
format O
and O
are O
usually O
continuous O
. O
For O
example O
, O
QA O
- O
based O
metrics O
often O
produce O
continuous O
logits O
in O
[ O
0 O
, O
1 O
] O
, O
and O
NLI O
- O
based O
metrics O
often O
produce O
discrete O
labels O
of O
entailment O
or O
contradiction O
. O
Thus O
, O
the O
first O
thing O
before O
training O
the O
labeling O
model O
is O
to O
unify O
weak O
supervision O
signals O
by O
a O
mapping O
function O
, O
m O
( O
λ O
i O
) O
→ O
{ O
0 O
, O
1 O
, O
−1 O
} O
. O
In O
this O
way O
, O
we O
can O
model O
the O
transformed O
λ O
by O
a O
data O
programming O
based O
labeling O
model O
. O

Weak O
Signal O
Unification O

We O
first O
unify O
all O
the O
weak O
supervision O
signals O
from O
different O
metrics O
into O
the O
same O
format O
, O
a O
logit O
λ O
i O
∈ O
[ O
0 O
, O
1 O
] O
. O
For O
the O
metric O
with O
single O
logit O
output O
, O
we O
directly O
use O
its O
output O
as O
λ O
i O
. O
For O
multi O
- O
label O
classification O
output O
, O
we O
select O
the O
probability O
of O
predicting O
entailment O
. O
Notice O
that O
all O
the O
signals O
predicted O
by O
imperfect O
metrics O
will O
introduce O
a O
portion O
of O
noises O
. O
For O
a O
more O
reliable O
signal O
, O
the O
core O
idea O
for O
designing O
a O
mapping O
function O
m O
is O
to O
map O
signals O
that O
the O
metric O
has O
high O
confidence O
into O
{ O
0 O
, O
1 O
} O
and O
abstain O
low O
- O
confidence O
signals O
by O
mapping O
them O
to O
−1 O
. O

Generally O
, O
this O
can O
be O
achieved O
by O
setting O
thresholds O
on O
signals O
. O
But O
another O
important O
issue O
to O
be O
noticed O
is O
that O
, O
as O
shown O
in O
Figure O
1 O
, O
signal O
distributions O
vary O
significantly O
across O
metrics O
and O
datasets O
, O
which O
makes O
threshold O
selection O
difficult O
. O
Thus O
, O
we O
instead O
dynamically O
determine O
thresholds O
by O
setting O
constant O
probability O
mass O
that O
contains O
the O
highest O
confidence O
. O
Specifically O
, O
we O
choose O
to O
map O
the O
lowest O
p B-HyperparameterName
− I-HyperparameterName
percent O
and O
the O
highest O
p B-HyperparameterName
+ I-HyperparameterName
percent O
of O
signal O
scores O
into O
label O
0 O
and O
1 O
, O
separately O
, O
and O
map O
the O
rest O
interval O
of O
low O
- O
confident O
scores O
into O
-1 O
. O
Given O
the O
inverse O
cumulative O
distribution O
function O
of O
the O
i O
- O
th O
signal O
F O
i O
, O
we O
can O
calculate O
its O
positive O
and O
negative O
threshold O
γ O
+ O
i O
and O
γ O
− O
i O
by O
: O

γ O
+ O
i O
= O
F O
i O
( O
1 O
− O
p O
+ O
) O
, O
γ O
− O
i O
= O
F O
i O
( O
p O
− O
) O
. O
( O
1 O
) O

The O
mapping O
function O
is O
then O
defined O
by O
: O

m O
( O
λ O
i O
) O
= O
 O
 O
 O
0 O
λ O
i O
≤ O
γ O
− O
i O
1 O
λ O
i O
≥ O
γ O
+ O
i O
−1 O
γ O
− O
i O
< O
λ O
i O
< O
γ O
+ O
i O
. O

( O
2 O
) O

For O
simplicity O
, O
we O
share O
p B-HyperparameterName
− I-HyperparameterName
and O
p B-HyperparameterName
+ I-HyperparameterName
across O
different O
resources O
and O
datasets O
. O
By O
applying O
the O
mapping O
function O
, O
we O
unify O
each O
λ O
i O
into O
a O
discrete O
label O
in O
{ O
0 O
, O
1 O
, O
−1 O
} O
. O

Labeling O
model O
We O
treat O
the O
true O
label O
y O
of O
x O
as O
a O
hidden O
variable O
and O
train O
the O
labeling O
model O
p O
ϕ O
to O
estimate O
y O
by O
aggregating O
λ O
4 O
. O
The O
generative O
model O
p O
ϕ O
models O
the O
generation O
process O
of O
λ O
and O
y O
by O
their O
joint O
probability O
. O
Because O
all O
the O
weak O
supervision O
signals O
are O
inferred O
from O
different O
resources O
, O
we O
treat O
them O
as O
independent O
variables O
. O
Then O
, O
given O
the O
prior O
p O
( O
y O
) O
5 O
, O
the O
joint O
probability O
is O
formulated O
by O

p O
ϕ O
( O
λ O
, O
y O
) O
= O
λ O
i O
∈λ O
p O
ϕ O
( O
λ O
i O
, O
y O
) O
= O
λ O
i O
∈λ O
p O
( O
λ O
i O
| O
y O
) O
p O
( O
y O
) O
, O
( O
3 O
) O

following O
Bayesian O
rule O
. O
Next O
, O
we O
need O
to O
model O
the O
likelihood O
p O
( O
λ O
i O
| O
y O
) O
that O
labels O
the O
sample O
with O
λ O
i O
based O
on O
the O
latent O
label O
y. O
Following O
, O
we O
define O
the O
labeling O
process O
of O
λ O
i O
as O
a O
sequence O
of O
Bernoulli O
process O
. O
Concretely O
, O
the O
i O
- O
th O
metric O
has O
a O
probability O
of O
β O
i O
not O
to O
abstain O
the O
sample O
and O
a O
probability O
α O
i O
to O
label O
it O
correctly O
. O
Then O
, O
we O
calculate O
the O
likelihood O
by O

p O
ϕ O
( O
λ O
i O
| O
y O
) O
= O
 O
 O
 O
β B-HyperparameterName
i I-HyperparameterName
α B-HyperparameterName
i I-HyperparameterName
λ O
i O
̸ O
= O
−1 O
∧ O
λ O
i O
= O
y O
β B-HyperparameterName
i I-HyperparameterName
( O
1 O
− O
α B-HyperparameterName
i I-HyperparameterName
) O
λ O
i O
̸ O
= O
−1 O
∧ O
λ O
i O
̸ O
= O
y O
1 O
− O
β B-HyperparameterName
i I-HyperparameterName
λ O
i O
= O
−1 O
, O
( O
4 O
) O

where O
α B-HyperparameterName
i I-HyperparameterName
, O
β B-HyperparameterName
i I-HyperparameterName
are O
learnable O
hyper O
- O
parameters O
. O
Given O
all O
samples O
, O
we O
train O
the O
labeling O
model O
by O
optimizing O
: O

L O
ϕ O
= O
min O
ϕ O
λ O
y∈ O
{ O
0,1 O
} O

log O
p O
ϕ O
( O
λ O
, O
y O
) O
. O

( O
5 O
) O

2.3 O
Noise O
Aware O
Fine O
- O
tuning O
NLI O
Warmup O
After O
we O
get O
the O
labeling O
model O
p O
ϕ O
, O
the O
next O
step O
is O
to O
train O
our O
metric O
model O
f O
θ O
with O
the O
weak O
supervision O
inferred O
by O
it O
. O
But O
in O
practice O
, O
we O
find O
direct O
training O
with O
weak O
supervision O
will O
cause O
the O
model O
easily O
converges O
to O
the O
local O
minima O
. O
This O
may O
because O
reasoning O
over O
a O
long O
range O
of O
context O
is O
challenging O
and O
weak O
supervisions O
are O
also O
potential O
to O
be O
noisy O
. O
These O
problems O
cause O
great O
difficulties O
in O
optimization O
. O
Inspired O
by O
the O
idea O
of O
curriculum O
learning O
( O
Bengio O
et O
al O
. O
, O
2009 O
) O
, O
we O
first O
warmup O
our O
metric O
model O
on O
NLI O
, O
an O
easier O
and O
closely O
related O
task O
. O
We O
use O
the O
mixture O
of O
four O
NLI O
datasets O
, O
MultiNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
Fever B-DatasetName
- I-DatasetName
NLI I-DatasetName
( O
Thorne O
et O
al O
. O
, O
2018 O
) O
, O
LingNLI B-DatasetName
( O
Parrish O
et O
al O
. O
, O
2021 O
) O
and O
Adversarial B-DatasetName
- I-DatasetName
NLI I-DatasetName
( O
Nie O
et O
al O
. O
, O
2020 O
) O
. O
Based O
on O
the O
warmed O
- O
up O
checkpoint O
, O
our O
metric O
model O
achieves O
much O
better O
results O
under O
weak O
supervision O
, O
which O
we O
will O
later O
show O
in O
our O
experiments O
. O

Noise O
Filtering O
and O
Training O

After O
warming O
up O
, O
we O
train O
our O
metric O
model O
with O
weak O
supervision O
. O

Because O
the O
estimated O
latent O
labels O
y O
can O
still O
be O
noisy O
due O
to O
the O
imperfect O
labeling O
model O
and O
weak O
supervision O
signals O
, O
we O
apply O
the O
likelihood O
of O
y O
that O
contains O
the O
certainty O
of O
the O
prediction O
as O
a O
soft O
probabilistic O
label O
instead O
of O
the O
discrete O
label O
for O
training O
. O
Based O
on O
the O
definition O
of O
joint O
probability O
in O
Eq O
. O
3 O
, O
we O
predict O
the O
likelihood O
of O
each O
sample O
by O

p O
ϕ O
( O
y O
= O
1|λ O
) O
= O
p O
ϕ O
( O
λ O
, O
1 O
) O
p O
ϕ O
( O
λ O
, O
1 O
) O
+ O
p O
ϕ O
( O
λ O
, O
0 O
) O
. O
( O
6 O
) O

With O
convenience O
, O
we O
abbreviate O
p O
ϕ O
( O
y O
= O
1|λ O
) O
as O
p O
( O
y O
+ O
) O
. O
Before O
training O
with O
p O
( O
y O
+ O
) O
, O
we O
first O
filter O
out O
estimated O
samples O
with O
low O
confidence O
, O
by O
applying O
the O
similar O
procedure O
in O
weak O
signal O
unification O
. O
By O
reusing O
mapping O
function O
m O
, O
we O
filter O
out O
the O
low O
confident O
probabilistic O
label O
and O
get O
the O
final O
training O
set O
by O

X O
= O
x O
, O
p O
( O
y O
+ O
) O
m O
p O
( O
y O
+ O
) O
̸ O
= O
−1 O
, O
( O
7 O

where O
p O
( O
y O
+ O
) O
is O
the O
corresponding O
probabilistic O
label O
of O
x. O
Then O
, O
given O
f O
θ O
after O
warming O
up O
, O
we O
finetune O
it O
by O

L O
f O
= O
min O
θ O
x∈X O
p O
( O
y O
+ O
) O
log O
( O
f O
θ O
( O
x O
) O
) O
+ O
( O
1 O
− O
p O
( O
y O
+ O
) O
) O
log O
( O
1 O
− O
f O
θ O
( O
x O
) O
) O
, O
( O
8 O
) O

where O
p O
( O
y O
+ O
) O
is O
kept O
fixed O
without O
gradient O
backpropagation O
to O
p O
ϕ O
during O
training O
. O

During O
inference O
, O
the O
model O
only O
needs O
to O
take O
the O
textual O
sequence O
x O
as O
input O
and O
output O
the O
logit O
prediction O
f O
θ O
( O
x O
) O
. O

Experimental O
Settings O

In O
this O
section O
, O
we O
introduce O
the O
experimental O
settings O
of O
WeCheck B-MethodName
including O
the O
evaluation O
benchmark O
, O
baseline O
models O
, O
and O
implementation O
details O
. O

TRUE B-DatasetName
Benchmark O

Recent O
works O
point O
out O
that O
the O
performance O
of O
a O
metric O
should O
be O
evaluated O
comprehensively O
across O
multiple O
tasks O
and O
datasets O
to O
reduce O
variance O
. O
Thus O
, O
we O
evaluate O
WeCheck B-MethodName
on O
TRUE B-DatasetName
( O
Honovich O
et O
al O
. O
, O
2022 O
) O
, O
a O
benchmark O
consisting O
of O
11 O
datasets O
of O
4 O
tasks O
including O
text B-TaskName
summarization I-TaskName
, O
dialogue B-TaskName
generation I-TaskName
, O
paraphrasing B-TaskName
, O
and O
fact O
checking O
, O
where O
each O
sample O
in O
datasets O
is O
annotated O
with O
a O
binary O
label O
manually O
. O
We O
only O
test O
on O
the O
first O
three O
tasks O
as O
fact O
checking O
is O
beyond O
our O
scope O
. O
Following O
TRUE B-DatasetName
, O
we O
normalize O
each O
metric O
score O
into O
a O
logit O
and O
report O
their O
Characteristic B-MetricName
Area I-MetricName
Under I-MetricName
the I-MetricName
Curve I-MetricName
( O
ROC B-MetricName
AUC I-MetricName
) O
w.r.t O
binary O
logits O
. O
Evaluation O
with O
ROC B-MetricName
AUC I-MetricName
does O
not O
require O
metrics O
to O
set O
specific O
decision O
thresholds O
. O
Details O
of O
tasks O
and O
datasets O
of O
TRUE B-DatasetName
are O
introduce O
in O
the O
Appendix O
A O
. O

Baseline O

We O
evaluate O
WeCheck B-MethodName
by O
comparing O
with O
recently O
proposed O
metrics O
. O
We O
categorize O
these O
baselines O
by O
types O
of O
their O
methods O
. O
( O
Kryscinski O
et O
al O
. O
, O
2020 O
) O
is O
a O
BERT O
- O
based O
metric O
with O
synthetic O
training O
samples O
constructed O
from O
rule O
- O
based O
data O
augmentation O
. O
SUMMAC B-MethodName
( O
SCZS B-MethodName
) O
( O
Laban O
et O
al O
. O
, O
2022 O
) O
aggregates O
sentence O
- O
level O
entailment O
scores O
for O
the O
final O
factual O
consistency O
score O
. O
We O
only O
report O
the O
zero O
- O
shot O
version O
SCZS B-MethodName
instead O
of O
the O
supervised O
version O
SCCONV O
because O
it O
is O
more O
effective O
on O
the O
TRUE B-DatasetName
benchmark O
. O
ANLI B-MethodName
( O
Honovich O
et O
al O
. O
, O
2022 O
) O
directly O
apply O
a O
large O
11B O
T5 O
trained O
on O
Adversarial O
- O
NLI O
( O
Nie O
et O
al O
. O
, O
2020 O
) O
dataset O
for O
fact O
checking O
and O
achieve O
SOTA O
performance O
on O
TRUE B-DatasetName
. O

NLI O
- O
based O
Metrics O
FactCC O

QA O
- O
QG O
based O
Metrics O
QuestEval O
( O
Scialom O
et O
al O
. O
, O
2021 O
) O
is O
a O
QA O
- O
QG O
based O
metric O
that O
jointly O
measures O
factual O
consistency O
and O
semantic O
relevance O
, O
where O
the O
importance O
of O
generated O
questions O
are O
weighted O
by O
a O
trained O
model O
. O
QAFactEval B-MethodName
( O
QAFact B-MethodName
) O
( O
Fabbri O
et O
al O
. O
, O
2022 O
) O
is O
a O
metric O
designed O
by O
carefully O
optimizing O
each O
component O
of O
the O
QG O
- O
QA O
framework O
. O
Q O
2 O
, O
from O
the O
version O
of O
Honovich O
et O
al O
. O
( O
2022 O
) O
, O
replace O
all O
the O
component O
of O
QA O
- O
QG O
framework O
into O
T5 O
11B O
large O
models O
. O

Other O
Types O
BERTScore B-MethodName
( O
BERTS B-MethodName
) O
( O
Zhang O
et O
al O
. O
, O
2019a O
) O
measure O
the O
similarity O
of O
a O
generated O
text O
and O
its O
reference O
by O
aggregating O
tokenlevel O
similarities O
of O
their O
contextual O
representations O
. O
BARTScore B-MethodName
( O
BARTS B-MethodName
) O
( O
Yuan O
et O
al O
. O
, O
2021 O
) O
evaluate O
the O
quality O
of O
generated O
text O
by O
its O
modeling O
perplexity O
of O
a O
fine O
- O
tuned O
BART O
. O

Implementation O
Details O

All O
the O
baseline O
metrics O
are O
tested O
based O
on O
their O
open O
- O
sourced O
codes O
. O
The O
metric O
model O
of O
WeCheck B-MethodName
is O
based O
on O
powerful O
pre O
- O
trained O
language O
model O
DeBERTaV3 O
( O
He O
et O
al O
. O
, O
2021 O
) O
. O
Following O
the O
description O
in O
§ O
2 O
, O
we O
first O
warm O
up O
DeBERTaV3 O
on O
NLI O
datasets O
and O
apply O
it O
for O
weak O
supervised O
training O
. O
As O
regards O
to O
training O
data O
, O
we O
sample O
text B-TaskName
summarization I-TaskName
examples O
from O
BART O
fine O
- O
tuned O
on O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
and O
XSum B-DatasetName
datasets O
. O
We O
sample O
dialogue B-TaskName
generation I-TaskName
examples O
from O
Mem O
- O
Net O
and O
dodecaDialogue O
( O
Shuster O
et O
al O
. O
, O
2020 O
) O
trained O
on O
WoW B-DatasetName
dataset O
following O
Honovich O
et O
al O
. O
( O
2021 O
) O
. O
For O
paraphrase B-TaskName
, O
we O
directly O
use O
samples O
in O
PAWS O
since O
it O
can O
be O
regard O
as O
a O
consistency O
checking O
dataset O
itself O
. O
For O
weak O
signals O
, O
we O
apply O
QAFact B-MethodName
( O
Fabbri O
et O
al O
. O
, O
2022 O
) O
, O
SUMMAC B-MethodName
( O
Laban O
et O
al O
. O
, O
2022 O
) O
, O
and O
the O
NLI B-MethodName
warmed I-MethodName
up I-MethodName
DeBERTaV3 I-MethodName
( O
NLI B-MethodName
- I-MethodName
warmup I-MethodName
) O
as O
to O
provide O
weak O
signals O
for O
each O
sample O
as O
default O
. O
For O
weak O
signal O
unification O
, O
we O
set O
p O
+ O
and O
p O
− O
in O
mapping O
function O
m O
to O
0.75 O
and O
0.25 O
based O
on O
validation O
. O
For O
labeling O
model O
p O
ϕ O
, O
we O
follow O
the O
implementation O
of O
Snorkel O
for O
efficiency O
and O
train O
it O
on O
CPUs O
with O
Adam O
optimizer O
. O
For O
noise O
- O
aware O
fine O
- O
tuning O
, O
we O
finetune O
the O
warmed O
up O
checkpoint O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e B-HyperparameterValue
−6 I-HyperparameterValue
, O
warmup B-HyperparameterName
steps I-HyperparameterName
of O
500 B-HyperparameterValue
, O
and O
the O
total B-HyperparameterName
training I-HyperparameterName
steps I-HyperparameterName
of O
3 B-HyperparameterValue
epoch O
. O
We O
train O
on O
4 O
NVIDIA O
Tesla O
V100 O
GPUs O
, O
and O
it O
takes O
around O
only O
5000 O
steps O
to O
reach O
the O
best O
performance O
. O

Results O

The O
experimental O
results O
on O
TRUE B-DatasetName
are O
reported O
in O
and O
closely O
related O
task O
, O
provides O
a O
much O
better O
initialization O
for O
training O
with O
weak O
supervision O
. O
For O
noise O
- O
aware O
finetuning O
, O
we O
study O
how O
filtering O
potential O
noisy O
samples O
( O
Eq O
. O
7 O
) O
and O
the O
probabilistic O
label O
( O
Eq O
. O
6 O
) O
affect O
the O
overall O
performance O
. O
After O
removing O
noise O
filtering O
( O
w O
/ O
o O
Noise O
Filter O
in O
Table O
2 O
) O
, O
the O
performance B-MetricName
drops O
around O
1 B-MetricValue
- I-MetricValue
2 I-MetricValue
points O
in O
each O
task O
and O
dataset O
in O
average O
. O
By O
replacing O
the O
probabilistic O
labels O
into O
hard O
labels O
( O
w O
/ O
Hard O
Label O
in O
Table O
2 O
) O
, O
we O
observe O
around O
0.1 B-MetricValue
- I-MetricValue
0.2 I-MetricValue
drops O
in O
performance B-MetricName
. O
This O
implies O
how O
to O
filter O
potential O
noisy O
samples O
is O
crucial O
in O
noise O
aware O
fine O
- O
tuning O
, O
and O
probabilistic O
labels O
also O
slightly O
help O
. O

Effects O
of O
Task O
We O
also O
analyse O
how O
each O
bootstrapped O
task O
affect O
WeCheck B-MethodName
. O
In O
1 O
) O
is O
much O
worse O
than O
others O
. O
Among O
the O
rest O
two O
methods O
with O
comparable O
performance O
, O
WeCheck B-MethodName
is O
2.9 B-MetricValue
times I-MetricValue
faster O
than O
SCZS B-MethodName
and O
30 B-MetricValue
times I-MetricValue
faster O
than O
QAFact B-MethodName
. O

Abstractiveness O
As O
mentioned O
above O
, O
abstractive O
hypotheses O
are O
challenging O
for O
current O
metrics O
, O
e.g. O
XSUM B-DatasetName
summaries O
from O
MNBM B-DatasetName
. O
We O
give O
an O
in O
- O
depth O
analysis O
of O
the O
effect O
of O
hypothesis O
abstractiveness O
on O
the O
metrics O
performance O
. O

Following O
See O
et O
al O
. O
( O
2017 O
) O
, O
we O
use O
the O
percentage B-MetricName
of I-MetricName
unique I-MetricName
unigrams I-MetricName
in O
a O
hypothesis O
w.r.t O
its O
premise O
to O
measure O
abstractivenss O
. O
Then O
, O
we O
spilt O
all O
the O
examples O
in O
TRUE B-DatasetName
into O
10 O
bins O
according O
to O
their O
abstractiveness O
. O
For O
each O
bin O
, O
we O
measure O
the O
ROC B-MetricName
AUC I-MetricName
of O
WeCheck B-MethodName
and O
the O
other O
three O
representative O
baselines O
: O
QAFact B-MethodName
, O
Summac B-MethodName
, O
and O
NLI B-MethodName
- I-MethodName
warmup I-MethodName
. O
From O
the O
results O
in O
Figure O
3 O
, O
we O
observe O
a O
significant O
drop O
in O
the O
performance O
for O
all O
baselines O
as O
the O
hypothesis O
becomes O
more O
abstractive O
, O
while O
, O
WeCheck B-MethodName
keeps O
its O
performance O
( O
around O
0.85 B-MetricValue
) O
. O
Moreover O
, O
WeCheck B-MethodName
consistently O
outperforms O
baseline O
metrics O
in O
every O
bin O
of O
ab- O
stractiveness O
. O
This O
further O
verifies O
the O
superiority O
of O
directly O
training O
with O
real O
task O
data O
. O

Labeling O
Model O

We O
compare O
how O
different O
data O
programming O
based O
labeling O
models O
affect O
the O
final O
metric O
performance O
. O

In O
WeCheck B-MethodName
, O
labeling O
model O
p O
ϕ O
learns O
to O
aggregate O
multi O
- O
resource O
labels O
to O
infer O
the O
hidden O
true O
label O
. O
Comparing O
concretely O
, O
our O
method O
is O
similar O
to O
Snorkel O
. O
Because O
, O
in O
our O
scenario O
, O
the O
number O
of O
weak O
supervision O
signals O
is O
small O
and O
their O
relationships O
are O
relatively O
simple O
as O
they O
are O
trained O
from O
different O
tasks O
, O
we O
prefer O
this O
method O
over O
other O
recent O
more O
advanced O
ones O
. O

In O
Table O
5 O
, O
we O
demonstrate O
the O
effectiveness O
of O
our O
labeling O
model O
by O
replacing O
it O
with O
other O
methods O
. O
In O
these O
baselines O
, O
simpler O
methods O
include O
: O
Average B-MethodName
Signals I-MethodName
, O
which O
simply O
averages O
all O
the O
weak O
signals O
as O
the O
probabilistic O
label O
p O
( O
y O
+ O
) O
; O
Major B-MethodName
Vote I-MethodName
, O
which O
select O
the O
most O
frequently O
appeared O
label O
in O
a O
unified O
weak O
signal O
set O
as O
the O
true O
label O
. O
More O
advanced O
methods O
include O
: O
Flying B-MethodName
Squid I-MethodName
( O
Fu O
et O
al O
. O
, O
2020 O
) O
, O
which O
applies O
an O
Ising O
model O
( O
Parsons O
, O
2011 O
) O
method O
, O
which O
uses O
a O
neural O
network O
as O
the O
labeling O
method O
and O
trains O
it O
end O
- O
to O
- O
end O
with O
the O
target O
tasks O
model O
; O
DWS B-MethodName
( O
Parker O
and O
Yu O
, O
2021 O
) O
treats O
the O
true O
label O
of O
a O
sample O
as O
the O
hidden O
variable O
and O
applies O
Estimation B-MethodName
- I-MethodName
Maximization I-MethodName
( O
EM B-MethodName
) O
for O
inference O
during O
training O
. O

From O
the O
results O
in O
Table O
5 O
, O
our O
default O
labeling O
model O
outperforms O
all O
others O
. O
Furthermore O
, O
more O
complex O
methods O
( O
Flying B-MethodName
Squid I-MethodName
, O
Weasel B-MethodName
, O
and O
EM B-MethodName
) O
perform O
worse O
than O
simpler O
methods O
( O
Ours O
, O
Average B-MethodName
Signal I-MethodName
, O
and O
Major B-MethodName
Vote I-MethodName
) O
. O
This O
further O
verifies O
that O
the O
relations O
between O
weak O
signals O
are O
simple O
, O
and O
complex O
modeling O
will O
not O
bring O
further O
improvements O
. O
From O
another O
perspective O
, O
overly O
simplistic O
approaches O
without O
any O
statistical O
modeling O
( O
Average B-MethodName
Signal I-MethodName
and O
Major B-MethodName
Vote I-MethodName
) O
also O
perform O
worse O
than O
our O
methods O
. O

Related O
Work O

Factual B-TaskName
Consistency I-TaskName
Evaluation I-TaskName
Recently O
, O
automatically O
checking O
factual O
consistency O
has O
become O
an O
increasingly O
popular O
topic O
. O
Reasoning O
over O
a O
long O
range O
of O
context O
for O
factual O
evaluation O
is O
a O
challenging O
task O
that O
even O
human O
annotators O
may O
frequently O
disagree O
with O
each O
other O
. O
Thus O
, O
it O
is O
hard O
to O
collect O
a O
large O
- O
scale O
high O
- O
quality O
dataset O
for O
training O
a O
fully O
supervised O
model O
, O
and O
previous O
works O
search O
for O
indirect O
methods O
. O
One O
branch O
of O
them O
leverage O
the O
reasoning O
ability O
of O
NLI O
. O
Based O
on O
the O
model O
trained O
on O
NLI O
datasets O
, O
e.g. O
MNLI O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
ANLI B-MethodName
( O
Nie O
et O
al O
. O
, O
2020 O
) O
, O
some O
works O
aggregate O
sentence O
- O
level O
entailment O
score O
for O
checking O
( O
Falke O
et O
al O
. O
, O
2019 O
; O
Laban O
et O
al O
. O
, O
2022 O
) O
, O
while O
others O
adopt O
document O
- O
level O
NLI O
which O
directly O
reasoning O
over O
the O
full O
context O
( O
Maynez O
et O
al O
. O
, O
2020 O
; O
Gehrmann O
et O
al O
. O
, O
2021 O
) O
. O
Another O
branch O
of O
methods O
apply O
QA O
- O
QG O
based O
pipeline O
for O
a O
more O
fine O
- O
grained O
checking O
. O
QAGS O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
and O
FEQA O
( O
Durmus O
et O
al O
. O
, O
2020 O
) O
are O
the O
earliest O
attempt O
on O
this O
method O
, O
and O
QuestEval B-MethodName
( O
Scialom O
et O
al O
. O
, O
2021 O
) O
and O
QAFactEval B-MethodName
( O
Fabbri O
et O
al O
. O
, O
2022 O
) O
further O
improve O
this O
type O
of O
methods O
by O
applying O
NLI O
for O
answer O
matching O
. O
Data O
Programming O
In O
this O
paper O
, O
we O
mainly O
focus O
on O
data O
programming O
( O
Ratner O
et O
al O
. O
, O
2016 O
) O
( O
DP O
) O
, O
a O
weak O
supervision O
paradigm O
proposed O
to O
infer O
correct O
labels O
based O
on O
noisy O
labels O
from O
labeling O
functions O
( O
LFs O
) O
, O
which O
are O
rule O
- O
based O
decision O
- O
making O
processes O
that O
generate O
discrete O
labels O
. O
Following O
the O
DP O
paradigm O
, O
Snorkel O
is O
proposed O
to O
for O
rapid O
training O
, O
more O
recent O
works O
study O
how O
to O
adapt O
label O
model O
in O
DP O
( O
Ratner O
et O
al O
. O
, O
2019 O
; O
Awasthi O
et O
al O
. O
, O
2020 O
) O
or O
modeling O
more O
complex O
structure O
between O
LFs O
( O
Fu O
et O
al O
. O
, O
2020 O
) O
. O
DP O
is O
also O
applied O
to O
several O
NLP O
tasks O
. O
DWS O
( O
Parker O
and O
Yu O
, O
2021 O
) O
combine O
DP O
and O
CRF O
for O
weakly O
supervised O
named O
entity O
recognition O
, O
Min O
et O
al O
. O
( O
2019 O
) O
apply O
DP O
for O
QA O
. O
Different O
from O
all O
previous O
tasks O
, O
our O
weak O
supervision O
signals O
are O
logits O
from O
other O
models O
, O
rather O
than O
discrete O
labels O
generated O
from O
rules O
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
a O
weakly O
supervised O
framework O
, O
WeCheck B-MethodName
, O
which O
aggregates O
weakly O
supervised O
signals O
from O
multiple O
resources O
and O
trains O
a O
target O
metric O
model O
in O
a O
noise O
- O
aware O
manner O
. O
Different O
from O
previous O
metrics O
that O
trains O
from O
synthetic O
data O
or O
transferred O
from O
other O
tasks O
, O
WeCheck B-MethodName
directly O
trains O
with O
the O
real O
generated O
text O
. O
WeCheck B-MethodName
first O
annotates O
each O
sample O
with O
a O
probabilistic O
label O
via O
a O
labeling O
function O
that O
aggregates O
multiple O
resources O
. O
Then O
, O
in O
the O
noise O
- O
aware O
finetuning O
stage O
, O
WeCheck B-MethodName
applies O
probabilistic O
labels O
to O
train O
the O
target O
metric O
model O
. O
Experimental O
results O
show O
that O
, O
WeCheck B-MethodName
not O
only O
surpass O
previous O
methods O
in O
performance O
but O
also O
time O
efficient O
. O
Moreover O
, O
WeCheck O
is O
potential O
to O
be O
compatible O
with O
future O
more O
stronger O
metrics O
, O
bring O
further O
improvements O
to O
the O
overall O
performance O
. O

Limitations O

Hyper O
- O
parameters O
Selection O
Some O
hyperparameters O
still O
acquire O
careful O
selection O
for O
WeCheck B-MethodName
, O
e.g. O
p B-HyperparameterName
+ I-HyperparameterName
, O
p B-HyperparameterName
− I-HyperparameterName
. O
Also O
, O
using O
different O
set O
of O
hyper O
- O
parameters O
for O
different O
tasks O
and O
datasets O
will O
further O
boost O
performance O
. O
Thus O
, O
we O
need O
to O
train O
the O
model O
several O
time O
and O
select O
the O
best O
performing O
parameters O
based O
on O
validation O
. O

End O
- O
to O
- O
End O
Training O
WeCheck B-MethodName
applies O
the O
weak O
annotation O
and O
noise O
- O
aware O
fine O
- O
tuning O
twostep O
pipeline O
, O
where O
the O
noises O
in O
the O
first O
step O
will O
greatly O
affect O
the O
performance O
of O
the O
second O
step O
. O
By O
modifying O
the O
overall O
framework O
into O
end O
- O
toend O
training O
will O
solve O
this O
problem O
. O

Acknowledgement O

This O
work O
was O
partially O
supported O
by O
National O
Key O
R O
& O
D O
Program O
of O
China O
( O
No O
. O
2022YFC3600402 O
) O
and O
National O
Social O
Science O
Foundation O
Project O
of O
China O
( O
21 O
& O
ZD287 O
) O
. O

A O
True B-DatasetName
Benchmark O

The O
TRUE B-DatasetName
benchmark O
is O
composed O
of O
the O
following O
tasks O
and O
datasets O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
No O
response O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
No O
response O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
No O
response O
. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
No O
response O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Section O
4,5,6 O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Section O
5 O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

A O
Unified O
Positive O
- O
Unlabeled O
Learning O
Framework O
for O
Document B-TaskName
- I-TaskName
Level I-TaskName
Relation I-TaskName
Extraction I-TaskName
with O
Different O
Levels O
of O
Labeling O

to O
identify O
relations O
between O
entities O
across O
multiple O
sentences O
. O
Most O
previous O
methods O
focused O
on O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
under O
full O
supervision O
. O
However O
, O
in O
real O
- O
world O
scenario O
, O
it O
is O
expensive O
and O
difficult O
to O
completely O
label O
all O
relations O
in O
a O
document O
because O
the O
number O
of O
entity O
pairs O
in O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
grows O
quadratically O
with O
the O
number O
of O
entities O
. O
To O
solve O
the O
common O
incomplete O
labeling O
problem O
, O
we O
propose O
a O
unified O
positive O
- O
unlabeled O
learning O
framework O
− O
shift B-MethodName
and I-MethodName
squared I-MethodName
ranking I-MethodName
loss I-MethodName
positive I-MethodName
- I-MethodName
unlabeled I-MethodName
( O
SSR B-MethodName
- I-MethodName
PU I-MethodName
) O
learning O
. O
We O
use O
positive B-MethodName
- I-MethodName
unlabeled I-MethodName
( O
PU B-MethodName
) O
learning O
on O
documentlevel B-TaskName
RE I-TaskName
for O
the O
first O
time O
. O
Considering O
that O
labeled O
data O
of O
a O
dataset O
may O
lead O
to O
prior O
shift O
of O
unlabeled O
data O
, O
we O
introduce O
a O
PU B-MethodName
learning O
under O
prior O
shift O
of O
training O
data O
. O
Also O
, O
using O
none O
- O
class O
score O
as O
an O
adaptive O
threshold O
, O
we O
propose O
squared O
ranking O
loss O
and O
prove O
its O
Bayesian O
consistency O
with O
multi O
- O
label O
ranking O
metrics O
. O
Extensive O
experiments O
demonstrate O
that O
our O
method O
achieves O
an O
improvement O
of O
about O
14 O
F1 B-MetricName
points O
relative O
to O
the O
previous O
baseline O
with O
incomplete O
labeling O
. O
In O
addition O
, O
it O
outperforms O
previous O
state O
- O
of O
- O
the O
- O
art O
results O
under O
both O
fully O
supervised O
and O
extremely O
unlabeled O
settings O
as O
well O
. O
1 O

Introduction O

Relation B-TaskName
extraction I-TaskName
( O
RE B-TaskName
) O
aims O
to O
identify O
the O
relations O
between O
two O
entities O
in O
a O
given O
text O
. O
It O
has O
rich O
applications O
in O
knowledge O
graph O
construction O
, O
question O
answering O
, O
and O
biomedical O
text O
understanding O
. O
Most O
of O
the O
previous O
work O
was O
to O
extract O
relations O
between O
entities O
in O
a O
single O
sentence O
( O
Miwa O
and O
Bansal O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O
Recently O
, O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
aiming O
to O
identify O
the O
relations O
among O
various O
entity O
pairs O
expressed O
in O
multiple O
sentences O
has O
received O
increasing O
research O
( O
Tan O
et O
al O
. O
, O
2022b O
) O
. O

attention O
( O
Yao O
et O
al O
. O
, O
2019 O
; O
Zhou O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2022 O
) O
. O

Previous O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
methods O
mainly O
deal O
with O
fully O
supervised O
scenarios O
. O
However O
, O
in O
real O
- O
world O
scenarios O
, O
incomplete O
labeling O
is O
a O
common O
problem O
in O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
because O
the O
number O
of O
entity O
pairs O
grows O
quadratically O
with O
the O
number O
of O
entities O
. O
DocRED B-DatasetName
( O
Yao O
et O
al O
. O
, O
2019 O
) O
is O
a O
popular O
dataset O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Recent O
studies O
( O
Huang O
et O
al O
. O
, O
2022 O
; O
Tan O
et O
al O
. O
, O
2022b O
) O
found O
that O
DocRED B-DatasetName
, O
which O
annotates O
data O
with O
a O
recommend O
- O
revise O
scheme O
, O
contains O
a O
large O
number O
of O
false O
negative O
samples O
, O
i.e. O
, O
many O
positive O
samples O
being O
unlabeled O
. O
As O
shown O
in O
Figure O
1 O
, O
document O
Alecu O
Russo O
contains O
a O
large O
number O
of O
unlabeled O
positive O
relations O
. O
Consequently O
, O
the O
models O
trained O
on O
this O
dataset O
tend O
to O
overfit O
in O
real O
scenarios O
and O
get O
lower O
recall O
. O
As O
a O
result O
, O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
with O
incomplete O
labeling O
has O
become O
an O
emergency O
need O
. O

To O
solve O
this O
problem O
, O
we O
propose O
a O
unified O
positive B-MethodName
- I-MethodName
unlabeled I-MethodName
learning I-MethodName
framework O
− O
shift B-MethodName
and I-MethodName
squared I-MethodName
ranking I-MethodName
loss I-MethodName
positive I-MethodName
- I-MethodName
unlabeled I-MethodName
( O
SSR B-MethodName
- I-MethodName
PU I-MethodName
) O
learning O
, O
which O
can O
be O
adapted O
to O
labeling O
under O
different O
levels O
. O
We O
use O
positive B-MethodName
- I-MethodName
unlabeled I-MethodName
( O
PU B-MethodName
) O
learning O
for O
the O
first O
time O
on O
the O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
task O
. O
Since O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
is O
a O
multi O
- O
label O
classification O
task O
, O
we O
apply O
a O
binary O
PU B-MethodName
learning O
method O
for O
each O
class O
( O
one O
- O
vs O
- O
all O
) O
, O
converting O
it O
to O
multi O
- O
label O
PU B-MethodName
learning O
. O
In O
addition O
, O
according O
to O
our O
observations O
, O
a O
considerable O
portion O
of O
the O
relations O
in O
DocRED B-DatasetName
, O
a O
dataset O
annotated O
by O
recommend O
- O
revise O
scheme O
, O
have O
already O
been O
annotated O
. O
This O
leads O
to O
the O
deviation O
between O
the O
prior O
distribution O
of O
the O
unlabeled O
data O
and O
the O
overall O
prior O
distribution O
. O
To O
address O
this O
problem O
, O
we O
introduce O
an O
adaptive B-MethodName
PU I-MethodName
learning O
under O
prior O
shift O
of O
training O
data O
that O
adjusts O
the O
model O
based O
on O
the O
estimated O
overall O
prior O
distribution O
and O
the O
labeled O
positive O
sample O
distribution O
to O
be O
similar O
to O
ordinary O
PN B-MethodName
learning O
or O
ordinary O
PU B-MethodName
learning O
. O
Here O
positive B-MethodName
- I-MethodName
negative I-MethodName
( O
PN B-MethodName
) O
learning O
means O
treating O
all O
unlabeled O
samples O
as O
negative O
samples O
. O
Also O
, O
to O
distinguish O
between O
none O
- O
class O
and O
predefined O
classes O
, O
we O
propose O
a O
squared O
ranking O
loss O
for O
none O
- O
class O
ranking O
such O
that O
positive O
predefined O
labels O
are O
ranked O
higher O
than O
none O
- O
class O
label O
and O
negative O
pre O
- O
defined O
labels O
are O
ranked O
lower O
. O
This O
is O
an O
ideal O
multi O
- O
label O
surrogate O
loss O
metric O
, O
and O
we O
theoretically O
prove O
its O
Bayesian O
consistency O
with O
the O
multi O
- O
label O
ranking O
metric O
proposed O
by O
( O
Zhou O
and O
Lee O
, O
2022 O
) O
. O
This O
loss O
function O
can O
be O
well O
adapted O
to O
PU B-MethodName
learning O
. O

We O
conduct O
extensive O
experiments O
on O
two O
multilabel O
document B-MethodName
- I-MethodName
level I-MethodName
RE I-MethodName
datasets O
with O
incomplete O
labeling O
, O
DocRED B-DatasetName
( O
Yao O
et O
al O
. O
, O
2019 O
) O
and O
ChemDis B-DatasetName
- I-DatasetName
Gene I-DatasetName
, O
a O
newly O
proposed O
multilabeled O
biomedical O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
dataset O
. O
Experimental O
results O
show O
that O
our O
method O
SSR B-MethodName
- I-MethodName
PU I-MethodName
outperforms O
previous O
baseline O
that O
did O
not O
consider O
the O
labeling O
incompleteness O
phenomenon O
by O
about O
14 B-MetricValue
F1 B-MetricName
points O
. O
In O
addition O
, O
we O
perform O
fully O
super O
- O
vised O
experiments O
, O
as O
well O
as O
experiments O
on O
an O
extremely O
unlabeled O
data O
that O
is O
newly O
constructed O
, O
in O
which O
the O
number O
of O
each O
relation O
type O
labeled O
in O
each O
document O
is O
limited O
to O
1 O
. O
Experiments O
under O
two O
complementary O
settings O
demonstrate O
the O
effectiveness O
of O
our O
method O
with O
different O
levels O
of O
labeling O
. O
The O
contributions O
of O
this O
paper O
are O
summarized O
as O
follows O
: O

• O
We O
propose O
a O
unified O
positive O
- O
unlabeled O
learning O
framework O
, O
SSR B-MethodName
- I-MethodName
PU I-MethodName
, O
to O
adapt O
documentlevel B-TaskName
RE I-TaskName
with O
different O
levels O
of O
incomplete O
labeling O
. O

• O
We O
apply O
PU B-MethodName
learning O
for O
the O
first O
time O
to O
the O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
task O
and O
introduce O
a O
PU B-MethodName
learning O
under O
prior O
shift O
of O
training O
data O
that O
can O
reach O
a O
balance O
between O
ordinary O
PN B-MethodName
learning O
and O
ordinary O
PU B-MethodName
learning O
based O
on O
the O
estimated O
prior O
and O
labeling O
distribution O
. O

• O
We O
propose O
squared O
ranking O
loss O
, O
which O
effectively O
improves O
performance O
relative O
to O
other O
loss O
functions O
, O
and O
prove O
its O
Bayesian O
consistency O
with O
multi O
- O
label O
ranking O
metrics O
. O

• O
Our O
method O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
in O
a O
variety O
of O
settings O
and O
provides O
a O
robust O
baseline O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
with O
incomplete O
labels O
. O

Related O
Work O

Document B-TaskName
- I-TaskName
level I-TaskName
relation I-TaskName
extraction I-TaskName
. O
Previous O
generally O
effective O
methods O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
are O
mainly O
graph O
- O
based O
models O
and O
transformerbased O
models O
. O
Graph O
- O
based O
models O
( O
Nan O
et O
al O
. O
, O
2020 O
; O
Zeng O
et O
al O
. O
, O
2020Zeng O
et O
al O
. O
, O
, O
2021Xu O
et O
al O
. O
, O
2021b O
) O
gather O
entity O
information O
for O
relational O
inference O
with O
graph O
neural O
networks O
, O
and O
transformer O
- O
based O
methods O
( O
Zhou O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2021a O
; O
Zhang O
et O
al O
. O
, O
2021 O
; O
Tan O
et O
al O
. O
, O
2022a O
) O
implicitly O
capture O
long O
- O
range O
dependencies O
. O
Recently O
, O
( O
Huang O
et O
al O
. O
, O
2022 O
; O
Tan O
et O
al O
. O
, O
2022b O
) O
found O
that O
a O
large O
number O
of O
positive O
relations O
remain O
unlabeled O
in O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
datasets O
, O
especially O
unpopular O
relations O
. O
However O
, O
the O
previous O
methods O
did O
not O
consider O
unlabeled O
data O
separately O
. O
They O
simply O
treated O
them O
all O
as O
negative O
samples O
, O
which O
led O
to O
a O
lower O
recall O
and O
a O
significant O
drop O
in O
performance O
in O
realistic O
scenarios O
. O
PU B-MethodName
learning O
. O
Positive B-MethodName
- I-MethodName
unlabeled I-MethodName
( O
PU B-MethodName
) O
learning O
( O
Elkan O
and O
Noto O
, O
2008 O
; O
du O
Plessis O
et O
al O
. O
, O
2014 O
; O
Plessis O
et O
al O
. O
, O
2015 O
; O
Kiryo O
et O
al O
. O
, O
2017 O
; O
Garg O
et O
al O
. O
, O
2021 O
) O
aims O
to O
learn O
a O
classifier O
from O
positive O
and O
unlabeled O
data O
. O
PU B-MethodName
learning O
is O
a O
kind O
of O
semisupervised O
learning O
but O
there O
is O
a O
fundamental O
difference O
between O
them O
: O
while O
semi O
- O
supervised O
learning O
requires O
labeled O
negative O
data O
, O
PU B-MethodName
learning O
requires O
only O
labeled O
positive O
data O
. O
Many O
current O
PU B-MethodName
learning O
methods O
rely O
on O
an O
overall O
prior O
estimate O
, O
while O
some O
recent O
studies O
( O
Charoenphakdee O
and O
Sugiyama O
, O
2019 O
; O
Nakajima O
and O
Sugiyama O
, O
2021 O
) O
have O
noticed O
a O
prior O
shift O
between O
the O
training O
set O
and O
the O
test O
set O
. O
On O
the O
other O
hand O
, O
PU B-MethodName
learning O
has O
been O
used O
in O
many O
NLP O
applications O
, O
e.g. O
, O
text O
classification O
( O
Li O
and O
Liu O
, O
2003 O
) O
, O
sentence O
embedding O
( O
Cao O
et O
al O
. O
, O
2021 O
) O
, O
named O
entity O
recognition O
Zhou O
et O
al O
. O
, O
2022 O
) O
, O
knowledge O
graph O
completion O
( O
Tang O
et O
al O
. O
, O
2022 O
) O
and O
sentence O
- O
level O
RE O
( O
He O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
this O
method O
is O
rarely O
applied O
to O
the O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
task O
. O

Multi O
- O
label O
classification O
. O
Multi O
- O
label O
classification O
is O
a O
widely O
investigated O
problem O
, O
and O
here O
we O
focus O
on O
the O
loss O
function O
. O
Binary O
cross O
entropy O
( O
BCE O
) O
is O
the O
most O
popular O
multi O
- O
label O
loss O
, O
reducing O
the O
multi O
- O
label O
problem O
to O
a O
number O
of O
independent O
binary O
( O
one O
- O
vs O
- O
all O
) O
classification O
tasks O
. O
Recently O
, O
( O
Hui O
and O
Belkin O
, O
2020 O
) O
have O
found O
that O
squared O
loss O
can O
also O
achieve O
better O
results O
in O
classification O
tasks O
. O
Another O
common O
multi O
- O
label O
loss O
function O
is O
pairwise O
ranking O
loss O
, O
which O
transforms O
multi O
- O
label O
learning O
into O
a O
ranking O
problem O
via O
pairwise O
( O
one O
- O
vs O
- O
one O
) O
comparison O
( O
Fürnkranz O
et O
al O
. O
, O
2008 O
; O
Li O
et O
al O
. O
, O
2017 O
) O
. O
For O
multi O
- O
label O
PU B-MethodName
learning O
, O
( O
Kanehira O
and O
Harada O
, O
2016 O
) O
treated O
it O
as O
a O
multi O
- O
label O
PU B-MethodName
ranking O
problem O
, O
and O
( O
Aota O
et O
al O
. O
, O
2021 O
) O
applied O
PU B-MethodName
learning O
to O
multi O
- O
label O
common O
vulnerabilities O
and O
exposure O
classification O
by O
using O
one O
- O
vs O
- O
all O
strategy O
. O
For O
document B-DatasetName
- I-DatasetName
level I-DatasetName
RE I-DatasetName
task O
, O
( O
Zhou O
and O
Lee O
, O
2022 O
) O
proposed O
a O
none O
- O
class O
ranking O
multi O
- O
label O
metric O
. O
This O
multi O
- O
label O
metric O
has O
not O
yet O
been O
applied O
to O
PU B-MethodName
learning O
. O

Methodology O

In O
this O
section O
, O
we O
introduce O
the O
details O
of O
our O
method O
shift B-MethodName
and I-MethodName
squared I-MethodName
ranking I-MethodName
loss I-MethodName
positiveunlabeled I-MethodName
( O
SSR B-MethodName
- I-MethodName
PU I-MethodName
) O
learning O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
with O
incomplete O
labeling O
. O
Firstly O
, O
we O
introduce O
the O
definition O
of O
positive B-MethodName
- I-MethodName
unlabeled I-MethodName
learning O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Next O
, O
we O
present O
the O
PU B-MethodName
learning O
under O
prior O
shift O
of O
training O
data O
. O
Finally O
, O
squared O
ranking O
loss O
using O
the O
none O
- O
class O
score O
as O
an O
adaptive O
threshold O
is O
proposed O
. O

Positive B-MethodName
- I-MethodName
unlabeled I-MethodName
learning O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName

Document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
can O
be O
viewed O
as O
a O
multi O
- O
label O
classification O
task O
, O
where O
each O
entity O
pair O
is O
an O
instance O
and O
the O
associated O
relations O
are O
label O
samples O
. O
Previous O
supervised O
learning O
methods O
only O
treated O
unlabeled O
relations O
as O
negative O
samples O
, O
which O
may O
lead O
to O
low O
recall B-MetricName
in O
the O
presence O
of O
a O
large O
number O
of O
false O
negatives O
. O
To O
address O
this O
problem O
, O
we O
adopt O
PU B-MethodName
learning O
( O
du O
Plessis O
et O
al O
. O
, O
2014 O
; O
Plessis O
et O
al O
. O
, O
2015 O
) O
for O
each O
class O
. O

Let O
X O
be O
an O
instance O
space O
and O
Y O
= O
{ O
−1 O
, O
+1 O
} O
K O
be O
a O
label O
space O
, O
where O
K O
is O
the O
number O
of O
predefined O
classes O
. O
An O
instance O
x O
∈ O
X O
is O
associated O
with O
a O
subset O
of O
labels O
, O
identified O
by O
a O
binary O
vector O
y O
∈ O
Y O
= O
( O
y O
1 O
, O
. O
. O
. O
, O
y O
K O
) O
, O
where O
y O
i O
= O
+1 O
if O
the O
i O
- O
th O
label O
is O
positive O
for O
x O
, O
and O
y O
i O
= O
−1 O
otherwise O
. O
A O
score O
function O
is O
defined O
as O
f O
( O
x O
) O
= O
( O
f O
1 O
( O
x O
) O
, O
f O
2 O
( O
x O
) O
, O
... O
, O
f O
K O
( O
x O
) O
) O
. O
In O
the O
following O
we O
use O
f O
i O
instead O
, O
to O
omit O
the O
dependency O
on O
x O
. O

For O
i O
- O
th O
class O
, O
assume O
that O
the O
data O
follow O
an O
unknown O
probability O
distribution O
with O
density O
p O
( O
x O
, O
y O
i O
) O
, O

p O
P O
i O
= O
p O
( O
x O
| O
y O
i O
= O
+1 O
) O
as O
the O
positive O
marginal O
, O
p O
N O
i O
= O
p O
( O
x O
| O
y O
i O
= O
−1 O
) O

as O
the O
negative O
marginal O
, O
and O
p O
i O
( O
x O
) O
as O
the O
marginal O
. O
In O
positivenegative B-MethodName
( O
PN B-MethodName
) O
learning O
, O
the O
goal O
is O
to O
minimize O
the O
expected O
classification O
risk O
: O

R O
PN O
( O
f O
) O
= O
K O
i=1 O
E O
x O
, O
y O
i O
∼p O
( O
x O
, O
y O
i O
) O
[ O
ℓ O
( O
f O
i O
, O
y O
i O
) O
] O
, O
( O
1 O
) O

Here O
, O
Eq.1 O
can O
be O
calculated O
by O
equivalently O
using O
the O
sum O
of O
the O
errors O
of O
positive O
and O
negative O
samples O
: O

R O
PN O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
+1 O
) O
] O
+ O
( O
1 O
− O
π O
i O
) O
E O
N O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
) O
, O
( O
2 O
) O

where O

π O
i O
= O
p O
( O
y O
i O
= O
+1 O
) O
and O
( O
1−π O
i O
) O
= O
( O
1−p O
( O
y O
i O
= O
+1 O
) O
) O
= O
p O
( O
y O
i O
= O
−1 O
) O

is O
the O
positive O
and O
negative O
prior O
of O
the O
i O
- O
th O
class O
. O

E O
P O
i O
[ O
• O
] O
= O
E O
x∼p O
( O
x|y O
i O
= O
+1 O
) O
[ O
• O
] O
, O
E O
N O
i O
[ O
• O
] O
= O
E O
x∼p O
( O
x|y O
i O
= O
−1 O
) O
[ O
• O
] O

and O
the O
loss O
function O
is O
represented O
by O
ℓ. O
Rewriting O
Eq.2 O
into O
a O
form O
that O
uses O
the O
data O
for O
approximation O
, O
we O
get O
: O

R O
PN O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
n O
P O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
+1 O
) O
+ O
( O
1 O
− O
π O
i O
) O
n O
N O
i O
n O
N O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
N O
i O
j O
) O
, O
−1 O
) O
) O
, O
( O
3 O
) O

where O
x O
P O
i O
j O
and O
x O
N O
i O
j O
denote O
cases O
that O
the O
j O
- O
th O
sample O
of O
class O
i O
is O
positive O
or O
negative O
. O
n O
P O
i O
and O
n O
N O
i O
are O
the O
number O
of O
positive O
and O
negative O
samples O
of O
class O
i O
, O
respectively O
. O

In O
positive B-MethodName
- I-MethodName
unlabeled I-MethodName
( O
PU B-MethodName
) O
learning O
, O
due O
to O
the O
absence O
of O
negative O
samples O
, O
we O
can O
not O
estimate O
E O
N O
i O
[ O
• O
] O
from O
the O
data O
. O
Following O
( O
du O
Plessis O
et O
al O
. O
, O
2014 O
) O
, O
PU B-MethodName
learning O
assumes O
that O
unlabeled O
data O
can O
reflect O
the O
true O
overall O
distribution O
, O
that O
is O
, O
p O
U O
i O
( O
x O
) O
= O
p O
i O
( O
x O
) O
. O
The O
expected O
classification O
risk O
formulation O
can O
be O
defined O
as O
: O

R O
PU O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
+1 O
) O
] O
+ O
E O
U O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
− O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
) O
, O
( O
4 O
) O

Here O

E O
U O
i O
[ O
• O
] O
= O
E O
x∼p O
i O
( O
x O
) O
[ O
• O
] O
and O
E O
U O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
−π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
can O
alternatively O
represent O
( O
1 O
− O
π O
i O
) O
E O
N O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
because O
p O
i O
( O
x O
) O
= O
π O
i O
p O
P O
i O
( O
x O
) O
+ O
( O
1 O
− O
π O
i O
) O
p O
N O
i O
( O
x O
) O
. O

By O
rewriting O
Eq.4 O
in O
a O
form O
that O
can O
be O
approximated O
using O
the O
data O
, O
we O
get O
the O
following O
: O

R O
PU O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
n O
P O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
+1 O
) O
+ O
[ O
1 O
n O
U O
i O
n O
U O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
U O
i O
j O
) O
, O
−1 O
) O
− O
π O
i O
n O
P O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
−1 O
) O
] O
) O
, O
( O
5 O
) O

where O
x O
U O
i O
j O
denote O
cases O
that O
the O
j O
- O
th O
sample O
is O
unlabeled O
as O
class O
i O
and O
n O
U O
i O
is O
the O
number O
of O
samples O
unlabeled O
as O
class O
i O
. O

However O
, O
the O
second O
term O
in O
Eq.5 O
can O
be O
negative O
and O
can O
be O
prone O
to O
overfitting O
when O
using O
a O
highly O
flexible O
model O
. O
Thus O
, O
a O
non O
- O
negative O
risk O
estimator O
( O
Kiryo O
et O
al O
. O
, O
2017 O
) O
is O
proposed O
to O
alleviate O
the O
overfitting O
problem O
: O

R O
PU O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
n O
P O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
+1 O
) O
+ O
max O
( O
0 O
, O
[ O
1 O
n O
U O
i O
n O
U O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
U O
i O
j O
) O
, O
−1 O
) O
− O
π O
i O
n O
P O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
−1 O
) O
] O
) O
) O
. O
( O
6 O
) O

For O
ℓ O
, O
we O
use O
the O
convex O
function O
squared O
loss O
: O
and O
we O
compare O
the O
performance O
of O
using O
squared O
loss O
and O
log O
- O
sigmoid O
loss O
functions O
in O
Section O
4.4 O
. O
The O
latter O
is O
a O
convex O
loss O
function O
commonly O
used O
in O
classification O
. O

ℓ O
( O
f O
i O
, O
y O
i O
) O
= O
1 O
4 O
( O
y O
i O
f O
i O
− O
1 O
) O
2 O
, O
( O
7 O
) O

In O
addition O
, O
to O
solve O
the O
heavy O
class O
imbalance O
problem O
, O
we O
multiply O
γ O
i O
= O
( O
1−π O
i O
π O
i O
) O
0.5 O
before O
positive O
risk O
estimations O
as O
the O
class O
weight O
. O

Class O
prior O
shift O
of O
training O
data O

Ordinary O
PU B-MethodName
learning O
requires O
an O
assumption O
that O
the O
overall O
distribution O
needs O
to O
be O
the O
same O
as O
the O
distribution O
of O
the O
unlabeled O
data O
. O
In O
contrast O
, O
with O
the O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
dataset O
constructed O
by O
a O
recommend O
- O
revise O
scheme O
, O
many O
relations O
are O
probably O
already O
annotated O
, O
especially O
the O
common O
ones O
. O
This O
leads O
to O
a O
prior O
shift O
in O
the O
unlabeled O
data O
of O
the O
training O
set O
. O
When O
this O
assumption O
is O
broken O
, O
ordinary O
PU B-MethodName
learning O
will O
yield O
a O
biased O
result O
. O
To O
address O
this O
problem O
, O
inspired O
by O
the O
method O
( O
Charoenphakdee O
and O
Sugiyama O
, O
2019 O
) O
for O
handling O
a O
prior O
shift O
between O
the O
test O
set O
and O
the O
training O
set O
, O
we O
introduce O
the O
PU B-MethodName
learning O
under O
prior O
shift O
of O
training O
data O
. O

For O
each O
class O
, O
assume O
that O
the O
original O
prior O

π O
i O
= O
p O
( O
y O
i O
= O
+1 O
) O
. O
We O
set O
π O
labeled O
, O
i O
= O
p O
( O
s O
i O
= O
+1 O
) O
and O
( O
1 O
− O
π O
labeled O
, O
i O
) O
= O
( O
1 O
− O
p O
( O
s O
i O
= O
+1 O
) O
) O
= O
p O
( O
s O
i O
= O
−1 O
) O

where O
s O
i O
= O
+1 O
or O
s O
i O
= O
−1 O
mean O
that O
the O
i O
- O
th O
class O
is O
labeled O
or O
unlabeled O
, O
respectively O
. O
As O
shown O
in O
Figure O
2 O
, O
the O
conditional O
probability O
of O
a O
positive O
sample O
under O
unlabeled O
data O
is O
different O
from O
the O
probability O
of O
an O
overall O
positive O
sample O
. O
The O
conditional O
probability O
of O
a O
positive O
sample O
under O
unlabeled O
data O
is O
: O

p O
( O
y O
i O
= O
1 O
| O
s O
i O
= O
−1 O
) O
= O
p O
( O
y O
i O
= O
1 O
, O
s O
i O
= O
−1 O
) O
p O
( O
s O
i O
= O
−1 O
) O
, O
( O
8 O
) O

where O
p O
( O
y O
i O
= O
1 O
, O
s O
i O
= O
−1 O
) O
= O
π O
i O
− O
π O
labeled O
, O
i O
, O
we O
can O
obtain O
the O
prior O
of O
positive O
samples O
in O
the O
new O
unlabeled O
data O
after O
labeling O
as O
π O
u O
, O
i O
= O
p O
( O
y O
i O
= O
1 O
| O

s O
i O
= O
−1 O
) O
= O
π O
i O
−π O
labeled O
, O
i O
1−π O
labeled O
, O
i O
. O

For O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
, O
the O
goal O
is O
to O
minimize O
the O
following O
misclassification O
risk O
for O
the O
original O
distribution O
of O
the O
training O
data O
: O

R O
ori O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
+1 O
) O
] O
+ O
( O
1 O
− O
π O
i O
) O
E O
N O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
) O
. O
( O
9 O
) O

We O
can O
express O
R O
ori O
( O
f O
) O
using O
the O
expectation O
of O
positive O
and O
unlabeled O
data O
by O
the O
following O
theorem O
. O

Theorem O
1 O
. O
The O
misclassification O
risk O
R O
ori O
( O
f O
) O
can O
be O
equivalently O
expressed O
as O

R O
S−PU O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
+1 O
) O
] O
+ O
1 O
− O
π O
i O
1 O
− O
π O
u O
, O
i O
E O
U O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
− O
π O
u O
, O
i O
− O
π O
u O
, O
i O
π O
i O
1 O
− O
π O
u O
, O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
) O
. O
( O
10 O
) O

Proof O
. O
Proof O
appears O
in O
Appendix O
A.1 O
. O

As O
a O
result O
, O
we O
can O
obtain O
the O
non O
- O
negative O
risk O
estimator O
( O
Kiryo O
et O
al O
. O
, O
2017 O
) O
under O
class O
prior O
shift O
of O
training O
data O
as O
follows O
: O

R O
S−PU O
( O
f O
) O
= O
K O
i=1 O
( O
1 O
n O
P O
i O
π O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
+1 O
) O
+ O
max O
( O
0 O
, O
[ O
1 O
n O
U O
i O
1 O
− O
π O
i O
1 O
− O
π O
u O
, O
i O
n O
U O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
U O
i O
j O
) O
, O
−1 O
) O
− O
1 O
n O
P O
i O
π O
u O
, O
i O
− O
π O
u O
, O
i O
π O
i O
1 O
− O
π O
u O
, O
i O
n O
P O
i O
j=1 O
ℓ O
( O
f O
i O
( O
x O
P O
i O
j O
) O
, O
−1 O
) O
] O
) O
) O
. O

( O
11 O
) O
We O
can O
observe O
that O
PN B-MethodName
learning O
and O
PU B-MethodName
learning O
are O
special O
cases O
of O
this O
function O
. O
When O
π O
u O
, O
i O
= O
0 O
, O
this O
equation O
reduces O
to O
the O
form O
of O
ordinary O
PN B-MethodName
learning O
, O
and O
when O
π O
u O
, O
i O
= O
π O
i O
, O
this O
equation O
reduces O
to O
the O
form O
of O
ordinary O
PU B-MethodName
learning O
. O

Squared O
ranking O
loss O

To O
better O
measure O
the O
performance O
of O
documentlevel B-TaskName
RE I-TaskName
, O
( O
Zhou O
and O
Lee O
, O
2022 O
) O
proposed O
a O
new O
multi O
- O
label O
performance O
measure O
: O
where O
positive O
pre O
- O
defined O
labels O
should O
be O
ranked O
higher O
than O
the O
none O
- O
class O
label O
and O
negative O
ones O
should O
be O
ranked O
below O
. O

L O
NA O
( O
f O
, O
y O
) O
= O
K O
i=1 O
( O
y O
i O
> O
0 O
f O
i O
< O
f O
0 O
+ O
y O
i O
≤ O
0 O
f O
i O
> O
f O
0 O
+ O
1 O
2 O
f O
i O
= O
f O
0 O
) O
, O
( O
12 O
) O

• O
is O
an O
indicator O
function O
that O
takes O
the O
value O
of O
1 O
when O
the O
conditions O
in O
the O
parentheses O
are O
met O
, O
otherwise O
0 O
. O
However O
, O
it O
is O
difficult O
to O
optimize O
the O
above O
equation O
directly O
. O
Thus O
, O
we O
propose O
the O
squared O
ranking O
surrogate O
loss O
by O
rewriting O
Eq.7 O
as O
: O

ℓ O
SR O
( O
f O
i O
, O
y O
i O
) O
= O
1 O
4 O
( O
y O
i O
( O
f O
i O
− O
f O
0 O
) O
− O
margin O
) O
2 O
, O
( O
13 O

where O
margin B-HyperparameterName
is O
a O
hyper O
- O
parameter O
and O
f O
0 O
is O
the O
none O
- O
class O
score O
, O
when O
f O
i O
is O
greater O
than O
f O
0 O
the O
label O
exists O
, O
and O
otherwise O
not O
. O

Next O
we O
prove O
the O
Bayesian O
consistency O
of O
ℓ O
SR O
with O
the O
multi O
- O
label O
ranking O
metric O
L O
NA O
when O
margin O
̸ O
= O
0 O
. O
Given O
an O
instance O
x O
, O
let O
∆ O
i O
= O
P O
( O
y O
i O
= O
1 O
| O
x O
) O
be O
the O
marginal O
probability O
when O
the O
i O
- O
th O
label O
is O
positive O
, O
the O
Bayes O
optimal O
score O
function O
f O
* O
NA O
that O
minimizes O
the O
multi O
- O
label O
risk O
E O
[ O
L O
NA O
( O
P O
, O
f O
) O
| O
x O
] O
is O
given O
by O
: O

f O
* O
NA O
∈ O
{ O
f O
: O
f O
i O
> O
f O
0 O
if O
∆ O
i O
> O
1 O
2 O
, O
and O
f O
i O
< O
f O
0 O
if O
∆ O
i O
< O
1 O
2 O
} O
. O
( O
14 O
) O

The O
next O
theory O
guarantees O
that O
the O
classifier O
obtained O
by O
minimizing O
the O
surrogate O
loss O
ℓ O
SR O
converges O
to O
the O
classifier O
with O
the O
lowest O
multi O
- O
label O
risk O
, O
thus O
making O
it O
possible O
to O
achieve O
a O
better O
classification O
performance O
w.r.t O
. O
corresponding O
to O
the O
multi O
- O
label O
performance O
metric O
. O

Theorem O
2 O
. O
ℓ O
SR O
( O
Eq.13 O
) O
is O
Bayes O
consistent O
w.r.t O
. O
L O
NA O
( O
Eq.12 O
) O
when O
margin O
̸ O
= O
0 O
. O

Proof O
. O
Proof O
appears O
in O
Appendix O
A.2 O
. O

As O
a O
supplement O
, O
we O
likewise O
compare O
the O
logsigmoid O
ranking O
loss O
performance O
in O
Section O
4.4 O
. O

Experiments O

In O
this O
section O
, O
we O
evaluate O
our O
method O
on O
two O
multi O
- O
label O
document B-DatasetName
- I-DatasetName
level I-DatasetName
RE I-DatasetName
datasets O
with O
in- O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
Large I-MethodName
for O
DocRED B-DatasetName
and O
PubmedBert B-DatasetName
( O
Gu O
et O
al O
. O
, O
2021 O
) O
for O
ChemDisGene B-DatasetName
. O
We O
use O
Huggingface O
's O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
to O
implement O
all O
the O
models O
and O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
as O
the O
optimizer O
, O
and O
apply O
a O
linear O
warmup O
( O
Goyal O
et O
al O
. O
, O
2017 O
) O
at O
the O
first O
6 O
% O
steps O
followed O
by O
a O
linear O
decay B-HyperparameterName
to O
0 B-HyperparameterValue
. O
For O
DocRED B-DatasetName
, O
we O
set O
the O
learning B-HyperparameterName
rates I-HyperparameterName
for O
BERT B-MethodName
Base I-MethodName
and O
RoBERTa B-MethodName
Large I-MethodName
settings O
to O
5e-5 B-MetricValue
and O
3e-5 B-MetricValue
, O
respectively O
, O
in O
the O
same O
way O
as O
ATLOP O
. O

For O
ChemDisGene B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
to O
2e-5 B-HyperparameterValue
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
( O
number O
of O
documents O
per O
batch O
) O
is O
set O
to O
4 B-HyperparameterValue
and O
8 B-HyperparameterValue
for O
two O
datasets O
, O
respectively O
. O
During O
our O
experiment O
, O
we O
set O
π O
i O
= O
3π O
labeled O
, O
i O
and O
margin B-HyperparameterName
= O
0.25 B-HyperparameterValue
. O
To O
evaluate O
the O
efficacy O
of O
our O
methods O
in O
realistic O
settings O
, O
we O
do O
not O
use O
any O
fully O
labeled O
validation O
or O
test O
sets O
in O
any O
stage O
of O
the O
training O
process O
. O
The O
training O
stopping O
criteria O
are O
set O
as O
follows O
: O
30 B-HyperparameterValue
epochs B-HyperparameterName
for O
both O
two O
dataset O
. O
We O
report O
the O
performance O
of O
the O
final O
model O
instead O
of O
the O
best O
checkpoint O
. O
All O
experiments O
are O
conducted O
with O
1 O
Tesla O
A100 O
- O
40 O
G O
GPU O
. O

Baseline O
. O
We O
re O
- O
implemented O
the O
existing O
fully O
supervised O
methods O
BiLSTM B-MethodName
( O
Yao O
et O
al O
. O
, O
2019 O
) O
, O
GAIN B-MethodName
( O
Zeng O
et O
al O
. O
, O
2020 O
) O
, O
DocuNET B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
for O
GAIN B-MethodName
and O
BiLSTM B-MethodName
we O
use O
a O
fixed O
threshold B-HyperparameterName
of O
0.5 B-HyperparameterValue
and O
all O
methods O
take O
the O
final O
result O
of O
the O
model O
instead O
of O
the O
best O
checkpoint O
. O
For O
ChemDis B-DatasetName
- I-DatasetName
Gene I-DatasetName
, O
we O
used O
BRAN B-MethodName
( O
Verga O
et O
al O
. O
, O
2018 O
) O
, O
Pubmed B-MethodName
- I-MethodName
Bert I-MethodName
( O
Gu O
et O
al O
. O
, O
2021 O
) O
and O
PubmedBert B-MethodName
+ I-MethodName
BRAN I-MethodName
mentioned O
in O
as O
the O
baseline O
models O
, O
and O
ATLOP O
is O
re O
- O
implemented O
as O
a O
supplementary O
baseline O
. O
Evaluation O
metric O
. O
For O
DocRED B-DatasetName
, O
we O
use O
the O
micro B-MetricName
F1 I-MetricName
( O
F1 B-MetricName
) O
, O
micro B-MetricName
ignore I-MetricName
F1 I-MetricName
( O
Ign B-MetricName
F1 I-MetricName
) O
, O
precision B-MetricName
( O
P B-MetricName
) O
and O
recall B-MetricName
( O
R B-MetricName
) O
as O
the O
evaluation O
metrics O
to O
evaluate O
the O
overall O
performance O
of O
a O
model O
. O
Ign B-MetricName
F1 I-MetricName
measures O
the O
F1 B-MetricName
score I-MetricName
excluding O
the O
relations O
shared O
by O
the O
training O
and O
test O
set O
. O
For O
ChemDis B-DatasetName
- I-DatasetName
Gene I-DatasetName
, O
we O
use O
micro B-MetricName
F1 I-MetricName
( O
F1 B-MetricName
) O
, O
precision B-MetricName
( O
P B-MetricName
) O
and O
recall B-MetricName
( O
R B-MetricName
) O
as O
the O
evaluation O
metrics O
. O

Main O
Results O

In O
this O
subsection O
, O
we O
present O
the O
results O
of O
comparison O
of O
PN B-MethodName
learning I-MethodName
( O
PN B-MethodName
) O
, O
squared B-MethodName
ranking I-MethodName
loss I-MethodName
PN I-MethodName
learning I-MethodName
( O
SR B-MethodName
- I-MethodName
PN I-MethodName
) O
, O
PU B-MethodName
learning I-MethodName
( O
PU B-MethodName
) O
, O
squared B-MethodName
ranking I-MethodName
loss I-MethodName
PU I-MethodName
learning I-MethodName
( O
SR B-MethodName
- I-MethodName
PU I-MethodName
) O
, O
PU B-MethodName
learning I-MethodName
under O
prior O
shift O
of O
training O
data O
( O
S B-MethodName
- I-MethodName
PU I-MethodName
) O
and O
SSR B-MethodName
- I-MethodName
PU I-MethodName
. O
All O
methods O
use O
the O
same O
encoder O
and O
different O
loss O
functions O
. O
For O
each O
method O
, O
we O
use O
the O
same O
hyper O
- O
parameter O
settings O
and O
report O
the O
mean O
and O
standard O
deviation O
on O
the O
test O
set O
by O
conducting O
5 O
runs O
with O
different O
random B-HyperparameterName
seeds I-HyperparameterName
( O
62,63,64,65,66 B-HyperparameterValue
) O
. O

Results O
on O
DocRED B-DatasetName
. O
As O
shown O
in O
Table O
2 O
, O
our O
SSR B-MethodName
- I-MethodName
PU I-MethodName
method O
achieves O
a O
state O
- O
of O
- O
the O
- O
art O
F1 B-MetricName
and O
Ign B-MetricName
F1 I-MetricName
in O
both O
BERT B-MethodName
Base I-MethodName
and O
RoBERTa B-MethodName
Large I-MethodName
settings O
and O
outperforms O
the O
original O
ATLOP B-MethodName
by O
13.58 B-MetricValue
and O
14.52 B-MetricValue
F1 B-MetricName
points O
, O
respectively O
. O
Meanwhile O
, O
consistent O
with O
the O
observation O
in O
the O
paper O
( O
Huang O
et O
al O
. O
, O
2022 O
) O
, O
existing O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
methods O
under O
full O
supervision O
have O
a O
significant O
performance O
degradation O
in O
the O
incompletely O
labeled O
scenario O
. O

The O
original O
ATLOP B-MethodName
method O
has O
the O
highest O
precision B-MetricName
( O
P B-MetricName
) O
but O
low B-MetricName
recall I-MetricName
( O
R B-MetricName
) O
, O
which O
implies O
that O
supervised O
learning O
methods O
that O
simply O
treat O
unlabeled O
data O
as O
negative O
samples O
lack O
the O
generalization O
ability O
to O
extract O
instances O
of O
relations O
that O
are O
systematically O
missed O
in O
the O
dataset O
. O
PN B-MethodName
learning O
uses O
an O
estimated O
prior O
, O
but O
will O
yield O
a O
biased O
result O
because O
there O
are O
still O
positive O
samples O
in O
the O
unlabeled O
data O
. O
While O
PU B-MethodName
learning O
uses O
both O
unlabeled O
and O
labeled O
data O
to O
better O
estimate O
the O
expectation O
of O
negative O
samples O
, O
which O
results O
in O
a O
higher O
recall B-MetricName
rate O
. O
In O
addition O
, O
ordinary O
PU B-MethodName
methods O
without O
prior O
shift O
overestimate O
the O
content O
of O
positive O
samples O
in O
unlabeled O
data O
, O
which O
means O
that O
the O
model O
will O
tend O
to O
identify O
more O
samples O
as O
positive O
, O
i.e. O
, O
higher O
recall B-MetricName
, O
but O
also O
leads O
to O
more O
false O
- O
positive O
prediction O
results O
, O
i.e. O
, O
lower O
precision B-MetricName
. O
In O
contrast O
, O
the O
S B-MethodName
- I-MethodName
PU I-MethodName
method O
with O
prior O
shift O
effectively O
mitigates O
this O
phenomenon O
by O
bringing O
the O
positive O
samples O
estimated O
by O
the O
model O
in O
the O
unlabeled O
data O
closer O
to O
their O
true O
distribution O
. O
For O
example O
, O
in O
experiments O
under O
the O
BERT B-MethodName
Base I-MethodName
setting O
, O
there O
is O
a O
small O
decrease O
in O
recall B-MetricName
of O
less O
than O
2 B-MetricValue
percentage I-MetricValue
points O
, O
while O
the O
precision B-MetricName
improves O
by O
about O
7 B-HyperparameterValue
percentage I-HyperparameterValue
points O
, O
leading O
to O
an O
improvement O
in O
the O
final O
results O
. O
And O
this O
phenomenon O
is O
more O
evident O
in O
common O
relations O
as O
analyzed O
in O
Section O
4.4 O
. O
Finally O
, O
applying O
squared O
ranking O
loss O
in O
PN B-MethodName
learning O
, O
PU B-MethodName
learning O
and O
S B-MethodName
- I-MethodName
PU I-MethodName
learning O
can O
further O
improve O
the O
performance O
of O
the O
model O
, O
demonstrating O
the O
effectiveness O
of O
the O
method O
with O
none O
- O
class O
score O
as O
an O
adaptive O
threshold O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
Results O
on O
ChemDisGene B-DatasetName
. O
As O
shown O
in O
Table O
3 O
, O
the O
improvement O
of O
our O
method O
agrees O
with O
the O
results O
on O
DocRED B-DatasetName
, O
reaching O
the O
state O
- O
of O
- O
the O
- O
art O
F1 B-MetricName
, O
which O
is O
5.83 B-MetricValue
F1 B-MetricName
points O
higher O
than O
the O
original O
ATLOP B-MethodName
. O
Notice O
that O
the O
improvement O
on O
ChemDis B-DatasetName
- I-DatasetName
Gene I-DatasetName
is O
not O
as O
dramatic O
as O
that O
on O
DocRED B-DatasetName
. O
We O
argue O
that O
this O
may O
be O
due O
to O
the O
fact O
that O
some O
of O
the O
documents O
in O
the O
extra O
annotated O
All O
relationships O
test O
set O
are O
from O
another O
corpus O
DrugProt O
( O
Miranda O
et O
al O
. O
, O
2021 O
) O
, O
and O
that O
the O
annotation O
by O
human O
experts O
has O
a O
large O
deviation O
from O
the O
original O
training O
set O
distribution O
. O
This O
suggests O
that O
it O
is O
a O
challenging O
direction O
to O
make O
the O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
model O
more O
generalizable O
when O
it O
is O
difficult O
to O
estimate O
the O
true O
distribution O
of O
the O
test O
set O
. O

Different O
Levels O
of O
Labeling O

Fully O
supervised O
setting O
. O
In O
this O
setting O
, O
we O
set O
π O
i O
= O
π O
labeled O
, O
i O
and O
other O
hyper O
- O
parameters O
identically O
. O
As O
shown O
in O
Table O
4 O
, O
we O
use O
the O
( O
Tan O
et O
al O
. O
, O
2022b O
) O
revised O
Re B-DatasetName
- I-DatasetName
DocRED I-DatasetName
dataset O
in O
the O
same O
fully O
supervised O
setting O
to O
compare O
with O
the O
current O
state O
- O
of O
- O
the O
- O
art O
baseline O
models O
ATLOP B-MethodName
( O
Zhou O
et O
al O
. O
, O
2021 O
) O
, O
DocuNET B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
and O
KD B-MethodName
- I-MethodName
DocRE I-MethodName
( O
Tan O
et O
al O
. O
, O
2022a O
) O
. O
Our O
method O
achieves O
the O
same O
state O
- O
of O
- O
the O
- O
art O
results O
, O
demonstrating O
the O
effectiveness O
of O
our O
method O
with O
full O
labeling O
. O
The O
result O
with O
this O
setting O
can O
be O
seen O
as O
an O
upper O
bound O
for O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
with O
in- O
complete O
labeling O
. O
More O
details O
of O
the O
experiment O
are O
shown O
in O
Appendix O
A.3 O
. O
Extremely O
unlabeled O
setting O
. O
In O
this O
setting O
, O
we O
use O
the O
original O
training O
set O
of O
DocRED B-DatasetName
to O
construct O
an O
extremely O
unlabeled O
training O
set O
, O
i.e. O
, O
the O
number O
of O
labels O
for O
each O
relation O
type O
in O
the O
document O
being O
limited O
to O
1 O
. O
The O
average O
number O
of O
relations O
in O
the O
processed O
documents O
is O
reduced O
to O
5.4 O
. O
We O
consider O
this O
a O
more O
difficult O
and O
challenging O
scenario O
. O
We O
set O
π O
i O
= O
12π O
labeled O
, O
i O
and O
other O
hyper O
- O
parameters O
identically O
. O
As O
shown O
in O
Table O
5 O
, O
traditional O
supervised O
learning O
methods O
fail O
, O
while O
our O
proposed O
SSR B-MethodName
- I-MethodName
PU I-MethodName
method O
still O
yields O
a O
robust O
result O
. O
It O
is O
worth O
noting O
that O
since O
the O
labeled O
sample O
is O
only O
a O
fraction O
of O
the O
true O
positive O
sample O
, O
i.e. O
, O
the O
biased O
distribution O
, O
which O
means O
p O
( O
x O
| O
y O
i O
= O
1 O
) O
is O
not O
equal O
to O
p O
( O
x O
| O
s O
i O
= O
1 O
) O
, O
the O
first O
term O
in O
Eq.11 O
is O
actually O
a O
biased O
approximation O
to O
the O
first O
term O
in O
Eq.10 O
. O
We O
consider O
this O
bias O
as O
one O
of O
the O
bottlenecks O
of O
the O
current O
method O
and O
the O
main O
reason O
why O
the O
method O
degrades O
a O
lot O
in O
extremely O
unlabeled O
scenarios O
, O
i.e. O
, O
the O
bias O
is O
widened O
in O
extremely O
unlabeled O
scenarios O
. O
This O
is O
a O
good O
direction O
for O
future O
research O
, O
where O
possible O
solutions O
might O
involve O
adding O
some O
data O
augmentation O
or O
bootstrapping O
methods O
for O
labeling O
to O
alleviate O
this O
bias O
. O
More O
details O
of O
the O
experiment O
are O
shown O
in O
Appendix O
A.4 O
. O

Additional O
Analysis O

Analysis O
of O
common O
relations O
. O
As O
shown O
in O
table O
6 O
and O
table O
7 O
, O
we O
show O
the O
results O
for O
common O
relations O
on O
DocRED B-DatasetName
and O
ChemDisGene B-DatasetName
, O
these O
frequent O
relation O
types O
account O
for O
about O
60 O
% O
of O
the O
relation O
triples O
( O
Tan O
et O
al O
. O
, O
2022b O
; O
a O
slightly O
higher O
recall B-MetricName
and O
much O
lower O
precision B-MetricName
, O
which O
corresponds O
to O
an O
overestimation O
of O
the O
positive O
sample O
size O
in O
the O
unlabeled O
data O
. O
The O
SSR B-MethodName
- I-MethodName
PU I-MethodName
method O
, O
on O
the O
other O
hand O
, O
can O
alleviate O
this O
problem O
well O
, O
contributing O
to O
a O
better O
balance O
among O
precision O
and O
recall O
and O
better O
performance O
. O
This O
indicates O
a O
large O
amount O
of O
prior O
shift O
in O
common O
relations O
, O
which O
is O
consistent O
with O
( O
Huang O
et O
al O
. O
, O
2022 O
) O
observation O
that O
common O
relations O
are O
more O
likely O
to O
be O
labeled O
in O
the O
dataset O
. O

Comparison O
with O
other O
loss O
functions O
. O
We O
compare O
the O
squared O
loss O
with O
the O
log O
- O
sigmoid O
loss O
, O
which O
is O
commonly O
used O
in O
multi O
- O
label O
classification O
at O
the O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
And O
again O
, O
this O
loss O
function O
is O
rewritten O
into O
a O
none O
- O
class O
ranking O
form O
for O
further O
comparison O
with O
squared O
ranking O
loss O
. O
The O
details O
of O
the O
loss O
function O
are O
listed O
in O
Appendix O
A.5 O
. O
As O
shown O
in O
Table O
8 O
, O
both O
the O
squared O
loss O
function O
and O
the O
squared O
ranking O
loss O
function O
are O
significantly O
improved O
compared O
to O
the O
other O
loss O
functions O
, O
which O
demonstrates O
the O
effectiveness O
of O
our O
proposed O
loss O
function O
in O
the O
multi O
- O
label O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
task O
. O

Conclusion O
and O
Future O
Work O

In O
this O
paper O
, O
we O
propose O
a O
unified O
positiveunlabeled B-MethodName
learning O
framework O
, O
SSR B-MethodName
- I-MethodName
PU I-MethodName
, O
which O
can O
effectively O
solve O
the O
incomplete O
labeling O
of O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
We O
use O
PU B-MethodName
learning O
on O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
for O
the O
first O
time O
and O
introduce O
a O
PU B-MethodName
learning O
under O
prior O
shift O
of O
training O
data O
to O
adapt O
to O
different O
levels O
of O
labeling O
. O
Also O
, O
we O
propose O
squared O
ranking O
loss O
, O
using O
none O
- O
class O
score O
as O
an O
adaptive O
threshold O
. O
Experiments O
demonstrate O
that O
our O
method O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
with O
different O
levels O
of O
labeling O
and O
provides O
a O
robust O
new O
baseline O
for O
incompletely O
labeled O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
. O
In O
the O
future O
, O
we O
will O
consider O
methods O
that O
do O
not O
require O
estimation O
of O
priors O
, O
allowing O
generalization O
to O
unknown O
distributions O
more O
accurately O
, O
as O
well O
as O
addressing O
the O
problem O
of O
biased O
distributions O
with O
incomplete O
labeled O
positive O
samples O
and O
further O
improving O
the O
extraction O
performance O
of O
long O
- O
tail O
relations O
. O

Limitations O

Regarding O
the O
limitations O
of O
our O
proposed O
method O
, O
our O
method O
requires O
an O
estimation O
of O
an O
overall O
prior O
that O
will O
affect O
the O
final O
result O
. O
In O
a O
realistic O
scenario O
, O
a O
very O
accurate O
prior O
estimation O
may O
be O
difficult O
to O
obtain O
. O
In O
addition O
, O
the O
biased O
distribution O
caused O
by O
the O
incomplete O
labeling O
of O
positive O
samples O
is O
one O
of O
the O
bottlenecks O
of O
the O
current O
method O
, O
and O
there O
is O
still O
much O
left O
to O
be O
improved O
for O
extremely O
unlabeled O
scenarios O
and O
scenarios O
where O
the O
gap O
between O
the O
test O
set O
and O
the O
training O
set O
distribution O
is O
too O
large O
, O
which O
can O
be O
a O
direction O
for O
further O
research O
. O
However O
, O
for O
now O
, O
we O
believe O
that O
our O
task O
is O
a O
valuable O
contribution O
to O
advancing O
the O
application O
of O
document B-TaskName
- I-TaskName
level I-TaskName
RE I-TaskName
in O
more O
realistic O
scenarios O
and O
provides O
a O
robust O
baseline O
for O
this O
direction O
. O

U O
i O
( O
x O
) O
= O
π O
u O
, O
i O
p O
P O
i O
( O
x O
) O
+ O
( O
1 O
− O
π O
u O
, O
i O
) O
p O
N O
i O
( O
x O
) O
, O
( O
1 O
− O
π O
u O
, O
i O
) O
E O
N O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
can O
be O
alternatively O
expressed O
as O
E O
U O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
− O
π O
u O
, O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
. O

We O
can O
rewrite O
R O
ori O
( O
f O
) O
as O
follows O
: O

R O
ori O
( O
f O
) O
= O
K O
i=1 O
( O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
+1 O
) O
] O
+ O
( O
1 O
− O
π O
i O
) O
E O
N O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
) O
= O
K O
i=1 O
( O
π O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
+1 O
) O
] O
+ O
1 O
− O
π O
i O
1 O
− O
π O
u O
, O
i O
( O
E O
U O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
− O
π O
u O
, O
i O
E O
P O
i O
[ O
ℓ O
( O
f O
i O
, O
−1 O
) O
] O
) O
) O
= O
R O
S−PU O
( O
f O
) O
. O
( O
15 O
) O

We O
conclude O
that O
R O
ori O
( O
f O
) O
= O
R O
S−PU O
( O
f O
) O
. O

A.2 O
Proof O
of O
Theorem O
2 O

Proof O
. O
Let O
∆ O
i O
= O
P O
( O
y O
i O
= O
1 O
| O
x O
) O
be O
the O
marginal O
probability O
when O
the O
i O
- O
th O
label O
is O
positive O
. O
The O
conditional O
risk O
of O
ℓ O
SR O
is O
: O

R O
ℓ O
SR O
( O
P O
, O
f O
) O
= O
K O
i=1 O
( O
∆ O
i O
1 O
4 O
( O
( O
f O
i O
− O
f O
0 O
) O
− O
margin B-HyperparameterName
) O
2 O
+ O
( O
1 O
− O
∆ O
i O
) O
1 O
4 O
( O
− O
( O
f O
i O
− O
f O
0 O
) O
− O
margin B-HyperparameterName
) O
2 O
) O
. O

( O
16 O
) O
For O
i O
= O
1 O
, O
... O
, O
K O
, O
the O
partial O
derivative O
can O
be O
computed O
by O

∂ O
f O
i O
E O
[ O
ℓ O
SR O
( O
P O
, O
f O
) O
| O
x O
] O
= O
K O
i=1 O
( O
∆ O
i O
1 O
2 O
( O
( O
f O
i O
− O
f O
0 O
) O
− O
margin B-HyperparameterName
) O
+ O
( O
1 O
− O
∆ O
i O
) O
1 O
2 O
( O
( O
f O
0 O
− O
f O
i O
) O
− O
margin B-HyperparameterName
) O
) O
, O
( O
17 O
) O

since O
ℓ O
SR O
is O
convex O
and O
differentiable O
, O
we O
can O
obtain O
the O
optimal O
f O
* O
by O
setting O
the O
partial O
derivatives O
to O
zero O
, O
which O
leads O
to O

f O
* O
i O
− O
f O
* O
0 O
= O
2∆ O
i O
margin B-HyperparameterName
− O
margin B-HyperparameterName
, O
i O
= O
1 O
, O
... O
, O
K. O
( O
18 O
) O

When O
margin B-HyperparameterName
̸ O
= O
0 O
, O
for O
the O
optimal O
score O
function O
f O
* O
, O
f O
* O
i O
> O
f O
* O
0 O
if O
and O
only O
if O
∆ O
i O
> O
1 O
2 O
, O
which O
minimizes O
the O
ℓ O
SR O
risk O
according O
to O
Eq.14 O
. O
Therefore O
, O
ℓ O
SR O
is O
Bayes O
consistent O
w.r.t O
. O
L O
NA O
. O

A.3 O
Results O
under O
the O
Fully O
Supervised O
Setting O

The O
detailed O
results O
under O
the O
fully O
supervised O
setting O
are O
shown O
in O
Table O
9 O
. O
We O
report O
the O
mean O
and O
standard O
deviation O
on O
the O
validation O
and O
test O
set O
by O
conducting O
5 O
runs O
with O
different O
random B-HyperparameterName
seeds I-HyperparameterName
( O
62,63,64,65,66 B-HyperparameterValue
) O
. O

A.4 O
Results O
under O
the O
Extremely O
Unlabeled O
Setting O

The O
detailed O
results O
under O
the O
extremely O
unlabeled O
setting O
are O
shown O
in O
Table O
10 O
. O
We O
report O
the O
mean O
and O
standard O
deviation O
on O
the O
test O
set O
by O
conducting O
5 O
runs O
with O
different O
random B-HyperparameterName
seeds I-HyperparameterName
( O
62,63,64,65,66 B-HyperparameterValue
) O
. O

A.5 O
Details O
of O
Other O
Loss O
Functions O

We O
first O
show O
the O
convex O
loss O
function O
log O
- O
sigmoid O
loss O
, O
which O
is O
commonly O
used O
in O
classification O
task O
: O

ℓ O
LS O
( O
f O
i O
, O
y O
i O
) O
= O
−log O
( O
σ O
( O
y O
i O
f O
i O
) O
) O
, O
( O
19 O
) O

where O
σ O
( O
x O
) O
is O
the O
sigmoid O
function O
. O
Since O
log O
- O
sigmoid O
loss O
is O
convex O
and O
differentiable O
, O
we O
can O
obtain O
its O
none O
- O
class O
ranking O
form O
. O
Log O
- O
sigmoid O
ranking O
loss O
: O

ℓ O
LSR O
( O
f O
i O
, O
y O
i O
) O
= O
−log O
( O
σ O
( O
y O
i O
( O
f O
i O
− O
f O
0 O
) O
) O
) O
. O
( O
20 O
) O

This O
ranking O
loss O
function O
remain O
Bayesian O
consistent O
with O
L O
NA O
( O
Eq.12 O
) O
. O

A.6 O
Sensitivity O
to O
Hyper O
- O
Parameter O
margin B-HyperparameterName

As O
shown O
in O
Table O
11 O
, O
the O
model O
fail O
to O
train O
when O
margin B-HyperparameterName
= O
0 B-HyperparameterValue
, O
and O
the O
model O
is O
insensitive O
to O
margin B-HyperparameterName
when O
margin B-HyperparameterName
̸ O
= O
0 B-HyperparameterValue
. O
This O
is O
consistent O
with O
our O
proof O
. O

A.7 O
Influence O
of O
Prior O
Estimation O

As O
shown O
in O
Table O
12 O
, O
the O
experimental O
results O
with O
different O
π O
i O
show O
that O
our O
method O
is O
insensitive O
to O
the O
estimation O
of O
π O
i O
. O
Smaller O
estimates O
of O
π O
i O
lead O
to O
higher O
precision B-MetricName
rates O
as O
well O
as O
lower O
recall B-MetricName
rates O
, O
while O
the O
opposite O
is O
true O
for O
higher O
estimates O
of O
π O
i O
. O

Acknowledgements O

We O
sincerely O
thank O
all O
anonymous O
reviewers O
for O
their O
valuable O
comments O
to O
improve O
our O
work O
. O
This O
research O
is O
funded O
by O
the O
Basic O
Research O
Project O
of O
Shanghai O
Science O
and O
Technology O
Commission O
( O
No.19JC1410101 O
) O
. O
The O
computation O
is O
supported O
by O
ECNU O
Multifunctional O
Platform O
for O
Innovation O
( O
001 O
) O
. O

Visual B-TaskName
Commonsense I-TaskName
in O
Pretrained O
Unimodal O
and O
Multimodal O
Models O

Our O
commonsense O
knowledge O
about O
objects O
includes O
their O
typical O
visual O
attributes O
; O
we O
know O
that O
bananas O
are O
typically O
yellow O
or O
green O
, O
and O
not O
purple O
. O
Text O
and O
image O
corpora O
, O
being O
subject O
to O
reporting O
bias O
, O
represent O
this O
worldknowledge O
to O
varying O
degrees O
of O
faithfulness O
. O
In O
this O
paper O
, O
we O
investigate O
to O
what O
degree O
unimodal O
( O
language O
- O
only O
) O
and O
multimodal O
( O
image O
and O
language O
) O
models O
capture O
a O
broad O
range O
of O
visually O
salient O
attributes O
. O
To O
that O
end O
, O
we O
create O
the O
Visual B-DatasetName
Commonsense I-DatasetName
Tests I-DatasetName
( O
ViComTe B-DatasetName
) O
dataset O
covering O
5 O
property O
types O
( O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
visual O
co O
- O
occurrence O
) O
for O
over O
5000 O
subjects O
. O
We O
validate O
this O
dataset O
by O
showing O
that O
our O
grounded O
color O
data O
correlates O
much O
better O
than O
ungrounded O
text O
- O
only O
data O
with O
crowdsourced O
color O
judgments O
provided O
by O
Paik O
et O
al O
. O
( O
2021 O
) O
. O
We O
then O
use O
our O
dataset O
to O
evaluate O
pretrained O
unimodal O
models O
and O
multimodal O
models O
. O
Our O
results O
indicate O
that O
multimodal O
models O
better O
reconstruct O
attribute O
distributions O
, O
but O
are O
still O
subject O
to O
reporting O
bias O
. O
Moreover O
, O
increasing O
model O
size O
does O
not O
enhance O
performance O
, O
suggesting O
that O
the O
key O
to O
visual O
commonsense O
lies O
in O
the O
data O
. O
1 O

Introduction O

The O
observation O
that O
human O
language O
understanding O
happens O
in O
a O
rich O
multimodal O
environment O
has O
led O
to O
an O
increased O
focus O
on O
visual O
grounding O
in O
natural O
language O
processing O
( O
NLP O
) O
( O
Baltrusaitis O
et O
al O
. O
, O
2019 O
; O
Bisk O
et O
al O
. O
, O
2020 O
) O
, O
driving O
comparisons O
between O
traditional O
unimodal O
text O
- O
only O
models O
and O
multimodal O
models O
which O
take O
both O
text O
and O
image O
inputs O
. O
In O
this O
work O
, O
we O
explore O
to O
what O
extent O
unimodal O
and O
multimodal O
models O
are O
able O
to O
capture O
commonsense O
visual O
concepts O
across O
five O
types O
of O
relations O
: O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
visual O
cooccurrence O
( O
cf O
. O
Fig O
. O
1 O
) O
. O
We O
further O
explore O
how O
this O
ability O
is O
influenced O
by O
reporting O
bias O
( O
Gordon O
and O
Van O
Durme O
, O
2013 O
) O
, O
the O
tendency O
of O
large O
corpora O
to O
over O
- O
or O
under O
- O
report O
events O
. O
We O
define O
visual O
commonsense O
as O
knowledge O
about O
generic O
visual O
concepts O
, O
e.g. O
" O
knobs O
are O
usually O
round O
" O
, O
and O
we O
measure O
this O
knowledge O
via O
frequency O
distributions O
over O
potential O
properties O
( O
e.g. O
round O
, O
square O
, O
etc O
) O
. O
A O
visually O
- O
informed O
language O
model O
should O
be O
able O
to O
capture O
such O
properties O
. O
Our O
color O
, O
shape O
, O
material O
, O
and O
co O
- O
occurrence O
data O
are O
mined O
from O
Visual B-DatasetName
Genome I-DatasetName
( O
Krishna O
et O
al O
. O
, O
2016 O
) O
, O
and O
our O
size O
data O
are O
created O
from O
object O
lists O
. O
They O
contain O
a O
large O
number O
of O
examples O
of O
per O
- O
object O
attribute O
distributions O
and O
" O
object O
- O
attribute O
" O
pairs O
. O
Paik O
et O
al O
. O
( O
2021 O
) O
evaluate O
language O
models O
' O
color O
perception O
using O
a O
human B-DatasetName
- I-DatasetName
annotated I-DatasetName
color I-DatasetName
dataset I-DatasetName
( O
CoDa B-DatasetName
) O
, O
finding O
that O
reporting O
bias O
negatively O
influences O
model O
performance O
and O
that O
multimodal O
training O
can O
mitigate O
those O
effects O
. O
In O
this O
work O
, O
we O
confirm O
those O
findings O
while O
extending O
the O
evaluation O
to O
a O
broader O
range O
of O
visually O
salient O
properties O
, O
resulting O
in O
a O
more O
comprehensive O
metric O
for O
visual O
commonsense O
. O
In O
order O
to O
elicit O
visual O
commonsense O
from O
language O
models O
, O
we O
utilize O
soft O
prompt O
tuning O
( O
Qin O
and O
Eisner O
, O
2021 O
) O
, O
which O
trains O
optimal O
templates O
by O
gradient O
descent O
for O
each O
model O
and O
relation O
type O
that O
we O
explore O
. O
We O
also O
utilize O
knowledge O
distillation O
to O
enhance O
a O
textonly O
model O
's O
visual O
commonsense O
ability O
, O
where O
the O
vision O
- O
language O
model O
serves O
as O
the O
teacher O
. O

The O
major O
contributions O
of O
this O
work O
are O
: O
( O
1 O
) O
we O
design O
a O
comprehensive O
analytic O
dataset O
, O
Vi B-DatasetName
- I-DatasetName
ComTe I-DatasetName
, O
for O
probing O
English O
visual O
commonsense O
, O
that O
is O
applicable O
to O
any O
language O
model O
; O
( O
2 O
) O
we O
use O
ViComTe B-DatasetName
to O
study O
models O
' O
ability O
to O
capture O
empirical O
distributions O
of O
visually O
salient O
properties O
. O
We O
examine O
unimodal O
language O
models O
, O
multimodal O
vision O
- O
language O
( O
VL O
) O
models O
, O
and O
a O
knowledgedistilled O
version O
of O
a O
VL O
model O
; O
and O
( O
3 O
) O
we O
analyze O
the O
effects O
of O
reporting O
bias O
on O
the O
visuallygrounded O
vs. O
ungrounded O
datasets O
and O
models O
. O

Does O
the O
model O
know O
… O

It O
is O
larger O
than O
: O
It O
is O
smaller O
than O
: O

Unimodal O
Multimodal O

BERT B-MethodName
, O
… O
Oscar B-MethodName
, O
… O

A O
girl O
is O
looking O
at O
the O
penguin O
. O

Penguins O
are O
a O
group O
of O
aquatic O
flightless O
birds O
. O

The O
word O
penguin O
first O
appears O
in O
the O
16th O
century O
as O
a O
name O
for O
the O
great O
auk O
. O
what O
is O
the O
color O
of O
a O
penguin O
? O

Figure O
1 O
: O
We O
compare O
unimodal O
and O
multimodal O
models O
' O
abilities O
to O
capture O
visual O
commonsense O
knowledge O
. O
The O
commonsense O
knowledge O
is O
evaluated O
on O
five O
relation O
types O
: O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
visual O
co O
- O
occurrence O
. O

We O
compare O
the O
model O
outputs O
with O
the O
gold O
distribution O
from O
ViComTe B-DatasetName
, O
which O
is O
mined O
from O
Visual B-DatasetName
Genome I-DatasetName
. O

2 O
Related O
Work O

Vision O
- O
Language O
Modeling O

Recent O
advances O
in O
vision O
- O
language O
( O
VL O
) O
modeling O
have O
led O
to O
increased O
success O
on O
benchmark O
tasks O
. O
Most O
VL O
models O
learn O
joint O
image O
and O
text O
representations O
from O
cross O
- O
modal O
training O
of O
transformers O
with O
self O
- O
attention O
, O
including O
LXMERT B-MethodName
( O
Tan O
and O
Bansal O
, O
2019 O
) O
, O
ViLBERT B-MethodName
( O
Lu O
et O
al O
. O
, O
2019 O
) O
, O
VisualBERT B-MethodName
( O
Li O
et O
al O
. O
, O
2019 O
) O
, O
UNITER B-MethodName
, O
etc O
. O
Oscar B-MethodName
additionally O
uses O
object O
tags O
in O
images O
as O
anchor O
points O
to O
facilitate O
the O
learning O
of O
image O
- O
text O
alignments O
and O
VinVL B-MethodName
presents O
an O
improved O
object O
detection O
model O
. O
CLIP B-MethodName
( O
Radford O
et O
al O
. O
, O
2021 O
) O
learns O
by O
predicting O
caption O
- O
image O
alignment O
from O
a O
large O
internet O
corpus O
of O
( O
image O
, O
text O
) O
pairs O
. O
While O
our O
work O
uses O
textual O
prompt O
tuning O
techniques O
, O
there O
have O
also O
been O
work O
on O
visual O
prompt O
engineering O
to O
enhance O
the O
performance O
of O
pretrained O
vision O
- O
language O
models O
. O
Zhou O
et O
al O
. O
( O
2021 O
) O
model O
context O
in O
prompts O
as O
continuous O
representations O
and O
learn O
to O
optimize O
that O
context O
. O
Yao O
et O
al O
. O
( O
2021 O
) O
develop O
a O
cross O
- O
modal O
prompt O
tuning O
framework O
that O
reformulates O
visual O
grounding O
as O
a O
fill O
- O
in O
- O
the O
- O
blank O
problem O
for O
both O
image O
and O
text O
. O

Visual O
Commonsense O

In O
one O
of O
the O
early O
attempts O
at O
learning O
visual O
commonsense O
, O
Vedantam O
et O
al O
. O
( O
2015 O
) O
measure O
the O
plausibility O
of O
a O
commonsense O
assertion O
in O
the O
form O
of O
( O
obj1 O
, O
relation O
, O
obj2 O
) O
based O
on O
its O
similarity O
to O
known O
plausible O
assertions O
, O
using O
both O
visual O
scenes O
and O
accompanying O
text O
. O
Zellers O
et O
al O
. O
( O
2021 O
) O
learn O
physical O
commonsense O
via O
interaction O
, O
and O
use O
this O
knowledge O
to O
ground O
language O
. O
Frank O
et O
al O
. O
( O
2021 O
) O
probe O
whether O
VL O
models O
have O
learned O
to O
construct O
cross O
- O
modal O
representations O
from O
both O
modalities O
via O
cross O
- O
modal O
input O
ablation O
. O

Note O
that O
our O
definition O
of O
visual O
commonsense O
differs O
from O
that O
of O
Zellers O
et O
al O
. O
( O
2019 O
) O
, O
where O
the O
model O
is O
required O
to O
perform O
commonsense O
reasoning O
based O
on O
an O
image O
. O
Our O
definition O
of O
visual O
commonsense O
is O
more O
similar O
to O
the O
idea O
of O
stereotypic O
tacit O
assumptions O
( O
Prince O
, O
1978 O
) O
the O
propositional O
beliefs O
that O
humans O
hold O
about O
generic O
concepts O
, O
such O
as O
" O
dogs O
have O
to O
be O
walked O
" O
. O
Weir O
et O
al O
. O
( O
2020 O
) O
probe O
neural O
language O
models O
for O
such O
human O
tacit O
assumptions O
and O
demonstrate O
the O
models O
' O
success O
. O
We O
extend O
this O
intuition O
to O
visual O
concepts O
and O
explore O
how O
visual O
information O
may O
help O
language O
models O
to O
capture O
such O
assumptions O
. O

There O
has O
also O
been O
earlier O
work O
on O
the O
McRae O
feature O
norms O
( O
McRae O
et O
al O
. O
, O
2005 O
) O
, O
in O
which O
human O
annotators O
wrote O
down O
attributes O
that O
describe O
the O
meaning O
of O
words O
. O
For O
instance O
, O
" O
car O
" O
can O
be O
labeled O
as O
" O
has O
four O
wheels O
" O
and O
" O
apple O
" O
can O
be O
labeled O
as O
" O
is O
green O
" O
. O
Silberer O
et O
al O
. O
( O
2013 O
) O
expand O
the O
McRae B-DatasetName
dataset O
into O
a O
set O
of O
images O
and O
their O
visual O
attributes O
and O
construct O
visually O
grounded O
distributional O
models O
that O
can O
represent O
image O
features O
with O
visual O
attributes O
. O
Zhu O
et O
al O
. O
( O
2020 O
) O
examine O
the O
" O
language O
prior O
" O
problem O
in O
Visual O
Question O
Answering O
models O
, O
where O
models O
tend O
to O
answer O
based O
on O
word O
frequencies O
in O
the O
data O
, O
ignoring O
the O
image O
contents O
. O
In O
this O
work O
, O
we O
explore O
to O
what O
extent O
such O
a O
language O
prior O
is O
recruited O
absent O
a O
visual O
input O
. O

Reporting O
Bias O

Pretrained O
language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
are O
trained O
on O
billions O
of O
tokens O
of O
text O
, O
capturing O
statistical O
regularities O
present O
in O
the O
training O
corpora O
. O
However O
, O
their O
textual O
training O
data O
can O
suffer O
from O
reporting O
bias O
, O
where O
the O
frequency O
distribution O
of O
specific O
events O
and O
properties O
in O
text O
may O
not O
reflect O
the O
real O
- O
world O
distribution O
of O
such O
properties O
( O
Gordon O
and O
Van O
Durme O
, O
2013 O
) O
. O
For O
example O
, O
while O
grass O
is O
typically O
green O
, O
this O
may O
be O
under O
- O
reported O
in O
web O
corpora O
( O
as O
it O
is O
assumed O
to O
be O
true O
) O
, O
and O
while O
motorcycle O
crashes O
may O
be O
more O
common O
in O
the O
real O
world O
, O
plane O
crashes O
are O
mentioned O
far O
more O
in O
news O
text O
( O
Gordon O
and O
Van O
Durme O
, O
2013 O
) O
. O
Misra O
et O
al O
. O
( O
2016 O
) O
highlight O
the O
reporting O
bias O
in O
" O
human O
- O
centric O
" O
image O
annotations O
and O
find O
that O
the O
noise O
in O
annotations O
exhibits O
a O
structure O
that O
can O
be O
modeled O
. O

3 O
Dataset O
: O
ViComTe B-DatasetName

Dataset O
Mining O

For O
each O
relation O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
object O
co O
- O
occurrence O
, O
our O
data O
take O
the O
form O
of O
( O
subject O
, O
object O
) O
tuples O
extracted O
from O
object O
distributions O
per O
subject O
. O
The O
goal O
is O
to O
predict O
the O
object O
and O
its O
distribution O
from O
the O
subject O
and O
relation O
. O
Table O
1 O
summarizes O
the O
number O
of O
classes O
and O
subject O
- O
object O
pairs O
for O
each O
relation O
. O
2 O
Color O
, O
Shape O
, O
Material O
For O
color O
, O
shape O
, O
and O
material O
, O
the O
subject O
is O
a O
noun O
and O
the O
object O
is O
the O
color O
, O
shape O
, O
or O
material O
property O
of O
the O
noun O
, O
mined O
from O
attributes O
of O
Visual B-DatasetName
Genome I-DatasetName
( O
VG B-DatasetName
) O
( O
Krishna O
et O
al O
. O
, O
2016 O
) O
. O
3 O
We O
manually O
create O
a O
list O
of O
single O
- O
word O
attributes O
for O
each O
relation O
, O
and O
only O
VG B-DatasetName
subjects O
that O
are O
matched O
with O
a O
specific O
attribute O
for O
more O
than O
a O
threshold O
number O
of O
times O
are O
recorded O
, O
in O
order O
to O
avoid O
noise O
in O
the O
dataset O
. O
The O
thresholds O
for O
color O
, O
material O
, O
and O
shape O
are O
5 O
, O
2 O
, O
and O
1 O
, O
respectively O
, O
chosen O
based O
on O
the O
availability O
of O
attributes O
of O
each O
relation O
in O
VG B-DatasetName
. O
VG B-DatasetName
attributes O
are O
filtered O
with O
the O
following O
steps O
: O
( O
1 O
) O
attribute O
" O
Y O
colored O
/ O
made O
/ O
shaped O
" O
is O
treated O
as O
" O
Y O
" O
; O
( O
2 O
) O
select O
only O
the O
last O
word O
for O
compound O
attributes O
( O
e.g. O
treat O
" O
forest O
green O
" O
as O
" O
green O
" O
) O
; O
( O
3 O
) O
similar O
attributes O
are O
merged O
into O
a O
main O
attribute O
class O
( O
e.g. O
" O
maroon O
" O
and O
" O
crimson O
" O
become O
" O
red O
" O
) O
. O

The O
above O
procedure O
produces O
a O
distribution O
over O
the O
set O
of O
attributes O
for O
each O
subject O
noun O
. O
From O
that O
distribution O
, O
a O
( O
subject O
, O
object O
) O
data O
instance O
is O
generated O
for O
each O
subject O
where O
the O
object O
is O
the O
attribute O
that O
associates O
with O
it O
the O
most O
. O
See O
the O
first O
three O
rows O
of O
Table O
1 O
for O
examples O
. O

Size O
Size O
is O
separated O
into O
size_smaller O
and O
size_larger O
, O
where O
the O
subject O
is O
a O
noun O
and O
the O
object O
is O
another O
noun O
that O
is O
smaller O
or O
larger O
, O
respectively O
, O
than O
the O
subject O
. O
To O
form O
the O
size O
dataset O
, O
we O
obtain O
a O
set O
of O
concrete O
nouns O
that O
appears O
in O
VG B-DatasetName
, O
which O
we O
manually O
classify O
into O
5 O
size O
categories O
( O
tiny O
, O
small O
, O
medium O
, O
large O
, O
and O
huge O
) O
. O
Typical O
objects O
in O
each O
category O
includes O
pill O
, O
book O
, O
table O
, O
lion O
, O
mountain O
, O
respectively O
. O
We O
randomly O
pick O
two O
nouns O
from O
different O
categories O
to O
form O
a O
( O
subject O
, O
object O
) O
pair O
. O

Visual O
Co O
- O
occurrence O
The O
visual O
co O
- O
occurrence O
dataset O
is O
generated O
in O
a O
similar O
way O
to O
the O
color O
, O
shape O
, O
and O
material O
datasets O
. O
Co O
- O
occurrence O
distribution O
is O
extracted O
from O
Visual B-DatasetName
Genome I-DatasetName
where O
two O
objects O
that O
occur O
in O
the O
same O
scene O
graph O
together O
for O
more O
than O
8 O
times O
are O
recorded O
, O
and O
a O
( O
subject O
, O
object O
) O
instance O
is O
generated O
for O
each O
subject O
, O
where O
the O
object O
is O
the O
noun O
that O
co O
- O
occurs O
with O
the O
subject O
the O
most O
. O

Data O
Grouping O

Following O
Paik O
et O
al O
. O
( O
2021 O
) O
, O
we O
split O
the O
color O
, O
shape O
, O
and O
material O
datasets O
each O
into O
three O
groups O
: O
SINGLE B-DatasetName
, O
MULTI B-DatasetName
, O
and O
ANY B-DatasetName
. O
The O
SINGLE B-DatasetName
group O
is O
for O
subjects O
whose O
most O
common O
attribute O
covers O
more O
than O
80 O
% O
of O
the O
probability O
, O
e.g. O
, O
the O
color O
of O
snow O
is O
almost O
always O
white O
. O
The O
MULTI B-DatasetName
group O
is O
defined O
as O
subjects O
not O
in O
the O
SINGLE B-DatasetName
group O
where O
more O
than O
90 O
% O
of O
the O
probability O
falls O
in O
the O
top O
4 O
attribute O
classes O
, O
e.g. O
, O
the O
color O
of O
a O
penguin O
in O
Fig O
. O
1 O
. O
The O
rest O
of O
the O
subjects O
are O
in O
the O
ANY B-DatasetName
group O
. O
Lower O
model O
performance O
for O
the O
SINGLE B-DatasetName
group O
would O
indicate O
the O
influence O
of O
reporting O
bias O
. O
For O
example O
, O
if O
the O
model O
is O
unable O
to O
correctly O
capture O
the O
distribution O
of O
the O
color O
of O
snow O
, O
it O
is O
likely O
because O
the O
color O
of O
snow O
has O
low O
probability O
of O
being O
reported O
in O
the O
training O
corpus O
, O
as O
people O
know O
it O
is O
white O
by O
default O
. O

Templates O

In O
order O
to O
elicit O
model O
response O
and O
extract O
target O
objects O
and O
distributions O
from O
text O
, O
we O
manually O
design O
a O
set O
of O
templates O
for O
each O
relation O
. O
There O
are O
7 O
templates O
for O
color O
, O
shape O
, O
and O
material O
each O
, O
8 O
for O
size O
, O
and O
4 O
for O
visual O
co O
- O
occurrence O
. O
See O
Table O
1 O
for O
example O
templates O
. O

Wikipedia B-DatasetName
Data O

In O
order O
to O
compare O
text O
- O
based O
and O
visuallygrounded O
data O
, O
we O
mine O
the O
color O
, O
shape O
, O
and O
material O
datasets O
from O
Wikipedia O
data O
, O
which O
is O
typically O
used O
in O
model O
pretraining O
. O
To O
mine O
these O
text O
- O
based O
datasets O
, O
we O
combine O
the O
sets O
of O
subjects O
in O
VG B-DatasetName
, O
take O
the O
manual O
list O
of O
attributes O
as O
objects O
again O
, O
and O
extract O
( O
subject O
, O
object O
) O
pairs O
if O
the O
pair O
matches O
any O
of O
the O
pre O
- O
defined O
templates O
. O
In O
Section O
3.5 O
we O
will O
show O
the O
advantages O
of O
the O
VG B-DatasetName
- O
mined O
dataset O
over O
this O
text O
- O
based O
dataset O
. O

Dataset O
Evaluation O

To O
ensure O
the O
validity O
of O
ViComTe B-DatasetName
, O
we O
compare O
our O
color O
dataset O
with O
the O
human O
- O
annotated O
CoDa B-DatasetName
dataset O
( O
Paik O
et O
al O
. O
, O
2021 O
) O
, O
which O
we O
assume O
is O
close O
to O
real O
- O
world O
color O
distributions O
and O
has O
minimal O
reporting O
bias O
. O
We O
see O
a O
reasonably O
strong O
correlation O
with O
CoDa B-DatasetName
, O
indicating O
that O
the O
ViComTe B-DatasetName
dataset O
is O
a O
good O
and O
cost O
- O
effective O
approximation O
to O
human O
annotations O
. O

Metrics O
We O
report O
the O
Spearman B-MetricName
's I-MetricName
rank I-MetricName
- I-MetricName
order I-MetricName
correlation I-MetricName
between O
the O
two O
distributions O
in O
comparison O
, O
averaged O
across O
all O
subjects O
. O
The O
Spearman O
correlation O
is O
used O
instead O
of O
the O
Pearson O
correlation O
since O
for O
our O
purpose O
the O
rank O
of O
the O
object O
distributions O
is O
more O
important O
than O
the O
exact O
values O
, O
which O
may O
change O
due O
to O
data O
variability O
. O
The O
top-1 B-MetricName
accuracy I-MetricName
( O
Acc B-MetricName
@ I-MetricName
1 I-MetricName
) O
is O
the O
percentage O
of O
the O
objects O
with O
the O
highest O
probability O
in O
the O
source O
distributions O
matching O
those O
in O
the O
target O
distributions O
. O
These O
two O
metrics O
are O
also O
used O
in O
later O
sections O
when O
evaluating O
model O
distributions O
. O

Analysis O
Table O
2 O
shows O
the O
detailed O
results O
of O
the O
evaluation O
of O
the O
ViComTe B-DatasetName
and O
Wikipedia B-DatasetName
color I-DatasetName
datasets O
by O
comparing O
with O
the O
human O
- O
annotated O
dataset O
, O
CoDa B-DatasetName
. O
We O
can O
see O
that O
ViComTe B-DatasetName
has O
much O
higher O
Spearman B-MetricName
correlation I-MetricName
with O
CoDa B-DatasetName
, O
as O
well O
as O
substantially O
higher O
top-1 B-MetricName
accuracy I-MetricName
for O
the O
SINGLE B-DatasetName
group O
. O
The O
correlation O
is O
expected O
to O
be O
low O
for O
the O
ANY B-DatasetName
group O
, O
because O
objects O
in O
the O
ANY O
group O
can O
have O
many O
possible O
colors O
. O

Reporting O
bias O
is O
present O
in O
both O
datasets O
, O
as O
the O
average O
number O
of O
occurrences O
of O
SINGLE B-DatasetName
group O
subjects O
are O
much O
fewer O
than O
that O
of O
the O
MULTI B-DatasetName
and O
ANY B-DatasetName
group O
subjects O
. O
Counter O
- O
intuitively O
, O
for O
ViComTe B-DatasetName
, O
the O
highly O
- O
correlated O
SINGLE B-DatasetName
group O
subjects O
have O
fewer O
average O
occurrences O
than O
the O
ones O
with O
low O
correlations O
. O
This O
is O
contrary O
to O
our O
expectation O
that O
more O
frequent O
objects O
would O
better O
reflect O
the O
human O
- O
perceived O
distribution O
and O
can O
be O
explained O
by O
SINGLE B-DatasetName
subjects O
being O
easier O
to O
represent O
even O
without O
a O
large O
amount O
of O
data O
. O

One O
example O
where O
the O
Wikipedia O
distribution O
diverges O
from O
the O
CoDa B-DatasetName
distribution O
is O
" O
penguin O
" O
, O
whose O
most O
likely O
color O
in O
CoDa B-DatasetName
is O
black O
, O
followed O
by O
white O
and O
gray O
; O
however O
, O
its O
top O
color O
in O
Wikipedia O
is O
blue O
, O
because O
" O
blue O
penguin O
" O
is O
a O
specific O
species O
with O
an O
entry O
in O
Wikipedia O
, O
even O
if O
it O
is O
not O
as O
common O
as O
black O
and O
white O
penguins O
. O
One O
example O
where O
the O
VG B-DatasetName
distributions O
diverge O
from O
CoDa B-DatasetName
is O
" O
mouse O
" O
, O
because O
in O
VG B-DatasetName
, O
most O
occurrences O
of O
" O
mouse O
" O
are O
computer O
mice O
, O
which O
are O
most O
commonly O
black O
, O
whereas O
when O
asked O
about O
the O
word O
" O
mouse O
" O
, O
human O
annotators O
typically O
think O
about O
the O
animal O
, O
so O
that O
the O
most O
likely O
colors O
in O
CoDa B-DatasetName
are O
white O
and O
gray O
. O
4 O

Dataset O
splits O

Each O
of O
the O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
cooccurrence O
datasets O
is O
split O
into O
80 O
% O
training O
data O
and O
20 O
% O
test O
data O
. O
All O
evaluation O
metrics O
are O
reported O
on O
the O
test O
set O
. O
The O
training O
set O
is O
used O
for O
the O
logistic O
regression O
and O
the O
soft O
prompt O
tuning O
algorithm O
( O
Section O
4.2 O
) O
. O

Probing O
Visual B-TaskName
Commonsense I-TaskName

Models O

We O
examine O
7 O
pretrained O
transformer O
- O
based O
models O
and O
2 O
variations O
of O
them O
, O
trained O
on O
a O
variety O
of O
data O
. O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
are O
trained O
on O
text O
only O
using O
a O
masked O
language O
modeling O
objective O
( O
MLM O
) O
. O
Oscar B-MethodName
) O
is O
a O
vision O
- O
language O
model O
based O
on O
the O
BERT O
architecture O
, O
trained O
with O
an O
combined O
MLM O
and O
contrastive O
loss O
on O
text O
- O
image O
pairs O
. O
VisualBERT B-MethodName
( O
Li O
et O
al O
. O
, O
2019 O
) O
is O
another O
vision O
- O
language O
model O
based O
on O
BERT O
that O
learns O
joint O
representation O
of O
images O
and O
text O
. O
Tan O
and O
Bansal O
( O
2020 O
) O
introduce O
the O
" O
vokenization O
" O
method O
, O
which O
aligns O
language O
tokens O
to O
their O
related O
images O
, O
mitigating O
the O
shortcomings O
of O
models O
trained O
on O
visually O
- O
grounded O
datasets O
in O
text O
- O
only O
tasks O
. O
Since O
our O
task O
is O
purely O
text O
- O
based O
, O
we O
also O
experiment O
with O
a O
pretrained O
vokenization O
model O
( O
BERT B-MethodName
+ I-MethodName
VLM I-MethodName
on I-MethodName
Wiki I-MethodName
) O
. O
Finally O
, O
we O
use O
representations O
from O
CLIP B-MethodName
( O
ViT B-MethodName
- I-MethodName
B I-MethodName
/ I-MethodName
32 I-MethodName
) O
( O
Radford O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
trained O
with O
a O
contrastive O
image O
- O
caption O
matching O
loss O
. O

Distilled B-MethodName
Oscar I-MethodName

As O
our O
experiments O
involve O
exclusively O
textual O
inputs O
, O
we O
develop O
a O
knowledgedistilled O
version O
of O
Oscar B-MethodName
( I-MethodName
" I-MethodName
Distilled I-MethodName
" I-MethodName
) I-MethodName
which O
corrects O
for O
the O
lack O
of O
image O
input O
in O
our O
task O
. O
Knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
; O
Sanh O
et O
al O
. O
, O
2019 O
) O
is O
the O
process O
of O
transferring O
knowledge O
from O
one O
model O
to O
another O
, O
where O
the O
student O
model O
is O
trained O
to O
produce O
the O
output O
of O
the O
teacher O
model O
. O
Here O
, O
we O
use O
Oscar B-MethodName
as O
the O
teacher O
and O
BERT B-MethodName
as O
the O
student O
. O
The O
training O
data O
is O
part O
of O
the O
Oscar O
pretraining O
corpus O
: O
COCO B-DatasetName
( O
Lin O
et O
al O
. O
, O
2014 O
) O
, O
Flickr30k B-DatasetName
( O
Young O
et O
al O
. O
, O
2014 O
) O
, O
and O
GQA B-DatasetName
( O
Hudson O
and O
Manning O
, O
2019 O
) O
, O
and O
the O
Distilled B-MethodName
Oscar I-MethodName
model O
has O
access O
to O
the O
text O
data O
only O
. O
We O
use O
the O
Kullback O
- O
Leibler O
loss O
to O
measure O
the O
divergence O
between O
the O
output O
logits O
of O
BERT B-MethodName
and O
Oscar B-MethodName
, O
and O
optimize O
the O
pretrained O
BERT B-MethodName
on O
that O
loss O
to O
match O
the O
outputs O
of O
Oscar B-MethodName
. O
Configurable O
parameters O
are O
set O
the O
same O
as O
for O
Oscar B-MethodName
pretraining O
. O

CaptionBERT B-MethodName
Since O
VL O
models O
are O
trained O
largely O
on O
caption O
data O
, O
it O
could O
be O
that O
the O
differences O
between O
a O
text O
- O
only O
model O
and O
a O
VL O
model O
come O
not O
from O
a O
difference O
in O
modalities O
-text O
vs. O
images O
and O
text O
-but O
from O
a O
difference O
in O
domainwebtext O
vs. O
image O
captions O
. O
In O
order O
to O
disentangle O
the O
effects O
of O
the O
domain O
difference O
from O
those O
of O
visual O
inputs O
, O
we O
train O
a O
BERT B-MethodName
model O
from O
scratch O
( O
" O
CaptionBERT B-MethodName
" O
) O
on O
Oscar B-MethodName
's O
caption O
- O
based O
text O
data O
( O
the O
same O
data O
as O
for O
the O
Distilled O
model O
) O
. O
If O
CaptionBERT B-MethodName
, O
which O
does O
not O
have O
exposure O
to O
visual O
inputs O
, O
performs O
better O
than O
BERT B-MethodName
and O
similarly O
to O
VL O
models O
( O
which O
are O
trained O
with O
visual O
inputs O
) O
, O
it O
would O
suggest O
that O
the O
training O
domain O
matters O
more O
than O
the O
modality O
. O
If O
, O
on O
the O
other O
hand O
, O
CaptionBERT B-MethodName
performs O
worse O
than O
VL O
models O
, O
it O
would O
highlight O
the O
importance O
of O
modality O
. O

Evaluation O
Methods O

We O
compare O
the O
visual O
commonsense O
abilities O
of O
pretrained O
unimodal O
and O
multimodal O
models O
. O
Given O
a O
list O
of O
prompts O
and O
a O
subject O
word O
, O
each O
model O
outputs O
the O
distribution O
of O
the O
target O
word O
. O
Following O
Paik O
et O
al O
. O
( O
2021 O
) O
, O
we O
apply O
zero O
- O
shot O
probes O
to O
models O
that O
are O
trained O
on O
a O
language O
modeling O
objective O
, O
and O
conduct O
representation O
probes O
for O
those O
that O
are O
not O
. O
We O
report O
the O
prediction O
accuracy O
and O
the O
Spearman O
correlation O
of O
the O
output O
distribution O
with O
the O
true O
distribution O
. O

We O
use O
models O
trained O
with O
an O
MLM O
objective O
( O
BERT O
, O
Distilled O
, O
etc O
) O
directly O
for O
zero O
- O
shot O
predic O
- O
tion O
of O
masked O
tokens O
. O
5 O
For O
Oscar B-MethodName
we O
add O
a O
wordprediction O
head O
on O
top O
of O
it O
. O
The O
results O
across O
templates O
are O
aggregated O
in O
two O
modes O
. O
In O
the O
" O
best O
template O
" O
mode O
, O
for O
each O
example O
, O
the O
highest O
Spearman B-MetricName
correlation I-MetricName
among O
all O
templates O
is O
reported O
, O
and O
the O
top-1 B-MetricName
result O
is O
regarded O
as O
correct O
if O
the O
true O
target O
object O
is O
the O
same O
as O
the O
top-1 B-MetricName
result O
of O
any O
of O
the O
templates O
. O
In O
the O
" O
average O
template O
" O
mode O
, O
the O
output O
distribution O
is O
the O
mean O
of O
the O
distributions O
across O
all O
templates O
. O

Since O
CLIP B-MethodName
is O
not O
trained O
on O
a O
token O
- O
prediction O
objective O
, O
we O
implement O
logistic O
regression O
on O
top O
of O
the O
frozen O
encoder O
output O
, O
to O
predict O
the O
target O
attribute O
or O
object O
. O
The O
input O
is O
each O
of O
the O
templates O
with O
the O
subject O
[ O
X O
] O
filled O
with O
an O
input O
in O
the O
dataset O
. O
Like O
Paik O
et O
al O
. O
( O
2021 O
) O
, O
to O
give O
the O
model O
ample O
chance O
of O
success O
, O
we O
take O
the O
template O
that O
results O
in O
the O
best O
test O
accuracy O
score O
, O
report O
that O
accuracy O
and O
the O
Spearman B-MetricName
correlation I-MetricName
associated O
with O
that O
template O
. O
For O
the O
classification O
head O
, O
we O
use O
the O
Scikit O
- O
Learn O
implementation O
of O
Logistic O
Regression O
( O
random_state=0 O
, O
C=0.316 O
, O
max_iter=2000 O
) O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
. O

Soft O
prompt O
tuning O
In O
order O
to O
overcome O
the O
limitation O
of O
self O
- O
designed O
prompts O
, O
we O
incorporate O
prompt O
tuning O
technique O
that O
learns O
soft O
prompts O
by O
gradient O
descent O
, O
from O
Qin O
and O
Eisner O
( O
2021 O
) O
. O
6 O
The O
algorithm O
minimizes O
the O
log O
loss O
: O

( O
x O
, O
y O
) O
∈Er O
− O
log O
t∈Tr O
p O
( O
y|t O
, O
x O
) O

for O
a O
set O
of O
example O
pairs O
E O
r O
and O
template O
set O
T O
r O
. O

Size O
Evaluation O

The O
size O
dataset O
differs O
from O
the O
other O
datasets O
in O
that O
we O
use O
relative O
sizes O
( O
X O
is O
larger O
/ O
smaller O
than O
Y O
) O
, O
as O
absolute O
size O
information O
is O
hard O
to O
obtain O
. O
Thus O
, O
we O
use O
two O
evaluation O
strategies O
for O
size O
. O

Rank O
partition O
First O
, O
as O
in O
the O
previous O
prediction O
task O
, O
given O
a O
template O
such O
as O
" O
[ O
X O
] O
is O
larger O
than O
[ O
Y O
] O
" O
and O
an O
object O
[ O
X O
] O
, O
we O
ask O
the O
model O
to O
predict O
the O
distribution O
of O
[ O
Y O
] O
, O
taking O
only O
the O
distribution O
D O
of O
nouns O
in O
the O
size O
dataset O
. O
For O
the O
current O
object O
[ O
X O
] O
, O
we O
take O
the O
nouns O
in O
size O
categories O
that O
are O
smaller O
than O
the O
category O
of O
[ O
X O
] O
( O
N O
sm O
) O
, O
and O
those O
that O
are O
in O
larger O
categories O
( O
N O
lg O
) O
. O

Let O
the O
length O
of O
N O
sm O
be O
m O
and O
the O
length O
of O
N O
lg O
be O
n. O
Then O
for O
the O
" O
larger O
" O
templates O
, O
we O
compute O
the O
average O
percentage O
of O
overlap O
between O
the O
top O
n O
objects O
in O
D O
and O
N O
lg O
and O
that O
between O
the O
bottom O
m O
objects O
in O
D O
and O
and O
N O
sm O
. O
For O
the O
" O
smaller O
" O
templates O
, O
the O
" O
top O
" O
and O
" O
bottom O
" O
are O
reversed O
. O

Adjective O
projection O

The O
second O
approach O
follows O
that O
of O
van O
Paridon O
et O
al O
. O
( O
2021 O
) O
, O
which O
projects O
the O
word O
to O
be O
evaluated O
onto O
an O
adjective O
scale O
. O
In O
this O
case O
, O
we O
compute O
the O
word O
embeddings O
of O
the O
adjectives O
" O
small O
" O
and O
" O
large O
" O
and O
the O
nouns O
from O
models O
, O
so O
the O
scale O
is O
−−→ O
large O
− O
− O
−− O
→ O
small O
and O
the O
projection O
is O
calculated O
by O
cosine O
similarity O
. O
For O
instance O
, O
for O
the O
example O
noun O
" O
bear O
" O
, O
the O
projection O
score O
is O
given O
by O
: O

cos_sim O
( O
−−→ O
large O
− O
− O
−− O
→ O
small O
, O
− O
− O
→ O
bear O
) O

With O
good O
word O
embeddings O
, O
larger O
nouns O
are O
expected O
to O
have O
higher O
projection O
scores O
. O
The O
validity O
of O
the O
adjective O
scales O
from O
word O
representations O
is O
shown O
by O
Kim O
and O
de O
Marneffe O
( O
2013 O
) O
. O

Measuring O
Model O
Reporting O
Bias O

We O
measure O
the O
reporting O
bias O
of O
our O
models O
by O
comparing O
model O
performance O
on O
datasets O
with O
different O
levels O
of O
reporting O
bias O
and O
on O
the O
SINGLE B-DatasetName
, O
MULTI B-DatasetName
, O
ANY B-DatasetName
groups O
of O
the O
ViComTe B-DatasetName
dataset O
. O
We O
assume O
that O
CoDa B-DatasetName
contains O
no O
reporting O
bias O
, O
in O
which O
case O
we O
can O
interpret O
Table O
2 O
as O
showing O
that O
ViComTe B-DatasetName
contains O
a O
relatively O
small O
amount O
of O
it O
, O
and O
Wikipedia O
contains O
a O
relatively O
large O
amount O
. O
Thus O
, O
a O
larger O
correlation O
of O
model O
outputs O
with O
ViComTe B-DatasetName
and O
a O
smaller O
one O
with O
Wikipedia O
would O
indicate O
less O
model O
reporting O
bias O
. O

Also O
, O
since O
the O
SINGLE B-DatasetName
group O
subjects O
are O
those O
whose O
attribute O
distribution O
concentrates O
on O
a O
single O
attribute O
, O
these O
subject O
- O
attribute O
pairs O
are O
less O
likely O
to O
be O
reported O
in O
text O
corpora O
or O
even O
image O
annotations O
. O
Therefore O
, O
lower O
model O
correlation O
on O
the O
SINGLE B-DatasetName
group O
than O
the O
MULTI B-DatasetName
and O
the O
ANY B-DatasetName
groups O
would O
be O
a O
sign O
of O
model O
reporting O
bias O
. O

Results O

The O
experimental O
results O
show O
that O
multimodal O
models O
outperform O
text O
- O
only O
models O
, O
suggesting O
their O
advantage O
in O
capturing O
visual O
commonsense O
. O
However O
, O
all O
models O
are O
subject O
to O
the O
influence O
of O
reporting O
bias O
, O
as O
they O
correlate O
better O
with O
the O
distributions O
from O
Wikipedia O
than O
those O
from O
CoDa B-DatasetName
Table O
3 O
: O
Spearman O
correlation O
and O
top-1 O
accuracy O
( O
both O
× O
100 O
) O
of O
zero O
shot O
probing O
, O
before O
and O
after O
soft O
prompt O
tuning O
( O
" O
N O
" O
and O
" O
Y O
" O
for O
the O
" O
Tune O
" O
column O
) O
. O
This O
is O
the O
" O
average O
template O
" O
case O
where O
the O
output O
distribution O
is O
the O
mean O
of O
distributions O
across O
all O
templates O
. O
The O
Spearman O
correlation O
reported O
is O
the O
mean O
across O
all O
subjects O
± O
standard O
deviation O
, O
comparing O
the O
output O
distribution O
and O
the O
Visual B-DatasetName
Genome I-DatasetName
distribution O
. O
The O
subscripts O
b O
and O
l O
indicate O
the O
size O
of O
the O
model O
, O
and O
Distilled O
is O
the O
BERT B-MethodName
model O
after O
distilling O
from O
Oscar B-MethodName
. O
Asterisk O
indicates O
where O
there O
is O
no O
significant O
difference O
between O
BERT B-MethodName
b I-MethodName
and O
Oscar B-MethodName
b I-MethodName
( O
t O
- O
test O
p O
- O
value O
> O
0.05 O
) O
. O

and O
ViComTe B-DatasetName
. O
Prompt O
tuning O
and O
knowledge O
distillation O
substantially O
enhance O
model O
performance O
, O
while O
increasing O
model O
size O
does O
not O
. O

Results O
with O
MLM O
Objective O

Color O
, O
Shape O
, O
Material O
The O
resulting O
model O
performance O
for O
the O
" O
average O
template O
" O
mode O
is O
shown O
in O
Table O
3 O
. O
Prompt O
tuning O
is O
done O
in O
this O
mode O
only O
. O
Note O
that O
because O
the O
top-1 B-MetricName
accuracy I-MetricName
is O
taken O
among O
all O
possible O
classes O
of O
each O
relation O
, O
it O
should O
be O
interpreted O
together O
with O
the O
number O
of O
classes O
( O
Table O
1 O
) O
. O

We O
can O
see O
from O
Table O
3 O
that O
Oscar O
does O
better O
than O
BERT B-MethodName
in O
almost O
all O
cases O
. O
Significant O
difference O
between O
Oscar B-MethodName
( O
base O
) O
and O
BERT B-MethodName
( O
base O
) O
is O
seen O
in O
most O
cases O
. O
Also O
, O
after O
soft O
prompt O
tuning O
, O
both O
the O
Spearman B-MetricName
correlation I-MetricName
and O
the O
accuracy O
substantially O
improved O
. O
Although O
there O
is O
considerable O
variation O
of O
the O
Spearman B-MetricName
correlations I-MetricName
, O
we O
find O
consistent O
improvement O
per O
example O
with O
both O
prompt O
tuning O
and O
multimodal O
pretraining O
( O
Appendix O
A.2 O
) O
. O

Table O
3 O
also O
shows O
that O
knowledge O
distillation O
helps O
improve O
the O
performance O
of O
BERT B-MethodName
in O
all O
cases O
, O
and O
the O
distilled O
model O
can O
sometimes O
even O
outperform O
the O
teacher O
model O
, O
Oscar B-MethodName
. O
Moreover O
, O
the O
large O
version O
of O
each O
model O
does O
not O
always O
outperform O
its O
base O
counterpart O
, O
suggesting O
that O
increasing O
the O
size O
of O
the O
model O
does O
not O
enhance O
the O
model O
's O
ability O
to O
understand O
visual O
commonsense O
. O
Instead O
, O
training O
with O
visually O
grounded O
data O
does O
. O

Fig O
. O
2 O
illustrates O
the O
Spearman O
correlations O
of O
different O
models O
with O
the O
color O
distributions O
from O
CoDa B-DatasetName
, O
ViComTe B-DatasetName
and O
Wikipedia O
, O
under O
the O
" O
best O
template O
" O
mode O
. O
7 O
All O
models O
correlate O
moderately O
Table O
5 O
: O
Per O
- O
group O
Spearman B-MetricName
correlation I-MetricName
and O
top-1 B-MetricName
accuracy I-MetricName
( O
both O
× O
100 O
) O
with O
a O
logistic O
regression O
head O
on O
model O
encoder O
outputs O
. O
Note O
that O
the O
ANY B-DatasetName
group O
for O
shape O
only O
has O
one O
example O
, O
so O
the O
accuracy O
is O
less O
meaningful O
and O
is O
omitted O
. O
All O
models O
have O
higher O
correlations O
in O
the O
MULTI B-DatasetName
and O
ANY B-DatasetName
groups O
than O
the O
SINGLE B-DatasetName
group O
, O
which O
is O
a O
sign O
of O
reporting O
bias O
. O

Results O
with O
Classification O
Head O

Table O
4 O
shows O
the O
results O
of O
BERT B-MethodName
, O
CLIP B-MethodName
, O
and O
Oscar B-MethodName
when O
topped O
with O
a O
classification O
head O
. O
We O
observe O
that O
Oscar B-MethodName
and O
CLIP B-MethodName
achieve O
similar O
performance O
and O
both O
outperform O
BERT B-MethodName
. O
Note O
that O
, O
while O
Visual B-DatasetName
Genome I-DatasetName
is O
part O
of O
Oscar B-MethodName
's O
pretraining O
corpus O
and O
one O
might O
suspect O
that O
that O
gives O
it O
an O
advantage O
, O
CLIP B-MethodName
is O
trained O
on O
a O
large O
corpus O
from O
web O
search O
that O
is O
unrelated O
to O
Visual B-DatasetName
Genome I-DatasetName
. O
Therefore O
, O
we O
can O
conclude O
that O
multimodal O
models O
pretrained O
on O
both O
images O
and O
text O
outperform O
text O
- O
only O
models O
. O
Table O
5 O
breaks O
down O
the O
results O
in O
Table O
4 O
into O
three O
subject O
groups O
. O
Oscar B-MethodName
and O
CLIP B-MethodName
outperform O
BERT B-MethodName
in O
almost O
all O
cases O
. O
The O
top-1 B-MetricName
accuracy I-MetricName
is O
higher O
for O
the O
SINGLE B-DatasetName
group O
than O
for O
the O
MULTI B-DatasetName
and O
ANY B-DatasetName
groups O
, O
perhaps O
because O
the O
SINGLE B-DatasetName
group O
subjects O
have O
only O
one O
most O
likely O
target O
attribute O
, O
which O
may O
be O
easier O
to O
predict O
. O
Note O
that O
the O
Spearman B-MetricName
correlations I-MetricName
for O
all O
three O
models O
become O
higher O
from O
group O
SINGLE B-DatasetName
to O
MULTI B-DatasetName
to O
ANY B-DatasetName
. O
Paik O
et O
al O
. O
( O
2021 O
) O
argue O
that O
higher O
correlation O
for O
the O
ANY B-DatasetName
and O
MULTI B-DatasetName
groups O
is O
a O
sign O
of O
model O
reporting O
bias O
, O
as O
objects O
in O
those O
two O
groups O
are O
more O
often O
reported O
. O
Thus O
, O
the O
results O
here O
indicate O
that O
reporting O
bias O
is O
still O
present O
in O
multimodal O
models O
. O

Results O
: O
Size O
Relation O

Table O
6 O
shows O
results O
of O
the O
rank O
partition O
method O
( O
Section O
4.3 O
) O
, O
before O
and O
after O
prompt O
tuning O
. O
Sur- O
prisingly O
, O
prompt O
tuning O
does O
not O
help O
in O
this O
case O
. O
Moreover O
, O
the O
performance O
for O
the O
" O
larger O
" O
templates O
is O
higher O
than O
that O
of O
the O
" O
smaller O
" O
templates O
, O
suggesting O
that O
the O
models O
contain O
inherent O
preference O
towards O
the O
" O
larger O
" O
templates O
. O
Fig O
. O
3 O
shows O
the O
results O
of O
the O
adjective O
projection O
method O
. O
8 O
For O
BERT B-MethodName
and O
Oscar B-MethodName
, O
we O
use O
the O
average O
embedding O
of O
the O
subword O
tokens O
of O
the O
nouns O
projected O
onto O
that O
of O
the O
adjectives O
" O
large O
" O
and O
" O
small O
" O
. O
For O
CLIP B-MethodName
, O
we O
take O
the O
textual O
encoder O
outputs O
as O
the O
embeddings O
, O
resulting O
in O
a O
different O
score O
range O
from O
that O
of O
BERT B-MethodName
and O
Oscar B-MethodName
. O
The O
results O
show O
the O
following O
trend O
: O
larger O
objects O
are O
projected O
onto O
the O
" O
large O
" O
end O
of O
the O
spectrum O
, O
although O
the O
trend O
is O
sometimes O
broken O
towards O
the O
" O
huge O
" O
end O
. O
This O
may O
be O
due O
to O
the O
" O
huge O
" O
group O
including O
nouns O
such O
as O
" O
pool O
" O
and O
" O
house O
" O
which O
can O
be O
modified O
by O
a O
relative O
size O
indicator O
" O
small O
" O
. O

Analysis O
and O
Limitations O

In O
Table O
3 O
, O
the O
accuracy B-MetricName
of O
BERT B-MethodName
for O
shape O
is O
particularly O
low O
( O
only O
6.7 B-MetricValue
% I-MetricValue
) O
, O
despite O
that O
shape O
has O
only O
12 O
classes O
. O
We O
hypothesize O
that O
this O
is O
due O
to O
reporting O
bias O
on O
shape O
in O
the O
text O
corpora O
that O
BERT B-MethodName
is O
trained O
on O
. O
This O
hypothesis O
is O
supported O
by O
mining O
sentences O
from O
Wikipedia O
that O
contain O
( O
noun O
, O
attribute O
) O
pairs O
, O
where O
we O
see O
that O
the O
relation O
shape O
has O
fewer O
number O
of O
occurrences O
than O
material O
and O
color O
( O
Appendix O
A.3 O
) O
. O
We O
also O
investigate O
whether O
the O
advantage O
of O
the O
visually O
- O
grounded O
models O
over O
pure O
- O
language O
models O
comes O
from O
the O
domain O
difference O
between O
web O
corpora O
and O
image O
captions O
, O
or O
the O
presence O
of O
actual O
visual O
input O
. O
Although O
its O
teacher O
is O
trained O
with O
visual O
inputs O
, O
the O
Distilled O
model O
is O
trained O
only O
on O
captions O
data O
and O
its O
performance O
matches O
that O
of O
Oscar B-MethodName
, O
so O
we O
hypothesize O
that O
grounded O
training O
data O
enhance O
models O
' O
ability O
to O
capture O
visual O
commonsense O
. O
The O
CaptionBERT B-MethodName
results O
support O
the O
hypothesis O
in O
favor O
of O
domain O
difference O
, O
since O
it O
performs O
better O
than O
BERT B-MethodName
in O
both O
CoDa B-DatasetName
and O
VG B-DatasetName
( O
Fig O
. O
2 O
) O
. O
Nevertheless O
, O
the O
visual O
inputs O
also O
have O
an O
effect O
, O
as O
Oscar B-MethodName
has O
a O
higher O
correlation O
than O
CaptionBERT B-MethodName
on O
CoDa B-DatasetName
. O
Thus O
, O
it O
seems O
that O
both O
domain O
and O
modality O
affect O
the O
ultimate O
model O
performance O
. O

Finally O
, O
although O
multimodal O
models O
show O
improvement O
on O
the O
task O
, O
sometimes O
the O
improvement O
is O
not O
significant O
and O
the O
resulting O
correlations O
are O
still O
weak O
. O
Further O
work O
is O
needed O
to O
enhance O
the O
visual O
commonsense O
abilities O
of O
the O
models O
and O
mitigate O
reporting O
bias O
, O
and O
our O
datasets O
can O
serve O
as O
an O
evaluation O
method O
. O

Conclusion O

In O
this O
paper O
, O
we O
probe O
knowledge O
about O
visually O
salient O
properties O
from O
pretrained O
neural O
networks O
. O
We O
automatically O
extract O
dataset O
of O
five O
visual O
relations O
: O
color O
, O
shape O
, O
material O
, O
size O
, O
and O
cooccurrence O
, O
and O
show O
that O
our O
ViComTe B-DatasetName
dataset O
has O
a O
much O
higher O
correlation O
with O
human O
perception O
data O
for O
color O
than O
data O
mined O
from O
Wikipedia O
. O
We O
then O
apply O
several O
probing O
techniques O
and O
discover O
that O
visually O
- O
supervised O
models O
perform O
better O
than O
pure O
language O
models O
, O
which O
indicates O
that O
they O
can O
better O
capture O
such O
visual O
properties O
. O
Distilling O
the O
knowledge O
from O
a O
visually O
- O
supervised O
model O
into O
a O
pure O
language O
model O
results O
in O
comparable O
performance O
with O
the O
teacher O
model O
. O

We O
also O
observe O
less O
reporting O
bias O
in O
both O
visually O
- O
grounded O
text O
( O
VG O
- O
mined O
datasets O
) O
than O
Wikipedia O
text O
and O
visually O
- O
grounded O
models O
( O
Oscar B-MethodName
, O
DistilledOscar B-MethodName
, O
VisualBERT B-MethodName
, O
and O
CLIP B-MethodName
) O
than O
pure O
language O
models O
. O
However O
, O
visuallygrounded O
models O
are O
still O
subject O
to O
the O
influence O
of O
reporting O
bias O
, O
as O
seen O
in O
the O
per O
- O
group O
analysis O
, O
where O
both O
types O
of O
models O
perform O
better O
for O
the O
MULTI B-DatasetName
group O
than O
the O
SINGLE B-DatasetName
group O
. O

A O
Appendix O

A.1 O
List O
of O
Objects O

Table O
7 O
shows O
the O
list O
of O
all O
possible O
attributes O
for O
relations O
color O
, O
shape O
, O
and O
material O
. O
Table O
8 O
shows O
the O
list O
of O
objects O
in O
the O
five O
categories O
of O
relation O
size O
. O
Visual O
co O
- O
ocurrence O
has O
a O
large O
number O
of O
objects O
that O
are O
not O
listed O
here O
for O
space O
reasons O
. O

A.2 O
Additional O
Probing O

Best O
template O
mode O
Table O
9 O
contains O
zero O
- O
shot O
results O
under O
the O
" O
best O
template O
" O
mode O
, O
for O
BERT B-MethodName
( O
base O
) O
, O
Oscar B-MethodName
( O
base O
) O
, O
BERT B-MethodName
distilled O
from O
Oscar B-MethodName
, O
RoBERTa B-MethodName
( O
base O
) O
, O
ALBERT B-MethodName
( O
base O
) O
, O
Vokenization B-MethodName
, O
and O
VisualBERT B-MethodName
( O
base O
) O
. O
These O
results O
demonstrate O
similar O
trends O
as O
the O
ones O
in O
the O
" O
average O
template O
" O
mode O
. O
Per O
- O
object O
analysis O
Fig O
. O
4 O
illustrates O
the O
finegrained O
Spearman B-MetricName
correlation I-MetricName
± O
standard O
deviation O
per O
object O
group O
for O
BERT B-MethodName
and O
CLIP B-MethodName
. O
Size O
per O
- O
object O
Fig O
. O
5 O
shows O
how O
the O
per O
- O
object O
projection O
scores O
on O
the O
size O
spectrum O
from O
BERT B-MethodName
and O
Oscar B-MethodName
are O
correlated O
. O
Per O
- O
Subject O
Comparison O
Fig O
. O
6 O
and O
Fig O
. O
7 O
show O
how O
the O
Spearman B-MetricName
correlations I-MetricName
of O
10 O
individual O
subjects O
improve O
after O
soft O
prompt O
tuning O
and O
after O
multimodal O
pretraining O
. O
Consistent O
improvement O
can O
be O
seen O
in O
color O
, O
material O
, O
and O
cooccurrence O
. O
Although O
we O
report O
average O
Spearman B-MetricName
correlations I-MetricName
in O
Table O
3 O
and O
there O
are O
large O
standard O
deviations O
, O
here O
we O
show O
that O
when O
improvement O
is O
observed O
collectively O
, O
it O
is O
also O
consistent O
across O
subjects O
. O
With O
shape O
, O
the O
improvement O
is O
less O
obvious O
( O
45.9 O
to O
50.4 O
for O
prompt O
tuning O
and O
49.2 O
to O
50.4 O
for O
multimodal O
pretraining O
) O
. O

A.3 O
Error O
Analysis O

Data O
The O
three O
subjects O
with O
the O
highest O
and O
lowest O
Spearman B-MetricName
correlation I-MetricName
are O
shown O
in O
Fig O
. O
8 O
and O
Fig O
. O
9 O
. O

Wikipedia O
Table O
10 O
shows O
the O
number O
of O
( O
noun O
, O
attribute O
) O
pairs O
of O
the O
three O
relation O
types O
in O
Wikipedia O
. O
Shape O
has O
fewer O
occurrences O
than O
material O
and O
color O
. O
Model O
Table O
11 O
shows O
the O
errors O
made O
by O
BERT B-MethodName
and O
Oscar B-MethodName
in O
the O
" O
average O
template O
" O
mode O
before O
prompt O
tuning O
. O
Overall O
, O
subjects O
with O
low O
correlation O
are O
those O
that O
are O
less O
often O
reported O
in O
Visual B-DatasetName
Genome I-DatasetName
as O
well O
as O
in O
textual O
data O
. O

Acknowledgments O

We O
would O
like O
to O
thank O
the O
reviewers O
for O
their O
comments O
and O
suggestions O
. O
Chenyu O
Zhang O
is O
supported O
by O
the O
Pistritto O
Research O
Fellowship O
. O
Elias O
Stengel O
- O
Eskin O
is O
supported O
by O
an O
NSF O
Graduate O
Research O
Fellowship O
. O
Zhuowan O
Li O
is O
supported O
by O
NSF O
1763705 O
. O

Is O
" O
My O
Favorite O
New O
Movie O
" O
My O
Favorite O
Movie O
? O
Probing B-TaskName
the I-TaskName
Understanding I-TaskName
of I-TaskName
Recursive I-TaskName
Noun I-TaskName
Phrases I-TaskName

Recursive O
noun O
phrases O
( O
NPs O
) O
have O
interesting O
semantic O
properties O
. O
For O
example O
, O
my O
favorite O
new O
movie O
is O
not O
necessarily O
my O
favorite O
movie O
, O
whereas O
my O
new O
favorite O
movie O
is O
. O
This O
is O
common O
sense O
to O
humans O
, O
yet O
it O
is O
unknown O
whether O
language O
models O
have O
such O
knowledge O
. O
We O
introduce O
the O
Recursive B-DatasetName
Noun I-DatasetName
Phrase I-DatasetName
Challenge I-DatasetName
( O
RNPC B-DatasetName
) O
, O
a O
dataset O
of O
three O
textual O
inference O
tasks O
involving O
textual O
entailment O
and O
event O
plausibility O
comparison O
, O
precisely O
targeting O
the O
understanding O
of O
recursive O
NPs O
. O
When O
evaluated O
on O
RNPC B-DatasetName
, O
state O
- O
of O
- O
theart O
Transformer O
models O
only O
perform O
around O
chance O
. O
Still O
, O
we O
show O
that O
such O
knowledge O
is O
learnable O
with O
appropriate O
data O
. O
We O
further O
probe O
the O
models O
for O
relevant O
linguistic O
features O
that O
can O
be O
learned O
from O
our O
tasks O
, O
including O
modifier O
semantic O
category O
and O
modifier O
scope O
. O
Finally O
, O
models O
trained O
on O
RNPC B-DatasetName
achieve O
strong O
zero O
- O
shot O
performance O
on O
an O
extrinsic O
Harm O
Detection O
evaluation O
task O
, O
showing O
the O
usefulness O
of O
the O
understanding O
of O
recursive O
NPs O
in O
downstream O
applications O
. O
1 O

Introduction O

Recursion O
, O
the O
self O
- O
embedding O
of O
a O
linguistic O
structure O
, O
constitutes O
a O
fundamental O
property O
of O
human O
language O
. O
Due O
to O
its O
hierarchical O
structure O
, O
it O
poses O
many O
challenges O
to O
human O
language O
acquisition O
. O
One O
such O
challenge O
occurs O
in O
the O
context O
of O
recursive O
Noun O
Phrases O
( O
NPs O
) O
, O
i.e. O
, O
NPs O
with O
multiple O
prenominal O
modifiers O
. O
For O
instance O
, O
in O
Figure O
1 O
, O
when O
asked O
to O
point O
to O
the O
second O
green O
ball O
in O
a O
series O
of O
balls O
, O
children O
sometimes O
erroneously O
point O
to O
the O
second O
and O
green O
ball O
( O
intersective O
interpretation O
) O
, O
instead O
of O
the O
second O
among O
green O
balls O
( O
recursive O
interpretation O
) O
( O
Matthei O
, O
1982 O
; O
Hamburger O
and O
Crain O
, O
1984 O
; O
Marcilese O
et O
al O
. O
, O
2013 O
) O
. O
1 O
Our O
code O
and O
data O
are O
available O
at O
https O
: O
/ O
/ O
github O
. O
com O
/ O
veronica320 O
/ O
Recursive O
- O
NPs O
. O
We O
investigate O
whether O
language O
models O
( O
LMs O
) O
make O
similar O
errors O
, O
since O
the O
understanding O
of O
recursive O
NPs O
is O
also O
fundamental O
in O
real O
- O
world O
AI O
applications O
. O
For O
example O
, O
a O
summarization O
system O
should O
know O
that O
the O
former O
US O
president O
can O
not O
be O
shortened O
as O
the O
president O
, O
since O
they O
are O
no O
longer O
in O
power O
. O
Also O
, O
a O
self O
- O
driving O
car O
asked O
to O
take O
the O
first O
left O
- O
hand O
exit O
should O
not O
assume O
that O
it O
is O
always O
the O
first O
exit O
. O

Previous O
work O
has O
studied O
the O
syntactic O
parsing O
of O
recursive O
NPs O
( O
Nakov O
and O
Hearst O
, O
2005 O
; O
Pitler O
et O
al O
. O
, O
2010 O
) O
, O
as O
well O
as O
the O
semantic O
categorization O
of O
modifiers O
in O
NPs O
with O
only O
one O
prenominal O
modifier O
( O
Kamp O
and O
Partee O
, O
1995 O
; O
McCrae O
et O
al O
. O
, O
2014 O
) O
. O
However O
, O
neither O
parsing O
nor O
modifier O
categorization O
alone O
can O
sufficiently O
capture O
the O
meaning O
of O
recursive O
NPs O
( O
§ O
2 O
) O
. O

In O
this O
paper O
, O
using O
recursive O
NPs O
with O
two O
modifiers O
as O
our O
test O
- O
bed O
, O
we O
address O
the O
following O
questions O
about O
LMs O
' O
understanding O
of O
recursion O
: O

( O
a O
) O
Is O
the O
knowledge O
of O
how O
to O
interpret O
recursive O
NPs O
present O
in O
LMs O
( O
§ O
5 O
) O
? O
We O
propose O
the O
Recursive B-DatasetName
Noun I-DatasetName
Phrase I-DatasetName
Challenge I-DatasetName
( O
RNPC B-DatasetName
) O
, O
a O
challenge O
set O
containing O
three O
classification O
tasks O
: O
Single O
- O
Premise O
Textual O
Entailment O
, O
Multi O
- O
Premise O
Textual O
Entailment O
, O
and O
Event O
Plausibility O
Comparison O
( O
§ O
3 O
) O
. O
Table O
1 O
provides O
examples O
for O
each O
task O
. O
Results O
show O
that O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
LMs O
finetuned O
on O
standard O
benchmarks O
of O
the O
same O
format O
( O
e.g. O
, O
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
) O
all O
struggle O
on O
our O
dataset O
, O
suggesting O
that O
the O
target O
knowledge O
is O
not O
readily O
available O
. O

( O
b O
) O
Is O
such O
knowledge O
learnable O
with O
appropriate O
data O
( O
§ O
6 O
) O
? O
We O
adopt O
the O
challenge O
set O
analysis O
technique O
proposed O
by O
Liu O
et O
al O
. O
( O
2019a O
) O
, O
which O
exposes O
models O
to O
a O
small O
amount O
of O
data O
and O
assesses O
how O
well O
they O
can O
adapt O
. O
All O
models O
achieve O
a O
noticeable O
performance O
improvement O
with O
as O
few O
as O
200 O
examples O
, O
indicating O
that O
the O
target O
knowledge O
is O
potentially O
learnable O
. O

( O
c O
) O
What O
can O
models O
learn O
from O
recursive O
NPs O
( O
§ O
7 O
) O
? O
We O
probe O
the O
finetuned O
models O
for O
two O
well O
- O
studied O
linguistic O
features O
in O
previous O
work O
, O
modifier O
semantic O
category O
and O
modifier O
scope O
. O
We O
show O
that O
both O
features O
can O
be O
learned O
from O
RNPC B-DatasetName
, O
with O
techniques O
including O
edge O
probing O
( O
Tenney O
et O
al O
. O
, O
2019 O
) O
and O
attention O
visualization O
( O
Vig O
, O
2019 O
) O
. O

( O
d O
) O
Is O
such O
knowledge O
useful O
for O
downstream O
tasks O
( O
§ O
8 O
) O
? O
When O
evaluated O
on O
an O
extrinsic O
Harm O
Detection O
task O
, O
models O
finetuned O
on O
RNPC B-DatasetName
achieve O
strong O
zero O
- O
shot O
performance O
. O
This O
shows O
that O
the O
understanding O
of O
recursive O
NPs O
can O
benefit O
downstream O
language O
understanding O
tasks O
. O

In O
summary O
, O
our O
work O
identifies O
an O
interesting O
linguistic O
phenomenon O
that O
is O
common O
sense O
to O
humans O
but O
challenging O
for O
models O
. O
It O
contributes O
to O
the O
characterization O
of O
LMs O
' O
limitations O
and O
capabilities O
in O
language O
understanding O
. O

Related O
Work O

Noun O
Phrases O
( O
NPs O
) O
have O
been O
extensively O
studied O
in O
both O
linguistics O
and O
NLP O
, O
primarily O
from O
the O
following O
perspectives O
. O
Syntactic O
structure O
. O
A O
line O
of O
work O
focuses O
on O
the O
syntactic O
structure O
of O
NPs O
, O
which O
essentially O
explains O
the O
modifier O
scope O
( O
Campbell O
, O
2002 O
) O
in O
NPs O
. O
One O
classic O
task O
is O
NP O
bracketing O
, O
i.e. O
, O
deciding O
whether O
an O
NP O
is O
right O
- O
branching O
( O
e.g. O
, O
[ O
world O
[ O
oil O
prices O
] O
] O
) O
or O
left O
- O
branching O
( O
e.g. O
, O
[ O
[ O
crude O
oil O
] O
prices O
] O
) O
( O
Lauer O
, O
1995 O
; O
Nakov O
and O
Hearst O
, O
2005 O
) O
. O
A O
harder O
task O
is O
full O
parsing O
( O
Vadas O
and O
Curran O
, O
2007 O
; O
Pitler O
et O
al O
. O
, O
2010 O
) O
, O
i.e. O
, O
reconstructing O
the O
complete O
dependency O
tree O
. O
Modifier O
semantics O
. O
Another O
line O
of O
research O
revolves O
around O
the O
semantics O
of O
simple O
modifiernoun O
composition O
, O
starting O
with O
ways O
to O
categorize O
modifiers O
based O
on O
their O
inference O
patterns O
( O
Kamp O
and O
Partee O
, O
1995 O
; O
Bouillon O
and O
Viegas O
, O
1999 O
; O
Chierchia O
and O
McConnell O
- O
Ginet O
, O
2000 O
) O
. O
With O
M O
as O
the O
modifier O
and O
N O
as O
the O
noun O
, O
a O
representative O
taxonomy O
summarized O
by O
McCrae O
et O
al O
. O
( O
2014 O
) O

is O
: O
( O
1 O
) O
intersective O
: O
X O
is O
a O
M O
N O
= O
⇒ O
X O
is O
M O
∧ O
X O
is O
a O
N O
, O
e.g O
. O

, O
" O
an O
American O
surgeon O
" O
describes O
someone O
who O
is O
both O
American O
and O
a O
surgeon O
; O

( O
2 O
) O
subsective O
: O

X O
is O
a O
M O
N O
= O
⇒ O
X O
is O
a O
N O
, O
but O
X O
is O
a O
M O
N O
= O
⇒ O
X O
is O
M O
, O
e.g O
. O

, O
someone O
who O
is O
" O
a O
skillful O
surgeon O
" O
is O
not O
necessarily O
skillful O
in O
all O
disciplines O
; O

( O
3 O
) O
privative O
: O
X O
is O
a O
M O
N O
= O
⇒ O
X O
is O
a O
N O
, O
e.g. O
, O
" O
a O
former O
surgeon O
" O
describes O
someone O
who O
is O
no O
longer O
a O
surgeon O
. O

Despite O
the O
variations O
2 O
and O
debates O
3 O
on O
the O
taxonomy O
, O
we O
follow O
these O
conventional O
terms O
in O
subsequent O
sections O
. O

With O
the O
advances O
in O
NLP O
, O
more O
recent O
works O
starts O
modeling O
the O
semantics O
of O
simple O
modifiernoun O
constructions O
with O
first O
- O
order O
logic O
( O
McCrae O
et O
al O
. O
, O
2014 O
) O
, O
linear O
mapping O
( O
Baroni O
and O
Zamparelli O
, O
2010 O
) O
, O
and O
other O
explicit O
compositional O
operations O
( O
Boleda O
et O
al O
. O
, O
2012 O
( O
Boleda O
et O
al O
. O
, O
, O
2013 O
. O
In O
particular O
, O
Pavlick O
and O
Callison O
- O
Burch O
( O
2016a O
, O
b O
) O
propose O
a O
novel O
contextualized O
inference O
- O
based O
approach O
. O
They O
define O
the O
Add O
- O
One O
Entailment O
task O
with O
natural O
contexts O
from O
textual O
corpora O
, O
where O
the O
hypothesis O
differs O
from O
the O
premise O
by O
the O
insertion O
of O
one O
modifier O
. O
For O
example O
, O
The O
crowd O
roared O
entails O
The O
enthusiastic O
crowd O
roared O
, O
though O
enthusiastic O
crowd O
denotes O
a O
subset O
of O
crowd O
without O
context O
. O
However O
, O
natural O
contexts O
also O
introduce O
complications O
from O
monotonicity O
( O
Van O
Benthem O
, O
1983 O
) O
. O
For O
instance O
, O
red O
apple O
entails O
apple O
, O
but O
He O
did O
n't O
eat O
any O
red O
apple O
does O
not O
entail O
He O
did O
n't O
eat O
any O
apple O
due O
to O
the O
downward O
entailment O
context O
. O
In O
our O
proposed O
approach O
, O
we O
handle O
this O
issue O
by O
controlling O
for O
context O
monotonicity O
. O

Other O
related O
work O
explores O
which O
attributes O
of O
the O
head O
noun O
are O
affected O
by O
the O
presence O
of O
modifiers O
. O
Mullenbach O
et O
al O
. O
( O
2019 O
) O
look O
at O
how O
modifiers O
project O
from O
a O
noun O
to O
its O
parts O
( O
e.g. O
, O
does O
a O
red O
jeep O
have O
red O
tires O
? O
) O
. O
Emami O
et O
al O
. O
( O
2021 O
) O
test O
the O
likelihood O
change O
of O
an O
event O
when O
a O
modifier O
is O
added O
( O
e.g. O
, O
a O
false O
key O
is O
less O
likely O
to O
open O
a O
door O
than O
a O
key O
) O
. O
Apidianaki O
and O
Garí O
Soler O
( O
2021 O
) O
study O
the O
prototypical O
properties O
of O
nouns O
( O
e.g. O
, O
a O
strawberry O
entails O
a O
red O
strawberry O
) O
. O
Researchers O
also O
examine O
the O
interpretation O
of O
noun O
compounds O
( O
Shwartz O
and O
Waterson O
, O
2018 O
; O
Hendrickx O
et O
al O
. O
, O
2013 O
) O
( O
e.g. O
, O
olive O
oil O
is O
made O
of O
olives O
, O
while O
baby O
oil O
is O
made O
for O
babies O
) O
. O
Summary O
. O
Neither O
syntactic O
parsing O
nor O
modifier O
semantics O
alone O
can O
fully O
capture O
the O
meaning O
of O
recursive O
NPs O
. O
In O
terms O
of O
syntax O
, O
modifier O
scope O
can O
not O
always O
explain O
NPs O
due O
to O
the O
influence O
from O
modifier O
semantics O
. O
For O
instance O
, O
a O
[ O
big O
[ O
fake O
gun O
] O
] O
and O
a O
[ O
big O
[ O
black O
gun O
] O
] O
have O
the O
same O
structure O
but O
different O
inference O
patterns O
, O
i.e. O
only O
the O
latter O
is O
a O
gun O
. O
Meanwhile O
, O
modifier O
category O
itself O
does O
not O
suffice O
without O
taking O
into O
account O
modifier O
scope O
. O
For O
example O
, O
a O
so O
- O
called O
healthy O
food O
and O
a O
so O
- O
called O
homeopathy O
expert O
start O
with O
the O
same O
privative O
modifier O
( O
so O
- O
called O
) O
. O
However O
, O
socalled O
questions O
truthfulness O
of O
the O
second O
modifier O
( O
healthy O
) O
in O
the O
former O
case O
while O
that O
of O
the O
noun O
( O
expert O
) O
in O
the O
latter O
. O
Therefore O
, O
we O
introduce O
a O
dataset O
containing O
three O
novel O
and O
challenging O
textual O
inference O
tasks O
, O
which O
rely O
on O
the O
interplay O
of O
syntax O
and O
semantics O
in O
determining O
the O
meaning O
of O
recursive O
NPs O
. O

Task O
Formulation O

Our O
dataset O
contains O
three O
tasks O
. O
Let O
us O
denote O
a O
canonical O
two O
- O
modifier O
recursive O
NP O
by O
Det O
M O
1 O
M O
2 O
N O
( O
Determiner O
, O
Modifier O
1 O
, O
Modifier O
2 O
, O
Noun O
) O
. O
With O
this O
notation O
, O
the O
tasks O
are O
outlined O
below O
. O
See O
Table O
1 O
for O
concrete O
examples O
. O

Single O
- O
Premise O
Textual O
Entailment O
( O
SPTE O
) O

follows O
the O
conventional O
TE O
task O
format O
. O
Given O
a O
premise O
and O
a O
hypothesis O
, O
the O
model O
decides O
whether O
the O
premise O
semantically O
entails O
the O
hypothesis O
. O
The O
labels O
include O
entailment O
and O
non O
- O
entailment O
. O
4 O
An O
SPTE O
example O
can O
be O
represented O
in O
regular O
expression O
as O
: O

Premise O
: O
P O
Det O
M O
1 O
M O
2 O
N O
Hypothesis O
: O
P O
Det O
( O
M O
1 O
|M O
2 O
) O
? O
N O
Label O
: O
entailment|non O
- O
entailment O

where O
P O
is O
a O
sentence O
prefix O
, O
which O
can O
be O
instantiated O
as O
This O
is O
/ O
He O
is O
/ O
She O
is O
, O
etc O
. O
, O
depending O
on O
the O
NP O
. O
Intuitively O
, O
this O
task O
tests O
whether O
an O
NP O
entails O
its O
various O
components O
. O
This O
holds O
for O
most O
simple O
NPs O
( O
e.g. O
, O
the O
second O
ball O
entails O
ball O
) O
, O
but O
recursive O
NPs O
offer O
interesting O
counterexamples O
( O
e.g. O
, O
( O
1b O
) O
in O
Table O
1 O
) O
. O

Multi O
- O
Premise O
Textual O
Entailment O
( O
MPTE O
) O
is O
adapted O
from O
the O
attributive O
propagation O
test O
described O
in O
Lalisse O
( O
2015 O
) O
. O
The O
format O
differs O
from O
SPTE O
only O
in O
that O
it O
has O
two O
premises O
instead O
of O
one O
. O
Given O
that O
both O
are O
true O
, O
the O
task O
is O
to O
determine O
whether O
the O
hypothesis O
is O
also O
true O
. O
The O
first O
premise O
is O
of O
the O
same O
form O
as O
in O
SPTE O
. O
The O
second O
premise O
contains O
a O
noun O
other O
than O
N O
, O
denoted O
by O
N O
2 O
. O
5 O
A O
regular O
expression O
representation O
is O
: O

Premise O
1 O
: O
P O
Det O
M O
1 O
M O
2 O
N O

Premise O
2 O
: O
P O
Det O
N O
2 O

Hypothesis O
: O
P O
Det O
( O
M O
1 O
|M O
2 O
) O
N O
2 O
Label O
: O
entailment|non O
- O
entailment O

This O
test O
targets O
the O
compositionality O
of O
modifiers O
and O
nouns O
. O
While O
most O
of O
the O
time O
a O
modifier O
can O
be O
freely O
" O
detached O
" O
and O
" O
attached O
" O
( O
e.g. O
, O
( O
2a O
) O
) O
, O
sometimes O
it O
can O
not O
( O
e.g. O
, O
( O
2b O
) O
) O
. O

Event O
Plausibility O
Comparison O
( O
EPC O
) O
follows O
the O
task O
formalization O
by O
Emami O
et O
al O
. O
( O
2021 O
) O
for O
single O
- O
modifier O
NPs O
. O
Given O
two O
events O
, O
Event1 O
and O
Event2 O
, O
a O
model O
needs O
to O
assess O
the O
plausibility O
of O
Event2 O
compared O
to O
that O
of O
Event1 O
. O
The O
two O
events O
have O
the O
same O
event O
predicate O
E O
, O
and O
differ O
only O
in O
the O
NP O
. O
A O
regular O
expression O
representation O
is O
: O

Event O
1 O
: O
Det O
( O
M O
1 O
|M O
2 O
) O
? O
N O
E O
Event O
2 O
: O
Det O
M O
1 O
M O
2 O
N O
E O
Label O
: O
more|equally|less O
plausible O

This O
task O
tests O
the O
influence O
of O
adding O
modifier O
( O
s O
) O
on O
the O
plausibility O
of O
different O
events O
about O
the O
noun O
. O
Not O
all O
events O
are O
affected O
in O
the O
same O
way O
: O
in O
( O
3 O
) O
, O
stars O
in O
many O
latest O
movies O
becomes O
less O
plausible O
, O
while O
is O
known O
by O
everyone O
is O
more O
so O
. O
We O
choose O
the O
three O
tasks O
defined O
above O
because O
they O
allow O
us O
to O
study O
different O
interesting O
properties O
of O
recursive O
NPs O
that O
conventional O
parsing O
tasks O
do O
not O
. O
For O
example O
, O
SPTE O
is O
convenient O
for O
comparing O
the O
impact O
of O
modifier O
order O
on O
the O
meaning O
of O
the O
NP O
( O
e.g. O
, O
( O
1a O
) O
and O
( O
1b O
) O
) O
; O
MPTE O
precisely O
reflects O
the O
property O
of O
subsective O
modifiers O
( O
e.g. O
, O
skillful O
) O
; O
whereas O
EPC O
is O
suitable O
for O
NPs O
with O
privative O
modifiers O
, O
since O
the O
other O
formats O
often O
cause O
ambiguity O
in O
this O
case O
. O
6 O

5 O
For O
both O
premises O
to O
hold O
at O
the O
same O
time O
, O
we O
need O
an O
N2 O
that O
can O
refer O
to O
the O
same O
entity O
as O
N O
. O

6 O
For O
example O
, O
fake O
fur O
might O
or O
might O
not O
be O
considered O
Modifier O
lexicon O
construction O
. O
We O
first O
construct O
a O
lexicon O
of O
modifiers O
following O
the O
taxonomy O
in O
Section O
2 O
( O
McCrae O
et O
al O
. O
, O
2014 O
) O
. O
We O
include O
modifiers O
studied O
in O
relevant O
linguistics O
literature O
( O
Nayak O
et O
al O
. O
, O
2014 O
; O
Lalisse O
, O
2015 O
) O
and O
complement O
the O
list O
with O
modifiers O
that O
are O
missing O
or O
have O
not O
been O
addressed O
before O
under O
this O
lens O
( O
for O
example O
, O
modifiers O
that O
describe O
material O
, O
such O
as O
wooden O
, O
can O
also O
be O
viewed O
as O
privative O
) O
. O
Each O
entry O
in O
the O
lexicon O
contains O
the O
modifier O
itself O
, O
its O
category O
( O
intersective O
, O
subsective O
, O
or O
privative O
) O
, O
and O
its O
attribute O
( O
e.g. O
, O
green O
is O
a O
COLOR O
) O
. O
In O
total O
, O
the O
lexicon O
contains O
689 O
modifiers O
, O
the O
largest O
resource O
of O
this O
kind O
. O
See O
Table O
2 O
( O
Matthei O
, O
1982 O
; O
Abdullah O
and O
Frost O
, O
2005 O
; O
Teodorescu O
, O
2006 O
; O
Morzycki O
, O
2016 O
) O
, O
text O
corpora O
( O
Penn O
Treebank O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
and O
the O
Annotated O
Gigaword O
corpus O
( O
Napoles O
et O
al O
. O
, O
2012 O
) O
) O
, O
and O
our O
creation O
. O
From O
text O
corpora O
, O
we O
extract O
all O
NPs O
with O
more O
than O
two O
consecutive O
modifiers O
in O
our O
lexicon O
, O
and O
manually O
select O
NPs O
considering O
a O
set O
of O
factors O
: O
lexical O
diversity O
, O
class O
balance O
, O
whether O
there O
is O
an O
interaction O
between O
the O
modifiers O
, O
etc O
. O
Finally O
, O
we O
complement O
the O
set O
with O
deliberately O
designed O
challenging O
cases O
of O
our O
invention O
, O
resulting O
in O
1,299 O
NPs O
in O
total O
. O

a O
kind O
of O
fur O
( O
Partee O
, O
2010 O
) O
. O
Annotators O
would O
thus O
probably O
disagree O
on O
the O
label O
if O
it O
were O
an O
SPTE O
example O
. O

Do O
LMs O
understand O
recursive O
NPs O
? O

To O
answer O
question O
( O
a O
) O
, O
whether O
the O
knowledge O
of O
how O
to O
interpret O
recursive O
NPs O
is O
present O
in O
pretrained O
LMs O
, O
we O
use O
the O
" O
behavioral O
test O
" O
probing O
method O
( O
Belinkov O
et O
al O
. O
, O
2020 O
) O
. O
Namely O
, O
we O
evaluate O
SOTA O
models O
finetuned O
on O
existing O
benchmark O
( O
s O
) O
of O
the O
same O
format O
as O
each O
RNPC B-DatasetName
task O
. O

The O
rationale O
is O
that O
LMs O
should O
acquire O
the O
ability O
of O
textual O
inference O
in O
the O
required O
format O
during O
finetuning O
, O
which O
allows O
us O
to O
elicit O
their O
potential O
knowledge O
about O
recursive O
NPs O
. O
9 O
Experimental O
setup O
. O
We O
consider O
the O
following O
datasets O
that O
address O
similar O
phenomena O
as O
our O
tasks O
: O
( O
1 O
) O
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
and O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
for O
our O
SPTE O
; O

( O
2 O
) O
MPE B-DatasetName
( O
Lai O
et O
al O
. O
, O
2017 O
) O
for O
our O
MPTE O
; O
and O
( O
3 O
) O
ADEPT B-DatasetName
( O
Emami O
et O
al O
. O
, O
2021 O
) O
for O
our O
EPC O
. O
We O
choose O
SOTA O
and O
close O
- O
to O
- O
SOTA O
models O
on O
these O
benchmarks O
as O
probing O
candidates O
, O
including O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
, O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
and O
GPT3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O
10 O
Results O
and O
analysis O
. O
We O
evaluate O
the O
finetuned O
models O
on O
each O
RNPC B-DatasetName
task O
. O
When O
the O
finetuning O
dataset O
has O
more O
classes O
than O
our O
task O
does O
, O
we O
map O
the O
model O
prediction O
to O
one O
of O
our O
classes O
by O
summing O
probability O
scores O
. O
11 O
Figure O
2 O
compares O
the O
performance O
of O
the O
models O
on O
the O
relevant O
9 O
LMs O
can O
also O
overfit O
the O
finetuning O
dataset O
and O
thus O
" O
forget O
" O
the O
target O
knowledge O
acquired O
during O
pretraining O
. O
Thus O
, O
we O
also O
directly O
probe O
the O
pretrained O
LMs O
in O
a O
complementary O
" O
likelihood O
scoring O
" O
experiment O
, O
described O
in O
Appendix O
C O
. O

10 O
Due O
to O
the O
size O
of O
MNLI B-DatasetName
and O
SNLI B-DatasetName
, O
we O
only O
evaluate O
available O
checkpoints O
from O
the O
Huggingface O
Transformers O
model O
hub O
. O
For O
the O
other O
two O
benchmarks O
, O
all O
models O
are O
trained O
by O
us O
. O
Also O
, O
the O
largest O
GPT3 B-MethodName
- O
davinci O
is O
unavailable O
for O
finetuning O
and O
thus O
excluded O
. O
See O
Appendices O
B O
and O
E.1 O
for O
dataset O
, O
model O
and O
hyperparameter O
details O
. O
11 O
For O
example O
, O
for O
a O
model O
trained O
on O
MNLI B-DatasetName
( O
with O
three O
labels O
) O
, O
we O
compare O
the O
score O
of O
entailment O
and O
the O
summed O
score O
of O
neutral O
and O
contradiction O
. O
If O
the O
former O
is O
higher O
, O
we O
predict O
entailment O
on O
SPTE O
; O
otherwise O
non O
- O
entailment O
. O
Empirically O
, O
this O
strategy O
results O
in O
higher O
performance O
than O
directly O
mapping O
the O
highest O
- O
score O
MNLI B-DatasetName
label O
to O
its O
corresponding O
SPTE O
label O
. O
benchmarks O
and O
our O
tasks O
. O
We O
also O
include O
human O
performance O
, O
calculated O
by O
averaging O
the O
accuracy O
of O
three O
college O
student O
annotators O
on O
a O
random O
sample O
of O
300 O
examples O
for O
each O
task O
. O

All O
models O
struggle O
on O
RNPC B-DatasetName
with O
performance O
around O
chance O
, O
while O
human B-MetricName
accuracy I-MetricName
is O
constantly O
above O
90 B-MetricValue
. O
On O
SPTE O
and O
MPTE O
, O
almost O
all O
models O
have O
a O
high O
false O
- O
positive O
rate O
. O
As O
long O
as O
all O
tokens O
in O
the O
hypothesis O
( O
e.g. O
, O
This O
is O
the O
second O
ball O
) O
appear O
in O
the O
premise O
( O
e.g. O
, O
This O
is O
the O
second O
green O
ball O
) O
, O
they O
tend O
to O
predict O
entailment O
, O
indicating O
that O
they O
are O
making O
the O
same O
intersective O
interpretation O
errors O
as O
children O
do O
. O
On O
EPC B-DatasetName
, O
most O
models O
over O
- O
predict O
equally O
plausible O
, O
arguably O
due O
to O
the O
class O
imbalance O
during O
finetuning O
. O
This O
also O
shows O
that O
our O
task O
is O
not O
trivially O
solvable O
by O
models O
that O
understand O
non O
- O
recursive O
NPs O
, O
which O
the O
finetuning O
dataset O
comprises O
. O

Next O
, O
we O
closely O
examine O
the O
best O
- O
performing O
models O
on O
each O
task O
, O
including O
RoBERTa B-MethodName
- O
large O
finetuned O
on O
MNLI B-DatasetName
, O
GPT3 B-MethodName
- O
curie O
finetuned O
on O
MPE B-DatasetName
, O
and O
RoBERTA B-MethodName
- O
large O
finetuned O
on O
ADEPT B-DatasetName
. O
On O
MPTE O
and O
EPC O
, O
even O
the O
best O
model O
barely O
surpasses O
chance O
performance O
. O
On O
SPTE O
, O
the O
best O
accuracy B-MetricName
( O
61.2 B-MetricValue
) O
is O
still O
unimpressive O
for O
a O
binary O
classification O
task O
. O
To O
understand O
where O
exactly O
the O
models O
fail O
, O
we O
further O
present O
a O
qualitative O
minimal O
- O
pair O
analysis O
in O
Table O
4 O
. O
On O
SPTE O
, O
the O
two O
examples O
differ O
only O
in O
the O
order O
of O
modifiers O
( O
new O
and O
favorite O
) O
in O
the O
premise O
, O
leading O
to O
opposite O
labels O
. O
However O
, O
the O
model O
predicts O
entailment O
for O
both O
, O
suggesting O
its O
insensitivity O
to O
subtle O
meaning O
differences O
incurred O
by O
modifier O
order O
changes O
. O
On O
MPTE O
, O
the O
difference O
between O
the O
two O
examples O
lies O
in O
the O
modifier O
in O
the O
hypothesis O
, O
an O
Ameri O
- O
can O
man O
vs. O
a O
short O
man O
. O
As O
basketball O
players O
are O
generally O
tall O
, O
the O
second O
hypothesis O
should O
not O
be O
entailed O
. O
Again O
, O
the O
model O
predicts O
entailment O
for O
both O
cases O
, O
which O
shows O
its O
lack O
of O
relevant O
world O
knowledge O
. O
Finally O
, O
on O
EPC O
, O
a O
dead O
dangerous O
animal O
and O
a O
dangerous O
dead O
animal O
have O
subtly O
different O
meanings O
-the O
former O
refers O
to O
a O
dangerous O
animal O
that O
is O
dead O
( O
e.g. O
, O
a O
dead O
lion O
, O
which O
is O
no O
longer O
harmful O
to O
people O
) O
, O
while O
the O
latter O
refers O
to O
a O
dead O
animal O
that O
has O
become O
dangerous O
( O
e.g. O
, O
a O
dead O
squirrel O
carrying O
viruses O
, O
which O
is O
indeed O
harmful O
) O
. O
The O
model O
fails O
to O
distinguish O
between O
them O
, O
predicting O
less O
plausible O
for O
both O
. O
All O
the O
above O
observations O
show O
that O
the O
knowledge O
for O
interpreting O
recursive O
NPs O
is O
not O
present O
in O
LM O
representations O
. O

Can O
LMs O
Learn O
the O
Meaning O
of O

Recursive O
NPs O
? O

We O
investigate O
the O
reasons O
behind O
the O
models O
' O
low O
performance O
on O
RNPC B-DatasetName
, O
specifically O
whether O
their O
failure O
is O
due O
to O
the O
lack O
of O
in O
- O
domain O
training O
data O
or O
an O
intrinsic O
deficiency O
in O
their O
architecture O
. O
Furthermore O
, O
SPTE O
may O
be O
the O
easiest O
task O
, O
since O
it O
only O
requires O
local O
knowledge O
about O
the O
meaning O
of O
the O
modifiers O
and O
the O
noun O
. O
By O
contrast O
, O
MPTE O
and O
EPC O
involve O
world O
knowledge O
( O
e.g. O
, O
basketball O
players O
are O
generally O
tall O
among O
the O
population O
) O
, O
as O
well O
as O
global O
reasoning O
between O
components O
in O
a O
sentence O
( O
e.g. O
, O
the O
relationship O
between O
the O
event O
and O
the O
modifiers O
) O
, O
which O
may O
explain O
the O
remaining O
large O
gap O
between O
model O
and O
human O
performance O
( O
> O
90 O
) O
. O

What O
can O
LMs O
learn O
from O
RNPC B-DatasetName
? O

Given O
that O
the O
target O
knowledge O
is O
learnable O
, O
we O
now O
address O
question O
( O
c O
) O
: O
What O
linguistic O
features O
have O
the O
models O
learned O
from O
RNPC B-DatasetName
? O
We O
probe O
for O
two O
features O
extensively O
studied O
in O
the O
relevant O
literature O
( O
cf O
. O
§ O
2 O
) O
, O
using O
different O
techniques O
. O
Modifier O
semantic O
category O
. O
We O
first O
investigate O
if O
models O
have O
learned O
the O
semantic O
category O
of O
modifiers O
using O
the O
" O
edge O
probing O
technique O
" O
( O
Tenney O
et O
al O
. O
, O
2019 O
) O
. O
Namely O
, O
each O
modifier O
is O
categorized O
as O
intersective O
, O
subsective O
, O
or O
privative O
( O
McCrae O
et O
al O
. O
, O
2014 O
individual O
modifiers O
is O
an O
important O
factor O
in O
determining O
the O
meaning O
of O
the O
entire O
NP O
. O
Given O
a O
finetuned O
model O
, O
we O
take O
the O
contextualized O
representation O
of O
each O
modifier O
in O
the O
last O
hidden O
layer O
. O
Then O
, O
we O
attach O
a O
linear O
head O
on O
top O
of O
the O
token O
representation O
as O
an O
" O
auxiliary O
classifier O
" O
. O
We O
choose O
linear O
classifiers O
because O
more O
expressive O
ones O
like O
Multi O
- O
Layer O
Perceptron O
are O
more O
likely O
to O
capture O
the O
target O
feature O
themselves O
( O
Hewitt O
and O
Liang O
, O
2019 O
) O
. O
The O
token O
representations O
are O
then O
frozen O
, O
while O
the O
linear O
head O
is O
trained O
to O
predict O
the O
semantic O
category O
of O
the O
modifiers O
. O
13 O
We O
probe O
the O
models O
finetuned O
on O
RNPC B-DatasetName
from O
Section O
6 O
, O
as O
well O
as O
the O
models O
finetuned O
on O
existing O
benchmarks O
for O
comparison O
. O
The O
results O
are O
shown O
in O
Figure O
4 O
. O
For O
all O
tasks O
, O
the O
probing O
accuracy O
is O
higher O
for O
models O
finetuned O
on O
RNPC B-DatasetName
than O
on O
existing O
benchmarks O
. O
The O
increase O
is O
small O
for O
SPTE O
( O
3.4 O
) O
and O
MPTE O
( O
2.8 O
) O
, O
but O
more O
obvious O
for O
EPC O
( O
7.1 O
) O
. O
This O
is O
somewhat O
counter O
- O
intuitive O
since O
modifier O
category O
is O
defined O
in O
terms O
of O
entailment O
patterns O
, O
but O
models O
learn O
it O
better O
from O
EPC O
than O
from O
TE O
tasks O
. O
Nonetheless O
, O
the O
overall O
trend O
shows O
that O
models O
can O
learn O
the O
semantic O
category O
of O
modifiers O
to O
some O
extent O
after O
being O
finetuned O
on O
our O
datasets O
. O
Since O
the O
absolute O
increase O
is O
limited O
, O
we O
plan O
to O
explore O
ways O
to O
quantify O
the O
actual O
amount O
of O
learned O
knowledge O
in O
future O
work O
. O
Modifier O
scope O
. O
We O
also O
probe O
for O
the O
scope O
of O
the O
first O
modifier O
( O
M O
1 O
) O
in O
recursive O
NPs O
( O
Det O
M O
1 O
M O
2 O
N O
) O
. O
Specifically O
, O
we O
focus O
on O
privative O
M O
1 O
's O
, O
since O
they O
can O
have O
different O
scopes O
when O
interacting O
with O
different O
M O
2 O
's O
and O
N O
's O
. O
For O
instance O
, O
in O
the O
NP O
a O
former O
American O
diplomat O
, O
former O
negates O
diplomat O
( O
N O
) O
, O
but O
the O
person O
is O
still O
American O
; O
while O
in O
a O
former O
beginner O
drummer O
, O
it O
negates O
beginner O
( O
M O
2 O
) O
, O
but O
the O
person O
may O
still O
be O
Figure O
5 O
: O
A O
case O
study O
of O
modifier O
scope O
. O
Each O
subfigure O
shows O
the O
frequency O
distribution O
of O
the O
attention O
ratio O
r O
( O
0 O
< O
r O
< O
1 O
) O
for O
an O
M O
1 O
, O
divided O
into O
two O
sides O
at O
0.5 O
. O
The O
M O
2 O
side O
contains O
NPs O
where O
M O
1 O
attends O
more O
to O
M O
2 O
than O
to O
N O
; O
vice O
versa O
for O
the O
N O
side O
. O
a O
drummer O
. O
14 O
This O
difference O
can O
not O
be O
captured O
by O
the O
semantic O
category O
of O
former O
. O

As O
a O
proxy O
for O
the O
scope O
of O
M O
1 O
, O
we O
use O
attention O
visualization O
, O
a O
widely O
adopted O
technique O
to O
study O
token O
correlations O
( O
Vig O
, O
2019 O
) O
. O
15 O
We O
choose O
BERT B-MethodName
- O
base O
finetuned O
on O
200 O
MPTE O
examples O
from O
Section O
6 O
as O
the O
model O
to O
be O
probed O
for O
a O
case O
study O
. O

Let O
us O
denote O
any O
token O
in O
a O
given O
NP O
as O
x. O
We O
define O
A O
x O
, O
the O
average O
of O
the O
weights O
of O
all O
attention O
heads O
from O
M O
1 O
to O
x O
in O
the O
final O
layer O
, O
representing O
how O
much O
M O
1 O
attends O
to O
token O
x. O
We O
then O
calculate O
the O
ratio O
r O
= O
A O
N O
/ O
( O
A O
N O
+ O
A O
M O
2 O
) O
( O
0 O
< O
r O
< O
1 O
) O
. O
If O
r O
< O
0.5 O
, O
then O
M O
1 O
attends O
more O
to O
M O
2 O
; O
else O
, O
M O
1 O
attends O
more O
to O
N. O
For O
each O
privative O
modifier O
, O
we O
take O
all O
NPs O
containing O
it O
in O
the O
M O
1 O
position O
in O
our O
dataset O
and O
plot O
the O
distribution O
of O
r. O
Figure O
5 O
shows O
three O
examples O
( O
alleged O
, O
counterfeit O
, O
or O
fraudulent O
) O
representing O
different O
patterns O
. O

As O
shown O
in O
the O
first O
sub O
- O
figure O
, O
alleged O
attends O
more O
to O
either O
M O
2 O
and O
N O
depending O
on O
the O
NP O
. O
For O
example O
, O
it O
attends O
more O
to O
M O
2 O
in O
an O
alleged O
antique O
bowl O
( O
0.454 O
) O
, O
since O
the O
NP O
describes O
a O
bowl O
14 O
Admittedly O
, O
there O
can O
be O
alternative O
interpretations O
: O
say O
, O
one O
can O
also O
imagine O
that O
a O
former O
beginner O
drummer O
describes O
a O
person O
who O
is O
no O
longer O
a O
drummer O
at O
all O
. O
However O
, O
in O
that O
case O
, O
it O
is O
enough O
to O
say O
a O
former O
drummer O
instead O
, O
considering O
the O
Gricean O
maxim O
of O
quantity O
. O
Therefore O
, O
here O
we O
still O
focus O
on O
the O
first O
interpretation O
, O
which O
is O
more O
straightforward O
. O
15 O
There O
have O
been O
recent O
debates O
on O
the O
faithfulness O
of O
this O
method O
( O
Jain O
and O
Wallace O
, O
2019 O
; O
Wiegreffe O
and O
Pinter O
, O
2019 O
) O
. O
Therefore O
, O
we O
do O
not O
use O
attention O
weights O
to O
make O
claims O
about O
how O
our O
models O
work O
, O
but O
only O
what O
they O
capture O
, O
with O
attention O
weights O
. O
that O
may O
not O
be O
antique O
. O
Inversely O
, O
an O
alleged O
male O
criminal O
is O
on O
the O
N O
side O
( O
0.517 O
) O
, O
since O
they O
are O
most O
likely O
male O
but O
may O
not O
be O
a O
criminal O
. O

The O
second O
sub O
- O
figure O
indicates O
that O
counterfeit O
mainly O
attends O
to O
M O
2 O
. O
For O
instance O
, O
a O
counterfeit O
Hollywood O
movie O
( O
0.382 O
) O
is O
still O
a O
movie O
, O
but O
is O
probably O
not O
made O
in O
Hollywood O
. O
This O
is O
similar O
to O
the O
cases O
of O
luxury O
bag O
, O
medical O
drugs O
, O
foreign O
cigarettes O
, O
etc O
. O
On O
the O
contrary O
, O
fraudulent O
mainly O
attends O
to O
N O
, O
as O
shown O
in O
the O
third O
sub O
- O
figure O
. O
The O
fraudulent O
medical O
claims O
( O
0.559 O
) O
are O
not O
valid O
claims O
but O
still O
on O
medical O
grounds O
. O
The O
same O
holds O
for O
electoral O
victory O
, O
medical O
excuse O
, O
etc O
. O

Additionally O
, O
we O
notice O
that O
there O
are O
some O
boundary O
cases O
close O
to O
the O
r O
= O
0.5 O
division O
line O
, O
like O
ruthless O
criminal O
and O
former O
thief O
in O
the O
alleged O
sub O
- O
figure O
. O
A O
plausible O
explanation O
is O
that O
M O
1 O
is O
questioning O
both O
M O
2 O
and O
N O
in O
these O
cases O
( O
e.g. O
, O
an O
alleged O
ruthless O
criminal O
is O
not O
necessarily O
ruthless O
or O
a O
criminal O
) O
. O
Overall O
, O
the O
above O
results O
indicate O
that O
models O
finetuned O
on O
our O
tasks O
can O
capture O
modifier O
scope O
in O
recursive O
NPs O
. O

Is O
RNPC B-DatasetName
useful O
for O
downstream O
tasks O
? O

We O
finally O
address O
question O
( O
d O
) O
: O
How O
can O
such O
knowledge O
benefit O
downstream O
tasks O
? O
We O
choose O
the O
task O
of O
Harm O
Detection O
( O
Banko O
et O
al O
. O
, O
2020 O
) O
for O
extrinsic O
evaluation O
. O
Concretely O
, O
we O
consider O
the O
scenario O
where O
a O
user O
interacts O
with O
a O
taskoriented O
agent O
like O
Siri O
or O
Alexa O
, O
and O
the O
agent O
needs O
to O
determine O
whether O
the O
involved O
activity O
in O
the O
user O
query O
is O
potentially O
harmful O
. O
The O
definition O
of O
" O
harm O
" O
can O
be O
user O
- O
dependent O
. O
Here O
, O
we O
consider O
an O
activity O
to O
be O
harmful O
if O
it O
may O
cause O
pain O
, O
physical O
injury O
, O
or O
be O
illegal O
for O
minors O
. O
We O
choose O
this O
task O
because O
many O
false O
positives O
come O
from O
recursive O
NPs O
. O
For O
example O
, O
how O
to O
make O
a O
homemade O
bomb O
is O
obviously O
harmful O
while O
how O
to O
make O
a O
homemade O
bath O
bomb O
is O
harmless O
. O

We O
collect O
a O
small O
test O
set O
from O
wikiHow O
, O
a O
website O
of O
how O
- O
to O
articles O
. O
Each O
article O
title O
is O
considered O
a O
query O
( O
e.g. O
, O
how O
to O
make O
a O
cake O
) O
. O
Then O
, O
we O
compile O
a O
list O
of O
74 O
keywords O
about O
harmful O
entities O
( O
e.g. O
, O
bomb O
, O
fire O
, O
drugs O
) O
, O
only O
12 O
of O
which O
occur O
in O
RNPC B-DatasetName
. O
We O
then O
select O
wikiHow O
queries O
containing O
at O
least O
an O
NP O
with O
one O
of O
the O
74 O
keywords O
as O
the O
head O
noun O
, O
and O
sample O
a O
small O
subset O
for O
manual O
annotation O
. O
Each O
query O
is O
labeled O
as O
harmful O
or O
harmless O
, O
depending O
on O
whether O
it O
involves O
a O
harmful O
activity O
as O
defined O
above O
. O
After O
data O
cleaning O
and O
re O
- O
balancing O
, O
we O
obtain O
170 O
queries O
, O
with O
a O
1:1 O
positive O
/ O
negative O
ratio O
. O
We O
design O
two O
zero O
- O
shot O
harm O
classifiers O
using O
models O
finetuned O
on O
our O
entire O
SPTE O
and O
EPC O
dataset O
. O
They O
share O
a O
few O
pre O
- O
processing O
steps O
: O
first O
, O
all O
NPs O
are O
extracted O
from O
the O
input O
query O
; O
then O
, O
NPs O
containing O
a O
keyword O
from O
our O
list O
in O
the O
head O
noun O
position O
are O
retained O
. O
For O
each O
retained O
NP O
( O
e.g. O
, O
a O
water O
gun O
) O
, O
we O
check O
if O
it O
is O
indeed O
a O
harmful O
entity O
using O
either O
the O
SPTE O
or O
the O
EPC O
model O
. O
The O
input O
to O
the O
SPTE O
model O
is O
a O
premise O
of O
the O
form O
" O
This O
is O
{ O
NP O
} O
" O
( O
e.g. O
, O
This O
is O
a O
water O
gun O
) O
and O
a O
hypothesis O
of O
the O
form O
" O
This O
is O
( O
a O
/ O
an O
) O
{ O
N O
} O
" O
( O
e.g. O
, O
This O
is O
a O
gun O
) O
. O
If O
the O
output O
label O
is O
entailment O
, O
we O
classify O
the O
query O
as O
harmful O
, O
otherwise O
harmless O
. O
Likewise O
, O
using O
the O
EPC O
model O
, O
we O
form O
two O
events O
given O
the O
retained O
NP O
: O
" O
( O
A O
/ O
An O
) O
{ O
N O
} O
is O
harmful O
" O
and O
" O
{ O
NP O
} O
is O
harmful O
" O
. O
If O
the O
second O
event O
is O
predicted O
as O
more O
or O
equally O
plausible O
compared O
to O
the O
first O
, O
the O
query O
is O
considered O
harmful O
. O

We O
compare O
our O
two O
classifiers O
to O
a O
simple O
baseline O
that O
always O
predicts O
harmful O
as O
well O
as O
to O
three O
GPT3 B-MethodName
models O
. O
16 O
Both O
classifiers O
meaningfully O
exceed O
the O
simple O
baseline O
, O
and O
the O
EPCbased O
classifier O
outperforms O
all O
the O
other O
methods O
by O
10 O
+ O
in O
terms O
of O
accuracy O
and O
F O
1 O
. O
This O
shows O
that O
the O
understanding O
of O
recursive O
NPs O
is O
beneficial O
for O
downstream O
tasks O
without O
any O
training O
data O
. O
To O
understand O
why O
EPC O
is O
more O
suitable O
than O
SPTE O
for O
this O
task O
, O
we O
further O
examine O
the O
errors O
they O
make O
. O
One O
major O
error O
type O
concerns O
polysemous O
keywords O
such O
as O
shot O
. O
For O
instance O
, O
the O
SPTE O
model O
mistakenly O
predicts O
how O
to O
have O
a O
good O
basketball O
shot O
to O
be O
harmful O
because O
a O
good O
basketball O
shot O
is O
still O
a O
shot O
( O
shot O
can O
mean O
both O
" O
shooting O
a O
gun O
" O
and O
" O
shooting O
a O
ball O
" O
) O
. O
There O
are O
also O
some O
queries O
out O
of O
the O
scope O
of O
the O
EPC O
model O
, O
e.g. O
, O
how O
to O
make O
a O
sake O
bomb O
. O
Since O
sake O
bomb O
is O
a O
cocktail O
, O
the O
gold O
label O
is O
harmful O
as O
our O
target O
users O
are O
minors O
. O
The O
EPC O
model O
correctly O
predicts O
that O
a O
sake O
bomb O
is O
less O
harmful O
than O
a O
bomb O
, O
but O
fails O
to O
capture O
that O
it O
may O
still O
be O
16 O
Used O
in O
a O
zero O
- O
shot O
setting O
; O
see O
Appendix O
E.4 O
for O
details O
. O
harmful O
( O
for O
minors O
) O
. O

Conclusion O

We O
introduce O
RNPC B-DatasetName
, O
a O
challenge O
set O
targeting O
the O
understanding O
of O
recursive O
NPs O
, O
a O
fundamental O
aspect O
of O
human O
common O
sense O
. O
Pretrained O
LMs O
with O
SOTA O
performance O
on O
Natural O
Language O
Understanding O
benchmarks O
have O
poor O
mastery O
of O
this O
knowledge O
, O
but O
can O
still O
learn O
it O
when O
exposed O
to O
small O
amounts O
of O
data O
from O
RNPC B-DatasetName
. O
Using O
different O
probing O
techniques O
, O
we O
show O
that O
models O
can O
learn O
relevant O
linguistic O
features O
, O
including O
modifier O
category O
and O
scope O
, O
from O
RNPC B-DatasetName
. O
They O
also O
achieve O
strong O
zero O
- O
shot O
performance O
on O
an O
extrinsic O
Harm O
Detection O
task O
, O
indicating O
the O
transferability O
of O
this O
knowledge O
. O
For O
future O
work O
, O
we O
hope O
to O
investigate O
other O
linguistic O
phenomena O
as O
a O
step O
towards O
comprehensively O
characterizing O
LMs O
' O
limitations O
and O
capabilities O
in O
language O
understanding O
. O

A O
Dataset O
Construction O
Details O

A.1 O
RNPC B-DatasetName
Statistics O
NPs O
. O
RNPC B-DatasetName
has O
1,299 O
NPs O
. O
For O
an O
NP O
in O
the O
form O
of O
Det O
M O
1 O
M O
2 O
N O
, O
the O
two O
modifiers O
M O
1 O
and O
M O
2 O
can O
each O
belong O
to O
one O
of O
three O
possible O
semantic O
categories O
( O
intersective O
, O
subsective O
, O
or O
privative O
) O
, O
resulting O
in O
nine O
possible O
combinations O
. O
We O
plot O
the O
distribution O
of O
NPs O
with O
different O
combinations O
in O
RNPC B-DatasetName
in O
Table O
6 O
. O
Note O
that O
the O
distribution O
is O
not O
balanced O
because O
certain O
categories O
( O
e.g. O
, O
NPs O
containing O
privative O
modifiers O
) O
yield O
many O
more O
minority O
class O
examples O
for O
our O
three O
tasks O
( O
e.g. O
, O
non O
- O
entailment O
in O
SPTE O
) O
. O
Thus O
, O
considering O
the O
final O
class O
balance O
in O
RNPC B-DatasetName
tasks O
, O
we O
include O
more O
NPs O
of O
certain O
categories O
. O
Training O
and O
test O
sets O
for O
finetuning O
. O
In O
the O
experiment O
where O
we O
finetune O
models O
on O
RNPC B-DatasetName
, O
described O
in O
Section O
6 O
, O
we O
split O
again O
the O
dataset O
for O
each O
task O

A.2 O
Crowdsourcing O
Details O

In O
the O
construction O
of O
RNPC B-DatasetName
, O
we O
hire O
college O
students O
as O
crowdworkers O
for O
instance O
creation O
and O
label O
verification O
. O
Specifically O
, O
they O
are O
undergraduate O
and O
graduate O
students O
in O
an O
Artificial O
Intelligence O
class O
( O
CIS O
421 O
/ O
521 O
and O
MCIT O
521 O
at O
the O
University O
of O
Pennsylvania O
) O
, O
with O
good O
English O
proficiency O
. O
Both O
tasks O
are O
given O
as O
optional O
extra O
credit O
assignments O
in O
the O
class O
. O
Participation O
is O
solely O
voluntary O
. O
Before O
participation O
, O
students O
can O
preview O
the O
tasks O
, O
and O
are O
given O
a O
clear O
description O
of O
how O
the O
data O
will O
be O
used O
at O
the O
beginning O
of O
the O
instructions O
. O

During O
instance O
creation O
, O
we O
provide O
detailed O
instructions O
on O
how O
to O
write O
high O
- O
quality O
examples O
for O
each O
task O
, O
which O
can O
be O
found O
in O
the O
Supplementary O
Materials O
. O
Annotations O
are O
collected O
via O
Google O
Forms O
. O
With O
100 O
valid O
instances O
( O
equivalent O
to O
2.5 O
- O
4.75 O
hours O
of O
work O
, O
depending O
on O
their O
proficiency O
) O
, O
students O
can O
earn O
1 O
% O
in O
extra O
credit O
of O
the O
overall O
course O
grade O
. O

During O
label O
verification O
, O
we O
host O
our O
questions O
on O
Amazon O
Mechanical O
Turk O
. O
We O
design O
a O
HIT O
type O
for O
each O
RNPC B-DatasetName
task O
, O
which O
is O
also O
included O
in O
the O
Supplementary O
Materials O
. O
With O
600 O
correctly O
answered O
questions O
( O
equivalent O
to O
3.5 O
- O
4 O
hours O
of O

M O
1 O
/ O
M O
2 O
Int O
. O
Sub O
. O
Pri O
. O
Int O
. O

13 O
37 O
74 O
Sub O
. O

138 O
109 O
162 O
Pri O
. O

99 O
420 O
250 O
work O
) O
, O
students O
can O
earn O
1 O
% O
in O
extra O
credit O
of O
the O
overall O
course O
grade O
. O
We O
calculate O
the O
interannotator O
agreement O
using O
Krippendorff O
's O
alpha O
. O
17 O
The O
agreement O
is O
0.843 O
for O
SPTE O
, O
0.575 O
for O
MPTE O
, O
and O
0.933 O
for O
EPC O
. O

A.3 O
Debiasing O
and O
Anonymization O

The O
collected O
data O
does O
not O
contain O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
. O
We O
ensure O
this O
by O
1 O
) O
manually O
reviewing O
the O
set O
of O
extracted O
NPs O
from O
corpora O
, O
and O
filtering O
out O
any O
NP O
that O
contains O
any O
sensitive O
/ O
offensive O
information O
, O
2 O
) O
not O
requesting O
any O
personal O
information O
during O
human O
annotation O
, O
and O
3 O
) O
manually O
reviewing O
each O
RNPC B-DatasetName
example O
written O
by O
the O
human O
participants O
. O

B O
Existing O
Benchmarks O
for O
Finetuning O

We O
use O
the O
following O
benchmark O
datasets O
for O
finetuning O
. O
Each O
of O
them O
has O
the O
same O
format O
as O
one O
of O
our O
RNPC B-DatasetName
tasks O
. O
Table O
8 O
shows O
the O
number O
of O
examples O
in O
each O
dataset O
. O
MNLI B-DatasetName
. O
The O
Multi B-DatasetName
- I-DatasetName
Genre I-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
corpus I-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
2017 O
) O
introduce O
a O
Multiple O
Premise O
Entailment O
Task O
dataset O
. O
This O
is O
a O
novel O
textual O
entailment O
task O
that O
requires O
inference O
over O
multiple O
premise O
sentences O
. O
Each O
example O
consists O
of O
four O
premise O
sentences O
( O
captions O
from O
a O
FLICKR30 O
K O
image O
) O
, O
one O
hypothesis O
sentence O
( O
a O
simplified O
FLICKR30 O
K O
caption O
) O
, O
and O
one O
label O
( O
entailment O
, O
neutral O
, O
or O
contradiction O
) O
that O
indicates O
the O
relationship O
between O
the O
set O
of O
four O
premises O
and O
the O
hypothesis O
. O
The O
language O
in O
the O
dataset O
is O
English O
. O
The O
license O
of O
the O
dataset O
is O
unspecified O
. O
ADEPT B-DatasetName
. O
Emami O
et O
al O
. O
( O
2021 O
) O
introduce O
a O
dataset O
of O
the O
Adjective B-DatasetName
- I-DatasetName
Dependent I-DatasetName
Plausibility I-DatasetName
Task I-DatasetName
( O
ADEPT B-DatasetName
) O
. O
Each O
example O
contains O
a O
base O
sentence O
, O
and O
a O
slightly O
modified O
sentence O
obtained O
by O
adding O
an O
adjective O
to O
a O
noun O
in O
the O
base O
sentence O
. O
The O
dataset O
is O
created O
to O
support O
explorations O
into O
how O
certain O
classes O
of O
adjectives O
might O
influence O
the O
plausibility O
of O
events O
depicted O
in O
natural O
language O
sentences O
. O
The O
textual O
data O
come O
from O
Wikipedia B-DatasetName
, O
the O
Common B-DatasetName
Crawl I-DatasetName
, O
and O
ConceptNet B-DatasetName
. O
The O
language O
of O
the O
dataset O
is O
English O
. O
ADEPT B-DatasetName
is O
released O
under O
the O
CC O
BY O
- O
SA O
3.0 O
license O
. O
It O
is O
intended O
to O
be O
used O
only O
for O
research O
, O
exploratory O
evaluation O
, O
and O
auditing O
, O
which O
our O
use O
is O
consistent O
with O
. O

C O
Probing O
Pretrained O
LMs O
C.1 O
Motivation O

When O
addressing O
question O
( O
a O
) O
, O
we O
finetune O
pretrained O
LMs O
on O
existing O
benchmarks O
of O
the O
same O
format O
as O
each O
RNPC B-DatasetName
task O
, O
assuming O
that O
the O
finetuning O
process O
allows O
models O
to O
do O
textual O
inference O
in O
the O
required O
format O
. O
However O
, O
it O
is O
possible O
that O
this O
assumption O
does O
not O
hold O
, O
because O
LMs O
can O
overfit O
the O
finetuning O
data O
beyond O
just O
learning O
the O
format O
. O
Then O
even O
if O
the O
target O
knowledge O
is O
present O
in O
pretrained O
LMs O
, O
catastrophic O
forgetting O
( O
Kemker O
et O
al O
. O
, O
2018 O
) O
can O
happen O
during O
finetuning O
. O

C.2 O
Task O
Conversion O

We O
complement O
Section O
5 O
with O
another O
experiment O
, O
where O
we O
directly O
probe O
pretrained O
LMs O
using O
a O
prompting O
method O
inspired O
by O
the O
line O
of O
work O
on O
LMs O
as O
knowledge O
bases O
( O
Petroni O
et O
al O
. O
, O
2019 O
) O
. O
Specifically O
, O
we O
convert O
each O
RNPC O
task O
to O
a O
likelihood O
comparison O
task O
: O
SPTE O
. O
Given O
the O
original O
formulation O
which O
has O
a O
premise O
and O
a O
hypothesis O
, O
we O
define O
L O
entail O
as O
the O
conditional O
likelihood O
that O
the O
hypothesis O
is O
necessarily O
true O
given O
the O
premise O
, O
assigned O
by O
an O
LM O
. O
Contrarily O
, O
L O
non−entail O
stands O
for O
the O
conditional O
likelihood O
that O
the O
hypothesis O
is O
NOT O
necessarily O
true O
given O
the O
premise O
. O
18 O
If O
L O
entail O
> O
L O
non−entail O
, O
the O
model O
is O
considered O
to O
predict O
entailment O
, O
and O
vice O
versa O
. O
MPTE O
. O
The O
conversion O
method O
is O
the O
same O
as O
that O
for O
SPTE O
, O
except O
that O
in O
the O
conditional O
likelihood O
computation O
, O
we O
now O
consider O
the O
concatenation O
of O
two O
premises O
as O
the O
given O
condition O
. O
EPC O
. O
Given O
the O
original O
formulation O
with O
two O
events O
, O
Event O
1 O
and O
Event O
2 O
, O
we O
define O
L O
1 O
and O
L O
2 O
as O
the O
( O
unconditional O
) O
likelihood O
of O
Event O
1 O
and O
Event O
2 O
assigned O
by O
an O
LM O
, O
respectively O
. O
We O
then O
choose O
a O
threshold O
θ O
, O
19 O
and O
compare O
it O
to O
the O
absolute O
difference O
between O
L O
1 O
and O
L O
2 O
. O
If O
the O
difference O
is O
smaller O
than O
θ O
, O
we O
consider O
the O
model O
prediction O
as O
equally O
likely O
. O
Otherwise O
, O
the O
model O
prediction O
is O
more O
likely O
if O
L O
2 O
is O
higher O
, O
and O
less O
likely O
if O
L O
1 O
is O
higher O
. O
For O
Causal O
LMs O
( O
e.g. O
, O
GPT O
) O
, O
the O
likelihood O
is O
computed O
with O
standard O
left O
- O
to O
- O
right O
language O
modeling O
scores O
. O
For O
Masked O
LMs O
( O
e.g. O
, O
BERT B-MethodName
, O
RoBERTa B-MethodName
, O
BART B-MethodName
) O
, O
the O
likelihood O
is O
computed O
with O
pseudo O
- O
log O
- O
likelihood O
scores O
( O
Salazar O
et O
al O
. O
, O
2020 O
) O
. O

C.3 O
Sanity O
Check O

Before O
evaluating O
LMs O
on O
the O
converted O
RNPC B-DatasetName
, O
we O
perform O
a O
sanity O
check O
to O
see O
if O
our O
formalization O
makes O
sense O
to O
LMs O
, O
i.e. O
, O
whether O
they O
understand O
the O
meaning O
of O
necessarily O
and O
not O
necessarily O
. O
We O
write O
50 O
sentence O
pairs O
for O
likelihood O
comparison O
, O
all O
consisting O
of O
simple O
commonsense O
knowledge O
. O
For O
example O
, O
comparing O
A O
human O
being O
is O
necessarily O
female O
and O
A O
human O
being O
is O
n't O
necessarily O
female O
, O
the O
second O
sentence O
should O
be O
more O
likely O
; O
while O
for O
Humans O
are O
necessarily O
mortal O
and O
Humans O
are O
n't O
necessarily O
mortal O
, O
the O
first O
sentence O
should O
be O
more O
likely O
. O
Such O
comparisons O
do O
not O
require O
any O
knowledge O
about O
recursive O
NPs O
, O
and O
involve O
only O
common O
entities O
and O
facts O
. O
If O
models O
understand O
necessarily O
and O
not O
necessarily O
correctly O
, O
they O
should O
find O
the O
task O
easy O
. O

To O
our O
surprise O
, O
almost O
all O
Masked O
LMs O
we O
test O
( O
BERT B-MethodName
- O
base O
/ O
large O
, O
RoBERTa B-MethodName
- O
base O
/ O
large O
) O
fail O
the O
sanity O
check O
, O
mostly O
performing O
around O
chance O
( O
50 B-MetricValue
accuracy B-MetricName
) O
. O
However O
, O
most O
Causal O
LMs O
( O
GPT-2base B-MethodName
/ O
medium O
/ O
large O
/ O
xl O
, O
GPT-3 B-MethodName
- I-MethodName
ada I-MethodName
) O
reasonably O
perform O
above O
chance O
, O
with O
accuracy B-MetricName
scores O
ranging O
from O
70 B-MetricValue
to O
80 B-MetricValue
. O
We O
suspect O
that O
pseudo O
- O
loglikelihood O
scores O
are O
not O
entirely O
suitable O
for O
our O
purposes O
; O
also O
, O
the O
task O
is O
harder O
than O
expected O
due O
to O
reporting O
bias O
, O
as O
the O
tested O
knowledge O
( O
e.g. O
, O
not O
all O
humans O
are O
female O
) O
is O
potentially O
too O
obvious O
to O
be O
explicitly O
stated O
in O
the O
pretraining O
data O
. O

C.4 O
Results O

We O
evaluate O
LMs O
that O
pass O
the O
sanity O
check O
on O
the O
converted O
RNPC B-DatasetName
, O
and O
report O
their O
performance O
in O
Table O
9 O
. O
Despite O
the O
decent O
performance O
on O
the O
sanity O
check O
examples O
( O
70 O
- O
80 O
) O
, O
the O
accuracy O
on O
RNPC B-DatasetName
is O
remarkably O
lower O
. O
Compared O
to O
our O
original O
results O
of O
probing O
the O
finetuned O
models O
, O
the O
optimal O
performance O
on O
SPTE O
and O
MPTE O
slightly O
improves O
, O
while O
accuracy O
on O
EPC O
decreases O
. O
However O
, O
the O
same O
patterns O
hold O
: O
most O
models O
perform O
around O
or O
slightly O
above O
chance O
, O
with O
a O
large O
difference O
from O
human O
performance O
. O
These O
findings O
further O
strengthen O
our O
answer O
to O
question O
( O
a O
) O
, O
i.e. O
LMs O
do O
not O
inherently O
have O
the O
knowledge O
to O
interpret O
recursive O
NPs O
. O

D O
Full O
Results O

In O
Section O
5 O
, O
we O
evaluate O
SOTA O
LMs O
on O
RNPC B-DatasetName
tasks O
. O
In O
addition O
to O
accuracy O
, O
we O
also O
report O
precision B-MetricName
, O
recall B-MetricName
, O
and O
F-1 B-MetricName
score I-MetricName
here O
. O
Tables O
10 O
, O
11 O
and O
12 O
show O
the O
full O
results O
for O
each O
task O
, O
respectively O
. O

E O
Implementation O
Details O
E.1 O
Models O
Finetuned O
on O
Existing O
Benchmarks O

In O
Section O
5 O
, O
we O
evaluate O
SOTA O
LMs O
finetuned O
on O
existing O
benchmarks O
of O
the O
same O
format O
on O
RNPC B-DatasetName
. O
We O
use O
four O
different O
pretrained O
models O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
, O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
and O
GPT3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
in O
different O
sizes O
. O
The O
first O
three O
are O
implemented O
with O
HuggingFace O
Transformers O
20 O
, O
and O
the O
last O
is O
from O
OpenAI O
's O
standard O
API O
21 O
. O
The O
pretrained O
model O
checkpoints O
we O
use O
include O
: O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
( O
110 O
M O
parameters O
) O
, O
bert B-MethodName
- I-MethodName
large I-MethodName
- I-MethodName
uncased I-MethodName
( O
336 O
M O
parameters O
) O
, O
roberta B-MethodName
- I-MethodName
base I-MethodName
( O
125 O
M O
param O
- O
eters O
) O
, O
roberta B-MethodName
- I-MethodName
large I-MethodName
( O
335 O
M O
parameters O
) O
, O
facebook B-MethodName
/ I-MethodName
bart I-MethodName
- I-MethodName
large I-MethodName
( O
406 O
M O
parameters O
) O
, O
GPT3 B-MethodName
- I-MethodName
ada I-MethodName
( O
350 O
M O
parameters O
) O
, O
and O
GPT3 B-MethodName
- I-MethodName
curie I-MethodName
( O
6.7B O
parameters O
) O
. O
22 O
Their O
licenses O
include O
Apache O
License O
2.0 O
( O
BERT B-MethodName
and O
BART B-MethodName
) O
, O
GNU O
General O
Public O
License O
v2.0 O
( O
RoBERTa B-MethodName
) O
, O
and O
MIT O
license O
( O
GPT3 B-MethodName
) O
. O

Due O
to O
the O
size O
of O
MNLI B-DatasetName
and O
SNLI B-DatasetName
, O
we O
use O
existing O
checkpoints O
available O
on O
the O
Huggingface O
Transformers O
model O
hub O
. O
For O
all O
other O
datasets O
, O
we O
finetune O
the O
pretrained O
models O
using O
the O
SequenceClassification O
pipeline O
on O
Huggingface O
, O
or O
the O
standard O
prompt O
completion O
finetuning O
API O
on O
OpenAI O
. O
23 O
The O
finetuning O
scripts O
are O
adapted O
from O
the O
text O
- O
classification O
example O
in O
the O
HuggingFace O
Transformers O
repository O
. O
24 O
We O
performed O
hyperparameter O
search O
in O
the O
following O
range O
: O

-batch B-HyperparameterName
size O
: O
[ O
4 B-HyperparameterValue
, O
8 B-HyperparameterValue
, O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue
] O
-learning B-HyperparameterName
rate I-HyperparameterName
: O
[ O
1e-5 B-HyperparameterValue
, O
1e-6 B-HyperparameterValue
] O
-number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
: O
[ O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
5 B-HyperparameterValue
] O
-max B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
: O
[ O
64,128 B-HyperparameterValue
] O
The O
optimal O
hyperparameter O
values O
and O
finetuned O
models O
are O
available O
on O
the O
HuggingFace O
model O
hub O
. O

We O
run O
our O
finetuning O
experiments O
on O
an O
NVIDIA O
GeForce O
RTX O
2080 O
Ti O
GPU O
, O
with O
halfprecision O
floating O
point O
format O
( O
FP16 O
) O
. O
The O
finetuning O
takes O
2 O
to O
5 O
hours O
depending O
on O
the O
task O
. O

E.2 O
Models O
Finetuned O
on O
RNPC B-DatasetName

In O
Section O
6 O
, O
we O
address O
the O
question O
of O
whether O
LMs O
can O
learn O
the O
meaning O
of O
recursive O
NPs O
. O
We O
finetune O
each O
model O
from O
Section O
E.1 O
on O
an O
increasing O
number O
of O
examples O
of O
each O
RNPC B-DatasetName
task O
. O
The O
model O
architectures O
, O
the O
pipelines O
used O
, O
the O
range O
of O
hyperparameter O
search O
, O
and O
the O
computing O
resources O
used O
are O
all O
the O
same O
as O
in O
the O
previous O
subsection O
. O
After O
being O
finetuned O
on O
200 O
examples O
, O
the O
best O
performing O
models O
are O
RoBERTalarge B-MethodName
( O
MNLI B-DatasetName
) O
for O
SPTE O
, O
RoBERTa B-MethodName
- I-MethodName
base I-MethodName
( O
MPE B-DatasetName
) O
for O
MPTE O
, O
and O
RoBERTA B-MethodName
- I-MethodName
large I-MethodName
( O
ADEPT B-DatasetName
) O
for O
EPC O
. O
The O
optimal O
hyperparameter O
values O
and O
finetuned O
models O
on O
the O
full O
200 O
examples O
of O
each O
RNPC B-DatasetName
task O
are O
available O
on O
the O
HuggingFace O
model O
hub O
. O

E.3 O
The O
" O
Edge O
Probing O
" O
Method O

In O
Section O
7 O
, O
we O
adopt O
the O
Edge O
Probing O
technique O
from O
Tenney O
et O
al O
. O
( O
2019 O
) O
to O
investigate O
if O
the O
modifier O
category O
feature O
can O
be O
learned O
from O
our O
tasks O
. O

To O
reintroduce O
the O
general O
idea O
of O
this O
method O
, O
consider O
the O
following O
setup O
: O
we O
have O
data O
D O
= O
{ O
( O
x O
1 O
, O
y O
1 O
) O
, O
( O
x O
2 O
, O
y O
2 O
) O
, O
... O
, O
( O
x O
n O
, O
y O
n O
) O
} O
, O
where O
( O
x O
1 O
, O
x O
2 O
, O
... O
, O
x O
n O
) O
are O
the O
model O
representations O
to O
be O
probed O
and O
( O
y O
1 O
, O
y O
2 O
, O
... O
, O
y O
n O
) O
are O
the O
labels O
of O
a O
linguistic O
feature O
we O
are O
interested O
in O
probing O
for O
. O
The O
goal O
is O
to O
see O
if O
( O
x O
1 O
, O
x O
2 O
, O
... O
, O
x O
n O
) O
encodes O
( O
y O
1 O
, O
y O
2 O
, O
... O
, O
y O
n O
) O
. O

In O
our O
case O
, O
given O
an O
NP O
of O
the O
form O
Det O
M O
1 O
M O
2 O
N O
, O
( O
x O
1 O
, O
x O
2 O
, O
... O
, O
x O
n O
) O
are O
the O
token O
representations O
of O
the O
best O
- O
performing O
models O
after O
being O
finetuned O
on O
each O
RNPC B-DatasetName
task O
, O
as O
mentioned O
in O
Section O
E.2 O
, O
and O
( O
y O
1 O
, O
y O
2 O
, O
... O
, O
y O
n O
) O
are O
the O
semantic O
categories O
of O
M O
1 O
and O
M O
2 O
. O

We O
freeze O
the O
representations O
( O
x O
1 O
, O
x O
2 O
, O
... O
, O
x O
n O
) O
, O
and O
attach O
a O
simple O
auxiliary O
classifier O
( O
or O
probing O
classifier O
) O
on O
top O
of O
them O
. O
The O
auxiliary O
classifier O
is O
trained O
to O
predict O
the O
semantic O
category O
y O
i O
corresponding O
to O
every O
modifier O
token O
representation O
x O
i O
. O
In O
our O
case O
, O
we O
choose O
a O
linear O
classifier O
as O
the O
probing O
classifier O
. O
The O
assumption O
is O
that O
the O
expressivity O
of O
the O
auxiliary O
classifier O
should O
be O
limited O
, O
or O
otherwise O
it O
might O
learn O
the O
target O
feature O
itself O
. O
Based O
on O
this O
assumption O
, O
the O
accuracy O
of O
predicting O
y O
indicates O
the O
extent O
to O
which O
the O
model O
representations O
encode O
the O
target O
feature O
. O
The O
method O
is O
illustrated O
in O
Figure O
6 O
. O

Specifically O
, O
the O
linear O
classifier O
is O
an O
SGDClassifier O
implemented O
with O
scikitlearn O
. O
25 O
We O
perform O
hyperparameter O
search O
in O
the O
following O
range O
: O
The O
optimal O
hyperparameter O
values O
for O
the O
bestperforming O
models O
of O
each O
RNPC B-DatasetName
task O
from O
Section O
E.2 O
are O
available O
on O
the O
HuggingFace O
model O
hub O
. O

E.4 O
Models O
Used O
for O
Extrinsic O
Evaluation O

In O
Section O
8 O
, O
one O
of O
the O
models O
we O
compare O
to O
is O
GPT3 B-MethodName
. O
We O
exploit O
three O
versions O
of O
different O
sizes O
, O
GPT3 B-MethodName
- I-MethodName
ada I-MethodName
, O
GPT3 B-MethodName
- I-MethodName
curie I-MethodName
, O
and O
GPT3 B-MethodName
- I-MethodName
davinci I-MethodName
, O
using O
the O
same O
API O
described O
in O
Section O
E.1 O
. O
The O
models O
are O
evaluated O
in O
a O
zeroshot O
setting O
, O
where O
we O
only O
provide O
the O
following O
task O
description O
: O

Given O
a O
" O
how O
- O
to O
" O
query O
, O
determine O
if O
the O
task O
is O
harmful O
to O
a O
teenager O
. O
The O
answer O
should O
be O
Yes O
or O
No O
. O
Each O
input O
question O
is O
then O
formatted O
as O
: O
Question O
: O
Is O
" O
{ O
how O
- O
to O
query O
} O
" O
harmful O
? O
Answer O
: O

For O
every O
how O
- O
to O
query O
in O
our O
test O
set O
, O
we O
use O
the O
concatenation O
of O
the O
task O
description O
and O
the O
instantiated O
input O
question O
as O
the O
prompt O
, O
and O
let O
the O
model O
generate O
a O
one O
- O
token O
continuation O
. O
The O
top O
generated O
token O
is O
always O
Yes O
or O
No O
, O
implying O
that O
GPT3 B-MethodName
has O
a O
good O
understanding O
of O
the O
task O
format O
. O

F O
Ethical O
Considerations O

F.1 O
Limitations O

Assumptions O
. O
One O
assumption O
we O
make O
in O
answering O
question O
( O
a O
) O
is O
that O
LMs O
finetuned O
on O
existing O
benchmarks O
can O
learn O
the O
required O
format O
without O
overfitting O
the O
specific O
domains O
of O
the O
finetuning O
data O
. O
Suppose O
this O
assumption O
does O
not O
hold O
, O
then O
even O
if O
the O
target O
knowledge O
is O
present O
in O
pretrained O
LMs O
, O
they O
can O
" O
forget O
" O
it O
during O
finetuning O
. O
Therefore O
, O
the O
finetuning O
process O
does O
not O
allow O
us O
to O
elicit O
the O
target O
knowledge O
from O
pretrained O
LMs O
. O
To O
address O
this O
issue O
, O
we O
complement O
the O
behavioral O
test O
probing O
method O
with O
another O
experiment O
to O
directly O
probe O
the O
pretrained O
LMs O
via O
likelihood O
scoring O
. O
See O
Section O
C O
for O
details O
. O

Another O
assumption O
occurs O
in O
our O
answer O
to O
question O
( O
d O
) O
. O
We O
assume O
that O
a O
query O
is O
harmful O
if O
it O
contains O
a O
harmful O
entity O
. O
However O
, O
in O
practice O
, O
there O
can O
be O
queries O
like O
How O
to O
prevent O
a O
fire O
, O
which O
does O
contain O
a O
harmful O
entity O
( O
fire O
) O
but O
is O
precautionary O
instead O
of O
harmful O
. O
Our O
model O
does O
not O
take O
into O
account O
factors O
like O
predicates O
in O
context O
, O
and O
will O
therefore O
identify O
all O
such O
cases O
as O
false O
positives O
. O
Scope O
of O
claims O
. O
Our O
first O
three O
claims O
( O
i.e. O
answers O
to O
question O
( O
a O
) O
- O
( O
c O
) O
) O
are O
only O
verified O
to O
hold O
on O
the O
RNPC B-DatasetName
dataset O
, O
which O
1 O
) O
is O
in O
English O
and O
2 O
) O
mainly O
consists O
of O
NPs O
in O
the O
news O
domain O
. O
Our O
last O
claim O
( O
i.e. O
answer O
to O
question O
( O
d O
) O
) O
is O
only O
verified O
to O
hold O
on O
the O
harm O
detection O
dataset O
we O
collect O
, O
which O
1 O
) O
is O
also O
in O
English O
, O
2 O
) O
consists O
of O
howto O
queries O
in O
the O
domain O
of O
human O
activities O
, O
and O
3 O
) O
is O
annotated O
based O
on O
a O
non O
- O
exhaustive O
keyword O
list O
of O
harmful O
entities O
. O

Moreover O
, O
part O
of O
our O
answer O
to O
question O
( O
b O
) O
( O
i.e. O
LMs O
have O
learned O
the O
feature O
of O
modifier O
semantic O
category O
from O
RNPC O
) O
is O
qualitative O
. O
The O
absolute O
increase O
in O
the O
probing O
accuracy O
after O
finetuning O
is O
limited O
, O
so O
it O
is O
likely O
not O
the O
entire O
picture O
. O
Quantifying O
to O
what O
extent O
LMs O
have O
learned O
this O
feature O
is O
an O
interesting O
direction O
for O
future O
work O
. O

F.2 O
Risks O

The O
risks O
associated O
with O
the O
study O
are O
minimal O
. O
Harm O
detection O
models O
. O
Our O
harm O
detection O
models O
are O
intended O
for O
research O
purposes O
only O
. O
They O
are O
designed O
for O
specific O
types O
of O
harmful O
queries O
, O
i.e. O
those O
with O
harmful O
entities O
. O
One O
should O
not O
deploy O
them O
directly O
in O
real O
life O
since O
they O
are O
by O
no O
means O
applicable O
under O
all O
scenarios O
. O
Data O
collection O
. O
Our O
human O
participants O
may O
experience O
slight O
discomfort O
due O
to O
boredom O
during O
data O
collection O
. O
To O
minimize O
this O
, O
we O
make O
sure O
that O
it O
is O
entirely O
voluntary O
to O
participate O
and O
discontinue O
at O
any O
time O
. O

F.3 O
Intended O
Use O

Our O
models O
and O
data O
should O
be O
used O
for O
research O
purposes O
only O
. O
They O
should O
not O
be O
deployed O
in O
the O
real O
world O
as O
anything O
other O
than O
a O
research O
prototype O
, O
especially O
commercially O
. O

Acknowledgments O

This O
research O
is O
based O
upon O
work O
supported O
in O
part O
by O
the O
DARPA O
KAIROS O
Program O
( O
contract O
FA8750 O
- O
19 O
- O
2 O
- O
1004 O
) O
, O
the O
DARPA O
LwLL O
Program O
( O
contract O
FA8750 O
- O
19 O
- O
2 O
- O
0201 O
) O
, O
and O
the O
IARPA O
BET O
- O
TER O
Program O
( O
contract O
2019 O
- O
19051600004 O
) O
. O
Approved O
for O
Public O
Release O
, O
Distribution O
Unlimited O
. O
The O
views O
and O
conclusions O
contained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
official O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
DARPA O
, O
IARPA O
, O
or O
the O
U.S. O
Government O
. O

Special O
thanks O
go O
to O
our O
annotators O
, O
students O
in O
CIS O
421 O
/ O
521 O
and O
MCIT O
521 O
at O
the O
University O
of O
Pennsylvania O
. O
We O
also O
thank O
Artemis O
Panagopoulou O
for O
providing O
the O
extrinsic O
evaluation O
data O
. O
Meanwhile O
, O
we O
appreciate O
the O
support O
from O
OpenAI O
on O
finetuning O
GPT-3 O
. O
Finally O
, O
we O
thank O
Haochen O
Zhang O
, O
Pengyuan O
Lu O
, O
Daniel O
Deutsch O
, O
Daphne O
Ippolito O
, O
Lara O
Martin O
, O
Young O
- O
Min O
Cho O
, O
Yi O
Zhang O
, O
Helen O
Jin O
, O
Siyi O
Liu O
, O
Eleni O
Miltsakaki O
, O
Jordan O
Kodner O
, O
Mingming O
Liu O
, O
Peng O
Zhou O
, O
Christopher O
Cieri O
, O
James O
J. O
Fiumara O
, O
Ellie O
Pavlick O
, O
Charles O
Yang O
, O
Yejin O
Choi O
, O
Alexander O
Koller O
, O
Chris O
Potts O
, O
and O
Mitch O
Marcus O
for O
their O
valuable O
feedback O
. O

kNN B-MethodName
- I-MethodName
Prompt I-MethodName
: O
Nearest O
Neighbor O
Zero O
- O
Shot O
Inference O

Retrieval B-MethodName
- I-MethodName
augmented I-MethodName
language I-MethodName
models I-MethodName
( O
LMs O
) O
use O
non O
- O
parametric O
memory O
to O
substantially O
outperform O
their O
non O
- O
retrieval O
counterparts O
on O
perplexity O
- O
based O
evaluations O
, O
but O
it O
is O
an O
open O
question O
whether O
they O
achieve O
similar O
gains O
in O
few O
- O
and O
zero O
- O
shot O
end O
- O
task O
accuracy B-MetricName
. O
We O
extensively O
study O
one O
such O
model O
, O
the O
k B-MethodName
- I-MethodName
nearest I-MethodName
neighbor I-MethodName
LM I-MethodName
( O
kNN B-MethodName
- I-MethodName
LM I-MethodName
) O
, O
showing O
that O
the O
gains O
marginally O
transfer O
. O
The O
main O
challenge O
is O
to O
achieve O
coverage O
of O
the O
verbalizer O
tokens O
that O
define O
the O
different O
end O
- O
task O
class O
labels O
. O
To O
address O
this O
challenge O
, O
we O
also O
introduce O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
, O
a O
simple O
and O
effective O
kNN B-MethodName
- I-MethodName
LM I-MethodName
with O
automatically O
expanded O
fuzzy O
verbalizers O
( O
e.g. O
to O
expand O
" O
terrible O
" O
to O
also O
include O
" O
silly O
" O
and O
other O
task O
- O
specific O
synonyms O
for O
sentiment O
classification O
) O
. O
Across O
nine O
diverse O
end O
- O
tasks O
, O
using O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
with O
GPT-2 B-MethodName
large I-MethodName
yields O
significant O
performance O
boosts O
over O
strong O
zeroshot O
baselines O
( O
13.4 B-MetricValue
% I-MetricValue
absolute O
improvement O
over O
the O
base O
LM O
on O
average O
) O
. O
We O
also O
show O
that O
other O
advantages O
of O
non O
- O
parametric O
augmentation O
hold O
for O
end O
tasks O
; O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
is O
effective O
for O
domain O
adaptation O
with O
no O
further O
training O
, O
and O
gains O
increase O
with O
the O
size O
of O
the O
retrieval O
model O
. O

Introduction O

Retrieval B-MethodName
- I-MethodName
augmented I-MethodName
language I-MethodName
models I-MethodName
( O
LMs O
) O
have O
access O
to O
a O
non O
- O
parametric O
memory O
, O
allowing O
them O
to O
directly O
access O
a O
large O
external O
text O
collection O
during O
inference O
. O
Previous O
work O
has O
shown O
that O
these O
models O
substantially O
outperform O
their O
nonretrieval O
- O
based O
counterparts O
on O
language O
modeling O
tasks O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
; O
He O
et O
al O
. O
, O
2021 O
; O
Borgeaud O
et O
al O
. O
, O
2021 O
) O
, O
but O
it O
is O
an O
open O
question O
whether O
they O
also O
achieve O
similar O
gains O
in O
fewshot O
and O
zero O
- O
shot O
end O
task O
evaluations O
( O
Radford O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020a O
) O
. O
In O
this O
paper O
, O
we O
demonstrate O
that O
, O
with O
some O
extensions O
to O
improve O
coverage O
of O
the O
verbalizer O
tokens O
, O
the O
performance O
gains O
of O
retrieval B-MethodName
- I-MethodName
augmented I-MethodName
LMs I-MethodName
generalize O
well O
to O
a O
wide O
range O
of O
downstream O
tasks O
. O

Figure O
1 O
: O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
incorporates O
information O
from O
a O
large O
, O
heterogeneous O
corpus O
( O
unlabeled O
texts O
from O
different O
domains O
) O
to O
facilitate O
few O
- O
and O
zero O
- O
shot O
inference O
. O
The O
datastore O
contains O
key O
- O
value O
pairs O
where O
the O
key O
is O
an O
encoding O
of O
a O
leftward O
context O
and O
the O
value O
is O
the O
next O
token O
following O
the O
context O
. O
Our O
fuzzy O
verbalizer O
expands O
" O
terrible O
" O
to O
include O
" O
silly O
" O
and O
" O
great O
" O
to O
include O
" O
excellent O
" O
. O
Because O
the O
encoded O
corpus O
is O
unlabeled O
plain O
text O
, O
some O
datastore O
entries O
contain O
next O
tokens O
not O
in O
the O
verbalizer O
tokens O
( O
e.g. O
, O
" O
cinema O
" O
) O
. O

We O
study O
the O
k B-MethodName
- I-MethodName
nearest I-MethodName
neighbors I-MethodName
language I-MethodName
model I-MethodName
( O
Khandelwal O
et O
al O
. O
, O
2020 O
, O
kNN B-MethodName
- I-MethodName
LM I-MethodName
) O
, O
which O
interpolates O
the O
LM O
softmax O
distribution O
with O
a O
nearest O
- O
neighbor O
distribution O
. O
The O
nearest O
neighbours O
are O
computed O
based O
on O
the O
distance O
in O
LM O
output O
embeddings O
and O
can O
be O
drawn O
from O
any O
text O
corpus O
, O
in O
our O
case O
, O
a O
heterogeneous O
corpus O
that O
contains O
unlabeled O
data O
from O
different O
domains O
. O
We O
are O
the O
first O
to O
study O
the O
zero O
- O
shot O
application O
of O
kNN B-MethodName
- I-MethodName
LM I-MethodName
to O
end O
tasks O
, O
and O
we O
find O
that O
applying O
the O
technique O
naïvely O
produces O
only O
marginal O
improvements O
( O
Section O
4 O
) O
. O
The O
main O
challenge O
is O
that O
the O
support O
of O
the O
kNN O
distribution O
is O
sparse O
( O
covering O
at O
most O
k B-HyperparameterName
tokens O
, O
often O
less O
) O
, O
as O
it O
only O
assigns O
probability O
mass O
to O
nearest O
neighbors O
. O
This O
means O
it O
often O
entirely O
misses O
the O
tokens O
that O
are O
used O
to O
verbalize O
the O
output O
label O
in O
the O
standard O
application O
of O
LMs O
to O
zero O
- O
shot O
classification O
: O
across O
the O
datasets O
we O
test O
, O
an O
output O
label O
receives O
nonzero O
probability O
under O
the O
kNN O
distribution O
only O
44.2 O
% O
of O
the O
time O
( O
see O
Section O
6 O
) O
. O

To O
address O
this O
challenge O
, O
we O
introduce O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
, O
a O
simple O
and O
effective O
method O
built O
on O
kNN B-MethodName
- I-MethodName
LM I-MethodName
for O
improving O
zero O
- O
shot O
inference O
with O
no O
further O
training O
. O
Key O
to O
our O
approach O
are O
fuzzy O
verbalizers O
, O
which O
automatically O
expand O
the O
set O
of O
tokens O
corresponding O
to O
each O
output O
label O
. O
For O
example O
, O
in O
Figure O
1 O
, O
the O
verbalized O
label O
of O
the O
negative O
sentiment O
is O
" O
terrible O
. O
" O
Our O
fuzzy O
verbalizer O
also O
maps O
" O
silly O
" O
to O
negative O
sentiment O
, O
allowing O
the O
model O
to O
better O
leverage O
the O
information O
available O
in O
the O
kNN O
distribution O
. O
Extensive O
experiments O
( O
Section O
3 O
) O
show O
that O
applying O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
using O
a O
purely O
unlabeled O
heterogeneous O
corpus O
consistently O
improves O
zero O
- O
shot O
performance O
on O
eleven O
tasks O
, O
including O
sentiment B-TaskName
analysis I-TaskName
, O
topic B-TaskName
classification I-TaskName
, O
entailment B-TaskName
, O
fact B-TaskName
retrieval I-TaskName
and O
question B-TaskName
answering I-TaskName
. O
These O
improvements O
hold O
for O
every O
model O
in O
the O
GPT-2 O
family O
. O

We O
also O
show O
that O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
can O
be O
used O
to O
adapt O
LMs O
to O
new O
domains O
and O
tasks O
with O
no O
further O
training O
( O
Section O
5 O
) O
. O
With O
a O
domain O
- O
specific O
datastore O
corpus O
, O
we O
achieve O
comparable O
or O
better O
performance O
to O
prompting O
the O
LM O
after O
domainadaptive O
pretraining O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
on O
that O
corpus O
. O
To O
better O
understand O
these O
gains O
, O
we O
conduct O
a O
thorough O
analysis O
( O
Section O
6 O
) O
, O
showing O
that O
fuzzy O
verbalizers O
are O
essential O
for O
leveraging O
the O
kNN O
distribution O
, O
the O
benefits O
of O
retrieval O
increase O
with O
retrieval O
model O
size O
, O
and O
even O
relatively O
small O
datastores O
can O
yield O
sizeable O
performance O
gains O
if O
they O
are O
tailored O
to O
the O
domain O
or O
task O
. O
Overall O
, O
our O
results O
show O
how O
retrieval O
can O
benefit O
zero O
- O
shot O
inference O
with O
LMs O
on O
a O
wide O
variety O
of O
tasks O
, O
and O
suggest O
that O
applying O
retrieval O
with O
larger O
models O
may O
yield O
even O
greater O
benefits O
. O
Code O
is O
available O
at O
github.com O
/ O
swj0419 O
/ O
kNN O
_ O
prompt O
. O

Method O

To O
perform O
zero O
- O
shot O
prediction O
on O
a O
downstream O
task O
using O
a O
pretrained O
language O
model O
, O
we O
recast O
the O
task O
as O
language O
modeling O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
by O
converting O
each O
input O
instance O
into O
a O
natural O
language O
prompt O
( O
Section O
2.1 O
) O
. O
We O
then O
augment O
the O
pretrained O
model O
with O
the O
knearest O
- O
neighbors O
language O
modeling O
technique O
from O
Khandelwal O
et O
al O
. O
( O
2020 O
) O
. O
To O
better O
benefit O
from O
the O
sparse O
kNN O
distribution O
, O
we O
introduce O
fuzzy O
verbalizers O
for O
mapping O
from O
the O
LM O
's O
outputs O
to O
a O
distribution O
over O
task O
- O
specific O
labels O
( O
Section O
2.3 O
) O
. O
Finally O
, O
we O
decode O
the O
output O
from O
this O
label O
distribution O
using O
the O
domain O
- O
conditional O
PMI O
scoring O
method O
of O
Holtzman O
et O
al O
. O
( O
2021 O
) O
. O

Prompting O
and O
Verbalizers O

We O
address O
classification O
problems O
where O
an O
instance O
consists O
of O
an O
input O
sequence O
of O
tokens O
x O
= O
( O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
|x| O
) O
from O
a O
vocabulary O
V O
and O
an O
output O
label O
y O
∈ O
Y. O
The O
output O
label O
set O
Y O
may O
be O
fixed O
for O
the O
task O
( O
text O
classification O
) O
. O
For O
example O
, O
in O
the O
sentiment B-TaskName
analysis I-TaskName
example O
in O
Figure O
2 O
, O
the O
input O
is O
x O
= O
" O
Mr. O
Tsai O
is O
one O
of O
world O
cinema O
's O
most O
gifted O
artists O
. O
" O
The O
output O
labels O
are O
Y O
= O
{ O
y O
+ O
, O
y O
- O
} O
, O
referring O
to O
positive O
and O
negative O
sentiment O
. O

To O
cast O
the O
task O
as O
language O
modeling O
, O
we O
deterministically O
transform O
each O
input O
example O
x O
into O
a O
prompt O
p O
( O
x O
) O
. O
Providing O
this O
prompt O
to O
an O
LM O
yields O
a O
probability O
distribution O
P O
LM O
( O
v O
| O
p O
( O
x O
) O
) O
. O
To O
extract O
an O
output O
label O
from O
this O
, O
we O
apply O
verbalizers O
V O
: O
y O
→ O
V O
* O
( O
Schick O
and O
Schütze O
, O
2021 O
) O
which O
map O
each O
output O
label O
y O
∈ O
Y O
to O
a O
label O
word O
V O
( O
y O
) O
= O
v. O
We O
can O
then O
compute O
a O
probability O
for O
each O
label O
: O

P O
( O
y O
| O
x O
) O
∝ O
P O
LM O
( O
V O
( O
y O
) O
| O
p O
( O
x O
) O
) O
, O
( O
1 O
) O

normalizing O
over O
all O
y O
∈ O
Y. O
For O
example O
, O
our O
prompt O
transformation O
for O
sentiment B-TaskName
analysis I-TaskName
adds O
It O
was O
after O
the O
input O
, O
and O
uses O
the O
verbalizer O
V O
( O
y O
+ O
) O
= O
great O
, O
V O
( O
y O
- O
) O
= O
terrible O
, O
which O
classifies O
sentiment O
according O
to O
the O
relative O
probabilities O
of O
It O
was O
great O
and O
It O
was O
terrible O
after O
the O
input O
sequence O
( O
see O
Figure O
2 O
, O
bottom O
left O
) O
. O

k B-MethodName
- I-MethodName
Nearest I-MethodName
Neighbors I-MethodName
Language I-MethodName
Modeling I-MethodName

Following O
Khandelwal O
et O
al O
. O
( O
2020 O
) O
, O
we O
augment O
the O
LM O
with O
a O
datastore O
from O
which O
it O
can O
retrieve O
tokens O
that O
inform O
its O
predictions O
, O
improving O
performance O
without O
further O
training O
. O

The O
datastore O
is O
a O
key O
- O
value O
store O
generated O
by O
running O
the O
LM O
over O
a O
corpus O
of O
text O
. O
Each O
value O
is O
a O
token O
w O
∈ O
V O
from O
the O
corpus O
, O
and O
its O
key O
is O
the O
vector O
hidden O
representation O
at O
the O
output O
layer O
of O
the O
LM O
running O
forward O
on O
the O
left O
context O
c O
∈ O
V O
* O
( O
call O
this O
f O
( O
c O
) O
) O
. O
At O
inference O
time O
, O
when O
predicting O
the O
next O
token O
for O
an O
input O
sequence O
c O
, O
the O
kNN B-MethodName
- I-MethodName
LM I-MethodName
retrieves O
the O
k B-HyperparameterValue
nearest O
neighbors O
of O
c O
from O
the O
datastore O
according O
to O
the O
distance O
d O
( O
• O
, O
f O
( O
c O
) O
) O
of O
their O
key O
vectors O
( O
squared O
L O
2 O
distance O
following O
Khandelwal O
et O
al O
. O
) O
. O
where O
each O
entry O
consists O
of O
a O
representation O
of O
a O
leftward O
context O
and O
its O
next O
token O
. O
During O
inference O
, O
a O
test O
example O
is O
mapped O
to O
a O
prompt O
form O
and O
used O
to O
retrieve O
the O
k B-HyperparameterName
most O
similar O
contexts O
and O
their O
next O
tokens O
from O
the O
datastore O
. O
The O
kNN O
distribution O
is O
a O
multinomial O
computed O
on O
the O
distance O
of O
the O
text O
example O
and O
similar O
contexts O
. O
The O
final O
prediction O
is O
formed O
by O
combining O
the O
kNN O
distribution O
with O
the O
language O
model O
output O
distribution O
. O

A O
softmax O
over O
the O
( O
negative O
) O
distances O
induces O
a O
distribution O
over O
the O
the O
tokens O
w O
i O
in O
the O
nearest O
neighbor O
set O
: O

P O
kNN O
( O
v O
| O
c O
) O
∝ O
( O
f O
( O
c O
i O
) O
, O
w O
i O
) O
1 O
v O
= O
w O
i O
e O
-d O
( O
f O
( O
c O
i O
) O
, O
f O
( O
c O
) O
) O
t O

where O
t B-HyperparameterName
is O
a O
temperature B-HyperparameterName
parameter O
. O
1 O
We O
can O
then O
interpolate O
this O
with O
the O
original O
LM O
as O
follows O
: O

P O
kNN O
- O
LM O
( O
v O
| O
c O
) O
= O
( O
1 O
-λ O
) O
P O
LM O
( O
v|c O
) O
+ O
λP O
kNN O
( O
v|c O
) O
. O

The O
hyperparameters O
for O
the O
kNN O
- O
LM O
approach O
are O
the O
number O
k B-HyperparameterName
of O
nearest O
neighbors O
, O
the O
interpolation B-HyperparameterName
constant I-HyperparameterName
λ B-HyperparameterName
, O
the O
temperature B-HyperparameterName
t B-HyperparameterName
, O
and O
the O
choice O
of O
datastore O
. O

Fuzzy O
verbalizers O

One O
challenge O
in O
performing O
zero O
- O
shot O
inference O
with O
LMs O
on O
downstream O
tasks O
is O
the O
choice O
of O
verbalizer O
. O
On O
one O
hand O
, O
LMs O
may O
be O
highly O
sensitive O
to O
the O
particular O
surface O
form O
in O
ways O
that O
are O
irrelevant O
to O
the O
classification O
task O
( O
Holtzman O
et O
al O
. O
, O
2021 O
) O
. O
On O
the O
other O
hand O
, O
for O
a O
kNN O
model O
, O
the O
k B-HyperparameterName
nearest O
neighbor O
set O
is O
sparse O
and O
may O
fail O
1 O
We O
have O
added O
the O
temperature B-HyperparameterName
adjustment O
in O
the O
softmax O
on O
top O
of O
the O
kNN B-MethodName
- I-MethodName
LM I-MethodName
formulation O
. O

to O
cover O
any O
of O
the O
tokens O
in O
the O
set O
of O
verbalizers O
( O
i.e. O
, O
P O
kNN O
( O
V O
( O
y O
) O
) O
= O
0 O
for O
all O
y O
∈ O
Y O
) O
, O
limiting O
its O
utility O
in O
those O
cases O
. O
To O
address O
these O
issues O
, O
we O
introduce O
fuzzy O
verbalizers O
, O
which O
associate O
each O
label O
y O
with O
a O
neighborhood O
of O
token O
sequences O
around O
a O
specific O
verbalization O
V O
( O
y O
) O
∈ O
V O
* O
. O

To O
do O
this O
, O
we O
first O
associate O
each O
token O
v O
∈ O
V O
with O
a O
neighborhood O
N O
( O
v O
) O
⊆ O
V O
of O
similar O
tokens O
. O
In O
particular O
, O
we O
use O
v O
's O
top-5 B-HyperparameterValue
most O
similar O
words O
according O
to O
the O
cosine O
similarity O
of O
their O
GloVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
as O
well O
as O
any O
of O
v O
's O
synonyms O
in O
ConceptNet O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
. O
2 O
Then O
, O
for O
the O
purposes O
of O
calculating O
the O
probability O
of O
a O
verbalized O
label O
v O
= O
V O
( O
y O
) O
, O
we O
treat O
a O
prediction O
of O
any O
token O
in O
each O
neighborhood O
of O
v O
as O
a O
viable O
substitute O
for O
it O
, O
marginalizing O
over O
N O
( O
z O
) O
: O

P O
FV O
( O
y O
| O
x O
) O
∝ O
v O
i O
∈N O
( O
v O
) O
P O
( O
v O
i O
| O
p O
( O
x O
) O
) O
( O
2 O
) O

The O
fuzzy O
verbalizer O
helps O
mitigate O
the O
effect O
the O
sparsity O
of O
the O
kNN O
distribution O
has O
on O
zero O
- O
shot O
prediction O
( O
see O
Section O
6 O
) O
. O

Full O
model O

To O
make O
a O
zero O
- O
shot O
prediction O
for O
an O
input O
x O
, O
we O
first O
transform O
it O
into O
a O
prompt O
p O
( O
x O
) O
and O
obtain O
a O
distribution O
over O
the O
label O
word O
v O
with O
a O
kNN B-MethodName
- I-MethodName
LM I-MethodName
: O
P O
kNN B-MethodName
- I-MethodName
LM I-MethodName
( O
v O
| O
p O
( O
x O
) O
) O
. O
We O
then O
apply O
domainconditional O
PMI O
scoring O
rule O
( O
Holtzman O
et O
al O
. O
, O
2021 O
) O
to O
calibrate O
the O
distribution O
: O

PMI O
DC O
( O
v O
, O
p O
( O
x O
) O
) O
= O
P O
( O
v O
| O
p O
( O
x O
) O
) O
P O
( O
v O
| O
p O
) O

where O
p O
is O
a O
task O
- O
dependent O
string O
which O
is O
independent O
of O
the O
particular O
input O
( O
generally O
the O
local O
context O
at O
the O
end O
of O
the O
prompt O
, O
e.g. O
, O
we O
use O
p O
= O
" O
It O
was O
" O
for O
sentiment B-TaskName
analysis I-TaskName
, O
as O
shown O
in O
Figure O
2 O
) O
. O
Finally O
, O
we O
convert O
this O
to O
the O
output O
label O
score O
P O
( O
y O
| O
p O
( O
x O
) O
) O
using O
a O
fuzzy O
verbalizer O
( O
Section O
2.3 O
) O
. O
When O
using O
the O
fuzzy O
verbalizer O
together O
with O
PMI O
calibration O
, O
instead O
of O
marginalizing O
over O
the O
tokens O
in O
the O
fuzzy O
verbalizer O
v O
i O
∈ O
N O
( O
v O
) O
( O
Equation O
2 O
) O
, O
we O
score O
each O
label O
according O
to O
the O
sum O
of O
the O
PMIs O
of O
its O
associated O
tokens O
: O

P O
( O
y O
| O
x O
) O
∝ O
v O
i O
∈N O
( O
v O
) O
PMI O
DC O
( O
v O
i O
, O
p O
( O
x O
) O
) O

3 O
Experimental O
Setup O

Tasks O

We O
experiment O
with O
9 O
tasks O
, O
including O
topic B-TaskName
classification I-TaskName
, O
sentiment B-TaskName
analysis I-TaskName
, O
entailment B-TaskName
and O
partisanship B-TaskName
classification I-TaskName
. O

Topic B-TaskName
Classification I-TaskName
We O
use O
the O
AG B-DatasetName
News I-DatasetName
( O
AGN B-DatasetName
) O
and O
Yahoo B-DatasetName
! I-DatasetName
Answers I-DatasetName
( O
Yahoo B-DatasetName
) O
corpora O
from O
Zhang O
et O
al O
. O
( O
2015 O
) O
for O
topic B-TaskName
classification I-TaskName
. O

Sentiment O
and O
Partisanship O

We O
study O
sentiment B-TaskName
analysis I-TaskName
using O
the O
Rotten B-DatasetName
Tomatoes I-DatasetName
( O
RT B-DatasetName
) O
and O
SST-2 B-DatasetName
corpora O
of O
Socher O
et O
al O
. O
( O
2013 O
) O
, O
movie O
reviews O
from O
Pang O
and O
Lee O
( O
2005 O
, O
MR B-DatasetName
) O
, O
the O
customer O
review O
dataset O
from O
Hu O
and O
Liu O
( O
2004 O
, O
CR B-DatasetName
) O
consisting O
of O
Amazon O
and O
Yelp O
reviews O
, O
and O
the O
hyperpartisan B-DatasetName
news I-DatasetName
detection I-DatasetName
dataset O
from O
Kiesel O
et O
al O
. O
( O
2019 O
, O
HYP B-DatasetName
) O
, O
which O
focuses O
on O
classifying O
whether O
a O
text O
exhibits O
extreme O
political O
views O
. O

Entailment B-TaskName
Entailment B-TaskName
datasets O
focus O
on O
classifying O
whether O
one O
sentence O
plausibly O
implies O
another O
to O
be O
true O
or O
false O
. O
We O
evaluate O
on O
the O
Com B-DatasetName
- I-DatasetName
mitmentBank I-DatasetName
( O
De O
Marneffe O
et O
al O
. O
, O
2019 O
, O
CB B-DatasetName
) O
and O
the O
Recognizing B-TaskName
Textual I-TaskName
Entailment I-TaskName
( O
Dagan O
et O
al O
. O
, O
2010 O
, O
RTE B-DatasetName
) O
dataset O
provided O
in O
GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O

kNN B-MethodName
- I-MethodName
Prompt I-MethodName
Model O
Details O

Inference O
Model O
For O
our O
main O
experiments O
, O
we O
directly O
use O
GPT-2 B-MethodName
large I-MethodName
from O
Huggingface O
3 O
as O
our O
base O
LM O
. O
We O
consider O
other O
model O
sizes O
in O
Section O
6 O
. O

Retriever O
Model O
Following O
the O
inference O
model O
, O
we O
use O
GPT-2 B-MethodName
large I-MethodName
to O
build O
the O
datastore O
. O
The O
keys O
are O
the O
1280 B-HyperparameterValue
- O
dimensional O
hidden B-HyperparameterName
representations I-HyperparameterName
before I-HyperparameterName
the I-HyperparameterName
final I-HyperparameterName
MLP I-HyperparameterName
which O
predicts O
the O
token O
distribution O
at O
each O
timestep O
, O
produced O
using O
a O
single O
forward O
pass O
over O
the O
datastore O
corpus O
. O
For O
efficient O
similarity O
search O
, O
we O
create O
a O
FAISS O
( O
Johnson O
et O
al O
. O
, O
2019 O
) O
index O
and O
search O
for O
nearest O
neighbors O
by O
Euclidean O
distance O
. O

Datastore B-DatasetName
Corpus O
For O
our O
datastore O
, O
we O
aim O
to O
curate O
a O
large O
, O
heteregenous O
corpus O
of O
data O
broadly O
relevant O
to O
the O
tasks O
we O
evaluate O
. O
To O
this O
end O
, O
we O
combine O
four O
sources O
of O
data O
including O
Wikitext-103 B-DatasetName
( O
Merity O
et O
al O
. O
, O
2016 O
) O
, O
the O
Amazon B-DatasetName
review I-DatasetName
corpus O
of O
He O
and O
McAuley O
( O
2016 O
) O
, O
and O
subsets O
of O
CC B-DatasetName
- I-DatasetName
NEWS I-DatasetName
4 O
and O
IMDB B-DatasetName
5 O
sampled O
uniformly O
from O
each O
. O
Table O
1 O
lists O
the O
specifics O
of O
each O
data O
source O
. O

Inference O
Procedure O
We O
retrieve O
k=1024 B-HyperparameterName
neighbors O
, O
soften O
the O
kNN O
distribution O
with O
a O
temperature B-HyperparameterName
value O
of O
3 B-HyperparameterValue
and O
use O
an O
interpolation B-HyperparameterName
factor I-HyperparameterName
of O
λ B-HyperparameterName
= O
0.3 B-HyperparameterValue
. O
Our O
primary O
evaluation O
is O
zero O
- O
shot O
. O
All O
hyperparameters O
were O
chosen O
on O
the O
basis O
of O
development O
experiments O
( O
see O
Section O
6 O
for O
more O
details O
) O
. O

Baselines O

LM O
is O
the O
result O
of O
prompting O
the O
base O
language O
model O
( O
GPT-2 B-MethodName
Large I-MethodName
) O
, O
choosing O
the O
output O
label O
whose O
verbalizer O
has O
the O
highest O
probability O
under O
the O
language O
model O
P O
LM O
( O
V O
( O
y O
) O
| O
p O
( O
x O
) O
) O
. O

Experimental O
Results O

Results O
for O
zero O
- O
shot O
prediction O
are O
in O
Table O
2 O
. O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
outperforms O
all O
baselines O
in O
all O
tasks O
, O
improving O
over O
the O
base O
LM O
by O
13.4 B-MetricValue
% I-MetricValue
on O
average O
. O
The O
gains O
are O
particularly O
pronounced O
for O
MR B-DatasetName
and O
RT B-DatasetName
( O
sentiment B-TaskName
analysis I-TaskName
on O
movie B-DatasetName
reviews I-DatasetName
) O
, O
Yahoo B-DatasetName
( O
topic B-TaskName
classification I-TaskName
) O
. O
For O
MR B-DatasetName
and O
RT B-DatasetName
, O
the O
gains O
seem O
to O
come O
mostly O
from O
PMI O
calibration O
. O
Interestingly O
, O
the O
kNN B-MethodName
- I-MethodName
LM I-MethodName
alone O
yields O
a O
fairly O
small O
improvement O
over O
the O
base O
LM O
( O
about O
0.4 B-MetricValue
% I-MetricValue
on O
average O
) O
. O
This O
suggests O
that O
the O
fuzzy O
verbalizer O
and O
PMI O
calibration O
methods O
may O
help O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
better O
leverage O
the O
information O
in O
the O
knearest O
neighbors O
distribution O
. O
We O
examine O
possible O
sources O
of O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
's O
performance O
gains O
in O
Section O
6 O
. O

Few O
- O
shot O
inference O
For O
a O
subset O
of O
tasks O
, O
we O
additionally O
compare O
to O
a O
few O
- O
shot O
setting O
where O
we O
prepend O
four O
examples O
uniformly O
sampled O
from O
the O
training O
data O
to O
the O
input O
( O
Table O
3 O
) O
. O
The O
demonstration O
examples O
are O
converted O
to O
prompt O
and O
verbalizer O
format O
. O
We O
report O
the O
mean O
accuracy B-MetricName
and O
standard O
deviation O
with O
4 O
different O
random B-HyperparameterName
seeds I-HyperparameterName
. O
We O
find O
that O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
consistently O
outperform O
baselines O
, O
demonstrating O
that O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
is O
ap- O
plicable O
to O
the O
few O
- O
shot O
setting O
as O
well O
. O
We O
leave O
further O
exploration O
of O
this O
phenomenon O
to O
future O
work O
. O

kNN B-MethodName
- I-MethodName
Prompt I-MethodName
for O
Domain O
Adaptation O

One O
of O
the O
advantages O
of O
retrieval B-MethodName
- I-MethodName
based I-MethodName
LMs I-MethodName
is O
that O
they O
can O
be O
adapted O
to O
new O
domains O
with O
no O
further O
training O
. O

To O
test O
this O
capability O
, O
we O
replace O
our O
heterogeneous O
datastore B-DatasetName
( O
Section O
3.2 O
) O
with O
domain O
- O
specific O
ones O
for O
several O
tasks O
. O
To O
build O
these O
domainspecific O
datastores B-DatasetName
, O
we O
select O
Amazon B-DatasetName
Reviews I-DatasetName
for O
CR B-DatasetName
, O
CC B-DatasetName
- I-DatasetName
NEWS I-DatasetName
for O
HYP B-DatasetName
and O
IMDB B-DatasetName
for O
MR B-DatasetName
, O
and O
encode O
them O
separately O
. O

For O
comparison O
, O
we O
consider O
domain O
- O
adaptive O
pretraining O
( O
Gururangan O
et O
al O
. O
, O
2020 O
, O
DAPT O
) O
, O
which O
further O
trains O
the O
LM O
on O
the O
domain O
- O
specific O
corpus O
. O
We O
train O
GPT-2 B-MethodName
Large I-MethodName
on O
each O
domain O
corpus O
for O
a O
single O
pass O
, O
then O
apply O
it O
to O
downstream O
tasks O
using O
our O
prompting O
and O
verbalizer O
setup O
and O
domain O
- O
conditional O
PMI O
scoring O
. O

As O
shown O
in O
Table O
4 O
, O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
performs O
comparably O
with O
DAPT O
. O
Specifically O
, O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
slightly O
outperforms O
DAPT O
on O
CR B-DatasetName
and O
MR B-DatasetName
. O
These O
results O
indicate O
that O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
is O
an O
effective O
method O
for O
domain O
adaptation O
. O
Critically O
, O
unlike O
DAPT O
, O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
does O
not O
require O
further O
training O
, O
which O
is O
more O
practical O
and O
efficient O
for O
adapting O
very O
large O
LMs O
. O

Effect O
of O
datastore B-DatasetName
distribution O
and O
size O

To O
better O
understand O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
's O
potential O
for O
domain O
adaptation O
, O
we O
experiment O
with O
varying O
sizes O
and O
5 O
3258 O

Figure O
3 O
: O
Effect O
of O
the O
number O
of O
tokens O
in O
the O
datastore B-DatasetName
on O
CR B-DatasetName
and O
MR B-DatasetName
. O
Each O
line O
represents O
the O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
model O
with O
a O
different O
datastore B-DatasetName
and O
the O
line O
ends O
when O
the O
entire O
available O
datastore O
is O
used O
. O
General O
, O
Domain O
, O
and O
Task O
refer O
to O
the O
heterogeneous O
corpus O
( O
Table O
1 O
) O
, O
domain O
- O
specific O
corpus O
, O
and O
task O
- O
specific O
corpus O
, O
respectively O
. O
We O
use O
IMDB B-DatasetName
as O
the O
domain O
- O
specific O
corpus O
for O
MR B-DatasetName
, O
and O
Amazon B-DatasetName
Reviews I-DatasetName
for O
CR B-DatasetName
. O
The O
task O
- O
specific O
corpus O
is O
the O
unlabeled O
training O
data O
of O
each O
task O
. O
GPT-2 B-MethodName
Large I-MethodName
is O
used O
for O
both O
retriever O
and O
inference O
models O
. O

distributions O
of O
the O
datastore B-DatasetName
. O
For O
each O
task O
, O
we O
consider O
three O
options O
for O
the O
datastore B-DatasetName
corpus O
: O
our O
heterogeneous O
corpus O
( O
Section O
3.2 O
) O
, O
a O
domain O
- O
specific O
corpus O
, O
and O
a O
task O
- O
specific O
corpus O
constructed O
from O
the O
task O
's O
( O
unlabeled O
) O
training O
data O
. O
Each O
of O
these O
data O
sources O
exhibits O
increasing O
levels O
of O
relevance O
to O
the O
task O
. O

Figure O
3 O
shows O
how O
model O
performance O
varies O
with O
the O
choice O
of O
datastore O
across O
different O
datastore O
sizes O
. O
For O
a O
fixed O
number O
of O
tokens O
, O
retrieving O
from O
a O
task O
- O
specific O
datastore O
is O
best O
. O
Furthermore O
, O
token O
- O
for O
- O
token O
, O
adding O
task O
- O
specific O
data O
leads O
to O
more O
gains O
than O
domain O
- O
specific O
data O
, O
which O
in O
turn O
is O
better O
than O
our O
heterogeneous O
corpus O
. O

Using O
domain O
- O
specific O
data O
is O
always O
better O
than O
retrieving O
from O
the O
large O
heterogeneous O
corpus O
. O
For O
example O
, O
for O
CR B-DatasetName
, O
using O
6 B-HyperparameterValue
M I-HyperparameterValue
tokens O
of O
domainspecific O
data O
outperforms O
using O
our O
465 B-DatasetName
M I-DatasetName
token O
heterogeneous O
corpus O
. O
These O
results O
suggest O
that O
while O
having O
a O
large O
datastore O
is O
beneficial O
, O
curating O
task O
- O
specific O
or O
domain O
- O
specific O
data O
can O
also O
be O
an O
effective O
way O
of O
improving O
model O
performance O
, O
especially O
if O
datastore O
size O
is O
limited O
( O
e.g. O
, O
due O
to O
memory O
constraints O
) O
. O

Analysis O

We O
perform O
several O
experiments O
to O
understand O
the O
contribution O
of O
each O
component O
of O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
and O
inform O
our O
choice O
of O
hyperparameters O
. O
interpolation O
( O
Section O
2.2 O
) O
, O
fuzzy O
verbalizers O
( O
Section O
2.3 O
) O
, O
and O
PMI O
scoring O
( O
Section O
2.4 O
) O
. O
Table O
5 O
shows O
the O
results O
of O
ablation O
experiments O
analyzing O
the O
contribution O
of O
each O
component O
. O
First O
, O
we O
find O
that O
adding O
kNN O
to O
LM O
gives O
trivial O
improvement O
( O
+0.4 B-MetricValue
% I-MetricValue
) O
, O
but O
much O
greater O
once O
we O
add O
fuzzy O
verbalizers O
on O
top O
of O
them O
( O
+10.3 B-MetricValue
% I-MetricValue
) O
, O
exceeding O
the O
contribution O
of O
the O
two O
components O
independently O
( O
with O
fuzzy O
verbalizers O
alone O
at O
+7.2 B-MetricValue
% I-MetricValue
) O
. O
This O
supports O
the O
argument O
that O
fuzzy O
verbalizers O
allow O
the O
model O
to O
make O
better O
use O
of O
the O
sparse O
support O
of O
the O
kNN O
distribution O
. O
Indeed O
, O
we O
find O
that O
across O
all O
tasks O
, O
an O
output O
label O
receives O
nonzero O
probability O
under O
the O
kNN O
distribution O
in O
kNN B-MethodName
- I-MethodName
LM I-MethodName
only O
45.8 O
% O
of O
the O
time O
. O
With O
fuzzy O
verbalizers O
, O
this O
increases O
to O
78 O
% O
. O

Second O
, O
we O
find O
that O
for O
the O
base O
LM O
, O
fuzzy O
verbalizers O
bring O
gains O
( O
+7.2 B-MetricValue
% I-MetricValue
) O
similar O
to O
PMI O
scoring O
( O
+8.8 B-MetricValue
% I-MetricValue
) O
, O
but O
the O
gains O
are O
only O
partially O
addi- O
tive O
when O
combining O
the O
two O
techniques O
( O
+10.9 B-MetricValue
% I-MetricValue
) O
. O
This O
suggests O
that O
by O
incorporating O
more O
varied O
surface O
forms O
into O
the O
score O
for O
each O
label O
, O
fuzzy O
verbalizers O
may O
partially O
-but O
not O
completely O
-mitigate O
the O
surface O
form O
competition O
problem O
which O
PMI O
scoring O
was O
designed O
to O
tackle O
( O
Holtzman O
et O
al O
. O
, O
2021 O
) O
. O
The O
effect O
of O
PMI O
scoring O
is O
increased O
, O
however O
, O
when O
we O
use O
fuzzy O
verbalizers O
and O
kNN O
retrieval O
together O
( O
+13.4 B-MetricValue
% I-MetricValue
for O
the O
full O
model O
versus O
+10.3 B-MetricValue
% I-MetricValue
for O
kNN O
with O
fuzzy O
verbalizers O
) O
, O
suggesting O
that O
the O
kNN O
distribution O
might O
suffer O
from O
more O
surface O
form O
competition O
problems O
than O
the O
base O
LM O
distribution O
. O
kNN O
retrieval O
hyperparameters O
Figure O
4 O
shows O
how O
the O
number B-HyperparameterName
of I-HyperparameterName
retrieved I-HyperparameterName
nearest I-HyperparameterName
neighbors I-HyperparameterName
( O
k B-HyperparameterName
) O
and O
softmax O
temperature B-HyperparameterName
affect O
model O
perfor O
- O
mance O
on O
three O
datasets O
. O
In O
most O
cases O
, O
performance O
monotonically O
improves O
with O
the O
number B-HyperparameterName
of I-HyperparameterName
neighbors I-HyperparameterName
when O
k B-HyperparameterName
is O
smaller O
than O
1024 B-HyperparameterValue
and O
deteriorates O
after O
that O
. O
When O
k B-HyperparameterName
< O
256 B-HyperparameterValue
, O
a O
temperature B-HyperparameterName
of O
1 B-HyperparameterValue
performs O
best O
, O
while O
flattening O
the O
distribution O
is O
useful O
when O
retrieving O
more O
neighbors O
. O
Overall O
, O
using O
1024 B-HyperparameterValue
neighbors O
and O
a O
temperature B-HyperparameterName
value O
of O
3 B-HyperparameterValue
performs O
consistently O
well O
across O
the O
tasks O
we O
test O
. O

Retrieval O
model O
size O
and O
inference O
model O
size O
Figure O
5 O
shows O
how O
performance O
varies O
with O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
retriever I-HyperparameterName
and O
inference O
models O
on O
three O
tasks O
. O
We O
observe O
substantial O
gains O
as O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
retriever I-HyperparameterName
increases O
, O
which O
hold O
regardless O
of O
inference O
model O
size O
. O

It O
should O
be O
noted O
that O
a O
larger O
retriever O
leads O
to O
a O
larger O
datastore O
and O
slower O
retrieval O
: O
increasing O
the O
retriever B-HyperparameterName
size I-HyperparameterName
from O
125 B-HyperparameterValue
M I-HyperparameterValue
to O
1600 B-HyperparameterValue
M I-HyperparameterValue
parameters O
doubles O
the O
memory O
footprint O
of O
the O
datastore B-DatasetName
, O
which O
scales O
with O
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
retriever I-HyperparameterName
model I-HyperparameterName
's O
output O
embeddings O
. O
These O
computational O
tradeoffs O
may O
inform O
the O
retriever B-HyperparameterName
size I-HyperparameterName
best O
suited O
for O
a O
particular O
application O
. O

Related O
Work O

Retrieval B-MethodName
- I-MethodName
augmented I-MethodName
LMs I-MethodName
Several O
studies O
propose O
the O
use O
of O
retrieval O
mechanisms O
with O
external O
datastores O
to O
improve O
language O
modeling O
performance O
( O
Khandelwal O
et O
al O
. O
, O
2020 O
) O
and O
opendomain O
question O
answering O
( O
Izacard O
and O
Grave O
, O
2020 O
; O
Lewis O
et O
al O
. O
, O
2020 O
) O
. O
Other O
work O
incorporates O
retrieval O
directly O
into O
the O
LM O
pretraining O
process O
( O
Guu O
et O
al O
. O
, O
2020 O
; O
Borgeaud O
et O
al O
. O
, O
2021 O
) O
. O
Khandelwal O
et O
al O
. O
( O
2021 O
) O
applies O
nearest O
neighbor O
retrieval O
to O
conditional O
sequence O
generation O
to O
improve O
the O
quality O
of O
machine O
translation O
systems O
. O
Our O
work O
is O
the O
first O
to O
show O
that O
retrieval O
augmentation O
, O
introduced O
at O
test O
time O
, O
improves O
the O
zero O
- O
shot O
inference O
of O
language O
models O
on O
a O
variety O
of O
end O
tasks O
. O

Zero O
- O
shot O
and O
few O
- O
shot O
inference O
Brown O
et O
al O
. O
( O
2020b O
) O
demonstrate O
that O
large O
LMs O
can O
perform O
zero O
- O
shot O
( O
given O
only O
a O
prompt O
) O
and O
few O
- O
shot O
learning O
( O
using O
a O
concatenation O
of O
training O
examples O
as O
demonstrations O
) O
without O
any O
finetuning O
. O
Subsequent O
work O
further O
improves O
their O
zero O
- O
shot O
and O
few O
- O
shot O
abilities O
with O
calibration O
( O
Holtzman O
et O
al O
. O
, O
2021 O
; O
Zhao O
et O
al O
. O
, O
2021 O
; O
Min O
et O
al O
. O
, O
2021a O
) O
, O
prompt O
engineering O
( O
Lu O
et O
al O
. O
, O
2021 O
; O
Shin O
et O
al O
. O
, O
2020 O
) O
and O
meta O
- O
tuning O
( O
Min O
et O
al O
. O
, O
2021b O
; O
Wei O
et O
al O
. O
, O
2022 O
; O
Zhong O
et O
al O
. O
, O
2021 O
) O
. O
Rubin O
et O
al O
. O
( O
2021 O
) O
and O
Liu O
et O
al O
. O
( O
2021 O
) O
apply O
retrieval O
methods O
to O
select O
incontext O
learning O
examples O
that O
are O
semanticallysimilar O
to O
a O
test O
example O
for O
few O
- O
shot O
inference O
. O
However O
, O
these O
retrieval O
methods O
require O
access O
to O
a O
large O
set O
of O
labeled O
data O
. O
In O
contrast O
, O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
only O
assumes O
the O
availability O
of O
a O
heterogeneous O
unlabeled O
corpus O
. O

Conclusions O

We O
present O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
, O
a O
new O
technique O
to O
augment O
LMs O
with O
nearest O
neighbor O
retrieval O
for O
zeroshot O
inference O
on O
end O
tasks O
. O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
substantially O
improves O
zero O
- O
shot O
performance O
on O
a O
wide O
range O
of O
multiple O
- O
choice O
and O
classification O
tasks O
. O
With O
a O
domain O
- O
or O
task O
- O
relevant O
datastore O
, O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
enables O
efficient O
domain O
adaptation O
with O
no O
additional O
training O
, O
and O
its O
benefits O
scale O
with O
the O
size O
of O
the O
retrieval O
model O
. O

Limitations O

Although O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
significantly O
improves O
GPT-2 B-MethodName
family O
models O
' O
zero O
- O
shot O
and O
few O
- O
shot O
performance O
, O
it O
stores O
high O
- O
dimensional O
vectors O
for O
every O
token O
in O
the O
datastore B-DatasetName
corpus O
and O
performs O
knearest O
neighbor O
search O
for O
every O
next O
token O
, O
which O
incurs O
significant O
inference O
overhead O
. O
Future O
work O
may O
study O
compressing O
the O
datastore B-DatasetName
and O
approximating O
kNN O
- O
search O
for O
efficient O
retrieval O
. O
Careful O
analysis O
could O
also O
explore O
datastore O
curation O
methods O
to O
balance O
task O
- O
relevancy O
, O
domain O
generality O
, O
and O
size O
. O
In O
addition O
, O
compared O
with O
sentence O
or O
document O
- O
level O
retrieval O
, O
retrieving O
tokens O
at O
each O
time O
step O
may O
limit O
the O
language O
model O
's O
ability O
to O
reason O
about O
the O
retrieved O
information O
. O
Future O
work O
may O
explore O
if O
more O
coarse O
- O
grained O
retrieval O
and O
interpolation O
such O
as O
chunks O
, O
sentences O
and O
documents O
- O
level O
lead O
to O
better O
end O
task O
performance O
. O

Our O
evaluation O
of O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
is O
limited O
to O
GPT-2 B-MethodName
family O
models O
and O
eleven O
end O
tasks O
. O
There O
are O
many O
other O
tasks O
and O
language O
models O
for O
which O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
can O
be O
useful O
. O
Future O
work O
may O
study O
the O
usefulness O
of O
kNN B-MethodName
- I-MethodName
Prompt I-MethodName
with O
larger O
inference O
models O
( O
i.e O
: O
GPT-3 O
) O
and O
more O
diverse O
tasks O
. O
Potentially O
, O
large O
inference O
models O
combined O
with O
larger O
retrieval O
models O
may O
result O
in O
better O
zeroshot O
performance O
. O
6 O
, O
the O
language O
model O
has O
to O
know O
the O
meaning O
of O
the O
entity O
" O
PSP O
" O
and O
" O
Sony O
" O
, O
otherwise O
it O
may O
associate O
" O
games O
" O
with O
a O
sport O
. O
kNN O
is O
able O
to O
match O
" O
Sony O
" O
in O
one O
of O
the O
retrieved O
neighbors O
, O
resolving O
the O
ambiguity O
of O
the O
word O
" O
games O
" O
. O
12 O
3265 O

A.2 O
Templates O

Scene O
Graph O
as O
Pivoting O
: O
Inference O
- O
time O
Image O
- O
free O
Unsupervised B-TaskName
Multimodal I-TaskName
Machine I-TaskName
Translation I-TaskName
with O
Visual O
Scene O
Hallucination O

In O
this O
work O
, O
we O
investigate O
a O
more O
realistic O
unsupervised B-TaskName
multimodal I-TaskName
machine I-TaskName
translation I-TaskName
( O
UMMT B-TaskName
) O
setup O
, O
inference O
- O
time O
image O
- O
free O
UMMT B-TaskName
, O
where O
the O
model O
is O
trained O
with O
sourcetext O
image O
pairs O
, O
and O
tested O
with O
only O
sourcetext O
inputs O
. O
First O
, O
we O
represent O
the O
input O
images O
and O
texts O
with O
the O
visual O
and O
language O
scene O
graphs O
( O
SG O
) O
, O
where O
such O
fine O
- O
grained O
visionlanguage O
features O
ensure O
a O
holistic O
understanding O
of O
the O
semantics O
. O
To O
enable O
pure O
- O
text O
input O
during O
inference O
, O
we O
devise O
a O
visual O
scene O
hallucination O
mechanism O
that O
dynamically O
generates O
pseudo O
visual O
SG O
from O
the O
given O
textual O
SG O
. O
Several O
SG O
- O
pivoting O
based O
learning O
objectives O
are O
introduced O
for O
unsupervised O
translation O
training O
. O
On O
the O
benchmark O
Multi30 B-DatasetName
K I-DatasetName
data O
, O
our B-MethodName
SG I-MethodName
- I-MethodName
based I-MethodName
method O
outperforms O
the O
best O
- O
performing O
baseline O
by O
significant O
BLEU B-MetricName
scores O
on O
the O
task O
and O
setup O
, O
helping O
yield O
translations O
with O
better O
completeness O
, O
relevance O
and O
fluency O
without O
relying O
on O
paired O
images O
. O
Further O
in O
- O
depth O
analyses O
reveal O
how O
our O
model O
advances O
in O
the O
task O
setting O
. O

Introduction O

Current O
neural O
machine O
translation O
( O
NMT O
) O
has O
achieved O
great O
triumph O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Zhu O
et O
al O
. O
, O
2020 O
) O
, O
however O
in O
the O
cost O
of O
creating O
large O
- O
scale O
parallel O
sentences O
, O
which O
obstructs O
the O
development O
of O
NMT O
for O
the O
minor O
languages O
. O
Unsupervised B-TaskName
NMT I-TaskName
( O
UMT B-TaskName
) O
has O
thus O
been O
proposed O
to O
relieve O
the O
reliance O
of O
parallel O
corpora O
( O
Artetxe O
et O
al O
. O
, O
2018 O
; O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
The O
core O
idea O
of O
UMT O
is O
to O
align O
the O
representation O
spaces O
between O
two O
languages O
with O
alternative O
pivot O
signals O
rather O
than O
parallel O
sentences O
, O
such O
as O
bilingual O
lexicons O
( O
Lample O
et O
al O
. O
, O
2018 O
) O
, O
multilingual O
language O
models O
( O
LM O
) O
( O
Conneau O
and O
Lample O
, O
2019 O
) O
and O
back O
- O
translation O
technique O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O
Recent O
trends O
have O
considered O
the O
incorporation O
of O
visual O
information O
, O
i.e. O
, O
multimodal B-TaskName
machine I-TaskName
translation I-TaskName
( O
MMT B-TaskName
) O
Huang O
et O
al O
. O
, O
2016 O
) O
. O
Intuitively O
, O
visual O
modality O
can O
serve O
as O
language O
- O
agnostic O
signals O
, O
pivoting O
different O
languages O
by O
grounding O
the O
same O
textual O
semantics O
into O
the O
common O
visual O
space O
. O
Therefore O
, O
solving O
UMT B-TaskName
with O
visual O
contents O
as O
pivot O
becomes O
a O
promising O
solution O
, O
a.k.a O
. O
, O
unsupervised B-TaskName
MMT I-TaskName
( O
UMMT B-TaskName
) O
( O
Huang O
et O
al O
. O
, O
2020 O
; O
Su O
et O
al O
. O
, O
2019 O
) O
. O

UMMT B-TaskName
systems O
are O
trained O
with O
only O
the O
textimage O
pairs O
( O
< O
text O
- O
img O
> O
) O
, O
which O
can O
be O
easier O
to O
collect O
than O
the O
parallel O
source O
- O
target O
sentence O
pairs O
( O
< O
src O
- O
tgt O
> O
) O
( O
Huang O
et O
al O
. O
, O
2020 O
) O
. O
Although O
exempting O
the O
parallel O
sentences O
for O
training O
, O
UMMT B-TaskName
still O
requires O
such O
text O
- O
image O
pairs O
as O
inputs O
for O
testing O
. O
Yet O
such O
assumption O
might O
be O
unrealistic O
, O
because O
in O
most O
of O
the O
real O
- O
world O
scenarios O
such O
as O
online O
translation O
systems O
, O
paired O
images O
are O
not O
available O
during O
inference O
. O
Especially O
for O
some O
scarce O
languages O
, O
the O
< O
text O
- O
img O
> O
pairs O
have O
difficult O
access O
. O
In O
other O
words O
, O
practical O
UMMT B-TaskName
systems O
should O
not O
only O
avoid O
the O
parallel O
sentences O
during O
training O
, O
but O
also O
the O
text O
- O
image O
pairs O
during O
inference O
. O
As O
summarized O
in O
Table O
1 O
, O
although O
some O
existing O
MMT B-TaskName
researches O
exempt O
the O
testing O
- O
time O
visual O
inputs O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2022 O
) O
, O
they O
all O
unfortunately O
are O
supervised O
methods O
, O
relying O
on O
large O
- O
scale O
parallel O
sentences O
for O
training O
. O
As O
emphasized O
above O
, O
the O
visual O
information O
is O
vital O
to O
UMMT B-TaskName
. O
However O
, O
for O
both O
the O
existing O
supervised B-TaskName
and I-TaskName
unsupervised I-TaskName
MMT I-TaskName
studies O
, O
they O
may O
suffer O
from O
ineffective O
and O
insufficient O
modeling O
of O
visual O
pivot O
features O
. O
For O
example O
, O
most O
of O
MMT B-TaskName
models O
perform O
vision O
- O
language O
( O
VL O
) O
grounding O
over O
the O
whole O
image O
and O
text O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
where O
such O
coarse O
- O
grained O
representation O
learning O
can O
cause O
mismatching O
and O
sacrifice O
the O
subtle O
VL O
semantics O
. O
Fang O
and O
Feng O
( O
2022 O
) O
recently O
introduce O
a O
fine O
- O
grained O
VL O
alignment O
learning O
via O
phrase O
- O
level O
grounding O
, O
while O
without O
a O
holistic O
understanding O
of O
the O
visual O
scene O
, O
such O
local O
- O
level O
method O
may O
lead O
to O
incomplete O
or O
missing O
alignments O
. O

In O
this O
work O
, O
we O
present O
a O
novel O
UMMT B-TaskName
method O
that O
solves O
all O
aforementioned O
challenges O
. O
First O
of O
all O
, O
to O
better O
represent O
the O
visual O
( O
also O
the O
textual O
) O
inputs O
, O
we O
consider O
incorporating O
the O
visual O
scene O
graph O
( O
VSG O
) O
( O
Johnson O
et O
al O
. O
, O
2015 O
) O
and O
language O
scene O
graph O
( O
LSG O
) O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
The O
scene O
graphs O
( O
SG O
) O
advance O
in O
intrinsically O
depicting O
the O
semantic O
structures O
of O
texts O
or O
images O
with O
rich O
details O
( O
cf O
. O
Fig O
. O
1 O
) O
, O
which O
offers O
a O
holistic O
viewpoint O
for O
more O
effective O
pivoting O
learning O
. O
Then O
, O
we O
build O
the O
UMMT B-TaskName
framework O
as O
illustrated O
in O
Fig O
. O
2 O
. O
The O
input O
src O
text O
and O
paired O
image O
are O
first O
transformed O
into O
LSG O
and O
VSG O
, O
which O
are O
further O
fused O
into O
a O
mixed O
SG O
, O
and O
then O
translated O
into O
the O
tgt O
- O
side O
LSG O
. O
And O
the O
tgt O
sentence O
will O
be O
finally O
produced O
conditioned O
on O
the O
tgt O
LSG O
. O
Several O
SGbased O
pivoting O
learning O
strategies O
are O
proposed O
for O
unsupervised O
training O
of O
UMMT B-TaskName
system O
. O
In O
addition O
, O
to O
support O
pure O
- O
text O
( O
image O
- O
free O
) O
input O
during O
inference O
, O
we O
devise O
a O
novel O
visual O
scene O
hallucination O
module O
, O
which O
dynamically O
generates O
a O
hallucinated O
VSG O
from O
the O
LSG O
compensatively O
. O
Our O
system O
is O
evaluated O
on O
the O
standard O
MMT B-TaskName
Multi30 B-DatasetName
K I-DatasetName
and O
NMT O
WMT B-DatasetName
data O
. O
Extensive O
experimental O
results O
verify O
that O
the O
proposed O
method O
outperforms O
strong O
baselines O
on O
unsupervised O
multimodal O
translation O
by O
above O
5 O
BLEU O
score O
on O
average O
. O
We O
further O
reveal O
the O
efficacy O
of O
the O
visual O
scene O
hallucination O
mechanism O
in O
relieving O
the O
reliance O
on O
image O
inputs O
during O
inference O
. O
Our O
SG O
- O
pivoting O
based O
UMMT O
helps O
yield O
translations O
with O
higher O
completeness O
, O
relevance O
and O
fluency O
, O
and O
especially O
obtains O
improvements O
on O
the O
longer O
sentences O
. O

Overall O
, O
we O
make O
the O
following O
contributions O
: O
▶ O
1 O
) O
We O
are O
the O
first O
to O
study O
the O
inferencetime O
image O
- O
free O
unsupervised B-TaskName
multimodal I-TaskName
machine I-TaskName
translation I-TaskName
, O
solved O
with O
a O
novel O
visual O
scene O
hallucination O
mechanism O
. O
▶ O
2 O
) O
We O
leverage O
the O
SGs O
to O
better O
represent O
the O
visual O
and O
language O
inputs O
. O
Moreover O
, O
we O
design O
SG O
- O
based O
graph O
pivoting O
learning O
strategies O
for O
UMMT B-TaskName
training O
. O
▶ O
3 O
) O
Our O
model O
achieves O
huge O
boosts O
over O
strong O
baselines O
on O
benchmark O
data O
. O
Code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
scofield7419 O
/ O
UMMT O
- O
VSH O
. O

Related O
Work O

Neural O
machine O
translation O
has O
achieved O
notable O
development O
in O
the O
era O
of O
deep O
learning O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Luong O
et O
al O
. O
, O
2015 O
) O
. O
The O
constructions O
of O
powerful O
neural O
models O
and O
training O
paradigms O
as O
well O
as O
the O
collection O
of O
large O
- O
scale O
parallel O
corpora O
are O
the O
driving O
forces O
to O
NMT O
's O
success O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
The O
key O
of O
NMT O
is O
to O
learn O
a O
good O
mapping O
between O
two O
( O
or O
more O
) O
languages O
. O
In O
recent O
years O
, O
visual O
information O
has O
been O
intro O
- O
duced O
for O
stronger O
NMT O
( O
i.e. O
, O
multimodal B-TaskName
machine I-TaskName
translation I-TaskName
) O
, O
by O
enhancing O
the O
alignments O
of O
language O
latent O
spaces O
with O
visual O
grounding O
Huang O
et O
al O
. O
, O
2016 O
) O
. O
Intuitively O
, O
people O
speaking O
different O
languages O
can O
actually O
refer O
to O
the O
same O
physical O
visual O
contents O
and O
conceptions O
. O

Unsupervised B-TaskName
machine I-TaskName
translation I-TaskName
aims O
to O
learn O
cross O
- O
lingual O
mapping O
without O
the O
use O
of O
largescale O
parallel O
corpora O
. O
The O
setting O
is O
practically O
meaningful O
to O
those O
minor O
languages O
with O
hard O
data O
accessibility O
. O
The O
basic O
idea O
is O
to O
leverage O
alternative O
pivoting O
contents O
to O
compensate O
the O
parallel O
signals O
based O
on O
the O
back O
- O
translation O
method O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
, O
such O
as O
third O
- O
languages O
, O
bilingual O
lexicons O
( O
Lample O
et O
al O
. O
, O
2018 O
) O
or O
multilingual O
LM O
( O
Conneau O
and O
Lample O
, O
2019 O
) O
. O
The O
visual O
information O
can O
also O
serve O
as O
pivot O
signals O
for O
UMT B-TaskName
, O
i.e. O
, O
unsupervised B-TaskName
multimodal I-TaskName
machine I-TaskName
translation I-TaskName
. O
Comparing O
to O
the O
standard O
MMT B-TaskName
that O
trains O
with O
< O
src O
- O
img O
- O
tgt O
> O
triples O
, O
UMMT B-TaskName
takes O
as O
input O
only O
the O
< O
src O
- O
img O
> O
. O
So O
far O
, O
few O
studies O
have O
explored O
the O
UMMT B-TaskName
setting O
, O
most O
of O
which O
try O
to O
enhance O
the O
back O
- O
translation O
with O
multimodal O
alignment O
mechanism O
( O
Nakayama O
and O
Nishida O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2018 O
; O
Su O
et O
al O
. O
, O
2019 O
; O
Huang O
et O
al O
. O
, O
2020 O
) O
. O

Scene O
graph O
describes O
a O
scene O
of O
an O
image O
or O
text O
into O
a O
structure O
layout O
, O
by O
connecting O
discrete O
objects O
with O
attributes O
and O
with O
other O
objects O
via O
pairwise O
relations O
( O
Krishna O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
As O
the O
SGs O
carry O
rich O
contextual O
and O
semantic O
information O
, O
they O
are O
widely O
integrated O
into O
downstream O
tasks O
for O
enhancements O
, O
e.g. O
, O
image O
retrieval O
( O
Johnson O
et O
al O
. O
, O
2015 O
) O
, O
image O
generation O
and O
image O
captioning O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O
This O
work O
inherits O
wisdom O
, O
incorporating O
both O
the O
visual O
scene O
graph O
and O
language O
scene O
graph O
as O
pivots O
for O
UMMT B-TaskName
. O

All O
the O
UMMT B-TaskName
researches O
assume O
that O
the O
< O
src O
- O
img O
> O
pairs O
are O
required O
during O
inference O
, O
yet O
we O
notice O
that O
this O
can O
be O
actually O
unrealistic O
. O
We O
thus O
propose O
a O
visual O
hallucination O
mechanism O
, O
achieving O
the O
inference O
- O
time O
image O
- O
free O
goal O
. O
There O
are O
relevant O
studies O
on O
supervised O
MMT O
that O
manage O
to O
avoid O
image O
inputs O
( O
with O
text O
only O
) O
during O
inference O
. O
The O
visual O
retrieval O
- O
base O
methods O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Fang O
and O
Feng O
, O
2022 O
) O

Framework O

As O
shown O
in O
Fig O
. O
2 O
, O
the O
system O
first O
represents O
the O
src O
- O
side O
LSG O
x O
and O
VSG O
features O
with O
two O
GCN O
graph O
encoders O
, O
respectively O
. O
Then O
the O
SG O
fus O
- O
ing O
& O
mapping O
module O
integrates O
and O
transforms O
two O
SG O
representations O
into O
a O
unified O
one O
as O
tgtside O
LSG O
, O
i.e. O
, O
LSG O
y O
. O
Another O
GSN O
model O
further O
encodes O
the O
LSG O
y O
, O
where O
the O
representations O
are O
used O
to O
generate O
the O
tgt O
sentence O
( O
i.e. O
, O
translation B-TaskName
) O
. O

Scene O
Graph O
Generating O
and O
Encoding O

We O
first O
employ O
two O
off O
- O
the O
- O
shelf O
SG O
parsers O
to O
obtain O
the O
LSG O
and O
VSG O
, O
separately O
( O
detailed O
in O
the O
experiment O
part O
) O
. O
For O
simplicity O
, O
here O
we O
unify O
the O
notations O
of O
LSG O
and O
VSG O
as O
SG O
. O
We O
denote O
a O
SG O
as O
G= O
( O
V O
, O
E O
) O
, O
where O
V O
are O
the O
nodes O
( O
including O
object O
o O
, O
attribute O
a O
and O
relation O
r O
types O
) O
, O
and O
E O
are O
the O
edges O
e O
i O
, O
j O
between O
any O
pair O
of O
nodes O
v O
i O
∈ O
V O
. O
We O
then O
encode O
both O
the O
VSG O
and O
LSG O
with O
two O
spatial O
Graph O
Convolution O
Networks O
( O
GCN O
) O
( O
Marcheggiani O
and O
Titov O
, O
2017 O
) O
respectively O
, O
which O
is O
formulated O
as O
: O

r O
1 O
, O
• O
• O
• O
, O
r O
n O
= O
GCN O
( O
G O
) O
, O
( O
1 O
) O

where O
r O
i O
is O
the O
representation O
of O
node O
v O
i O
. O
We O
here O
denote O
r O
L O
i O
as O
LSG O
's O
node O
representation O
, O
and O
r O
V O
i O
as O
VSG O
's O
node O
representation O
. O

Visual O
Scene O
Hallucinating O
During O
inference O
, O
the O
visual O
scene O
hallucination O
( O
VSH O
) O
module O
is O
activated O
to O
perform O
two O
- O
step O
inference O
to O
generate O
the O
hallucinated O
VSG O
′ O
, O
as O
illustrated O
in O
Fig O
. O
3 O
. O
Step1 O
: O
sketching O
skeleton O
aims O
to O
build O
the O
skeleton O
VSG O
. O
We O
copy O
all O
the O
nodes O
from O
the O
raw O
LSG O
x O
to O
the O
target O
VSG O
, O
and O
transform O
the O
textual O
entity O
nodes O
into O
the O
visual O
object O
nodes O
. O

Step2 O
: O
completing O
vision O
aims O
to O
enrich O
and O
augment O
the O
skeleton O
VSG O
into O
a O
more O
realistic O
one O
. O
It O
is O
indispensable O
to O
add O
new O
nodes O
and O
edges O
in O
the O
skeleton O
VSG O
, O
since O
in O
real O
scenarios O
, O
visual O
scenes O
are O
much O
more O
concrete O
and O
vivid O
than O
textual O
scenes O
. O
Specifically O
, O
we O
develop O
a O
node O
augmentor O
and O
a O
relation O
augmentor O
, O
where O
the O
former O
decides O
whether O
to O
attach O
a O
new O
node O
to O
an O
existing O
one O
, O
and O
the O
later O
decides O
whether O
to O
create O
an O
edge O
between O
two O
disjoint O
nodes O
. O
To O
ensure O
the O
fidelity O
of O
the O
hallucinated O
VSG O
′ O
, O
during O
training O
, O
the O
node O
augmentor O
and O
relation O
augmentor O
will O
be O
updated O
( O
i.e. O
, O
with O
the O
learning O
target O
L O
VSH O
) O
with O
the O
input O
LSG O
and O
VSG O
supervisions O
. O
Appendix O
§ O
A.1 O
details O
the O
VSH O
module O
. O

SG O
Fusing O
& O
Mapping O
Now O
we O
fuse O
the O
heterogeneous O
LSG O
x O
and O
VSG O
into O
one O
unified O
scene O
graph O
with O
a O
mixed O
view O
. O
The O
key O
idea O
is O
to O
merge O
the O
information O
from O
two O
SGs O
serving O
similar O
roles O
. O

In O
particular O
, O
we O
first O
measure O
the O
representation O
similarity O
of O
each O
pair O
of O
< O
text O
- O
img O
> O
nodes O
from O
two O
GCNs O
. O
For O
those O
pairs O
with O
high O
alignment O
scores O
, O
we O
merge O
them O
as O
one O
by O
averaging O
their O
representations O
, O
and O
for O
those O
not O
, O
we O
take O
the O
union O
structures O
from O
two O
SGs O
. O
This O
results O
in O
a O
pseudo O
tgt O
- O
side O
LSG O
y O
. O
We O
then O
use O
another O
GCN O
model O
for O
further O
representation O
propagation O
. O
Finally O
, O
we O
employ O
a O
graph O
- O
to O
- O
text O
generator O
to O
transform O
the O
LSG O
y O
representations O
to O
the O
tgt O
sentence O
y. O
Appendix O
§ O
A.2 O
presents O
all O
the O
technical O
details O
in O
this O
part O
. O

Learning O
with O
Scene O
Graph O
Pivoting O

In O
this O
part O
, O
based O
on O
the O
SG O
pivot O
we O
introduce O
several O
learning O
strategies O
to O
accomplish O
the O
unsupervised B-TaskName
training I-TaskName
of I-TaskName
machine I-TaskName
translation I-TaskName
. O
We O
mainly O
consider O
1 O
) O
cross B-MethodName
- I-MethodName
SG I-MethodName
visual I-MethodName
- I-MethodName
language I-MethodName
learning I-MethodName
, O
and O
2 O
) O
SG B-MethodName
- I-MethodName
pivoted I-MethodName
back I-MethodName
- I-MethodName
translation I-MethodName
training O
. O
Fig O
. O
4 O
illustrates O
these O
learning O
strategies O
. O

Cross B-MethodName
- I-MethodName
SG I-MethodName
Visual I-MethodName
- I-MethodName
language I-MethodName
Learning I-MethodName

The O
visual O
- O
language O
SG O
cross O
- O
learning O
aims O
to O
enhance O
the O
structural O
correspondence O
between O
the O
LSG O
and O
VSG O
. O
Via O
cross O
- O
learning O
we O
also O
teach O
the O
SG O
encoders O
to O
automatically O
learn O
to O
highlight O
those O
shared O
visual O
- O
language O
information O
while O
deactivating O
those O
trivial O
substructures O
, O
i.e. O
, O
denoising O
. O

Cross O
- O
modal O
SG O
Aligning O

The O
idea O
is O
to O
encourage O
the O
text O
and O
visual O
nodes O
that O
serve O
a O
similar O
role O
in O
VSG O
and O
LSG O
to O
be O
closer O
. O
To O
align O
the O
fine O
- O
grained O
structures O
between O
SGs O
, O
we O
adopt O
the O
contrastive O
learning O
( O
CL O
) O
technique O
( O
Logeswaran O
and O
Lee O
, O
2018 O
; O
Yan O
et O
al O
. O
, O
2021 O
; O
Huang O
et O
al O
. O
, O
2022 O
) O
. O
In O
particular O
, O
CL O
learns O
effec O
- O
tive O
representation O
by O
pulling O
semantically O
close O
content O
pairs O
together O
, O
while O
pushing O
apart O
those O
different O
ones O
. O
Technically O
, O
we O
measure O
the O
similarities O
between O
pairs O
of O
nodes O
from O
two O
VSG O
and O
LSG O
: O

si O
, O
j O
= O
( O
r O
L O
i O
) O
T O
• O
r O
V O
j O
||r O
L O
i O
|| O
||r O
V O
j O
|| O
. O
( O
2 O
) O

A O
threshold O
value O
α O
is O
pre O
- O
defined O
to O
decide O
the O
alignment O
confidence O
, O
i.e. O
, O
pairs O
with O
s O
i O
, O
j O
> O
α O
are O
considered O
similar O
. O
Then O
we O
put O
on O
the O
CL O
loss O
: O

L O
CMA O
= O
− O
i∈LSG O
x O
, O
j O
* O
∈VSG O
log O
exp O
( O
si O
, O
j O
* O
/ O
τ O
) O
Z O
, O
( O
3 O
) O

Z O
= O
i∈LSG O
x O
, O
k∈VSG O
, O
k̸ O
= O
j O
* O
exp O
( O
s O
i O
, O
k O
/ O
τ O
) O
, O
( O
4 O
) O

where O
τ O
> O
0 O
is O
an O
annealing O
factor O
. O
j O
* O
means O
a O
positive O
pair O
with O
i O
, O
i.e. O
, O
s O
i O
, O
j O
* O
> O
α O
. O

Cross O
- O
modal O
Cross O
- O
reconstruction O
We O
further O
strengthen O
the O
correspondence O
between O
VSG O
and O
LSG O
via O
cross O
- O
modal O
cross O
- O
reconstruction O
. O
Specifically O
, O
we O
try O
to O
reconstruct O
the O
input O
sentence O
from O
the O
VSG O
, O
and O
the O
image O
representations O
from O
the O
LSG O
. O
In O
this O
way O
we O
force O
both O
two O
SGs O
to O
focus O
on O
the O
VL O
- O
shared O
parts O
. O
To O
realize O
VSG→x O
we O
employ O
the O
aforementioned O
graph O
- O
to O
- O
text O
generator O
. O

For O
LSG→z O
, O
we O
use O
the O
graph O
- O
to O
- O
image O
generator O
. O
The O
learning O
loss O
can O
be O
marked O
as O
L O
REC O
. O

SG B-MethodName
- I-MethodName
pivoted I-MethodName
Back I-MethodName
- I-MethodName
translation I-MethodName
Training O

Back O
- O
translation O
is O
a O
key O
method O
to O
realize O
unsupervised O
machine O
translation O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O

In O
this O
work O
, O
we O
further O
aid O
the O
back O
- O
translation O
with O
structural O
SG O
pivoting O
. O

Visual O
- O
concomitant O
Back O
- O
translation O
We O
perform O
the O
back O
- O
translation O
with O
the O
SG O
pivoting O
. O
We O
denote O
the O
X O
→Y O
translation O
direction O
as O
y O
= O
F O
xz→y O
( O
x O
, O
z O
) O
, O
and O
Y→Z O
as O
x O
= O
F O
yz→x O
( O
y O
, O
z O
) O
. O

As O
we O
only O
have O
src O
- O
side O
sentences O
, O
the O
backtranslation O
is O
uni O
- O
directional O
, O
i.e. O
, O
x→ȳ→x O
. O

L O
VCB O
= O
E O
[ O
− O
log O
p O
yz→x O
( O
x|F O
xz→y O
( O
x O
, O
z O
) O
, O
z O
) O
] O
. O

( O
5 O
) O
Captioning O
- O
pivoted O
Back O
- O
translation O
Image O
captioning O
is O
partially O
similar O
to O
MMT O
besides O
the O
non O
- O
text O
part O
of O
the O
input O
. O
Inspired O
by O
Huang O
et O
al O
. O
( O
2020 O
) O
, O
based O
on O
the O
SG O
pivoting O
, O
we O
incorporate O
two O
captioning O
procedures O
, O
Z→X O
and O
Z→Y O
, O
to O
generate O
pseudo O
parallel O
sentences O
< O
x O
- O
ȳ O
> O
for O
back O
- O
translation O
and O
better O
align O
the O
language O
latent O
spaces O
. O
We O
denote O
Z→X O
asx O
= O
C O
z→x O
( O
z O
) O
, O
Z→Y O
as O
y O
= O
C O
z→y O
( O
z O
) O
. O
The O
back O
- O
translation O
loss O
will O
be O
: O

L O
CPB O
= O
E O
[ O
− O
log O
p O
( O
x|F O
xz→y O
( O
x O
, O
z O
) O
, O
z O
) O
] O
+ O
E O
[ O
− O
log O
p O
( O
ȳ|F O
yz→x O
( O
ȳ O
, O
z O
) O
, O
z O
) O
] O
. O
( O
6 O
) O

⋆ O
Remarks O
In O
the O
initial O
stage O
, O
each O
of O
the O
above O
learning O
objectives O
will O
be O
executed O
separately O
, O
in O
a O
certain O
order O
, O
so O
as O
to O
maintain O
a O
stable O
and O
effective O
UMMT B-TaskName
system O
. O
We O
first O
perform O
L O
CMA O
and O
L O
REC O
, O
because O
the O
cross O
- O
SG O
visual O
- O
language O
learning O
is O
responsible O
for O
aligning O
the O
VL O
SGs O
, O
based O
on O
which O
the O
high O
- O
level O
translation O
can O
happen O
. O
Then O
we O
perform O
back O
- O
translation O
training O
L O
VCB O
and O
L O
CPB O
, O
together O
with O
VSH O
updating O
L O
VSH O
. O

Once O
the O
system O
tends O
to O
converge O
, O
we O
put O
them O
all O
together O
for O
further O
fine O
- O
tuning O
: O

L O
= O
L O
CMA O
+ O
L O
REC O
+ O
L O
VCB O
+ O
L O
CPB O
+ O
L O
VSH O
. O
( O
7 O
) O

5 O
Experiments O

Setups O

The O
experiments O
are O
carried O
out O
on O
Multi30 B-DatasetName
K I-DatasetName
data O
, O
a O
benchmark O
for O
MMT B-TaskName
, O
where O
each O
image O
comes O
with O
three O
parallel O
descriptions O
in O
English O
/ O
German O
/ O
French O
. O
Following O
Huang O
et O
al O
. O

( O
2020 O
) O
, O
we O
mainly O
consider O
the O
English O
- O
French O
( O
En↔Fr O
) O
and O
English O
- O
German O
( O
En↔De O
) O
. O
For O
each O
translation O
direction O
, O
we O
only O
use O
the O
src O
sentence O
& O
img O
for O
training O
, O
and O
only O
the O
src O
sentence O
for O
testing O
. O
We O
also O
test O
on O
the O
WMT16 B-DatasetName
En→Ro O
and O
WMT14 O
En→De O
, O
En→Fr O
. O
WMT B-DatasetName
( O
Bojar O
et O
al O
. O
, O
2014 O
( O
Bojar O
et O
al O
. O
, O
, O
2016 O
) O
is O
widely O
- O
used O
text O
- O
only O
translation O
corpora O
, O
where O
following O
Li O
et O
al O
. O
( O
2022 O
) O
, O
we O
use O
CLIP O
( O
Radford O
et O
al O
. O
, O
2021 O
) O
to O
retrieve O
images O
from O
Multi30 B-DatasetName
K I-DatasetName
for O
sentences O
. O
Following O
prior O
research O
, O
we O
employ O
the O
Faster O
- O
RCNN O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
as O
an O
object O
detector O
, O
and O
MOTIFS O
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
as O
a O
relation O
classifier O
and O
an O
attribute O
classifier O
, O
where O
these O
three O
together O
form O
a O
VSG O
generator O
. O
For O
LSG O
generation O
, O
we O
convert O
the O
sentences O
into O
dependency O
trees O
with O
a O
parser O
( O
Anderson O
et O
al O
. O
, O
2018 O
) O
, O
which O
is O
then O
transformed O
into O
the O
scene O
graph O
based O
on O
certain O
rules O
( O
Schuster O
et O
al O
. O
, O
2015 O
) O
. O
For O
text O
preprocessing O
, O
we O
use O
Moses O
( O
Koehn O
et O
al O
. O
, O
2007 O
) O
image O
- O
free O
setup O
, O
we O
also O
re O
- O
implement O
the O
UMMT B-MethodName
and O
PVP B-MethodName
by O
integrating O
the O
phrase O
- O
level O
retrievalbased O
visual O
hallucination O
method O
( O
Fang O
and O
Feng O
, O
2022 O
) O
. O
All O
models O
use O
the O
same O
fair O
configurations O
, O
and O
we O
do O
not O
use O
pre O
- O
trained O
LM O
. O
On O
WMT B-DatasetName
we O
also O
test O
the O
supervised B-TaskName
MMT I-TaskName
setup O
, O
where O
we O
use O
these O
baselines O
: O
UVR B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
RMMT B-MethodName
( O
Wu O
et O
al O
. O
, O
2021b O
) O
, O
PUVR B-MethodName
( O
Fang O
and O
Feng O
, O
2022 O
) O
and O
VALHALLA B-MethodName
( O
Li O
et O
al O
. O
, O
2022 O
) O
. O
We O
report O
the O
BLEU B-MetricName
and O
METEOR B-MetricName
scores O
for O
model O
evaluation O
. O

Our O
results O
are O
computed O
with O
a O
model O
averaging O
over O
5 O
latest O
checkpoints O
with O
significance O
test O
. O
Our O
experiments O
are O
based O
on O
the O
NVIDIA O
A100 O
Tensor O
Core O
GPUs O
. O

Main O
Results O

Results O
on O
Multi30 B-DatasetName
K I-DatasetName

In O
Table O
2 O
we O
show O
the O
overall O
results O
on O
Multi30 B-DatasetName
K I-DatasetName
data O
. O
First O
, O
we O
inspect O
the O
performance O
where O
gold O
- O
paired O
images O
are O
given O
as O
inputs O
for O
testing O
. O
We O
see O
that O
our B-MethodName
method I-MethodName
( O
Ours B-MethodName
# I-MethodName
) O
, O
by O
integrating O
the O
LSG O
and O
VSG O
information O
, O
shows O
clear O
superiority O
over O
baselines O
on O
all O
translation O
jobs O
, O
while O
ablating O
the O
SGs O
, O
the O
performance O
drops O
rapidly O
. O
This O
shows O
the O
importance O
of O
leveraging O
scene O
graphs O
for O
more O
effective O
En→Ro O
En→De O
En→Fr O
Avg O
. O
zwei O
fahrräder O
stehen O
hinter O
zwei O
mann O
mit O
den O
eingetopften O
graspflanzen O
in O
der O
nähe O
des O
meeres O
. O
man O
in O
t O
- O
shirt O
and O
shorts O
kicking O
football O
off O
tee O
. O

< O
Query O
> O
< O
Query O
> O
< O
Query O
> O
< O
Query O
> O
mann O
in O
hemd O
und O
hose O
, O
der O
fußball O
spielt O
. O

( O
man O
in O
shirt O
and O
pants O
playing O
football O
. O
) O
mann O
in O
t O
- O
shirt O
und O
shorts O
tritt O
fußball O
vom O
tee O
. O

( O
man O
in O
t O
- O
shirt O
and O
shorts O
kicks O
football O
off O
the O
tee O
. O
) O
( O
two O
bicycles O
stand O
behind O
two O
people O
sitting O
on O
the O
grass O
near O
a O
body O
of O
water O
. O
) O
back O
- O
translation O
influences O
the O
results O
the O
biggest O
, O
with O
an O
average O
4.3 B-MetricValue
BLEU B-MetricName
score O
. O
Overall O
, O
two O
SG O
- O
pivoted O
back O
- O
translation O
training O
targets O
show O
much O
higher O
influences O
than O
the O
two O
cross O
- O
SG O
visual O
- O
language O
learning O
objectives O
. O
When O
removing O
both O
two O
back O
- O
translation O
targets O
, O
we O
witness O
the O
most O
dramatic O
decrease O
, O
i.e. O
, O
average O
-5.7 B-MetricValue
BLEU B-MetricName
. O
This O
validates O
the O
long O
- O
standing O
finding O
that O
the O
back O
- O
translation O
mechanism O
is O
key O
to O
unsupervised O
translation O
( O
Sennrich O
et O
al O
. O
, O
2016 O
; O
Huang O
et O
al O
. O
, O
2020 O
) O
. O
unsupervised O
MMT O
. O
We O
can O
find O
that O
our B-MethodName
unsupervised I-MethodName
method I-MethodName
only O
loses O
within O
1 B-MetricValue
BLEU B-MetricName
score O
to O
supervised O
models O
, O
e.g. O
, O
UVR B-MethodName
and O
PUVR B-MethodName
. O

Results O
on O
WMT B-DatasetName

Further O
Analyses O
and O
Discussions O

In O
this O
part O
we O
try O
to O
dive O
deep O
into O
the O
model O
, O
presenting O
in O
- O
depth O
analyses O
to O
reveal O
what O
and O
how O
our O
proposed O
method O
really O
works O
and O
improves O
. O

• O
Integration O
of O
the O
vision O
and O
language O
SGs O
helps O
gain O
a O
holistic O
understanding O
of O
input O
. O

Both O
VSG O
and O
LSG O
advance O
in O
comprehensively O
depicting O
the O
intrinsic O
structure O
of O
the O
content O
semantics O
, O
which O
ensures O
a O
holistic O
understanding O
of O
the O
input O
texts O
and O
images O
. O
By O
encoding O
the O
vision O
and O
language O
SGs O
, O
it O
is O
expected O
to O
completely O
capture O
the O
key O
components O
from O
src O
inputs O
, O
and O
thus O
achieve O
better O
translations O
. O
However O
, O
without O
such O
structural O
features O
, O
some O
information O
may O
be O
lost O
during O
the O
translation O
. O
In O
Table O
5 O
via O
human B-MetricName
evalua- I-MetricName
tion I-MetricName
we O
can O
see O
that O
our B-MethodName
system I-MethodName
obtains O
significantly O
higher O
scores O
in O
terms O
of O
the O
completeness B-MetricName
, O
comparing O
to O
those O
baselines O
without O
considering O
SGs O
. O
Also O
in O
Fig O
. O
5 O
, O
we O
can O
find O
that O
the O
baseline O
system O
PVP B-MethodName
* I-MethodName
( I-MethodName
PR I-MethodName
) I-MethodName
, O
with O
only O
the O
local O
- O
level O
phrase O
- O
level O
visual O
retrieval O
, O
has O
frequently O
missed O
the O
key O
entities O
during O
the O
translation O
, O
e.g. O
, O
the O
object O
' O
tee O
' O
in O
case O
# O
2 O
. O

• O
SG O
- O
based O
multimodal O
feature O
modeling O
helps O
achieve O
more O
accurate O
alignment O
between O
vision O
and O
language O
. O
Another O
merit O
to O
integrating O
the O
SGs O
is O
that O
the O
fine O
- O
grained O
graph O
modeling O
of O
visual O
and O
language O
scenes O
obviously O
aids O
more O
precise O
multimodal O
feature O
alignment O
. O
In O
this O
way O
, O
the O
translated O
texts O
have O
higher O
fidelity O
to O
the O
original O
texts O
. O
Inaccurate O
multimodal O
alignment O
without O
considering O
the O
SG O
modeling O
will O
otherwise O
lead O
to O
worse O
ambiguity O
. O
Observing O
the O
ambiguity O
in O
Table O
5 O
, O
we O
see O
that O
our B-MethodName
model I-MethodName
exhibits O
the O
lowest O
ambiguity O
. O
In O
Fig O
. O
5 O
for O
the O
case O
# O
3 O
, O
PVP B-MethodName
* I-MethodName
( I-MethodName
PR I-MethodName
) I-MethodName
confuses O
the O
verb O
' O
saw O
' O
as O
' O
see O
' O
as O
it O
fails O
to O
accurately O
refer O
' O
saw O
' O
to O
a O
certain O
lumbering O
tool O
, O
while O
ours B-MethodName
gives O
a O
correct O
prediction O
. O
Besides O
, O
accurate O
multimodal O
alignment O
greatly O
enhances O
the O
utility O
of O
visual O
information O
. O
In O
Table O
6 O
we O
compare O
the O
relevance O
of O
vision O
- O
language O
counterparts O
by O
different O
models O
, O
where O
our B-MethodName
model I-MethodName
gives O
the O
highest O
performance O
on O
both O
the O
overall O
text O
- O
image O
matching O
and O
the O
regional O
phrase O
- O
object O
matching O
. O
In O
addition O
, O
two O
proposed O
cross O
- O
SG O
learning O
targets O
display O
big O
impacts O
on O
the O
VL O
- O
aligning O
ability O
. O

• O
The O
longer O
and O
more O
complex O
the O
sentences O
, O
the O
higher O
the O
translation O
quality O
benefiting O
from O
the O
SGs O
features O
. O
In O
this O
work O
, O
we O
investigate O
the O
SG O
structures O
to O
model O
the O
input O
texts O
. O
Graph O
modeling O
of O
the O
texts O
has O
proven O
effective O
for O
resolving O
the O
long O
- O
range O
dependency O
issue O
( O
Marcheggiani O
and O
Titov O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2022 O
) O
. O
In O
Fig O
. O
6 O
we O
group O
the O
translation O
performance O
based O
on O
the O
lengths O
of O
source O
sentences O
. O
We O
see O
that O
our O
SG O
- O
based O
model O
gives O
very O
considerable O
gains O
over O
the O
two O
non O
- O
SG O
baselines O
, O
where O
the O
longer O
the O
sentences O
the O
higher O
the O
improvements O
. O

• O
Incorporating O
SGs O
into O
MMT O
advances O
in O
more O
fluent O
translation O
. O
Also O
, O
modeling O
the O
semantic O
scene O
graph O
of O
the O
input O
features O
contributes O
a O
lot O
to O
the O
language O
fluency O
of O
the O
translation O
texts O
. O

Looking O
at O
the O
Fluency B-MetricName
item O
in O
Table O
5 O
, O
we O
find O
that O
our O
system O
gives O
the O
best O
fluency B-MetricName
with O
the O
lowest O
grammar O
errors O
. O

• O
SG O
- O
based O
visual O
scene O
hallucination O
mechanism O
helps O
gain O
rich O
and O
correct O
visual O
features O
. O
Different O
from O
the O
baseline O
retrieval O
- O
based O
methods O
that O
directly O
obtain O
the O
whole O
images O
( O
or O
local O
regions O
) O
, O
our O
proposed O
VSH O
mechanism O
instead O
compensatively O
generates O
the O
VSGs O
from O
the O
given O
LSGs O
. O
In O
this O
way O
, O
the O
hallucinated O
visual O
features O
enjoy O
two O
- O
fold O
advantages O
. O
On O
the O
one O
hand O
, O
the O
pseudo O
VSG O
has O
high O
correspondence O
with O
the O
textual O
one O
, O
both O
of O
which O
will O
enhance O
the O
shared O
feature O
learning O
between O
the O
two O
modalities O
. O
On O
the O
other O
hand O
, O
the O
hallucinated O
VSG O
will O
produce O
some O
vision O
- O
specific O
scene O
components O
and O
structures O
, O
providing O
additional O
clues O
to O
facilitate O
back O
to O
the O
textual O
features O
for O
overall O
better O
semantic O
understanding O
. O
Fig O
. O
7 O
illustrates O
the O
node O
increasing O
rate O
during O
the O
vision O
scene O
graph O
hallucination O
. O
We O
see O
that O
the O
numbers O
of O
all O
three O
types O
of O
nodes O
increase O
, O
to O
different O
extents O
, O
where O
object O
nodes O
grow O
rapidest O
. O
Also O
, O
during O
the O
two O
transition O
steps O
of O
the O
VSH O
mechanism O
we O
get O
two O
VSGs O
, O
skeleton O
VSG O
and O
hallucinated O
VSG O
. O
From O
Fig O
. O
8 O
we O
see O
that O
after O
two O
full O
hallucination O
steps O
, O
we O
can O
obtain O
high O
- O
fidelity O
vision O
features O
, O
demonstrating O
the O
necessity O
of O
the O
second O
completing O
- O
vision O
step O
. O

Conclusion O

We O
investigate O
an O
inference O
- O
time O
image O
- O
free O
setup O
in O
unsupervised B-TaskName
multimodal I-TaskName
machine I-TaskName
translation I-TaskName
. O

In O
specific O
, O
we O
integrate O
the O
visual O
and O
language O
scene O
graph O
to O
learn O
the O
fine O
- O
grained O
visionlanguage O
representations O
. O
Moreover O
, O
we O
present O
a O
visual O
scene O
hallucination O
mechanism O
to O
generate O
pseudo O
visual O
features O
during O
inference O
. O
We O
then O
propose O
several O
SG O
- O
pivoting O
learning O
objectives O
for O
unsupervised O
translation O
training O
. O
Experiments O
demonstrate O
the O
effectiveness O
of O
our O
SG B-MethodName
- I-MethodName
pivoting I-MethodName
based I-MethodName
UMMT I-MethodName
. O
Further O
experimental O
analyses O
present O
a O
deep O
understanding O
of O
how O
our O
method O
advances O
the O
task O
and O
setup O
. O

Limitations O

Our O
paper O
has O
the O
following O
potential O
limitations O
. O
First O
of O
all O
, O
we O
take O
advantage O
of O
the O
external O
scene O
graph O
structures O
to O
achieve O
the O
inference O
- O
time O
visual O
hallucination O
and O
secure O
significant O
improvements O
of O
the O
target O
task O
, O
while O
it O
could O
be O
a O
doubleedged O
sword O
. O
This O
makes O
our O
method O
subject O
to O
the O
quality O
of O
the O
external O
structure O
parsers O
. O
When O
the O
parsed O
structures O
of O
visual O
scene O
graphs O
and O
language O
scene O
graphs O
are O
with O
much O
noise O
, O
it O
will O
deteriorate O
our O
methods O
. O
Fortunately O
, O
the O
existing O
scene O
graph O
parsers O
have O
already O
achieved O
satisfactory O
performance O
for O
the O
majority O
language O
( O
e.g. O
, O
English O
) O
, O
which O
can O
meet O
our O
demands O
. O
Second O
, O
the O
effectiveness O
of O
our B-MethodName
approach I-MethodName
depends O
on O
the O
availability O
of O
good O
- O
quality O
images O
, O
which O
however O
shares O
the O
pitfalls O
associated O
with O
the O
standard O
unsupervised O
multimodal O
translation O
setup O
. O

A O
Appendix O

In O
§ O
3.2 O
we O
give O
a O
brief O
induction O
to O
the O
overall O
model O
framework O
. O
Here O
we O
extend O
the O
details O
of O
each O
module O
of O
the O
scene O
graph O
- O
based O
multimodal O
translation O
backbone O
. O
In O
Fig O
. O
9 O
we O
outline O
our O
framework O
. O

A.1 O
Visual O
Scene O
Hallucination O
Learning O
Module O

First O
of O
all O
, O
we O
note O
that O
VSH O
only O
will O
be O
activated O
to O
produce O
VSG O
hallucination O
at O
inference O
time O
. O
CLIP O
tool O
1 O
to O
search O
for O
the O
best O
matching O
visual O
node O
( O
proposal O
) O
in O
D O
o O
as O
the O
counterpart O
visual O
object O
, O
resulting O
in O
the O
skeleton O
VSG O
. O
After O
this O
step O
, O
we O
obtain O
the O
sketch O
structure O
of O
the O
target O
VSG O
. O

Step2 O
: O
Completing O
Vision O
This O
step O
completes O
the O
skeleton O
VSG O
into O
a O
more O
realistic O
one O
, O
i.e. O
, O
the O
final O
hallucinated O
VSG O
′ O
. O
With O
the O
skeleton O
VSG O
at O
hand O
, O
we O
aim O
to O
further O
enrich O
skeleton O
VSG O
. O
Because O
intuitively O
, O
in O
actual O
world O
the O
visual O
scenes O
are O
always O
much O
more O
concrete O
and O
vivid O
than O
textual O
scenes O
. O
For O
example O
, O
given O
a O
caption O
text O
' O
boys O
are O
playing O
baseball O
on O
playground O
' O
, O
the O
LSG O
only O
mentions O
' O
boys O
' O
, O
' O
baseball O
' O
and O
' O
playground O
' O
objects O
. O
But O
imaginarily O
, O
there O
must O
be O
a O
' O
baseball O
bat O
' O
in O
the O
scene O
of O
vision O
, O
and O
also O
both O
the O
pairs O
of O
' O
boys'-'playground O
' O
and O
' O
baseball'-'playground O
' O
has O
' O
on O
' O
relation O
. O
Thus O
it O
is O
indispensable O
to O
add O
new O
nodes O
and O
more O
edges O
, O
i.e. O
, O
scene O
graph O
augmentation O
. O
To O
reach O
the O
goal O
, O
we O
propose O
a O
node O
augmentor O
and O
a O
relation O
augmentor O
, O
as O
shown O
in O
Fig O
. O
10 O
. O
First O
of O
all O
, O
we O
downgrade O
all O
the O
relation O
nodes O
as O
the O
edge O
itself O
, O
i.e. O
, O
an O
edge O
with O
a O
relation O
label O
. O
By O
this O
, O
we O
obtain O
a O
VSG O
that O
only O
contains O
object O
and O
attribute O
nodes O
, O
and O
labeled O
edges O
, O
which O
is O
illustrated O
in O
Fig O
. O
11 O
. O
▶ O
For O
the O
node O
augmentor O
, O
we O
first O
traverse O
all O
the O
object O
nodes O
in O
the O
skeleton O
VSG O
. O
For O
each O
object O
node O
v O
i O
, O
we O
then O
perform O
k O
- O
order O
routing O
over O
its O
neighbor O
nodes O
. O
We O
denote O
its O
neighbor O
nodes O
as O

V O
na O
i O
= O
{ O
• O
• O
• O
, O
v O
k O
, O
• O
• O
• O
} O
. O

Then O
we O
use O
the O
attention O
to O
learn O
the O
neighbor O
influence O
to O
v O
i O
, O
and O
obtain O
the O
k O
- O
order O
feature O
representation O
h O
i O
of O
v O
i O
: O

α O
n O
k O
= O
exp O
r O
i O
• O
r O
k O

and O
v O
k O
, O
which O
are O
obtained O
from O
GCN O
encoder O
. O

Then O
we O
use O
a O
classifier O
to O
make O
prediction O
over O
the O
total O
vocabularies O
of O
D O
o O
and O
D O
a O
, O
to O
determine O
which O
nodev O
′ O
i O
( O
either O
an O
object O
or O
an O
attribute O
node O
) O
should O
be O
attached O
to O
v O
i O
, O
if O
any O
: O
( O
FFN O
( O
[ O
h O
na O
i O
; O
r O
i O
) O
) O
. O
▶ O
For O
the O
relation O
augmentor O
, O
we O
first O
traverse O
all O
the O
node O
- O
pairs O
( O
object O
or O
attribute O
nodes O
, O
excluding O
the O
relation O
nodes O
) O
in O
the O
VSG O
, O
i.e. O
, O
v O
i O
& O
v O
j O
. O
Then O
, O
for O
each O
node O
in O
the O
pair O
we O
use O
a O
triaffine O
attention O
( O
Wang O
et O
al O
. O
, O
2019 O
; O
Wu O
et O
al O
. O
, O
2021a O
) O
to O
directly O
determine O
which O
new O
relation O
typeê O
′ O
i O
, O
j O
should O
be O
built O
between O
them O
, O
if O
exists O
: O

v O
′ O
i O
← O
Softmax O
D O
na O
( O
FFN O
( O
h O
na O
i O
) O
) O
, O
where O
D O
na O
= O
D O
o O
∪ O
D O
a O
∪ O

h O
pa O
i−j O
= O
Sigmoid O
( O
r O
i O
1 O
T O
( O
r O
j O
) O
T O
W O
r O
i−j O
1 O
) O
, O
e O
′ O
i O
, O
j O
← O
Softmax O
D O
pa O
( O
FFN O
( O
h O
pa O
i−j O
) O
) O
, O

where O
D O
pa O
= O
D O
r O
∪ O
{ O
ϵ O
} O
, O
where O
the O
dummy O
token O
ϵ O
indicates O
no O
new O
edge O
should O
be O
created O
between O
two O
nodes O
. O
The O
new O
edgeê O
′ O
i O
, O
j O
has O
a O
relation O
label O
. O
r O
i−j O
is O
the O
representation O
of O
the O
path O
from O
v O
i O
to O
v O
j O
, O
which O
is O
obtained O
by O
the O
pooling O
function O
over O
all O
the O
nodes O
in O
the O
path O
: O
h O
pa O
i−j O
= O
Pool O
( O
r O
i O
, O
• O
• O
• O
, O
r O
j O
) O
. O
Note O
that O
the O
triaffine O
scorer O
is O
effective O
in O
modeling O
the O
high O
- O
order O
ternary O
relations O
, O
which O
will O
provide O
a O
precise O
determination O
on O
whether O
to O
add O
a O
new O
edge O
. O

During O
training O
, O
the O
node O
augmentor O
and O
the O
relation O
augmentor O
are O
trained O
and O
updated O
based O
on O
the O
gold O
LSG O
and O
VSG O
, O
to O
learn O
the O
correct O
mapping O
between O
LSG O
and O
VSG O
. O

L O
N O
A O
= O
[ O
log O
p O
( O
v O
′ O
i O
|V O
SG O
← O
LSG O
) O
+ O
log O
p O
( O
ê O
′ O
i O
, O
j O
|V O
SG O
← O
LSG O
) O
] O
, O
L O
P O
A O
= O
log O
p O
( O
ê O
′ O
i O
, O
j O
|V O
SG O
← O
LSG O
) O
, O
L O
VSH O
= O
L O
N O
A O
+ O
L O
P O
A O
. O

Such O
supervised O
learning O
is O
also O
important O
for O
ensuring O
that O
the O
final O
generated O
hallucinated O
visual O
scenes O
are O
basically O
coincident O
with O
the O
caption O
text O
, O
instead O
of O
random O
or O
groundless O
vision O
scenes O
. O

A.2 O
SG O
Fusing O
& O
Mapping O
Module O

Here O
we O
extend O
the O
contents O
in O
§ O
3.2 O
. O
As O
shown O
in O
Fig O
. O
9 O
, O
first O
of O
all O
, O
the O
SG O
fusing O
module O
merges O
the O
LSG O
x O
and O
VSG O
into O
a O
mixed O
cross O
- O
modal O
scene O
graph O
, O
such O
that O
the O
merged O
scene O
graph O
are O
highly O
compact O
with O
less O
redundant O
. O
Before O
the O
merging O
, O
we O
first O
measure O
the O
similarity O
of O
each O
pair O
of O
< O
text O
- O
img O
> O
node O
representations O
via O
cosine O
distance O
: O

s O
f O
i O
, O
j O
= O
( O
r O
L O
i O
) O
T O
• O
r O
V O
j O
||r O
L O
i O
|| O
||r O
V O
j O
|| O
. O

This O
is O
a O
similar O
process O
as O
in O
Eq O
. O
( O
2 O
) O
. O
For O
those O
pairs O
with O
high O
alignment O
scores O
, O
i.e. O
, O
s O
i O
, O
j O
> O
α O
( O
we O
use O
the O
same O
pre O
- O
defined O
threshold O
as O
in O
crossmodal O
alignment O
learning O
) O
, O
we O
consider O
them O
as O
serving O
a O
similar O
role O
. O
Since O
we O
will O
perform O
the O
cross O
- O
modal O
SG O
aligning O
learning O
L O
CMA O
, O
the O
accuracy O
of O
the O
alignment O
between O
LSG O
x O
and O
VSG O
can O
be O
guaranteed O
. O
Then O
, O
we O
average O
the O
representations O
of O
the O
image O
- O
text O
node O
pair O
from O
their O
GCNs O
. O
And O
for O
the O
rest O
of O
nodes O
in O
LSG O
x O
and O
VSG O
, O
we O
take O
the O
union O
structures O
of O
them O
. O
The O
resulting O
mixed O
SG O
fully O
inherits O
the O
semantic O
- O
rich O
scene O
nodes O
from O
both O
the O
textual O
SG O
and O
the O
visual O
SG O
, O
which O
will O
benefit O
the O
following O
text O
generation O
. O
Now O
we O
treat O
the O
mixed O
SG O
as O
a O
pseudo O
tgt O
- O
side O
LSG O
y O
. O
We O
use O
another O
GCN O
to O
model O
LSG O
y O
for O
further O
feature O
propagation O
: O
r O
y O
1 O
, O
• O
• O
• O
, O
r O
y O
m O
= O
GCN O
( O
V O
SG O
y O
) O
. O
The O
initial O
node O
representations O
of O
GCN O
are O
from O
the O
GCNs O
of O
VSG O
and O
LSG O
x O
, O
i.e. O
, O
r O
L O
and O
r O
V O
as O
in O
Eq O
. O
( O
1 O
) O
. O
Based O
on O
the O
node O
representation O
r O
y O
i O
of O
VSG O
y O
, O
we O
finally O
employ O
a O
graph O
- O
to O
- O
text O
model O
2 O
to O
generate O
the O
final O
tgt O
- O
side O
sentence O
. O
Specifically O
, O
all O
the O
node O
representation O
r O
i O
will O
be O
first O
summarized O
into O
one O
unified O
graph O
- O
level O
feature O
via O
pooling O
: O

r O
y O
= O
Pool O
( O
r O
y O
1 O
, O
• O
• O
• O
, O
r O
y O
m O
) O
. O

Then O
, O
an O
autoregressive O
sequential O
decoder O
( O
Se O
- O
qDec O
) O
will O
take O
r O
y O
to O
generate O
tgt O
- O
side O
token O
over O
the O
tgt O
- O
side O
vocabulary O
at O
each O
setp O
, O
sequentially O
: O
e O
i O
= O
SeqDec O
( O
e O
≤i O
, O
r O
y O
) O
, O
y O
i O
← O
Softmax O
( O
e O
i O
) O
. O

Acknowledgments O

This O
research O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62176180 O
) O
, O
and O
also O
the O
Sea O
- O
NExT O
Joint O
Lab O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
No O
response O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Appendix O
B O
B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Appendix O
B O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Appendix O
B O
C O
Did O
you O
run O
computational O
experiments O
? O

Appendix O
B O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Appendix O
B O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

5993 O

C2 O
. O
Did O
you O
discuss O
the O
experimental O
setup O
, O
including O
hyperparameter O
search O
and O
best O
- O
found O
hyperparameter O
values O
? O
Appendix O
B O
C3 O
. O
Did O
you O
report O
descriptive O
statistics O
about O
your O
results O
( O
e.g. O
, O
error O
bars O
around O
results O
, O
summary O
statistics O
from O
sets O
of O
experiments O
) O
, O
and O
is O
it O
transparent O
whether O
you O
are O
reporting O
the O
max O
, O
mean O
, O
etc O
. O
or O
just O
a O
single O
run O
? O
5 O
C4 O
. O
If O
you O
used O
existing O
packages O
( O
e.g. O
, O
for O
preprocessing O
, O
for O
normalization O
, O
or O
for O
evaluation O
) O
, O
did O
you O
report O
the O
implementation O
, O
model O
, O
and O
parameter O
settings O
used O
( O
e.g. O
, O
NLTK O
, O
Spacy O
, O
ROUGE O
, O
etc O
. O
) O
? O
Appendix O
B O
D O
Did O
you O
use O
human O
annotators O
( O
e.g. O
, O
crowdworkers O
) O
or O
research O
with O
human O
participants O
? O
5 O
D1 O
. O
Did O
you O
report O
the O
full O
text O
of O
instructions O
given O
to O
participants O
, O
including O
e.g. O
, O
screenshots O
, O
disclaimers O
of O
any O
risks O
to O
participants O
or O
annotators O
, O
etc O
. O
? O
Appendix O
B O
D2 O
. O
Did O
you O
report O
information O
about O
how O
you O
recruited O
( O
e.g. O
, O
crowdsourcing O
platform O
, O
students O
) O
and O
paid O
participants O
, O
and O
discuss O
if O
such O
payment O
is O
adequate O
given O
the O
participants O
' O
demographic O
( O
e.g. O
, O
country O
of O
residence O
) O
? O
Appendix O
B O
D3 O
. O
Did O
you O
discuss O
whether O
and O
how O
consent O
was O
obtained O
from O
people O
whose O
data O
you O
're O
using O
/ O
curating O
? O
For O
example O
, O
if O
you O
collected O
data O
via O
crowdsourcing O
, O
did O
your O
instructions O
to O
crowdworkers O
explain O
how O
the O
data O
would O
be O
used O
? O
Appendix O
B O
D4 O
. O
Was O
the O
data O
collection O
protocol O
approved O
( O
or O
determined O
exempt O
) O
by O
an O
ethics O
review O
board O
? O
Not O
applicable O
. O
Left O
blank O
. O

D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
Not O
applicable O
. O
Left O
blank O
. O

Learning B-MethodName
to I-MethodName
Adapt I-MethodName
to I-MethodName
Low I-MethodName
- I-MethodName
Resource I-MethodName
Paraphrase I-MethodName
Generation I-MethodName

Paraphrase B-TaskName
generation I-TaskName
is O
a O
longstanding O
NLP O
task O
and O
achieves O
great O
success O
with O
the O
aid O
of O
large O
corpora O
. O
However O
, O
transferring O
a O
paraphrasing O
model O
to O
another O
domain O
encounters O
the O
problem O
of O
domain O
shifting O
especially O
when O
the O
data O
is O
sparse O
. O
At O
the O
same O
time O
, O
widely O
using O
large O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
faces O
the O
overfitting O
problem O
when O
training O
on O
scarce O
labeled O
data O
. O
To O
mitigate O
these O
two O
issues O
, O
we O
propose O
, O
LAPA B-MethodName
, O
an O
effective O
adapter O
for O
PLMs O
optimized O
by O
meta O
- O
learning O
. O
LAPA B-MethodName
has O
three O
- O
stage O
training O
on O
three O
types O
of O
related O
resources O
to O
solve O
this O
problem O
: O
1 O
. O
pre O
- O
training O
PLMs O
on O
unsupervised O
corpora O
, O
2 O
. O
inserting O
an O
adapter O
layer O
and O
meta O
- O
training O
on O
source O
domain O
labeled O
data O
, O
and O
3 O
. O
fine O
- O
tuning O
adapters O
on O
a O
small O
amount O
of O
target O
domain O
labeled O
data O
. O
This O
method O
enables O
paraphrase B-TaskName
generation I-TaskName
models O
to O
learn O
basic O
language O
knowledge O
first O
, O
then O
learn O
the O
paraphrasing O
task O
itself O
later O
, O
and O
finally O
adapt O
to O
the O
target O
task O
. O
Our O
experimental O
results O
demonstrate O
that O
LAPA B-MethodName
achieves O
state O
- O
of O
- O
the O
- O
art O
in O
supervised O
, O
unsupervised O
, O
and O
low O
- O
resource O
settings O
on O
three O
benchmark O
datasets O
. O
With O
only O
2 O
% O
of O
trainable O
parameters O
and O
1 O
% O
labeled O
data O
of O
the O
target O
task O
, O
our O
approach O
can O
achieve O
a O
competitive O
performance O
with O
previous O
work O
. O

Introduction O

Paraphrase B-TaskName
generation I-TaskName
can O
comprehend O
a O
sentence O
and O
generate O
another O
with O
the O
same O
semantics O
but O
with O
variations O
in O
lexicon O
or O
syntax O
, O
which O
has O
various O
applications O
on O
downstream O
tasks O
including O
query O
rewriting O
( O
Dong O
et O
al O
. O
, O
2017 O
) O
, O
data O
augmentation O
( O
Iyyer O
et O
al O
. O
, O
2018 O
) O
and O
language O
model O
pre O
- O
training O
( O
Lewis O
et O
al O
. O
, O
2020a O
) O
. O
Conventional O
approaches O
( O
Prakash O
et O
al O
. O
, O
2016 O
; O
Chowdhury O
et O
al O
. O
, O
2022 O
) O
model O
the O
paraphrase B-TaskName
generation I-TaskName
as O
a O
supervised O
encoding O
- O
decoding O
problem O
, O
inspired O
by O
machine O
translation O
systems O
. O
However O
, O
the O
success O
of O
these O
methods O
often O
relies O
on O
a O
large O
number O
of O
parallel O
paraphrases O
, O
whose O
collection O
is O
timeconsuming O
and O
requires O
a O
lot O
of O
domain O
knowledge O
. O
Therefore O
, O
in O
real O
scenarios O
with O
a O
small O
amount O
of O
parallel O
data O
, O
the O
model O
suffers O
from O
performance O
drops O
facing O
domain O
gaps O
. O
This O
phenomenon O
, O
known O
as O
domain O
shift O
problem O
( O
Pan O
and O
Yang O
, O
2009 O
) O
, O
comes O
from O
the O
representation O
gap O
between O
training O
and O
testing O
domains O
with O
different O
writing O
styles O
or O
forms O
. O

To O
tackle O
this O
problem O
, O
unsupervised O
methods O
such O
as O
editing O
- O
based O
approaches O
( O
Bowman O
et O
al O
. O
, O
2016 O
; O
Miao O
et O
al O
. O
, O
2019 O
) O
or O
reinforcement O
learning O
( O
Li O
et O
al O
. O
, O
2018 O
; O
Siddique O
et O
al O
. O
, O
2020 O
) O
, O
and O
weakly O
- O
supervised O
methods O
such O
as O
retrievalenhanced O
( O
Ding O
et O
al O
. O
, O
2021 O
; O
Yin O
et O
al O
. O
, O
2022 O
) O
or O
prompt O
- O
based O
do O
not O
introduce O
or O
only O
introduce O
a O
small O
number O
of O
supervised O
signals O
, O
which O
limits O
their O
performance O
such O
that O
underperforms O
supervised O
methods O
. O
In O
fact O
, O
largescale O
unlabeled O
corpus O
data O
( O
UCD O
) O
and O
labeled O
source O
domain O
data O
( O
LSDD O
) O
, O
as O
well O
as O
a O
few O
labeled O
target O
domain O
data O
( O
LTDD O
) O
, O
can O
be O
easily O
achieved O
. O
Therefore O
, O
we O
propose O
a O
new O
three O
- O
stage O
learning O
paradigm O
: O
pre O
- O
training O
, O
meta O
- O
learning O
, O
and O
fine O
- O
tuning O
, O
aiming O
to O
leverage O
the O
pre O
- O
trained O
knowledge O
on O
UCD O
, O
source O
domain O
knowledge O
on O
LSDD O
, O
and O
adapt O
to O
target O
domain O
on O
LSDD O
to O
improve O
the O
performance O
of O
low O
- O
resource O
paraphrase O
generation O
. O
In O
order O
to O
successfully O
implement O
this O
learning O
paradigm O
, O
we O
propose O
a O
simple O
yet O
effective O
model O
which O
combined O
pre O
- O
trained O
language O
model O
( O
PLM O
) O
and O
MAML O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
, O
named O
Learning B-MethodName
to I-MethodName
Adapt I-MethodName
to I-MethodName
low I-MethodName
- I-MethodName
resource I-MethodName
PAraphrase I-MethodName
generation I-MethodName
( O
LAPA B-MethodName
) O
. O
Specifically O
, O
before O
meta O
- O
learning O
, O
we O
insert O
an O
adapter O
layer O
into O
each O
transformer O
layer O
of O
PLM O
. O
An O
adapter O
layer O
is O
composed O
of O
a O
few O
parameters O
of O
feedforward O
layer O
and O
residual O
connection O
. O
During O
meta O
- O
training O
and O
fine O
- O
tuning O
, O
only O
the O
adapter O
layer O
and O
normalization O
layer O
are O
trainable O
. O
Parameter O
freezing O
and O
residual O
connection O
can O
retain O
the O
prior O
knowledge O
of O
PLM O
to O
avoid O
negative O
transfer O
effects O
. O
Smaller O
- O
scale O
parameter O
updating O
can O
prevent O
MAML O
from O
gradient O
explosion O
or O
diminishing O
problems O
when O
the O
number O
of O
MAML O
inner O
loop O
iterations O
and O
model O
depth O
increase O
( O
Antoniou O
et O
al O
. O
, O
2019 O
) O
or O
training O
data O
is O
extremely O
scarce O
. O

Overall O
, O
we O
hold O
the O
idea O
that O
paraphrasing O
is O
a O
fundamental O
ability O
of O
human O
beings O
. O
The O
paraphrase O
model O
should O
not O
rely O
on O
domain O
and O
seen O
data O
. O
Therefore O
, O
we O
are O
committed O
to O
characterizing O
the O
basic O
ability O
of O
the O
paraphrase O
model O
, O
obtaining O
gains O
from O
each O
domain O
, O
and O
applying O
it O
to O
a O
specific O
domain O
. O
Our O
contributions O
are O
summarized O
as O
follows O
: O

• O
We O
define O
a O
novel O
three O
stages O
learning O
paradigm O
for O
low O
- O
resource O
paraphrase O
generation O
in O
data O
scarcity O
scenarios O
. O

• O
We O
propose O
that O
LAPA B-MethodName
implement O
this O
learning O
paradigm O
, O
which O
transferred O
the O
PLM O
knowledge O
and O
source O
domain O
knowledge O
to O
complete O
the O
low O
- O
resource O
learning O
in O
the O
target O
domain O
quickly O
and O
with O
high O
quality O
. O

• O
The O
supervised O
, O
unsupervised O
and O
weakly O
supervised O
experimental O
results O
of O
LAPA B-MethodName
on O
three O
benchmark O
datasets O
achieve O
state O
- O
of O
- O
theart O
( O
SOTA O
) O
. O
LAPA B-MethodName
with O
only O
2 O
% O
of O
trainable O
parameters O
and O
1 O
% O
target O
task O
labeled O
data O
can O
achieve O
a O
competitive O
performance O
with O
previous O
works O
. O

Related O
Work O

While O
the O
paraphrase B-TaskName
generation I-TaskName
performance O
is O
greatly O
improved O
with O
various O
supervised O
techniques O
( O
Zhao O
et O
al O
. O
, O
2008 O
; O
Prakash O
et O
al O
. O
, O
2016 O
; O
Egonmwan O
and O
Chali O
, O
2019 O
; O
Cao O
and O
Wan O
, O
2020 O
; O
Hosking O
and O
Lapata O
, O
2021 O
; O
Chowdhury O
et O
al O
. O
, O
2022 O
) O
, O
there O
are O
few O
studies O
regarding O
the O
lowresource O
setting O
. O
West O
et O
al O
. O
( O
2021 O
) O
and O
Meng O
et O
al O
. O
( O
2021 O
) O
proposed O
novel O
unsupervised O
paraphrasing O
strategies O
by O
data O
augmentation O
based O
on O
reflective O
decoding O
or O
diverse O
decoding O
. O
Ding O
et O
al O
. O
( O
2021 O
) O
and O
Yin O
et O
al O
. O
( O
2022 O
) O
achieved O
improvements O
on O
various O
low O
- O
resource O
datasets O
with O
retrieved O
data O
and O
meta O
reinforcement O
learning O
. O
However O
, O
these O
studies O
only O
use O
a O
single O
large O
corpus O
for O
training O
the O
full O
PLM O
, O
which O
suffers O
from O
domainshifting O
problems O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
Besides O
, O
under O
the O
extreme O
low O
- O
resource O
setting O
, O
directly O
fine O
- O
tuning O
the O
full O
PLM O
will O
cause O
an O
over O
- O
fitting O
problem O
( O
Antoniou O
et O
al O
. O
, O
2019 O
) O
. O
Meta O
- O
learning O
helps O
improve O
low O
- O
resource O
performance O
in O
various O
recent O
studies O
, O
such O
as O
image O
classification O
( O
Soh O
et O
al O
. O
, O
2020 O
) O
, O
vehicle O
tracking O
and O
natural O
language O
processing O
( O
Park O
et O
al O
. O
, O
2021 O
; O
Chen O
and O
Shuai O
, O
2021 O
; O
Hong O
and O
Jang O
, O
2022 O
) O
. O
Finn O
et O
al O
. O
( O
2017 O
) O
proposed O
a O
meta O
learner O
named O
MAML O
, O
which O
uses O
other O
example O
tasks O
to O
learn O
how O
to O
effectively O
initialize O
a O
basic O
learner O
, O
which O
can O
be O
quickly O
generalized O
to O
new O
tasks O
. O
Adapter O
modules O
have O
been O
mainly O
used O
for O
parameter O
- O
efficient O
and O
quick O
fine O
- O
tuning O
of O
a O
basic O
PLMs O
to O
new O
tasks O
( O
Houlsby O
et O
al O
. O
, O
2019 O
; O
Bapna O
and O
Firat O
, O
2019 O
; O
Pfeiffer O
et O
al O
. O
, O
2020Pfeiffer O
et O
al O
. O
, O
, O
2021 O
. O
Our O
paper O
proposes O
to O
incorporate O
meta O
- O
learning O
approaches O
to O
realize O
multi O
- O
domain O
migration O
and O
task O
adapter O
to O
realize O
parameter O
effective O
transfer O
learning O
( O
i.e. O
, O
limited O
trainable O
parameters O
) O
to O
mitigate O
the O
above O
problems O
of O
paraphrase B-TaskName
generation I-TaskName
. O

3 O
The O
Approach O

Learning O
Paradigm O

As O
shown O
in O
Figure O
1 O
, O
the O
workflow O
of O
our O
learning O
paradigm O
including O
three O
stages O
: O
1 O
. O
Backbone O
model O
pre O
- O
training O
on O
large O
unlabeled O
corpora O
2 O
. O
Adapter O
model O
meta O
- O
training O
on O
large O
source O
corpora O
using O
the O
meta O
- O
learning O
and O
3 O
. O
Adapter O
model O
fine O
- O
tuning O
on O
target O
corpora O
and O
evaluate O
model O
performance O
. O
The O
prior O
knowledge O
K O
pri O
comes O
from O
first O
two O
stages O
: O
pre O
- O
training O
and O
meta O
- O
learning O
. O
We O
denote O
our O
backbone O
model O
by O
f O
( O
θ O
) O
with O
parameters O
θ O
. O
The O
first O
stage O
is O
pretraining O
on O
unlabeled O
corpora O
D O
pre O
, O
and O
we O
get O
f O
( O
θ O
pre O
) O
. O
The O
second O
stage O
is O
meta O
- O
training O
on O
adapter O
model O
f O
[ O
θ O
pre O
, O
Φ O
] O
with O
additional O
parameters O
Φ O
and O
frozen O
θ O
pre O
on O
related O
source O
corpora O
D O
src O
, O
and O
we O
got O
f O
[ O
θ O
pre O
, O
Φ O
src O
] O
. O
Finally O
, O
we O
initialize O
the O
adapter O
model O
with O
[ O
θ O
pre O
, O
Φ O
src O
] O
and O
finetune O
Φ O
src O
on O
the O
target O
corpus O
D O
tgt O
to O
obtain O
a O
target O
model O
f O
[ O
θ O
pre O
, O
Φ O
tgt O
] O
which O
are O
model O
parameters O
after O
target O
adapter O
, O
i.e. O
, O
the O
posterior O
knowledge O
K O
por O
. O

Backbone O
Model O

Because O
PLM O
is O
equipped O
with O
prior O
knowledge O
K O
pri O
and O
exhibits O
strong O
capabilities O
in O
a O
range O
of O
different O
generative O
tasks O
, O
we O
choose O
the O
pretrained O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020b O
) O
as O
the O
backbone O
model O
for O
paraphrase O
generation O
. O
Specifically O
, O
given O
a O
labeled O
paraphrase O
pair O
i O
= O
( O
x O
, O
ŷ O
) O
, O
where O
x O
= O
[ O
x O
1 O
, O
. O
. O
. O
, O
x O
N O
] O
, O
ŷ O
= O
[ O
ŷ O
1 O
, O
. O
. O
. O
, O
ŷ O
M O
] O
, O
and O
inputting O
x O
, O
the O
model O
has O
produced O
a O
predicted O
segment O
sequence O
y O
< O
t O
= O
[ O
y O
1 O
, O
. O
. O
. O
, O
y O
t−1 O
] O
before O
time O
t O
, O
then O
the O
probability O
that O
the O
token O
generated O
at O
time O
t O
is O
y O
t O
is O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
, O
θ O
) O
. O
The O
model O
is O
optimized O
by O
minimizing O
the O
negative O
log O
- O
likelihood O
: O

L O
i O
( O
f O
( O
θ O
) O
) O
= O
− O
M O
t=1 O
log O
p O
( O
ŷ O
t O
|y O
< O
t O
, O
x O
, O
θ O
) O
. O

Adapter O
Model O

The O
adapter O
model O
is O
obtained O
by O
inserting O
the O
adapter O
layer O
into O
each O
transformer O
layer O
of O
the O
backbone O
model O
. O
An O
adapter O
layer O
is O
a O
bottlenecked O
feed O
- O
forward O
network O
consisting O
of O
a O
downproject O
layer O
, O
a O
nonlinearity O
function O
and O
an O
upproject O
layer O
. O
In O
addition O
, O
a O
skip O
connection O
layer O
from O
input O
to O
output O
prevents O
the O
noised O
initialization O
from O
interference O
with O
the O
training O
initially O
. O
For O
the O
adapter O
in O
layer O
l O
, O
the O
function O
can O
be O
formulated O
as O
: O
Adapter O
( O
z O
l O
) O
= O
W O
l O
u O
ReLU O
( O
W O
l O
d O
z O
l O
) O
+ O
z O
l O
where O
z O
l O
represents O
the O
inputs O
of O
the O
adapter O
in O
layer O
l. O
Besides O
, O
the O
normalization O
layers O
are O
trainable O
and O
initialized O
from O
the O
previous O
training O
stage O
. O

Meta O
- O
Learning O

The O
second O
stage O
is O
adapter O
model O
meta O
traning O
based O
on O
MAML O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
. O
The O
learning O
process O
is O
shown O
in O
Algorithm O
1 O
. O
First O
, O
we O
freeze O
the O
backbone O
model O
parameters O
θ O
pre O
that O
have O
been O
pre O
- O
trained O
in O
the O
pre O
- O
training O
stage O
, O
then O
, O
add O
new O
adapters O
with O
parameters O
Φ O
to O
get O
adapter O
model O
f O
[ O
θ O
pre O
, O
Φ O
] O
. O
Based O
on O
Algorithm O
1 O
, O
we O
first O
complete O
the O
meta O
- O
learning O
of O
the O
adapter O
model O
on O
the O
source O
corpus O
D O
src O
to O
help O
the O
adapters O
Φ O
find O
the O
initialization O
parameters O
Φ O
src O
suitable O
for O
paraphrase O
generation O
to O
adapt O
faster O
target O
task O
. O
At O
this O
Compute O
adapted O
parameters O
with O
gradient O
descent O
: O

[ O
θ O
, O
Φ O
] O
= O
[ O
θ O
, O
Φ O
] O
− O
α∇ O
Φ O
L O
i O
( O
f O
[ O
θ O
, O
Φ O
] O
) O
8 O
: O
end O
for O
9 O
: O
Update O
[ O
θ O
, O
Φ O
] O
← O
[ O
θ O
, O
Φ O
] O
− O
β∇ O
Φ O
T O
i O
∼p O
( O
T O
) O
L O
i O
( O
f O
[ O
θ O
, O
Φ O
] O

Experimental O
Settings O

Datasets O

We O
conducted O
experiments O
on O
Quora B-DatasetName
1 O
, O
Twitter B-DatasetName
( O
Lan O
et O
al O
. O
, O
2017 O
) O
and O
MSCOCO B-DatasetName
( O
Lin O
et O
al O
. O
, O
2014 O
) O
benchmark O
datasets O
, O
and O
followed O
the O
same O
setting O
in O
previous O
works O
( O
Lin O
et O
al O
. O
, O
2014 O
; O
Liu O
et O
al O
. O
, O
2020 O
; O
Ding O
et O
al O
. O
, O
2021 O
) O
. O
For O
meta O
- O
learning O
, O
we O
choose O
a O
different O
source O
task O
's O
labeld O
train O
- O
set O
from O
the O
target O
task O
to O
randomly O
construct O
meta O
tasks O
. O
Appendix O
Table O
4 O
describes O
more O
details O
. O

Baselines O

Supervised O
methods O
are O
trained O
with O
all O
parallel O
sentences O
of O
target O
task O
. O
Unsupervised O
baselines O
( O
Lewis O
et O
al O
. O
, O
2020b O
) O
. O
Like O
our O
work O
, O
they O
all O
used O
BART B-MethodName
as O
PLM O
. O
To O
compare O
the O
performance O
of O
our O
method O
against O
the O
previous O
works O
, O
we O
use O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
iBLEU B-MetricName
( O
Sun O
and O
Zhou O
, O
2012 O
) O
and O
ROUGE B-MetricName
( O
Hovy O
et O
al O
. O
, O
2006 O
) O
metrics O
. O
All O
metrics O
are O
computed O
between O
the O
generated O
and O
the O
reference O
paraphrases O
in O
the O
test O
set O
( O
Kumar O
et O
al O
. O
, O
2020 O
) O
. O
We O
also O
separately O
analyze O
the O
impact O
of O
target O
task O
Example O
Input O
Can O
we O
ever O
store O
energy O
produced O
in O
lightning O
? O

Experimental O
Results O

How O
does O
a O
pencil O
and O
a O
liquid O
eyeliner O
differ O
? O

How O
come O
there O
's O
no O
physical O
evidence O
for O
sea O
dragons O
existing O
if O
they O
're O
the O
largestanimal O
in O
the O
sea O
. O
Table O
2 O
: O
Examples O
of O
the O
generated O
paraphrases O
on O
Quora O
dataset O
. O
We O
highlight O
the O
key O
phrases O
in O
the O
paraphrases O
generated O
and O
use O
wavy O
underline O
to O
show O
the O
matched O
parts O
between O
LAPA B-MethodName
and O
reference O
. O

labeled O
data O
scale O
under O
low O
- O
resource O
setting O
. O
Figure O
2 O
shows O
the O
experimental O
results O
on O
the O
Quora B-DatasetName
dataset O
. O
It O
can O
be O
conclused O
that O
LAPA B-MethodName
has O
a O
significant O
effect O
compared O
with O
BART B-MethodName
under O
the O
same O
small O
data O
size O
. O
LAPA B-MethodName
can O
achieve O
the O
effect O
of O
89 O
% O
to O
93 O
% O
of O
the O
full O
amount O
of O
data O
when O
not O
using O
any O
target O
task O
labeled O
data O
; O
when O
using O
a O
very O
small O
amount O
of O
data O
such O
as O
0.5k O
( O
i.e O
0.5 O
% O
of O
the O
full O
data O
) O
, O
it O
can O
be O
improved O
to O
94 O
% O
to O
96 O
% O
; O
when O
the O
amount O
of O
data O
increases O
to O
10k O
( O
i.e O
10 O
% O
of O
the O
full O
data O
) O
, O
the O
performance O
is O
almost O
the O
same O
as O
the O
full O
amount O
of O
data O
100k O
. O
It O
should O
be O
pointed O
out O
that O
which O
dataset O
is O
selected O
as O
the O
source O
data O
can O
not O
have O
a O
substantial O
impact O
on O
the O
migration O
results O
, O
as O
shown O
in O
Figure O
3 O
. O
The O
results O
independent O
of O
the O
source O
dataset O
prove O
that O
LAPA B-MethodName
can O
learn O
the O
paraphrasing B-TaskName
task O
itself O
on O
any O
dataset O
, O
so O
it O
has O
strong O
adaptability O
to O
the O
target O
task O
. O
We O
conduct O
an O
ablation O
study O
with O
three O
variants O
under O
the O
low O
- O
resource O
setting O
of O
the O
Quora B-DatasetName
dataset O
to O
investigate O
the O
contribution O
of O
each O
component O
in O
the O
proposed O
method O
. O
The O
experimental O
results O
are O
shown O
in O
Table O
3 O
. O
We O
can O
get O
: O
first O
, O
using O
pre B-MethodName
- I-MethodName
trained I-MethodName
BART I-MethodName
can O
get O
good O
results O
; O
second O
, O
by O
adding O
the O
source O
task O
dataset O
for O
pre B-MethodName
- I-MethodName
trained I-MethodName
BART I-MethodName
, O
the O
knowledge O
of O
the O
source O
domain O
can O
be O
effectively O
learned O
, O
thereby O
improving O
the O
performance O
of O
the O
model O
in O
the O
target O
domain O
; O
third O
, O
adding O
our O
proposed O
meta O
- O
learning O
framework O
can O
again O
effectively O
improve O
the O
speed O
and O
quality O
of O
learning O
the O
source O
domain O
( O
LAPA B-MethodName
only O
has O
2.8 O
% O
training O
parameters O
compared O
with O
BART B-MethodName
) O
and O
achieve O
the O
best O
performance O
. O

Ablation O

Case O
Study O

Table O
2 O
lists O
some O
paraphrases O
generated O
by O
LAPA B-MethodName
and O
BART B-MethodName
with O
different O
experimental O
settings O
. O
We O
can O
observe O
that O
paraphrases O
produced O
by O
LAPA B-MethodName
are O
not O
only O
grammatically O
correct O
but O
preserve O
the O
semantics O
of O
Input O
more O
completely O
, O
and O
the O
expression O
is O
closer O
to O
Reference O
than O
the O
other O
methods O
. O
This O
benefits O
from O
the O
fact O
that O
our O
LAPA B-MethodName
approach O
can O
make O
full O
use O
of O
source O
domain O
data O
and O
task O
features O
, O
and O
better O
preserve O
the O
prior O
knowledge O
of O
PLM O
, O
so O
as O
to O
adapt O
to O
new O
target O
tasks O
quickly O
and O
efficiently O
. O

Conclusion O

In O
this O
work O
, O
we O
investigate O
the O
problem O
of O
paraphrase B-TaskName
generation I-TaskName
under O
the O
low O
- O
resource O
setting O
and O
propose O
a O
simple O
yet O
effective O
approach O
LAPA B-MethodName
. O
We O
effectively O
combine O
transfer O
learning O
and O
meta O
- O
learning O
by O
using O
adapter O
modules O
as O
the O
bridge O
. O
Whether O
in O
supervised O
, O
unsupervised O
or O
low O
- O
resource O
setting O
, O
the O
results O
that O
our O
approach O
achieves O
the O
SOTA O
results O
on O
benchmark O
datasets O
. O
In O
the O
future O
, O
we O
plan O
to O
explore O
how O
to O
choose O
a O
smaller O
but O
suitable O
high O
- O
quality O
source O
corpus O
for O
learning O
in O
the O
source O
domain O
to O
improve O
the O
effect O
of O
transferring O
to O
the O
target O
domain O
, O
because O
not O
all O
source O
domain O
data O
has O
a O
positive O
effect O
. O
Second O
, O
we O
plan O
to O
extend O
this O
framework O
to O
other O
AI O
fields O
to O
solve O
low O
- O
resource O
problems O
in O
other O
scenarios O
and O
enable O
more O
industrial O
applications O
. O

Limitations O

The O
major O
limitation O
of O
present O
study O
is O
the O
need O
for O
source O
domain O
annotated O
data O
that O
can O
adapt O
to O
the O
target O
domain O
. O
Because O
this O
is O
the O
source O
of O
data O
for O
the O
knowledge O
of O
the O
learning O
task O
itself O
, O
it O
can O
not O
be O
avoided O
. O
In O
the O
real O
world O
, O
we O
can O
find O
it O
from O
public O
free O
datasets O
, O
exchange O
it O
commercially O
with O
other O
institutions O
, O
or O
annotate O
a O
batch O
of O
raw O
data O
ourselves O
as O
a O
cold O
start O
to O
solve O
this O
problem O
. O
Secondly O
, O
this O
study O
also O
has O
insufficient O
research O
on O
related O
variables O
. O
Due O
to O
the O
limitation O
of O
time O
and O
article O
length O
, O
we O
have O
not O
been O
able O
to O
study O
. O
These O
findings O
provide O
the O
following O
insights O
for O
future O
research O
: O
What O
is O
the O
lower O
bound O
of O
the O
amount O
of O
source O
domain O
data O
that O
can O
be O
well O
adapted O
to O
the O
target O
task O
? O
Whether O
we O
can O
apply O
weak O
supervision O
, O
data O
augmentation O
and O
other O
methods O
to O
create O
source O
domain O
data O
? O
How O
to O
select O
high O
- O
quality O
source O
domain O
data O
to O
get O
a O
better O
adapter O
model O
? O
We O
leave O
these O
questions O
to O
future O
research O
. O
Twitter O
The O
twitter O
URL O
paraphrasing O
corpus O
is O
built O
by O
Lan O
et O
al O
. O
( O
2017 O
) O
for O
paraphrase O
identification O
. O
We O
follow O
the O
setting O
in O
Li O
et O
al O
. O
( O
2018 O
) O
, O
Kazemnejad O
et O
al O
. O
( O
2020 O
) O
and O
Siddique O
et O
al O
. O
( O
2020 O
) O
. O

The O
detailed O
dataset O
statistics O
are O
summarized O
in O
Table O
4 O
. O

A.2 O
Evaluation O
Details O

To O
make O
a O
fair O
and O
comprehensive O
assessment O
, O
we O
follow O
the O
same O
experiment O
setting O
of O
each O
comparison O
work O
( O
Li O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2020 O
; O
Ding O
et O
al O
. O
, O
2021 O
) O
and O
conduct O
the O
comparison O
respectively O
. O
For O
data O
preprocessing O
, O
all O
the O
sentences O
are O
lower O
cased O
, O
and O
truncate O
all O
sentences O
to O
up O
to O
20 B-HyperparameterValue
words B-HyperparameterName
. O
< O
s O
> O
and O
< O
/ O
s O
> O
are O
spliced O
to O
the O
front O
and O
back O
end O
of O
the O
sentence O
as O
start O
and O
end O
markers O
. O

For O
evaluation O
metrics O
, O
we O
use O
BLEU B-MetricName
, O
i B-MetricName
- I-MetricName
BLEU I-MetricName
and O
ROUGE B-MetricName
that O
have O
been O
widely O
used O
in O
the O
previous O
work O
to O
measure O
the O
quality O
of O
the O
paraphrases O
. O
The O
i B-MetricName
- I-MetricName
BLUE I-MetricName
aims O
to O
measure O
the O
diversity O
of O
expression O
in O
the O
generated O
paraphrases O
by O
penalizing O
copying O
words O
from O
input O
sentences O
. O
Specifically O
, O
we O
follow O
the O
unsupervised O
paraphrase O
generation O
baselines O
and O
set O
the O
balancing O
parameter O
α B-HyperparameterName
= O
0.9 B-HyperparameterValue
. O

A.3 O
Implementation O

Our O
experiments O
were O
conducted O
with O
PyToch O
on O
NVIDIA O
Tesla O
V100 O
16 O
GB O
GPU O
. O
Following O
the O
comparison O
methods O
, O
we O
used O
BART B-MethodName
- I-MethodName
large I-MethodName
as O
the O
pre O
- O
trained O
language O
model O
and O
use O
its O
pre O
- O
trained O
parameters O
. O
For O
adapter O
modules O
, O
the O
hidden B-HyperparameterName
size I-HyperparameterName
is O
128 B-HyperparameterValue
. O
For O
meta O
- O
training O
, O
unless O
otherwise O
specified O
, O
a O
meta B-HyperparameterName
batch I-HyperparameterName
includes O
3 B-HyperparameterValue
tasks O
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
of O
each O
task O
is O
10 B-HyperparameterValue
. O
Both O
basic O
learners O
and O
meta O
learners O
use O
the O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
optimizer O
for O
optimization O
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
by O
grid O
search O
in O
1e-5 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
, O
1e-6 B-HyperparameterValue
and O
5e-6 B-HyperparameterValue
. O
The O
internal B-HyperparameterName
gradient I-HyperparameterName
step I-HyperparameterName
size I-HyperparameterName
is O
4 B-HyperparameterValue
, O
and O
the O
whole O
model O
has O
enough O
step B-HyperparameterName
size I-HyperparameterName
for O
training O
. O
For O
meta O
verification O
, O
we O
use O
a O
corpus O
excluded O
from O
the O
source O
task O
and O
the O
target O
task O
. O
For O
fine O
- O
tuning O
, O
we O
use O
validation O
set O
to O
select O
the O
best O
model O
for O
metrics O
calculation O
. O

SeaD B-MethodName
: O
End O
- O
to O
- O
end O
Text O
- O
to O
- O
SQL O
Generation O
with O
Schema B-MethodName
- I-MethodName
aware I-MethodName
Denoising I-MethodName

On O
the O
WikiSQL B-DatasetName
1 O
benchmark O
, O
most O
methods O
tackle O
the O
challenge O
of O
text O
- O
to O
- O
SQL O
with O
predefined O
sketch O
slots O
and O
build O
sophisticated O
sub O
- O
tasks O
to O
fill O
these O
slots O
. O
Though O
achieving O
promising O
results O
, O
these O
methods O
suffer O
from O
over O
- O
complex O
model O
structure O
. O
In O
this O
paper O
, O
we O
present O
a O
simple O
yet O
effective O
approach O
that O
enables O
auto O
- O
regressive O
sequenceto O
- O
sequence O
model O
to O
robust O
text O
- O
to O
- O
SQL O
generation O
. O
Instead O
of O
formulating O
the O
task O
of O
text O
- O
to O
- O
SQL O
as O
slot O
- O
filling O
, O
we O
propose O
to O
train O
sequence O
- O
to O
- O
sequence O
model O
with O
Schemaaware B-MethodName
Denoising I-MethodName
( O
SeaD B-MethodName
) O
, O
which O
consists O
of O
two O
denoising O
objectives O
that O
train O
model O
to O
either O
recover O
input O
or O
predict O
output O
from O
two O
novel O
erosion O
and O
shuffle O
noises O
. O
These O
modelagnostic O
denoising O
objectives O
act O
as O
the O
auxiliary O
tasks O
for O
structural O
data O
modeling O
during O
sequence O
- O
to O
- O
sequence O
generation O
. O
In O
addition O
, O
we O
propose O
a O
clause B-MethodName
- I-MethodName
sensitive I-MethodName
execution I-MethodName
guided I-MethodName
( I-MethodName
EG I-MethodName
) I-MethodName
decoding I-MethodName
strategy I-MethodName
to O
overcome O
the O
limitation O
of O
EG O
decoding O
for O
generative O
model O
. O
The O
experiments O
show O
that O
the O
proposed O
method O
improves O
the O
performance O
of O
sequence O
- O
to O
- O
sequence O
model O
in O
both O
schema O
linking O
and O
grammar O
correctness O
and O
establishes O
new O
state O
- O
of O
- O
the O
- O
art O
on O
WikiSQL B-DatasetName
benchmark O
. O
Our O
work O
indicates O
that O
the O
capacity O
of O
sequence O
- O
to O
- O
sequence O
model O
for O
text O
- O
to O
- O
SQL O
may O
have O
been O
under O
- O
estimated O
and O
could O
be O
enhanced O
by O
specialized O
denoising O
task O
. O

Introduction O

Text O
- O
to O
- O
SQL O
aims O
at O
translating O
natural O
language O
into O
valid O
SQL O
query O
. O
It O
enables O
layman O
to O
explore O
structural O
database O
information O
with O
semantic O
question O
instead O
of O
dealing O
with O
the O
complex O
grammar O
required O
by O
logical O
-form O
query O
. O
On O
the O
WikiSQL B-DatasetName
benchmark O
, O
most O
models O
adopt O
a O
sketch O
- O
based O
slot O
filling O
approach O
. O
It O
decomposes O
the O
task O
of O
convert O
query O
to O
SQL O
into O
several O
sub O
- O
tasks O
that O
are O
relatively O
easy O
to O
handle O
, O
e.g. O
, O
the O
' O
SELECT O
' O
column O
mentioned O
or O
the O
query O
span O
corresponding O
to O
a O
condition O
value O
. O
The O
entire O
SQL O
can O
be O
recovered O
from O
the O
results O
of O
the O
sub O
- O
tasks O
deterministically O
. O

Though O
being O
a O
typical O
sequence O
- O
to O
- O
sequence O
( O
seq2seq O
) O
task O
, O
auto O
- O
regressive O
models O
( O
LSTM B-MethodName
, O
Transformer B-MethodName
, O
etc O
. O
) O
, O
however O
, O
fail O
to O
achieve O
state O
- O
ofthe O
- O
art O
results O
for O
text O
- O
to O
- O
SQL O
task O
. O
Previous O
works O
attribute O
the O
sub O
- O
optimal O
results O
of O
seq2seq O
models O
to O
three O
major O
limitations O
. O
First O
, O
SQL O
queries O
with O
different O
clause O
order O
may O
have O
exact O
same O
semantic O
meaning O
and O
return O
same O
results O
by O
execution O
. O
The O
token O
interchangeability O
may O
confusion O
model O
that O
based O
on O
seq2seq O
generation O
. O
Second O
, O
the O
grammar O
constraint O
induced O
by O
structural O
logical O
form O
is O
ignored O
during O
auto O
- O
regressive O
decoding O
, O
therefore O
the O
model O
may O
predict O
SQL O
with O
invalid O
logical O
form O
. O
Third O
, O
schema O
linking O
, O
which O
has O
been O
suggested O
to O
be O
the O
crux O
of O
text O
- O
to O
- O
SQL O
task O
, O
is O
not O
specially O
addressed O
by O
vanilla O
seq2seq O
model O
. O

In O
this O
paper O
, O
we O
present O
a O
simple O
yet O
effective O
method O
to O
boost O
the O
performance O
of O
seq2seq O
model O
for O
text O
- O
to O
- O
SQL O
task O
. O
Instead O
of O
building O
extra O
sub O
- O
module O
or O
putting O
constraint O
on O
model O
output O
, O
we O
propose O
two O
novel O
schema O
- O
awared O
denoising O
objectives O
trained O
along O
with O
the O
original O
seq2seq O
generation O
task O
. O
These O
denoising O
objectives O
deal O
with O
the O
intrinsic O
attribute O
of O
logical O
form O
and O
could O
facilitate O
schema O
linking O
required O
for O
text O
- O
to O
- O
SQL O
task O
. O
The O
inductive O
schema O
- O
awared O
noises O
can O
be O
categorized O
into O
two O
types O
: O
erosion O
and O
shuffle O
. O
Erosion O
acts O
on O
schema O
input O
by O
randomly O
permute O
, O
drop O
and O
add O
columns O
into O
the O
current O
schema O
set O
. O
The O
related O
schema O
entity O
in O
target O
SQL O
query O
will O
be O
jointly O
modified O
according O
to O
the O
erosion O
result O
. O
Shuffle O
is O
applied O
via O
randomly O
re O
- O
ordering O
the O
mentioned O
entity O
and O
values O
in O
natural O
language O
( O
NL O
) O
or O
SQL O
with O
respect O
to O
the O
schema O
columns O
. O

During O
training O
procedure O
, O
shuffle O
is O
performed O
during O
monolingual O
self O
- O
supervision O
that O
trains O
model O
to O
recover O
original O
text O
given O
the O
noised O
one O
. O
Erosion O
is O
applied O
to O
seq2seq O
task O
that O
trains O
model O
to O
generate O
corrupted O
SQL O
sequence O
, O
given O
NL O
and O
eroded O
schema O
as O
input O
. O
These O
proposed O
denoising O
objectives O
are O
combined O
along O
with O
the O
origin O
seq2seq O
task O
to O
train O
a O
SeaD B-MethodName
model O
. O
In O
addition O
, O
to O
deal O
with O
the O
limitation O
of O
execution B-MethodName
- I-MethodName
guided I-MethodName
( I-MethodName
EG I-MethodName
) I-MethodName
decoding I-MethodName
, O
we O
propose O
a O
clause O
- O
sensitive O
EG O
strategy O
that O
decide O
beam O
size O
with O
respect O
to O
the O
clause O
token O
that O
is O
predicted O
. O
The O
proposed O
method O
establish O
new O
state O
- O
of O
- O
the O
- O
art O
on O
the O
WikiSQL B-DatasetName
benchmark O
. O

The O
main O
contribution O
of O
this O
work O
is O
the O
schemaaware O
denoising O
objectives O
that O
are O
designed O
for O
text O
- O
to O
- O
SQL O
task O
. O
The O
denoising O
objectives O
are O
model O
- O
agnostic O
and O
could O
apply O
to O
any O
seq2seq O
model O
that O
are O
trained O
in O
auto O
- O
regressive O
manner O
. O
In O
addition O
, O
we O
also O
propose O
a O
clause B-MethodName
- I-MethodName
sensitive I-MethodName
EG I-MethodName
decoding I-MethodName
strategy I-MethodName
, O
which O
can O
improve O
the O
searching O
efficiency O
of O
EG O
during O
seq2seq O
generation O
. O
The O
results O
of O
the O
work O
demonstrate O
the O
effectiveness O
of O
the O
schema O
- O
aware O
denoising O
approach O
and O
shad O
lights O
on O
the O
importance O
of O
task O
- O
oriented O
denoising O
objective O
. O

Related O
Work O

Semantic O
Parsing O
The O
problem O
of O
mapping O
natural O
language O
to O
meaningful O
executable O
programs O
has O
been O
widely O
studied O
in O
natural O
language O
processing O
research O
. O
Logic O
forms O
( O
Zettlemoyer O
and O
Collins O
, O
2012 O
; O
Zettlemoyer O
, O
2011 O
, O
2013 O
; O
Cai O
and O
Yates O
, O
2013 O
; O
Reddy O
et O
al O
. O
, O
2014 O
; O
Liang O
et O
al O
. O
, O
2013 O
; O
Quirk O
et O
al O
. O
, O
2015 O
; O
Chen O
et O
al O
. O
, O
2016 O
) O
can O
be O
considered O
as O
a O
special O
instance O
to O
the O
more O
generic O
semantic O
parsing O
problem O
. O
As O
a O
sub O
- O
task O
of O
semantic O
parsing O
, O
the O
text O
- O
to O
- O
SQL O
problem O
has O
been O
studied O
for O
decades O
. O
( O
Warren O
and O
Pereira O
, O
1982 O
; O
Popescu O
et O
al O
. O
, O
2003 O
; O
Li O
et O
al O
. O
, O
2006 O
; O
Giordani O
and O
Moschitti O
, O
2012 O
; O
Wang O
et O
al O
. O
, O
2017 O
) O
. O
Slotfilling O
model O
( O
Hwang O
et O
al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2019a O
; O
Lyu O
et O
al O
. O
, O
2020 O
) O
translates O
the O
clauses O
of O
SQL O
into O
subtasks O
, O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
treat O
this O
task O
as O
a O
twostage O
sequence O
labeling O
model O
. O
However O
, O
the O
convergence O
rate O
between O
subtasks O
is O
inconsistent O
or O
the O
interaction O
between O
multiple O
subtasks O
may O
lead O
to O
the O
model O
may O
not O
converge O
well O
. O
Like O
lots O
of O
previous O
work O
( O
Dong O
and O
Lapata O
, O
2016 O
; O
Lin O
et O
al O
. O
, O
2018 O
; O
Zhong O
et O
al O
. O
, O
2017 O
; O
Suhr O
et O
al O
. O
, O
2020 O
; O
Raffel O
et O
al O
. O
, O
2019 O
) O
, O
we O
treat O
text O
- O
to O
- O
SQL O
as O
a O
translation O
problem O
, O
and O
taking O
both O
the O
natural O
language O
question O
and O
the O
DB O
as O
input O
. O

Hybrid B-MethodName
Pointer I-MethodName
Networks I-MethodName
Proposed O
by O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
, O
copying O
mechanism O
( O
CM O
) O
uses O
attention O
as O
a O
pointer O
to O
copy O
several O
discrete O
tokens O
from O
input O
sequence O
as O
the O
output O
and O
have O
been O
successfully O
used O
in O
machine O
reading O
comprehension O
( O
Wang O
and O
Jiang O
, O
2016 O
; O
Trischler O
et O
al O
. O
, O
2016 O
; O
Kadlec O
et O
al O
. O
, O
2016 O
; O
Xiong O
et O
al O
. O
, O
2016 O
) O
, O
interactive O
conversation O
( O
Gu O
et O
al O
. O
, O
2016 O
; O
Yu O
and O
Joty O
, O
2020 O
; O
He O
et O
al O
. O
, O
2019b O
) O
, O
geometric O
problems O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
and O
program O
generation O
( O
Zhong O
et O
al O
. O
, O
2017 O
; O
Xu O
et O
al O
. O
, O
2017 O
; O
Dong O
and O
Lapata O
, O
2016 O
; O
Yu O
et O
al O
. O
, O
2018 O
; O
McCann O
et O
al O
. O
, O
2018 O
; O
Hwang O
et O
al O
. O
, O
2019 O
) O
. O
In O
text O
- O
to O
- O
SQL O
, O
CM O
can O
not O
only O
facilitate O
the O
condition O
value O
extraction O
from O
source O
input O
, O
but O
also O
help O
to O
protect O
the O
privacy O
of O
the O
database O
. O
In O
this O
paper O
, O
we O
use O
a O
Hybrid B-MethodName
Pointer I-MethodName
Generator I-MethodName
Network I-MethodName
which O
is O
similar O
to O
( O
Jia O
and O
Liang O
, O
2016 O
; O
Rongali O
et O
al O
. O
, O
2020 O
) O
to O
generate O
next O
step O
token O
. O

Denoising O
Self O
- O
training O
Language O
model O
pretraining O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Lan O
et O
al O
. O
, O
2019 O
) O
has O
been O
shown O
to O
improve O
the O
downstream O
performance O
on O
many O
NLP O
tasks O
and O
brought O
significant O
gains O
. O
( O
Radford O
et O
al O
. O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Song O
et O
al O
. O
, O
2019 O
) O
are O
beneficial O
to O
seq2seq O
task O
, O
while O
they O
are O
problematic O
for O
some O
tasks O
. O
While O
Figure O
2 O
: O
The O
proposed O
schema O
- O
aware O
denoising O
procedure O
. O
( O
a O
) O
Erosion O
denoising O
randomly O
drops O
, O
adds O
and O
repermutes O
schema O
columns O
. O
The O
related O
column O
entities O
in O
ground O
- O
truth O
SQL O
sequence O
will O
be O
jointly O
modified O
or O
masked O
out O
with O
respect O
to O
the O
erosion O
results O
of O
the O
current O
schema O
set O
. O
Erosion O
objective O
trains O
model O
to O
predict O
the O
modified O
SQL O
sequence O
under O
noised O
input O
. O
( O
b O
) O
Shuffle O
denoising O
objective O
re O
- O
permutes O
the O
mentioned O
entities O
in O
SQL O
or O
NL O
sequence O
, O
and O
trains O
model O
to O
reconstruct O
the O
sequence O
with O
the O
correct O
entity O
order O
. O

Methodology O

Given O
natural O
language O
question O
Q O
and O
a O
schema O
S O
, O
our O
goal O
is O
to O
obtain O
the O
corresponding O
SQL O
query O
Y O
. O
Here O
the O
natural O
question O
Q O
= O
{ O
q O
1 O
, O
... O
, O
q O
|Q| O
} O
denotes O
a O
word O
sequence O
, O
the O
schema O
S O
= O
{ O
c O
1 O
, O
... O
, O
c O
|S| O
} O
is O
composed O
of O
a O
set O
of O
columns O
, O
where O
each O
column O
c O
i O
= O
{ O
c O
1 O
, O
... O
, O
c O
|c O
i O
| O
} O
is O
a O
sequence O
of O
words O
. O
Y O
= O
y O
1 O
, O
... O
, O
y O
|Y O
| O
denotes O
the O
token O
- O
wise O
raw O
SQL O
sequence O
. O
We O
approach O
this O
task O
with O
directly O
auto O
- O
regressive O
generation O
, O
i.e. O
, O
predicting O
the O
SQL O
sequence O
token O
by O
token O
. O
We O
choose O
Transformer O
as O
our O
base O
architecture O
, O
which O
is O
a O
widely O
adopted O
in O
seq2seq O
translation O
and O
generation O
tasks O
. O
In O
this O
section O
, O
we O
first O
present O
the O
sample O
formulation O
that O
transform O
text O
- O
to O
- O
SQL O
into O
typical O
seq2seq O
task O
, O
followed O
by O
a O
brief O
introduce O
of O
the O
Transformer O
architecture O
with O
pointer O
generator O
. O
Then O
we O
describe O
the O
proposed O
schema O
- O
aware O
denoising O
method O
and O
clause O
- O
sensitive O
EG O
decoding O
strategy O
. O

Sample O
Formulation O

Given O
training O
samples O
{ O
X O
i O
, O
Y O
i O
} O
, O
i O
= O
1 O
, O
... O
, O
N O
, O
X O
= O
{ O
Q O
, O
S O
} O
, O
where O
Q O
denotes O
the O
NL O
sequence O
and O
S O
denotes O
the O
schema O
set O
, O
Y O
is O
the O
SQL O
sequence O
. O
Sample O
formulation O
is O
a O
function O
X O
, O
Y O
= O
f O
ormat O
( O
X O
, O
Y O
) O
S O
that O
transforms O
heterogeneous O
data O
into O
pairwise O
token O
sequence O
. O
It O
is O
performed O
by O
filling O
template O
that O
acts O
as O
a O
prompt O
to O
guide O
seq2seq O
model O
to O
generate O
different O
types O
of O
token O
with O
respected O
to O
various O
contexts O
. O
For O
schema O
formulation O
, O
each O
column O
name O
is O
prefixed O
with O
a O
separate O
special O
token O
< O
coli O
> O
, O
where O
i O
denotes O
the O
ith O
column O
in O
the O
schema O
set O
. O
The O
column O
type O
of O
each O
column O
is O
also O
append O
to O
the O
name O
sequence O
to O
form O
the O
template O
for O
a O
schema O
column O
< O
coli O
> O
col O
name O
: O
col O
type O
. O
All O
columns O
in O
schema O
is O
formulated O
and O
concatenated O
together O
to O
compose O
the O
input O
sequence O
for O
schema O
. O
The O
schema O
sequence O
is O
further O
concatenated O
with O
the O
NL O
sequence O
for O
model O
input O
. O
We O
explicitly O
introduce O
schema O
- O
mention O
alignment O
to O
NL O
sequence O
by O
surrounding O
schema O
names O
that O
are O
mentioned O
in O
NL O
sequence O
with O
bracket O
tokens O
[ O
] O
, O
in O
order O
to O
improve O
the O
learning O
of O
schema O
linking O
, O
For O
SQL O
sequence O
, O
we O
initialize O
it O
with O
raw O
SQL O
query O
and O
perform O
several O
modifications O
on O
it O
: O
1 O
) O
surrounding O
entities O
and O
values O
in O
SQL O
query O
with O
a O
" O
' O
" O
token O
, O
and O
dropping O
other O
surroundings O
if O
exist O
; O
2 O
) O
replacing O
col O
entities O
with O
their O
corresponding O
separate O
token O
in O
schema O
; O
3 O
) O
inserting O
spaces O
between O
punctuation O
and O
words O
. O
The O
formulated O
SQL O
sequence O
is O
illustrated O
in O
Figure O
1 O
. O
The O
formatting O
procedure O
improves O
consistency O
between O
tokenized O
sequences O
of O
source O
and O
target O
, O
and O
contributes O
to O
the O
identification O
and O
linking O
of O
schema O
entities O
. O

Transformer O
with O
Pointer O

Following O
the O
previous O
works O
on O
seq2seq O
semantic O
parsing O
, O
we O
use O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
as O
the O
backbone O
of O
our O
model O
. O
The O
vanilla O
Transformer O
generate O
tokens O
with O
a O
feed O
- O
forward O
layer O
that O
computes O
the O
unnormalized O
score O
over O
the O
target O
vocabulary O
. O
In O
text O
- O
to O
- O
SQL O
task O
, O
however O
, O
most O
schema O
and O
value O
mentions O
can O
be O
extracted O
from O
the O
input O
sequence O
. O
Therefore O
, O
we O
adopt O
a O
Hybrid B-MethodName
Pointer I-MethodName
Generator I-MethodName
Network I-MethodName
( O
Jia O
and O
Liang O
, O
2016 O
) O
in O
our O
architecture O
to O
generate O
tokens O
from O
the O
target O
vocabulary O
V O
or O
copy O
from O
the O
input O
context O
. O

During O
inference O
, O
input O
sequence O
X O
is O
first O
encoded O
into O
a O
sequence O
of O
hidden O
states O
H O
enc O
. O
Then O
, O
the O
decoder O
produces O
the O
hidden O
states O
h O
dec O
for O
step O
t O
based O
on O
previously O
generated O
sequence O
and O
encoded O
output O
. O
The O
unnormalized O
scores O
scores O
v O
= O
{ O
s O
1 O
, O
... O
, O
s O
|V O
| O
} O
over O
V O
can O
be O
obtained O
from O
h O
dec O
through O
a O
feed O
- O
forward O
layer O
. O

V O
= O
{ O
V O
q O
, O
V O
c O
, O
V O
s O
} O

is O
the O
target O
vocabulary O
, O
where O
V O
q O
denotes O
corpora O
token O
vocabulary O
, O
V O
c O
denotes O
column O
token O
set O
and O
V O
s O
denotes O
avaliable O
SQL O
keywords O
, O
e.g. O
SELECT O
, O
MAX O
, O
MIN O
, O
etc O
. O
The O
decoder O
output O
h O
dec O
is O
also O
used O
to O
compute O
the O
unnormalized O
attention O
scores O
score O
s O
= O
{ O
i O
1 O
, O
... O
, O
i O
|X| O
} O
over O
the O
input O
sequence O
tokens O
, O
where O
|X| O
is O
the O
sequence O
length O
. O

We O
concatenate O
scores O
v O
and O
score O
s O
to O
get O
the O
hybrid O
score O
score O
hybrid O
= O
{ O
s O
1 O
, O
... O
, O
s O
|V O
| O
, O
i O
1 O
, O
... O
, O
i O
|X| O
} O
, O
where O
the O
first O
|V O
| O
elements O
represent O
the O
output O
distribution O
of O
the O
target O
vocabulary O
V O
and O
the O
remained O
|X| O
are O
pointers O
tokens O
referred O
to O
corresponding O
input O
tokens O
. O
The O
final O
probability O
distribution O
is O
computed O
by O
P O
= O
softmax O
( O
score O
hybrid O
) O
, O
to O
determine O
the O
next O
token O
during O
generation O
. O

Schema B-MethodName
- I-MethodName
aware I-MethodName
Denoising I-MethodName

Similar O
to O
masked O
language O
modeling O
and O
other O
denoising O
task O
, O
we O
propose O
two O
schema O
- O
aware O
objectives O
, O
erosion O
and O
shuffle O
, O
that O
train O
model O
to O
either O
reconstruct O
the O
origin O
sequence O
from O
noising O
input O
or O
predict O
corrupted O
output O
otherwise O
. O
The O
denoising O
procedure O
is O
illustrated O
in O
Figure O
2 O
. O

Erosion O

Given O
input O
sample O
{ O
Q O
, O
S O
, O
Y O
} O
, O
erosion O
corrupts O
the O
schema O
sequence O
S O
with O
a O
serial O
compositions O
of O
three O
noising O
operations O
: O
Permutation O
Re O
- O
order O
the O
concatenation O
sequence O
of O
schema O
columns O
during O
schema O
formulation O
. O
Removal O
For O
each O
column O
, O
remove O
it O
with O
a O
dropping O
probability O
p O
drop O
. O
Addition O
With O
a O
addition O
probability O
p O
add O
, O
extract O
a O
column O
from O
another O
schema O
that O
exists O
in O
the O
training O
database O
and O
insert O
it O
into O
current O
schema O
set O
. O
During O
all O
operations O
above O
, O
the O
order O
of O
separating O
special O
tokens O
remains O
unchanged O
, O
therefore O
the O

X O
= O
{ O
( O
Q O
i O
, O
S O
i O
, O
Y O
i O
) O
} O
, O
i O
∈ O
1 O
, O
... O
|X O
| O
, O
S2S O
Transformer O
Θ O
foreach O
( O
Q O
i O
, O
S O
i O
, O
Y O
i O
) O
∈ O
X O
do O
T O
src O
, O
T O
tgt O
← O
Q O
i O
, O
Y O
i O
; O
T O
tgt O
, O
S O
i O
← O
Erosion O
( O
T O
tgt O
, O
S O
i O
) O
with O
P O
shuf O
f O
le O
do O
with O
P O
swap O
do O
T O
src O
, O
T O
tgt O
← O
T O
tgt O
, O
T O
src O
; O
end O
T O
src O
← O
Shuffle O
( O
T O
tgt O
) O
end O
T O
type O
← O
SeqType O
( O
T O
tgt O
) O
if O
T O
type O
= O
SQL O
then O
T O
pref O
ix O
← O
< O
2sql O
> O
; O
else O
T O
pref O
ix O
← O
< O
2nl O
> O
; O
end O
T O
src O
← O
T O
pref O
ix O
+ O
T O
src O
+ O
S O
i O
; O
TrainOneSample O
( O
T O
src O
, O
T O
tgt O
, O
Θ O
) O
end O

corresponding O
anonymous O
entities O
in O
SQL O
query O
should O
be O
updated O
along O
with O
the O
erosion O
operations O
in O
schema O
sequence O
. O
In O
particular O
, O
if O
a O
column O
entity O
mentioned O
in O
SQL O
query O
is O
removed O
during O
erosion O
, O
we O
substitute O
the O
corresponding O
column O
token O
in O
SQL O
with O
a O
masking O
token O
< O
unk O
> O
to O
cope O
with O
the O
absence O
of O
the O
schema O
information O
. O
With O
such O
joint O
modification O
for O
schema O
and O
SQL O
sequence O
, O
the O
model O
is O
required O
to O
identify O
the O
schema O
entities O
that O
are O
truly O
related O
to O
the O
NL O
question O
and O
learns O
to O
raise O
an O
unknown O
exception O
whenever O
the O
schema O
information O
is O
insufficient O
to O
compose O
the O
target O
SQL O
. O

Shuffle O

Given O
input O
sequence O
X O
= O
{ O
Q O
, O
S O
} O
, O
where O
Q O
= O
{ O
Q O
, O
Y O
} O
, O
the O
shuffle O
noise O
reorders O
the O
mentioning O
sequence O
of O
entities O
in O
the O
source O
query O
while O
the O
schema O
sequence O
S O
is O
fixed O
. O
The O
denoising O
objective O
trains O
model O
to O
reconstruct O
the O
query O
sequence O
Q O
with O
entities O
in O
correct O
order O
. O
The O
objective O
of O
recovering O
shuffled O
entity O
orders O
trains O
model O
to O
capture O
the O
inner O
relation O
between O
different O
entities O
and O
therefore O
contributes O
to O
the O
schema O
linking O
performance O
. O
It O
is O
also O
notable O
that O
, O
as O
a O
selfsupervision O
objective O
, O
both O
Q O
and O
Y O
are O
engaged O
in O
this O
denoising O
task O
and O
get O
trained O
separately O
. O
Though O
we O
dependent O
on O
the O
SQL O
query O
to O
identify O
the O
value O
entities O
in O
NL O
query O
, O
order O
shuffling O
with O
only O
column O
entities O
is O
sufficient O
to O
obtain O
promising O
performance O
. O
Since O
no O
parallel O
data O
is O
required O
, O
additional O
corpus O
with O
monolingual O
data O
for O
both O
SQL O
and O
NL O
could O
help O
with O
the O
re O
- O
order O
task O
and O
will O
be O
one O
of O
the O
further O
direction O
of O
this O
work O
. O

Training O
Procedure O

Inspired O
by O
previous O
works O
on O
denoising O
selftraining O
( O
Song O
et O
al O
. O
, O
2019 O
; O
, O
we O
propose O
to O
train O
the O
schema O
- O
aware O
denoising O
objectives O
along O
with O
the O
primary O
seq2seq O
task O
. O
During O
training O
, O
for O
each O
training O
sample O
, O
we O
apply O
a O
nosing O
pipeline O
to O
it O
before O
feeding O
it O
into O
the O
model O
. O
The O
noises O
with O
different O
type O
are O
applied O
to O
the O
sample O
individually O
. O
Through O
the O
control O
of O
activate O
probability O
, O
they O
could O
share O
the O
same O
weights O
in O
the O
overall O
objective O
. O
Such O
continual O
noising O
pipeline O
generates O
random O
- O
wise O
corrupted O
samples O
during O
training O
. O
It O
prevents O
the O
model O
from O
fast O
over O
- O
fitting O
and O
could O
yield O
results O
with O
better O
generalization O
( O
Siddhant O
et O
al O
. O
, O
2020 O
) O
. O
In O
practice O
, O
such O
simple O
combination O
noising O
strategy O
could O
perform O
better O
comparing O
to O
model O
- O
based O
curriculum O
method O
. O
The O
whole O
procedure O
is O
summarized O
in O
Algorithm O
1 O
. O

Clause B-MethodName
- I-MethodName
sensitive I-MethodName
EG I-MethodName
Decoding I-MethodName

During O
the O
inference O
of O
text O
- O
to O
- O
SQL O
task O
, O
the O
predicted O
SQL O
may O
contain O
errors O
related O
to O
inappropriate O
schema O
linking O
or O
grammar O
. O
EG O
decoding O
is O
proposed O
to O
amend O
these O
errors O
through O
an O
executor O
- O
in O
- O
loop O
iteration O
. O
It O
is O
performed O
by O
feeding O
SQL O
queries O
in O
the O
candidate O
list O
into O
the O
executor O
in O
sequence O
and O
discarding O
those O
queries O
that O
fail O
to O
execute O
or O
return O
empty O
result O
. O
Such O
decoding O
strategy O
, O
while O
effective O
, O
suggests O
that O
the O
major O
disagreement O
in O
the O
candidate O
list O
focuses O
on O
schema O
linking O
or O
grammar O
. O
Directly O
perform O
EG O
to O
the O
candidates O
generated O
with O
beam O
search O
leads O
to O
trivial O
improvement O
, O
as O
the O
candidates O
consist O
of O
redundant O
variations O
focuses O
on O
selection O
or O
schema O
naming O
, O
etc O
. O
This O
problem O
can O
be O
addressed O
by O
setting O
the O
beam O
length O
of O
most O
of O
the O
predicted O
tokens O
to O
1 O
and O
releasing O
those O
tokens O
related O
to O
schema O
linking O
( O
e.g. O
, O
WHERE O
) O
. O
We O
also O
notice O
that O
there O
are O
cases O
that O
combine O
incorrect O
schema O
linking O
with O
some O
aggregation O
in O
SELECT O
clause O
, O
which O
return O
some O
trivial O
results O
such O
as O
0 O
, O
thus O
suppress O
the O
EG O
filter O
. O
To O
mitigate O
the O
issue O
, O
we O
suggest O
to O
drop O
aggregate O
operator O
in O
SELECT O
during O
EG O
to O
maximize O
the O
effectiveness O
of O
it O
. O
Note O
that O
with O
such O
strategy O
, O
the O
condition O
with O
inequation O
in O
WHERE O
clause O
should O
be O
dropped O
together O
to O
ensure O
the O
validity O
of O
the O
ground O
- O
truth O
SQL O
results O
. O

Experiment O

To O
demonstrate O
the O
effectiveness O
of O
the O
proposed O
method O
, O
we O
evaluate O
the O
proposed O
model O
on O
Wik B-DatasetName
- I-DatasetName
iSQL I-DatasetName
benchmark O
and O
compare O
it O
to O
other O
state O
- O
ofthe O
- O
art O
methods O
. O

Dataset O

As O
the O
largest O
human O
- O
annotated O
dataset O
of O
textto O
- O
SQL O
, O
WikiSQL B-DatasetName
consists O
of O
56 O
, O
355 O
, O
8 O
, O
421 O
and O
15 O
, O
878 O
NL O
- O
SQL O
pairs O
for O
training O
, O
validation O
and O
inference O
respectively O
. O
All O
ground O
- O
truth O
SQL O
queries O
are O
guaranteed O
with O
at O
least O
one O
query O
result O
. O
Each O
SQL O
contains O
SELECT O
clause O
with O
at O
most O
one O
aggregation O
operator O
and O
WHERE O
clause O
with O
at O
most O
4 O
conditions O
that O
connected O
by O
AND O
. O

Each O
SQL O
is O
associated O
with O
a O
schema O
in O
database O
. O

Implementation O
details O

We O
implement O
our O
method O
using O
AllenNLP O
( O
Gardner O
et O
al O
. O
) O
and O
Pytorch O
( O
Paszke O
et O
al O
. O
) O
. O
For O
the O
model O
architecture O
, O
we O
use O
Transformer O
with O
12 B-HyperparameterValue
layers B-HyperparameterName
in O
each O
of O
the O
encoder O
and O
decoder O
with O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
. O
We O
initialize O
the O
model O
weight O
with O
bart O
- O
large O
pretrained O
model O
provided O
by O
Huggingface O
community O
( O
Wolf O
et O
al O
. O
) O
and O
finetune O
it O
on O
training O
dataset O
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
during O
training O
is O
set O
to O
8 B-HyperparameterValue
with O
a O
gradient B-HyperparameterName
accumulation I-HyperparameterName
step I-HyperparameterName
of O
2 B-HyperparameterValue
. O
We O
choose O
Adam B-HyperparameterName
( O
Kingma O
and O
Ba O
) O
as O
the O
optimizer O
and O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
7e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
with O
a O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
step I-HyperparameterName
ratio I-HyperparameterName
of O
1 B-HyperparameterValue
% I-HyperparameterValue
. O
We O
searched O
for O
the O
best O
learning B-HyperparameterName
rate I-HyperparameterName
for O
our O
model O
out O
of O
[ O
1e-4 B-HyperparameterValue
, O
7e-5 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
, O
5e-6 B-HyperparameterValue
, O
5e-7 B-HyperparameterValue
] O
. O
The O
weight B-HyperparameterName
decay I-HyperparameterName
for O
regulation O
is O
set O
to O
0.01 B-HyperparameterValue
. O
We O
set O
the O
activation O
probability O
P B-HyperparameterName
swap I-HyperparameterName
= O
0.5 B-HyperparameterValue
and O
P B-HyperparameterName
shuf I-HyperparameterName
f I-HyperparameterName
le I-HyperparameterName
= O
0.3 B-HyperparameterValue
, O
which O
lets O
the O
self O
- O
supervision O
and O
seq2seq O
objectives O
share O
equal O
weight O
during O
training O
process O
. O
P B-HyperparameterName
drop I-HyperparameterName
for O
column O
removal O
in O
erosion O
is O
set O
to O
0.1 B-HyperparameterValue
. O

The O
early B-HyperparameterName
stop I-HyperparameterName
patience I-HyperparameterName
is O
set O
to O
5 B-HyperparameterValue
with O
respect O
to O
the O
BLUE B-MetricName
metric O
( O
Papineni O
et O
al O
. O
) O
on O
validation O
set O
. O
The O
overall O
training O
procedure O
spend O
around O
3 O
hours O
on O
an O
Ubuntu O
server O
with O
8 O
NVIDIA O
V100 O
GPUs O
. O

Competitors O

We O
compare O
the O
proposed O
method O
to O
the O
following O
models O
: O
( O
1 O
) O
SQLNet B-MethodName
( O
Xu O
et O
al O
. O
, O
2017 O
) O
is O
a O
sketch O
- O
based O
method O
; O

( O
2 O
) O
SQLova B-MethodName
( O
Hwang O
et O
al O
. O
, O
2019 O
) O
is O
a O
sketch O
- O
based O
method O
which O
leverage O
the O
pre O
- O
trained O
language O
model O
for O
representation O
; O
( O
3 O
) O
X B-MethodName
- I-MethodName
SQL I-MethodName
( O
He O
et O
al O
. O
, O
2019a O
) O
enhances O
the O
structural O
schema O
representation O
with O
contextual O
embedding O
; O

( O
4 O
) O
HydraNet B-MethodName
( O
Lyu O
et O
al O
. O
, O
2020 O
) O
transforms O
schema O
linking O
into O
column O
- O
wise O
matching O
and O
ranking O
; O

( O
5 O
) O
IESQL B-MethodName
( O
Ma O
et O
al O
. O
, O
2020 O
) O
treats O
text O
- O
to O
- O
SQL O
as O
a O
sequence O
labeling O
task O
; O
( O
6 O
) O
BRIDGE B-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
is O
a O
sequential O
architecture O
for O
modeling O
dependencies O
between O
natural O
language O
question O
and O
related O
schema O
; O
( O
7 O
) O
SDSQL B-MethodName
( O
Hui O
et O
al O
. O
, O
2021 O
) O
is O
a O
multi O
- O
task O
model O
with O
explicitly O
schema O
dependency O
guided O
module O
. O

Comparison O
with O
State O
- O
of O
- O
the O
- O
art O
Models O

The O
comparison O
results O
are O
summarized O
in O
achieves O
best O
performance O
on O
four O
out O
of O
five O
components O
among O
all O
competitors O
. O

Ablation O
Study O

To O
evaluate O
the O
contribution O
of O
each O
proposed O
objective O
, O
we O
perform O
ablation O
study O
to O
SeaD B-MethodName
( O
Table O
4 O
) O
with O
WikiSQL B-DatasetName
dataset O
. O
We O
start O
from O
the O
Bart O
model O
and O
add O
components O
to O
it O
in O
sequence O
. O
The O
pointer O
net O
contributes O
to O
1.3 B-MetricValue
% I-MetricValue
absolute O
improvement O
of O
Acc B-MetricName
lf O
on O
test O
set O
. O
Combine O
text O
infilling O
, O
an O
effective O
denoising O
objective O
utilized O
by O
Bart O
, O
into O
training O
procedure O
brings O
0.2 B-MetricValue
absolute O
Acc B-MetricName
lf O
improvement O
. O
On O
the O
other O
hand O
, O
erosion O
and O
shuffle O
objectives O
contribute O
to O
1.5 B-MetricValue
% I-MetricValue
and O
0.5 B-MetricValue
% I-MetricValue
absolute O
Acc B-MetricName
lf O
improvement O
for O
SeaD B-MethodName
on O
test O
set O
respectively O
. O
It O
demonstrates O
the O
effectiveness O
of O
the O
schema O
- O
aware O
denoising O
objective O
for O
improving O
seq2seq O
generation O
in O
text O
- O
to O
- O
SQL O
task O
. O

Conclusions O

In O
this O
paper O
, O
we O
proposed O
to O
train O
model O
with O
novel O
schema O
- O
aware O
denoising O
objectives O
, O
which O
could O
improve O
performance O
of O
seq2seq O
generation O
for O
text O
- O
to O
- O
SQL O
task O
. O
These O
objectives O
are O
applied O
individually O
with O
respective O
to O
their O
activate O
probabilities O
, O
which O
are O
fixed O
during O
the O
training O
procedure O
. O

A O
noise O
re O
- O
weighting O
model O
will O
be O
considered O
for O
future O
work O
. O
Combined O
with O
the O
proposed O
clausesensitive B-MethodName
EG I-MethodName
decoding I-MethodName
strategy I-MethodName
, O
our O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
on O
the O
WikiSQL B-DatasetName
benchmark O
. O
The O
success O
of O
the O
SeaD B-MethodName
highlights O
the O
potential O
of O
utilizing O
task O
- O
oriented O
denoising O
objective O
for O
seq2seq O
model O
enhancement O
. O

Product B-TaskName
Question I-TaskName
Answering I-TaskName
in O
E O
- O
Commerce O
: O
A O
Survey O

Product B-TaskName
question I-TaskName
answering I-TaskName
( O
PQA B-TaskName
) O
, O
aiming O
to O
automatically O
provide O
instant O
responses O
to O
customer O
's O
questions O
in O
E O
- O
Commerce O
platforms O
, O
has O
drawn O
increasing O
attention O
in O
recent O
years O
. O
Compared O
with O
typical O
QA B-TaskName
problems O
, O
PQA B-TaskName
exhibits O
unique O
challenges O
such O
as O
the O
subjectivity O
and O
reliability O
of O
user O
- O
generated O
contents O
in O
Ecommerce O
platforms O
. O
Therefore O
, O
various O
problem O
settings O
and O
novel O
methods O
have O
been O
proposed O
to O
capture O
these O
special O
characteristics O
. O
In O
this O
paper O
, O
we O
aim O
to O
systematically O
review O
existing O
research O
efforts O
on O
PQA B-TaskName
. O
Specifically O
, O
we O
categorize O
PQA B-TaskName
studies O
into O
four O
problem O
settings O
in O
terms O
of O
the O
form O
of O
provided O
answers O
. O
We O
analyze O
the O
pros O
and O
cons O
, O
as O
well O
as O
present O
existing O
datasets O
and O
evaluation O
protocols O
for O
each O
setting O
. O
We O
further O
summarize O
the O
most O
significant O
challenges O
that O
characterize O
PQA B-TaskName
from O
general O
QA B-TaskName
applications O
and O
discuss O
their O
corresponding O
solutions O
. O
Finally O
, O
we O
conclude O
this O
paper O
by O
providing O
the O
prospect O
on O
several O
future O
directions O
. O

1 O
Introduction O
E O
- O
Commerce O
is O
playing O
an O
increasingly O
important O
role O
in O
our O
daily O
life O
. O
During O
the O
online O
shopping O
, O
potential O
customers O
inevitably O
have O
some O
questions O
about O
their O
interested O
products O
. O
To O
settle O
down O
their O
concerns O
and O
improve O
the O
shopping O
experience O
, O
many O
AI O
conversational O
assistants O
have O
been O
developed O
to O
solve O
customers O
' O
problems O
, O
such O
as O
Alexa O
( O
Carmel O
et O
al O
. O
, O
2018 O
) O
and O
AliMe O
( O
Li O
et O
al O
. O
, O
2017a O
) O
. O
The O
core O
machine O
learning O
problem O
underlying O
them O
, O
namely O
Product B-TaskName
Question I-TaskName
Answering I-TaskName
( O
PQA B-TaskName
) O
, O
thus O
receives O
extensive O
attention O
in O
both O
academia O
and O
industries O
recently O
. O
tremendous O
amount O
of O
product O
- O
related O
data O
available O
within O
the O
product O
page O
, O
which O
contains O
natural O
language O
user O
- O
generated O
content O
( O
UGC O
) O
( O
e.g. O
, O
product O
reviews O
, O
community O
QA O
pairs O
) O
, O
structured O
product O
- O
related O
information O
( O
e.g. O
, O
attribute O
- O
value O
pairs O
) O
, O
images O
, O
etc O
. O
Generally O
, O
PQA B-TaskName
aims O
to O
automatically O
answer O
the O
customer O
- O
posted O
question O
in O
the O
natural O
language O
form O
about O
a O
specific O
product O
, O
based O
on O
the O
product O
- O
related O
data O
. O

Typical O
QA O
studies O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
and O
some O
other O
domain O
- O
specific O
QA O
studies O
( O
e.g. O
, O
biomedical O
QA O
( O
Jin O
et O
al O
. O
, O
2023 O
) O
and O
legal O
QA O
( O
Gil O
, O
2021 O
) O
) O
mainly O
focus O
on O
the O
questions O
that O
ask O
for O
a O
certain O
factual O
and O
objective O
answer O
. O
Differently O
, O
product O
- O
related O
questions O
in O
PQA B-TaskName
typically O
involve O
consumers O
' O
opinion O
about O
the O
products O
or O
aspects O
of O
products O
. O
Therefore O
, O
early O
studies O
( O
Moghaddam O
and O
Ester O
, O
2011 O
; O
Yu O
et O
al O
. O
, O
2012 O
) O
regard O
PQA B-TaskName
as O
a O
special O
opinion O
mining O
problem O
, O
where O
the O
answers O
are O
generated O
by O
aggregating O
opinions O
in O
the O
retrieved O
documents O
. O
Most O
of O
recent O
works O
essentially O
follow O
the O
same O
intuition O
, O
but O
formulate O
PQA B-TaskName
as O
different O
problems O
in O
terms O
of O
the O
form O
of O
target O
answers O
. O
Accordingly O
, O
existing O
PQA B-TaskName
studies O
Besides O
the O
task O
- O
specific O
challenges O
in O
each O
type O
of O
PQA B-TaskName
systems O
, O
there O
are O
several O
common O
challenges O
across O
all O
types O
of O
PQA B-TaskName
systems O
, O
which O
differentiate O
PQA B-TaskName
from O
other O
QA B-TaskName
systems O
. O
( O
1 O
) O
Subjectivity O
. O
Subjective O
questions O
constitute O
a O
large O
proportion O
of O
questions O
in O
PQA O
, O
which O
requires O
to O
aggregate O
the O
crowd O
's O
opinions O
about O
the O
questions O
, O
reflected O
through O
related O
reviews O
and O
QAs B-TaskName
. O

( O
2 O
) O
Reliability O
& O
Answerability O
. O
Different O
from O
those O
supporting O
documents O
constructed O
by O
professionals O
in O
biomedical O
or O
legal O
QA B-TaskName
, O
product O
reviews O
and O
community O
QA B-TaskName
pairs O
come O
directly O
from O
nonexpert O
users O
, O
which O
may O
suffer O
from O
some O
typical O
flaws O
as O
other O
UGC O
, O
such O
as O
redundancy O
, O
inconsistency O
, O
spam O
, O
and O
even O
malice O
. O
( O
3 O
) O
Multi O
- O
type O
resources O
. O
The O
supporting O
documents O
usually O
consist O
of O
heterogeneous O
information O
from O
multi O
- O
type O
data O
resources O
, O
such O
as O
text O
, O
table O
, O
knowledge O
graph O
, O
image O
, O
etc O
. O
( O
4 O
) O
Low O
- O
resource O
. O
PQA B-TaskName
systems O
often O
encounter O
the O
low O
- O
resource O
issue O
, O
since O
different O
product O
categories O
may O
need O
different O
training O
data O
, O
and O
it O
is O
generally O
time O
- O
consuming O
and O
costly O
to O
manually O
annotate O
sufficient O
labeled O
data O
for O
each O
domain O
. O
Accordingly O
, O
we O
introduce O
existing O
solutions O
to O
each O
challenge O
. O

To O
our O
knowledge O
, O
this O
survey O
is O
the O
first O
to O
focus O
on O
Product B-TaskName
Question I-TaskName
Answering I-TaskName
. O
We O
first O
systematically O
summarize O
recent O
studies O
on O
PQA B-TaskName
into O
four O
problem O
settings O
as O
well O
as O
introduce O
the O
available O
datasets O
and O
corresponding O
evaluation O
protocols O
in O
Section O
2 O
. O
Then O
we O
analyze O
the O
most O
significant O
challenges O
that O
characterize O
PQA B-TaskName
from O
other O
QA B-TaskName
applications O
and O
discuss O
their O
corresponding O
solutions O
in O
Section O
3 O
. O
Finally O
, O
we O
discuss O
several O
promising O
research O
directions O
for O
future O
PQA B-TaskName
studies O
and O
conclude O
this O
paper O
in O
Section O
4 O
and O
5 O
. O

Problems O
and O
Approaches O

Product B-TaskName
question I-TaskName
answering I-TaskName
( O
PQA B-TaskName
) O
aims O
to O
produce O
an O
answer O
a O
to O
a O
given O
natural O
language O
question O
q O
based O
on O
a O
set O
of O
supporting O
documents O
D O
, O
1 O
. O
We O
present O
an O
overview O
of O
the O
general O
framework O
for O
each O
problem O
setting O
in O
Figure O
2 O
. O
In O
addition O
, O
the O
key O
information O
of O
the O
datasets O
adopted O
in O
existing O
PQA B-TaskName
studies O
is O
summarized O
in O
Table O
2 O
. O

Opinion O
- O
based O
PQA B-TaskName

Opinion O
- O
based O
PQA B-TaskName
studies O
focus O
on O
yes O
- O
no O
type O
questions O
, O
i.e. O
, O
questions O
that O
can O
be O
answered O
by O
" O
Yes O
" O
or O
" O
No O
" O
, O
which O
constitute O
a O
large O
proportion O
on O
PQA B-TaskName
platforms O
. O

Problem O
Definition O

Given O
a O
product O
- O
related O
question O
q O
and O
a O
set O
of O
supporting O
documents O
D O
( O
product O
reviews O
in O
most O
cases O
) O
, O
the O
goal O
is O
to O
predict O
a O
binary O
answer O
a O
∈ O
{ O
Yes O
, O
No O
} O
. O
Some O
studies O
also O
consider O
the O
neutral O
answer O
, O
e.g. O
, O
" O
Not O
Sure O
" O
. O

Datasets O
& O
Evaluation O
Protocols O

One O
of O
the O
largest O
and O
widely O
- O
adopted O
public O
PQA B-TaskName
datasets O
is O
the O
Amazon B-DatasetName
Product I-DatasetName
Dataset I-DatasetName
( O
denoted O
as O
" O
Amazon B-DatasetName
" O
in O
Table O
1 O
and O
hereafter O
) O
, O
composed O
by O
Amazon B-DatasetName
Question I-DatasetName
/ O
Answer B-DatasetName
Data I-DatasetName
( O
McAuley O
and O
Yang O
, O
2016 O
; O
Wan O
and O
McAuley O
, O
2016 O
) O
and O
Amazon B-DatasetName
Review I-DatasetName
Data I-DatasetName
( O
He O
and O
McAuley O
, O
2016 O
; O
Ni O
et O
al O
. O
, O
2019 O
) O
. O
It O
consists O
of O
around O
1.4 O
million O
answered O
questions O
and O
233.1 O
million O
product O
reviews O
across O
over O
20 O
different O
product O
categories O
. O
The O
Amazon B-DatasetName
dataset O
contains O
the O
information O
of O
question O
types O
( O
" O
yes O
- O
no O
" O
or O
" O
open O
- O
ended O
" O
) O
, O
answer O
types O
( O
" O
yes O
" O
, O
" O
no O
" O
, O
or O
" O
not O
sure O
" O
) O
, O
helpful O
votes O
by O
customers O
, O
and O
product O
metadata O
, O
which O
is O
suitable O
for O
opinionbased O
PQA B-TaskName
evaluation O
. O
Due O
to O
the O
existence O
of O
a O
certain O
proportion O
of O
unanswerable O
questions O
based O
on O
the O
available O
reviews O
, O
it O
is O
difficult O
to O
achieve O
an O
acceptable O
performance O
with O
the O
ordinary O
classification B-MetricName
accuracy I-MetricName
metric O
Acc B-MetricName
( O
Q O
) O
for O
any O
method O
. O
Therefore O
, O
McAuley O
and O
Yang O
( O
2016 O
) O
propose O
Acc B-MetricName
@ I-MetricName
k I-MetricName
, O
which O
has O
become O
the O
de O
facto O
metric O
for O
evaluating O
opinion O
- O
based O
PQA B-TaskName
methods O
, O
which O
only O
calculates O
the O
classification O
accuracy O
of O
top O
- O
k O
questions O
ranked O
by O
the O
prediction O
confidence O
. O
The O
confidence O
with O
each O
classification O
is O
its O
distance O
from O
the O
decision O
boundary O
, O
i.e. O
, O
| O
1 O
2 O
− O
P O
( O
a|q O
, O
D O
) O
| O
. O
A O
good O
model O
is O
supposed O
to O
assign O
high O
confidence O
to O
those O
questions O
that O
can O
be O
correctly O
addressed O
. O

Acc B-MetricName
@ I-MetricName
k I-MetricName
= O
Acc B-MetricName
( O
arg O
max O
Q O
′ O
∈P O
k O
( O
Q O
) O
q∈Q O
′ O
| O
1 O
2 O
− O
P O
( O
a|q O
, O
D O
) O
| O
) O
( O
1 O
) O

where O
P O
k O
( O
Q O
) O
is O
the O
set O
of O
k O
- O
sized O
subsets O
of O
Q O
, O
and O
k O
is O
commonly O
set O
to O
be O
50 O
% O
of O
the O
total O
number O
of O
questions O
. O

Methods O

McAuley O
and O
Yang O
( O
2016 O
) O
propose O
a O
Mixtures B-MethodName
of I-MethodName
experts I-MethodName
( O
MoEs B-MethodName
) O
( O
Jacobs O
et O
al O
. O
, O
1991 O
) O
based O
model O
, O
namely O
Mixtures B-MethodName
of I-MethodName
Opinions I-MethodName
for I-MethodName
Question I-MethodName
Answering I-MethodName
( O
Moqa B-MethodName
) O
, O
to O
answer O
yes O
- O
no O
questions O
in O
PQA B-TaskName
, O
where O
each O
review O
is O
regarded O
as O
an O
" O
expert O
" O
to O
make O
a O
binary O
prediction O
for O
voting O
in O
favor O
of O
a O
" O
yes O
" O
or O
" O
no O
" O
answer O
. O
The O
confidence O
of O
each O
review O
is O
further O
weighted O
by O
its O
relevance O
to O
the O
question O
as O
follows O
: O

P O
( O
a|q O
, O
D O
) O
= O
d∈D O
P O
( O
d|q O
) O
how O
relevant O
is O
d O
• O
P O
( O
a|d O
, O
q O
) O
prediction O
from O
d O
( O
2 O
) O

Moqa B-MethodName
is O
later O
enhanced O
by O
modeling O
the O
ambiguity O
and O
subjectivity O
of O
answers O
and O
reviews O
( O
Wan O
and O
McAuley O
, O
2016 O
) O
. O
further O
improve O
Moqa B-MethodName
by O
computing O
the O
aspect O
- O
specific O
embeddings O
of O
reviews O
and O
questions O
via O
a O
three O
- O
order O
auto O
- O
encoder O
network O
in O
an O
unsupervised O
manner O
. O
In O
these O
early O
studies O
, O
the O
features O
either O
extracted O
by O
heuristic O
rules O
or O
acquired O
from O
unsupervised O
manners O
may O
limit O
the O
performance O
and O
application O
of O
opinion O
- O
based O
PQA B-TaskName
approaches O
. O

To O
better O
model O
the O
relation O
between O
the O
question O
and O
each O
review O
, O
Fan O
et O
al O
. O
( O
2019 O
) O
and O
Zhang O
et O
al O
. O
( O
2019 O
) O
explore O
the O
utility O
of O
neural O
networks O
( O
e.g. O
, O
BiLSTM B-MethodName
( O
Schuster O
and O
Paliwal O
, O
1997 O
) O
) O
and O
pretrained O
language O
models O
( O
e.g. O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
) O
to O
learn O
the O
distributed O
feature O
representations O
, O
which O
largely O
outperform O
previous O
methods O
. O
Recently O
, O
Rozen O
et O
al O
. O
( O
2021 O
) O
propose O
an O
approach O
, O
called O
SimBA B-MethodName
( O
Similarity B-MethodName
Based I-MethodName
Answer I-MethodName
Prediction I-MethodName
) O
, O
which O
leverages O
existing O
answers O
from O
similar O
resolved O
questions O
about O
similar O
products O
to O
predict O
the O
answer O
for O
the O
target O
question O
. O

Pros O
and O
Cons O

Opinion O
- O
based O
PQA B-TaskName
approaches O
can O
tackle O
a O
large O
proportion O
of O
product O
- O
related O
questions O
that O
ask O
for O
certain O
opinion O
by O
using O
comparatively O
simple O
and O
easy O
- O
to O
- O
deploy O
methods O
. O
However O
, O
opinion O
- O
based O
approaches O
could O
only O
provide O
the O
classification O
result O
of O
the O
opinion O
polarity O
, O
based O
on O
the O
common O
opinion O
reflected O
in O
the O
supporting O
documents O
, O
without O
detailed O
and O
question O
- O
specific O
information O
. O

Extraction O
- O
based O
PQA B-TaskName

Similar O
to O
typical O
extraction O
- O
based O
QA B-TaskName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
( O
also O
called O
Machine B-TaskName
Reading I-TaskName
Comprehension I-TaskName
( O
MRC B-TaskName
) O
) O
, O
extraction O
- O
based O
PQA B-TaskName
studies O
aim O
at O
extracting O
a O
certain O
span O
of O
a O
document O
to O
be O
the O
answer O
for O
the O
given O
product O
- O
related O
questions O
. O

Problem O
Definition O

Given O
a O
product O
- O
related O
question O
q O
and O
a O
supporting O
document O
d O
= O
{ O
t O
1 O
, O
... O
, O
t O
n O
} O
∈ O
D O
, O
which O
consists O
of O
one O
or O
more O
product O
reviews O
, O
the O
goal O
is O
to O
find O
a O
sequence O
of O
tokens O
( O
a O
text O
span O
) O
a O
= O
{ O
t O
s O
, O
... O
, O
t O
e O
} O
in O
d O
that O
answers O
q O
correctly O
, O
where O
1 O
≤ O
s O
≤ O
n O
, O
1 O
≤ O
e O
≤ O
n O
, O
and O
s O
≤ O
e O
. O

Datasets O
& O
Evaluation O
Protocols O

Xu O
et O
al O
. O
( O
2019 O
) O
build O
the O
first O
extraction O
- O
based O
PQA B-TaskName
dataset O
, O
called O
ReviewRC B-DatasetName
, O
using O
reviews O
from O
SemEval-2016 B-DatasetName
Task I-DatasetName
5 I-DatasetName
( O
Pontiki O
et O
al O
. O
, O
2016 O
) O
. O
Similarly O
, O
Gupta O
et O
al O
. O
( O
2019 O
) O
conduct O
extensive O
pre O
- O
processing O
on O
the O
Amazon B-DatasetName
dataset O
( O
McAuley O
and O
Yang O
, O
2016 O
; O
He O
and O
McAuley O
, O
2016 O
) O
to O
build O
a O
dataset O
for O
extraction O
- O
based O
PQA B-TaskName
, O
called O
Ama B-DatasetName
- I-DatasetName
zonQA I-DatasetName
. O
It O
annotates O
each O
question O
as O
either O
answerable O
or O
unanswerable O
based O
on O
the O
available O
reviews O
, O
and O
heuristically O
creates O
an O
answer O
span O
from O
the O
reviews O
that O
best O
answer O
the O
question O
. O
Bjerva O
et O
al O
. O
( O
2020 O
) O
propose O
SubjQA B-DatasetName
dataset O
to O
investigate O
the O
relation O
between O
subjectivity O
and O
PQA B-TaskName
in O
the O
context O
of O
product O
reviews O
, O
which O
contains O
6 O
different O
domains O
that O
are O
built O
upon O
Tri B-DatasetName
- I-DatasetName
pAdvisor I-DatasetName
( O
Wang O
et O
al O
. O
, O
2010 O
) O
, O
Yelp B-DatasetName
6 I-DatasetName
, O
and O
Amazon B-DatasetName
( O
McAuley O
and O
Yang O
, O
2016 O
) O
datasets O
. O

Given O
the O
same O
setting O
as O
typical O
MRC B-TaskName
, O
extraction O
- O
based O
PQA B-TaskName
adopts O
the O
same O
evaluation O
metrics O
, O
including O
Exact B-MetricName
Match I-MetricName
( O
EM B-MetricName
) O
and O
F1 B-MetricName
scores O
. O
EM B-MetricName
requires O
the O
predicted O
answer O
span O
to O
exactly O
match O
with O
the O
human O
annotated O
answer O
, O
while O
F1 B-MetricName
score O
is O
the O
averaged O
F1 O
scores O
of O
individual O
answers O
in O
the O
token O
- O
level O
. O

Methods O

Due O
to O
the O
limited O
training O
data O
for O
extraction O
- O
based O
PQA B-TaskName
, O
Xu O
et O
al O
. O
( O
2019 O
) O
employ O
two O
popular O
pretraining O
objectives O
, O
i.e. O
, O
masked O
language O
modeling O
and O
next O
sentence O
prediction O
, O
to O
post O
- O
train O
the O
BERT O
encoder O
on O
both O
the O
general O
MRC B-DatasetName
dataset O
, O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
and O
E O
- O
Commerce O
review O
datasets O
, O
including O
Amazon B-DatasetName
Review I-DatasetName
( O
He O
and O
McAuley O
, O
2016 O
) O
and O
Yelp B-DatasetName
datasets O
. O
In O
realworld O
applications O
, O
there O
will O
be O
a O
large O
number O
of O
irrelevant O
reviews O
and O
the O
question O
might O
be O
unanswerable O
. O
To O
this O
end O
, O
Gupta O
et O
al O
. O
( O
2019 O
) O
first O
extract O
top O
review O
snippets O
for O
each O
question O
based O
on O
IR O
techniques O
and O
build O
an O
answerability O
classifier O
to O
identify O
unanswerable O
questions O
based O
on O
the O
available O
reviews O
. O
Then O
, O
a O
span O
- O
based O
QA O
model O
, O
namely O
R B-MethodName
- I-MethodName
Net I-MethodName
( O
Wang O
et O
al O
. O
, O
2017 O
) O
, O
is O
adopted O
for O
the O
extraction O
- O
based O
PQA B-TaskName
. O
Besides O
, O
Bjerva O
et O
al O
. O
( O
2020 O
) O
develop O
a O
subjectivity O
- O
aware O
QA O
model O
, O
which O
performs O
the O
multi O
- O
task O
learning O
of O
the O
extraction O
- O
based O
PQA B-TaskName
and O
subjectivity O
classification O
. O
Experimental O
results O
show O
that O
incorporating O
subjectivity O
effectively O
boosts O
the O
performance O
. O

Pros O
and O
Cons O

Extraction O
- O
based O
PQA B-TaskName
approaches O
can O
provide O
pinpointed O
answers O
to O
the O
given O
questions O
, O
but O
it O
may O
be O
less O
user O
- O
friendly O
to O
provide O
an O
incomplete O
sentence O
to O
users O
and O
may O
also O
lose O
some O
additional O
information O
. O
Since O
there O
are O
a O
large O
proportion O
of O
questions O
that O
ask O
for O
certain O
user O
experiences O
or O
opinions O
based O
on O
the O
statistics O
in O
( O
McAuley O
and O
Yang O
, O
2016 O
; O
Deng O
et O
al O
. O
, O
2022 O
) O
, O
extractionbased O
paradigm O
is O
less O
practical O
and O
favorable O
in O
real O
- O
world O
PQA O
applications O
. O
Therefore O
, O
it O
can O
be O
observed O
that O
there O
are O
relatively O
few O
works O
in O
extraction O
- O
based O
PQA B-TaskName
studies O
in O
recent O
years O
. O

Retrieval O
- O
based O
PQA B-TaskName

Retrieval O
- O
based O
PQA B-TaskName
studies O
treat O
PQA B-TaskName
as O
an O
answer O
( O
sentence O
) O
selection O
task O
, O
which O
retrieves O
the O
best O
answer O
from O
a O
set O
of O
candidates O
to O
appropriately O
answer O
the O
given O
question O
. O

Problem O
Definition O

Given O
a O
question O
q O
and O
a O
set O
of O
supporting O
documents O
D O
, O
the O
goal O
is O
to O
find O
the O
best O
answer O
a O
by O
ranking O
the O
list O
of O
documents O
according O
to O
the O
relevancy O
score O
between O
the O
question O
q O
and O
each O
document O
d O
∈ O
D O
, O
i.e. O
, O
a O
= O
arg O
max O
d∈D O
R O
( O
q O
, O
d O
) O
. O

Datasets O
& O
Evaluation O
Protocols O

Due O
to O
the O
absence O
of O
ground O
- O
truth O
question O
- O
review O
( O
QR O
) O
pairs O
, O
several O
efforts O
Zhang O
et O
al O
. O
, O
2020f O
) O
have O
been O
made O
on O
annotating O
additional O
QR O
pairs O
into O
the O
Amazon B-DatasetName
dataset O
for O
retrieval O
- O
based O
PQA B-TaskName
. O
Nevertheless O
, O
the O
original O
Amazon B-DatasetName
dataset O
can O
be O
directly O
adopted O
for O
retrieval O
- O
based O
PQA B-TaskName
studies O
( O
Zhang O
et O
al O
. O
, O
2020e O
, O
c O
) O
that O
aim O
to O
select O
reliable O
or O
helpful O
answers O
from O
candidate O
community O
answers O
. O

Since O
the O
retrieval O
- O
based O
PQA B-TaskName
methods O
are O
essentially O
solving O
a O
ranking O
problem O
, O
most O
studies O
adopt O
standard O
ranking O
metrics O
for O
evaluation O
, O
including O
mean B-MetricName
average I-MetricName
precision I-MetricName
( O
MAP B-MetricName
) O
, O
mean B-MetricName
reciprocal I-MetricName
rank I-MetricName
( O
MRR B-MetricName
) O
, O
and O
normalized B-MetricName
discounted I-MetricName
cumulative I-MetricName
gain I-MetricName
( O
NDCG B-MetricName
) O
. O
et O
al O
. O
( O
2017 O
) O
first O
demonstrate O
a O
retrieval O
- O
based O
PQA B-TaskName
chatbot O
, O
namely O
SuperAgent B-MethodName
, O
which O
contains O
different O
ranking O
modules O
that O
select O
the O
best O
answer O
from O
different O
data O
sources O
within O
the O
product O
page O
, O
including O
community O
QA O
pairs O
, O
product O
reviews O
, O
and O
product O
information O
. O
further O
propose O
a O
pipeline O
system O
that O
first O
classifies O
the O
question O
into O
one O
of O
the O
predefined O
question O
categories O
with O
a O
question O
category O
classifier O
, O
and O
then O
uses O
an O
ensemble O
matching O
model O
to O
rank O
the O
candidate O
answers O
. O
However O
, O
these O
systems O
usually O
contain O
multiple O
modules O
with O
different O
purposes O
, O
which O
require O
a O
large O
amount O
of O
annotated O
data O
from O
different O
sources O
. O
Therefore O
, O
most O
recent O
retrieval O
- O
based O
PQA B-TaskName
works O
use O
one O
or O
two O
sources O
as O
the O
supporting O
documents O
and O
build O
the O
model O
in O
an O
end O
- O
to O
- O
end O
manner O
. O

Methods O

Cui O

When O
facing O
a O
newly O
posted O
product O
- O
related O
question O
, O
a O
straight O
- O
forward O
answering O
strategy O
is O
to O
retrieve O
a O
similar O
resolved O
question O
and O
provide O
the O
corresponding O
answer O
to O
the O
target O
question O
. O
However O
, O
such O
a O
solution O
relies O
heavily O
on O
a O
large O
amount O
of O
domain O
- O
specific O
labeled O
data O
, O
since O
QA B-TaskName
data O
differs O
significantly O
in O
language O
characteristics O
across O
different O
product O
categories O
. O
To O
handle O
the O
low O
- O
resource O
issue O
, O
propose O
a O
general O
transfer O
learning O
framework O
that O
adapts O
the O
shared O
knowledge O
learned O
from O
large O
- O
scale O
paraphrase O
identification O
and O
natural O
language O
inference O
datasets O
( O
e.g. O
, O
Quora B-DatasetName
7 I-DatasetName
and O
MultiNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
) O
to O
enhance O
the O
performance O
of O
reranking O
similar O
questions O
in O
retrieval O
- O
based O
PQA B-TaskName
systems O
. O
Besides O
, O
Mittal O
et O
al O
. O
( O
2021 O
) O
propose O
a O
distillation O
- O
based O
distantly O
supervised O
training O
algorithm O
, O
which O
uses O
QA O
pairs O
retrieved O
by O
a O
syntactic O
matching O
system O
, O
to O
help O
learn O
a O
robust O
question O
matching O
model O
. O

Another O
approach O
to O
obtain O
answers O
for O
new O
questions O
is O
to O
select O
sentences O
from O
product O
reviews O
. O
The O
main O
challenge O
is O
that O
the O
information O
distributions O
of O
explicit O
answers O
and O
review O
contents O
that O
can O
address O
the O
corresponding O
questions O
are O
quite O
different O
and O
there O
are O
no O
annotated O
ground O
- O
truth O
question O
- O
review O
( O
QR O
) O
pairs O
which O
can O
be O
used O
for O
training O
. O
develop O
a O
distant O
supervision O
paradigm O
for O
incorporating O
the O
knowledge O
contained O
in O
QA O
collections O
into O
question O
- O
based O
response O
review O
ranking O
, O
where O
the O
top O
ranked O
reviews O
are O
more O
relevant O
to O
the O
QA O
pair O
and O
are O
useful O
for O
capturing O
the O
knowledge O
of O
response O
review O
ranking O
. O
propose O
a O
multi O
- O
task O
deep O
learning O
method O
, O
namely O
QARnet B-MethodName
, O
which O
can O
exploit O
both O
user O
- O
generated O
QA O
data O
and O
manually O
labeled O
QR O
pairs O
to O
train O
an O
end O
- O
toend O
deep O
model O
for O
answer O
identification O
in O
review O
data O
. O
aim O
at O
improving O
the O
interpretability O
of O
retrieval O
- O
based O
PQA B-TaskName
by O
identifying O
important O
keywords O
within O
the O
question O
and O
associating O
relevant O
words O
from O
large O
- O
scale O
QA O
pairs O
. O
Zhang O
et O
al O
. O
( O
2020f O
) O
employ O
pre O
- O
trained O
language O
models O
( O
e.g. O
, O
BERT O
) O
to O
obtain O
weak O
supervision O
signals O
from O
the O
community O
QA O
pairs O
for O
measuring O
the O
relevance O
between O
the O
question O
and O
heterogeneous O
information O
, O
including O
natural O
language O
reviews O
and O
structured O
attribute O
- O
value O
pairs O
. O

For O
the O
situation O
where O
multiple O
user O
- O
generated O
answers O
have O
already O
been O
posted O
, O
Zhang O
et O
al O
. O
( O
2020c O
) O
propose O
an O
answer O
ranking O
model O
, O
namely O
MUSE B-MethodName
, O
which O
models O
multiple O
semantic O
relations O
among O
the O
question O
, O
answers O
, O
and O
relevant O
reviews O
, O
to O
rank O
the O
candidate O
answers O
in O
PQA B-TaskName
platforms O
. O

Pros O
and O
Cons O

Retrieval O
- O
based O
approaches O
select O
complete O
and O
informative O
sentences O
as O
the O
answer O
, O
which O
may O
not O
answer O
the O
given O
question O
precisely O
since O
the O
supporting O
document O
( O
e.g. O
, O
reviews O
) O
is O
not O
specifically O
written O
for O
answering O
the O
given O
question O
. O

Generation O
- O
based O
PQA B-TaskName

Inspired O
by O
successful O
applications O
of O
sequence O
- O
tosequence O
( O
Seq2seq O
) O
models O
on O
other O
natural O
language O
generation O
tasks O
, O
several O
attempts O
have O
been O
made O
on O
leveraging O
Seq2seq O
model O
to O
automatically O
generate O
natural O
sentences O
as O
the O
answer O
to O
the O
given O
product O
- O
related O
question O
. O

Problem O
Definition O

Given O
a O
product O
- O
related O
question O
q O
and O
a O
set O
of O
supporting O
documents O
D O
that O
are O
relevant O
to O
the O
given O
question O
, O
the O
goal O
is O
to O
generate O
a O
natural O
language O
answer O
a O
= O
{ O
t O
a O
1 O
, O
t O
a O
2 O
, O
... O
} O
based O
on O
the O
question O
q O
and O
supporting O
documents O
D O
. O

Datasets O
& O
Evaluation O
Protocols O

The O
Amazon B-DatasetName
dataset O
can O
be O
directly O
adopted O
for O
generation O
- O
based O
PQA B-TaskName
. O
Another O
popular O
dataset O
used O
for O
geneartive O
PQA B-TaskName
is O
from O
JD B-DatasetName
( O
Gao O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
one O
of O
the O
largest O
e O
- O
commerce O
websites O
in O
China O
. O
In O
total O
, O
the O
JD B-DatasetName
dataset O
contains O
469,953 O
products O
and O
38 O
product O
categories O
, O
where O
each O
QA B-TaskName
pair O
is O
associated O
with O
the O
reviews O
and O
attributes O
of O
the O
corresponding O
product O
. O

Evaluating O
generation O
- O
based O
methods O
often O
involves O
both O
automatic O
evaluation O
and O
human B-MetricName
evaluation I-MetricName
. O
Common O
automatic O
evaluation O
metrics O
include O
( O
i O
) O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
and O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
for O
evaluating O
lexical O
similarity O
between O
generated O
answers O
and O
ground O
- O
truth O
answers O
, O
( O
ii O
) O
Embedding B-MetricName
- I-MetricName
based I-MetricName
Similarity I-MetricName
( O
Forgues O
et O
al O
. O
, O
2014 O
) O
, O
BertScore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
, O
and O
BleuRT B-MetricName
( O
Sellam O
et O
al O
. O
, O
2020 O
) O
for O
evaluating O
semantic O
relevance O
, O
( O
iii O
) O
Distinct B-MetricName
scores I-MetricName
( O
Li O
et O
al O
. O
, O
2016 O
) O
for O
evaluating O
the O
diversity O
of O
the O
generated O
answers O
. O
Human O
evaluation O
protocols O
are O
designed O
for O
evaluating O
different O
perspectives O
of O
the O
generated O
answer O
by O
human O
annotations O
, O
such O
as O
fluency O
, O
consistency O
, O
informativeness O
, O
helpfulness O
, O
etc O
. O

Methods O

Generation O
- O
based O
PQA B-TaskName
studies O
typically O
regard O
the O
retrieval O
of O
relevant O
documents O
as O
a O
pre O
- O
processing O
step O
, O
and O
build O
the O
method O
upon O
the O
retrieved O
documents O
. O
Due O
to O
the O
noisy O
nature O
of O
retrieved O
documents O
, O
Gao O
et O
al O
. O
( O
2019 O
) O
employ O
a O
Wasserstein O
distance O
based O
adversarial O
learning O
method O
to O
denoise O
the O
irrelevant O
information O
in O
the O
supporting O
reviews O
, O
while O
Chen O
et O
al O
. O
( O
2019c O
) O
design O
an O
attention O
- O
based O
weighting O
strategy O
to O
highlight O
the O
relevant O
words O
appearing O
in O
the O
retrieved O
review O
snippets O
. O
Besides O
identifying O
relevant O
information O
from O
the O
retrieved O
documents O
, O
Deng O
et O
al O
. O
( O
2020 O
) O
find O
that O
the O
rich O
personal O
opinion O
information O
in O
product O
reviews O
also O
attaches O
great O
importance O
in O
generation O
- O
based O
methods O
, O
as O
there O
are O
a O
large O
number O
of O
subjective O
questions O
in O
PQA O
. O
To O
this O
end O
, O
a O
joint O
learning O
model O
of O
answer O
generation O
and O
opinion O
mining O
is O
proposed O
to O
generate O
opinion O
- O
aware O
answers O
. O
Likewise O
, O
Lu O
et O
al O
. O
( O
2020 O
) O
propose O
a O
cross O
- O
passage O
hierarchical O
memory O
network O
to O
identify O
the O
most O
prominent O
opinion O
across O
different O
reviews O
for O
answer O
generation O
in O
PQA B-TaskName
. O
Some O
recent O
works O
focus O
on O
leveraging O
documents O
from O
multi O
- O
type O
resources O
to O
generate O
the O
answer O
. O
Feng O
et O
al O
. O
( O
2021 O
) O
model O
the O
logical O
relation O
between O
unstructured O
documents O
( O
reviews O
) O
and O
structured O
documents O
( O
product O
attributes O
) O
with O
a O
heterogeneous O
graph O
neural O
network O
. O
Gao O
et O
al O
. O
( O
2021 O
) O
aim O
at O
solving O
the O
safe O
answer O
problem O
dur O
- O
ing O
the O
generation O
( O
i.e. O
, O
neural O
models O
tend O
to O
generate O
meaningless O
and O
general O
answers O
) O
, O
by O
systematically O
modeling O
product O
reviews O
, O
product O
attributes O
, O
and O
answer O
prototypes O
. O
Shen O
et O
al O
. O
( O
2022b O
) O
propose O
present O
the O
semiPQA B-DatasetName
dataset O
to O
benchmark O
PQA B-TaskName
over O
semi O
- O
structured O
data O
. O

Pros O
and O
Cons O

Generation O
- O
based O
methods O
can O
provide O
natural O
forms O
of O
answers O
specific O
to O
the O
given O
questions O
. O
However O
, O
the O
hallucination O
and O
factual O
inconsistency O
issues O
are O
prevalent O
in O
generation O
- O
based O
methods O
. O
In O
addition O
, O
it O
is O
still O
lack O
of O
robust O
automatic O
evaluation O
protocols O
for O
generation O
- O
based O
methods O
. O

Challenges O
and O
Solutions O

Although O
the O
aforementioned O
PQA B-TaskName
methods O
are O
developed O
based O
on O
different O
problem O
settings O
, O
there O
are O
some O
common O
challenges O
in O
PQA B-TaskName
, O
as O
presented O
in O
Table O
1 O
. O
Several O
main O
challenges O
and O
their O
corresponding O
solutions O
are O
summarized O
as O
follows O
. O

Subjectivity O

Different O
from O
typical O
QA B-TaskName
whose O
answers O
are O
usually O
objective O
and O
unique O
, O
a O
large O
proportion O
of O
questions O
in O
PQA B-TaskName
platforms O
are O
asking O
for O
subjective O
information O
or O
opinions O
. O
Meanwhile O
, O
the O
UGC O
in O
E O
- O
commerce O
such O
as O
product O
reviews O
also O
provides O
rich O
information O
about O
other O
customers O
' O
opinion O
. O
Therefore O
, O
early O
studies O
regard O
PQA B-TaskName
as O
a O
special O
opinion O
mining O
problem O
( O
Moghaddam O
and O
Ester O
, O
2011 O
; O
Yu O
et O
al O
. O
, O
2012 O
) O
, O
which O
is O
followed O
by O
recent O
opinion O
- O
based O
PQA B-TaskName
studies O
( O
McAuley O
and O
Yang O
, O
2016 O
; O
Wan O
and O
McAuley O
, O
2016 O
) O
. O
Ideal O
answers O
to O
this O
kind O
of O
questions O
require O
information O
describing O
personal O
opinions O
and O
experiences O
. O
There O
are O
two O
specific O
challenges O
in O
exploiting O
such O
subjective O
information O
to O
facilitate O
PQA B-TaskName
: O

• O
Detect O
question O
- O
related O
opinion O
. O
A O
common O
solution O
is O
to O
regard O
the O
question O
as O
the O
target O
aspect O
for O
aspect O
- O
based O
opinion O
extraction O
. O
For O
example O
, O
Bjerva O
et O
al O
. O
( O
2020 O
) O
use O
OpineDB O
and O
some O
syntactic O
extraction O
patterns O
to O
extract O
opinion O
spans O
. O
Deng O
et O
al O
. O
( O
2020 O
) O
employ O
a O
dual O
attention O
mechanism O
to O
highlight O
the O
question O
- O
related O
information O
in O
reviews O
for O
the O
joint O
learning O
with O
an O
auxiliary O
opinion O
mining O
task O
. O
study O
aspect O
- O
based O
sentiment O
analysis O
in O
PQA B-TaskName
, O
which O
classifies O
the O
sentiment O
polarity O
towards O
certain O
product O
aspects O
in O
the O
question O
from O
the O
community O
answers O
. O

• O
Aggregate O
diverse O
opinion O
information O
. O
Since O
users O
may O
differ O
in O
opinions O
towards O
the O
same O
question O
, O
a O
good O
PQA B-TaskName
system O
should O
avoid O
expressing O
a O
random O
opinion O
, O
or O
even O
being O
contradictory O
to O
the O
common O
opinion O
. O
To O
this O
end O
, O
Deng O
et O
al O
. O
( O
2020 O
) O
employ O
an O
opinion O
selfmatching O
layer O
and O
design O
two O
kinds O
of O
opinion O
fusion O
strategies O
to O
uncover O
the O
common O
opinion O
among O
multiple O
reviews O
for O
generation O
- O
based O
PQA O
. O
Likewise O
, O
Lu O
et O
al O
. O
( O
2020 O
) O
propose O
a O
crosspassage O
hierarchical O
memory O
network O
to O
identify O
the O
most O
prominent O
opinion O
. O
However O
, O
existing O
studies O
pay O
little O
attention O
on O
resolving O
conflicting O
user O
opinions O
, O
which O
is O
a O
common O
issue O
in O
opinion O
summarization O
of O
product O
reviews O
( O
Pecar O
, O
2018 O
; O
Suhara O
et O
al O
. O
, O
2020 O
) O
and O
worth O
exploring O
in O
the O
future O
studies O
of O
PQA B-TaskName
. O

Answer O
Reliability O
& O
Answerability O

Similar O
to O
other O
UGC O
, O
product O
reviews O
and O
community O
answers O
in O
E O
- O
commerce O
sites O
, O
which O
are O
also O
provided O
by O
online O
users O
instead O
of O
professionals O
, O
vary O
significantly O
in O
their O
qualities O
and O
inevitably O
suffer O
from O
some O
reliability O
issues O
such O
as O
spam O
, O
redundancy O
, O
and O
even O
malicious O
content O
. O
Therefore O
, O
it O
is O
of O
great O
importance O
to O
study O
the O
answer O
reliability O
and O
answerability O
issue O
when O
building O
automatic O
PQA B-TaskName
systems O
using O
these O
UGC O
. O
In O
terms O
of O
the O
availability O
of O
candidate O
answers O
, O
existing O
solutions O
can O
be O
categorized O
into O
two O
groups O
: O

• O
Reliability O
of O
user O
- O
generated O
answers O
. O
When O
there O
are O
a O
set O
of O
candidate O
user O
- O
generated O
answers O
for O
the O
concerned O
question O
, O
the O
reliability O
measurement O
of O
these O
answers O
has O
been O
investigated O
from O
different O
perspectives O
. O
For O
example O
, O
Zhang O
et O
al O
. O
( O
2020e O
) O
predict O
the O
helpfulness O
of O
user O
- O
generated O
answers O
by O
investigating O
the O
opinion O
coherence O
between O
the O
answer O
and O
crowds O
' O
opinions O
reflected O
in O
the O
reviews O
, O
while O
Zhang O
et O
al O
. O
( O
2020d O
) O
tackle O
the O
veracity O
prediction O
of O
the O
user O
- O
generated O
answers O
for O
factual O
questions O
as O
an O
evidence O
- O
based O
fact O
checking O
problem O
. O
However O
, O
these O
studies O
mainly O
focus O
on O
the O
content O
reliability O
while O
neglecting O
the O
reliability O
degree O
of O
the O
answerer O
( O
Li O
et O
al O
. O
, O
2017b O
. O

• O
Unanswerable O
questions O
based O
on O
the O
available O
documents O
. O
Question O
answerability O
detection O
has O
drawn O
extensive O
attention O
in O
typical O
QA B-TaskName
studies O
( O
Rajpurkar O
et O
al O
. O
, O
2018 O
) O
. O
Similarly O
, O
Gupta O
et O
al O
. O
( O
2019 O
) O
train O
an O
binary O
classifier O
to O
classify O
the O
question O
answerability O
for O
PQA B-TaskName
. O
Zhang O
et O
al O
. O
( O
2020a O
) O
propose O
a O
conformal O
prediction O
based O
framework O
to O
reject O
unreliable O
answers O
and O
return O
nil O
answers O
for O
unanswerable O
questions O
. O
Meanwhile O
, O
the O
answerablity O
in O
PQA B-TaskName
is O
also O
highly O
related O
to O
the O
reliability O
of O
product O
reviews O
( O
Roy O
et O
al O
. O
, O
2022a O
; O
Shen O
et O
al O
. O
, O
2022a O
) O
. O

Multi O
- O
type O
Resources O

Another O
characteristic O
of O
PQA B-TaskName
is O
the O
necessity O
of O
processing O
heterogeneous O
information O
from O
multitype O
resources O
, O
including O
natural O
language O
UGC O
( O
e.g. O
, O
reviews O
, O
community O
QA O
pairs O
) O
, O
structured O
product O
information O
( O
e.g. O
, O
attribute O
- O
value O
pairs O
( O
Lai O
et O
al O
. O
, O
2018 O
; O
Roy O
et O
al O
. O
, O
2020 O
) O
, O
knowledge O
graph O
( O
Li O
et O
al O
. O
, O
2019a O
) O
) O
, O
E O
- O
manuals O
( O
Nandy O
et O
al O
. O
, O
2021 O
) O
, O
images O
, O
etc O
. O
Early O
works O
( O
Cui O
et O
al O
. O
, O
2017 O
; O
design O
separated O
modules O
to O
handle O
the O
questions O
that O
require O
different O
types O
of O
data O
resources O
. O
However O
, O
these O
PQA B-TaskName
systems O
rely O
heavily O
on O
annotated O
data O
from O
different O
types O
of O
resources O
and O
neglect O
the O
relation O
among O
heterogeneous O
data O
. O
Therefore O
, O
some O
recent O
studies O
focus O
on O
manipulating O
heterogeneous O
information O
from O
multi O
- O
type O
resources O
in O
a O
single O
model O
for O
better O
answering O
product O
- O
related O
questions O
. O
For O
instance O
, O
Zhang O
et O
al O
. O
( O
2020f O
) O
design O
a O
unified O
heterogeneous O
encoding O
scheme O
that O
transforms O
structured O
attribute O
- O
value O
pairs O
into O
a O
pesudo O
- O
sentence O
. O
Gao O
et O
al O
. O
( O
2019 O
) O
employ O
a O
key O
- O
value O
memory O
network O
to O
store O
and O
encode O
product O
attributes O
for O
answer O
decoding O
with O
the O
encoded O
review O
representations O
, O
which O
is O
further O
combined O
with O
answer O
prototypes O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
. O
Feng O
et O
al O
. O
( O
2021 O
) O
propose O
a O
heterogeneous O
graph O
neural O
network O
to O
track O
the O
information O
propagation O
among O
different O
types O
of O
information O
for O
modeling O
the O
relational O
and O
logical O
information O
. O

Low O
- O
resource O

Since O
there O
are O
a O
large O
amount O
of O
new O
questions O
posted O
in O
PQA B-TaskName
platforms O
every O
day O
and O
the O
required O
information O
to O
answer O
the O
questions O
varies O
significantly O
across O
different O
product O
categories O
( O
even O
across O
different O
single O
products O
) O
, O
traditional O
supervised O
learning O
methods O
become O
data O
hungry O
in O
this O
situation O
. O
However O
, O
it O
is O
time O
- O
consuming O
and O
laborintensive O
to O
obtain O
sufficient O
domain O
- O
specific O
annotations O
. O
Existing O
solutions O
typically O
leverage O
external O
resources O
to O
mitigate O
the O
low O
- O
resource O
issue O
. O
In O
terms O
of O
the O
external O
resources O
, O
these O
solutions O
can O
be O
categorized O
into O
two O
groups O
: O

• O
Transfer O
learning O
from O
out O
- O
domain O
data O
. O
This O
group O
of O
solutions O
typically O
leverages O
large O
- O
scale O
open O
- O
domain O
labeled O
datasets O
and O
design O
appropriate O
TL O
strategy O
for O
domain O
adaptation O
in O
PQA B-TaskName
. O
For O
example O
, O
transfer O
the O
knowledge O
learned O
from O
Quora B-DatasetName
and O
MultiNLI B-DatasetName
datasets O
to O
retrieval O
- O
based O
PQA B-TaskName
models O
, O
by O
imposing O
a O
regularization O
term O
on O
the O
weights O
of O
the O
output O
layer O
to O
capture O
both O
the O
inter O
- O
domain O
and O
the O
intra O
- O
domain O
relationships O
. O
Xu O
et O
al O
. O
( O
2019 O
) O
perform O
post O
- O
training O
on O
the O
SQuAD B-DatasetName
dataset O
to O
inject O
task O
- O
specific O
knowledge O
into O
BERT O
for O
extraction O
- O
based O
PQA B-TaskName
. O

• O
Distant O
supervision O
from O
in O
- O
domain O
data O
. O
Another O
line O
of O
solutions O
adopt O
the O
resolved O
QA B-TaskName
pairs O
from O
similar O
products O
( O
Rozen O
et O
al O
. O
, O
2021 O
) O
or O
products O
in O
the O
same O
categories O
Roy O
et O
al O
. O
, O
2022b O
) O

Prospects O
and O
Future O
Directions O

Considering O
the O
challenges O
summarized O
in O
this O
paper O
, O
we O
point O
out O
several O
promising O
prospects O
and O
future O
directions O
for O
PQA B-TaskName
studies O
: O

• O
Question O
Understanding O
. O
Due O
to O
the O
diversity O
of O
product O
- O
related O
questions O
, O
some O
attempts O
have O
been O
made O
on O
identifying O
the O
user O
's O
intents O
( O
Yu O
and O
Lam O
, O
2018a O
) O
, O
the O
question O
types O
( O
Cui O
et O
al O
. O
, O
2017 O
) O
, O
and O
even O
the O
user O
's O
purchase O
- O
state O
( O
Kuchy O
et O
al O
. O
, O
2021 O
) O
from O
the O
questions O
. O
In O
addition O
, O
some O
researches O
investigate O
the O
user O
's O
uncertainty O
or O
the O
question O
's O
ambiguity O
towards O
the O
product O
by O
asking O
clarifying O
questions O
( O
Majumder O
et O
al O
. O
, O
2021 O
; O
Zhang O
and O
Zhu O
, O
2021 O
) O
. O
Despite O
the O
extensive O
studies O
for O
QA B-TaskName
, O
question O
understanding O
has O
not O
been O
deeply O
studied O
in O
the O
context O
of O
PQA B-TaskName
. O
For O
example O
, O
the O
system O
should O
be O
capable O
of O
identifying O
the O
subjectivity O
from O
the O
productrelated O
questions O
( O
Bjerva O
et O
al O
. O
, O
2020 O
) O
, O
such O
as O
opinionated O
questions O
( O
Deng O
et O
al O
. O
, O
2020 O
) O
, O
comparative O
questions O
( O
Bondarenko O
et O
al O
. O
, O
2022 O
) O
, O
etc O
. O

• O
Personalization O
. O
As O
mentioned O
before O
, O
compared O
with O
typical O
QA B-TaskName
studies O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
there O
is O
a O
large O
proportion O
of O
subjective O
questions O
( O
McAuley O
and O
Yang O
, O
2016 O
) O
on O
PQA B-TaskName
platforms O
, O
which O
involve O
user O
preference O
or O
require O
personal O
information O
to O
answer O
, O
rather O
than O
objective O
or O
factoid O
questions O
that O
look O
for O
a O
certain O
answer O
. O
Besides O
, O
in O
E O
- O
Commerce O
, O
different O
customers O
often O
have O
certain O
preferences O
over O
product O
aspects O
or O
information O
needs O
( O
Chen O
et O
al O
. O
, O
2019b O
; O
Li O
et O
al O
. O
, O
2019b O
) O
, O
leading O
to O
various O
expectations O
for O
the O
provided O
answers O
. O
Therefore O
, O
Carmel O
et O
al O
. O
( O
2018 O
) O
state O
that O
a O
good O
PQA B-TaskName
system O
should O
answer O
the O
customer O
's O
questions O
with O
the O
context O
of O
her O
/ O
his O
encounter O
history O
, O
taking O
into O
consideration O
her O
/ O
his O
preference O
and O
interest O
. O
Such O
personalization O
can O
make O
the O
answer O
more O
helpful O
for O
customers O
and O
better O
clarify O
their O
concerns O
about O
the O
product O
( O
Deng O
et O
al O
. O
, O
2022 O
) O
. O

• O
Multi O
- O
modality O
. O
Compared O
with O
the O
widelystudied O
natural O
language O
UGC O
and O
structured O
product O
knowledge O
data O
, O
image O
data O
has O
received O
little O
attention O
in O
PQA B-TaskName
studies O
. O
On O
E O
- O
Commerce O
sites O
, O
there O
exist O
not O
only O
a O
great O
number O
of O
official O
product O
images O
, O
but O
also O
increasing O
usershared O
images O
about O
their O
actual O
experiences O
, O
which O
benefit O
many O
other O
E O
- O
Commerce O
applications O
( O
Liu O
et O
al O
. O
, O
2021 O
; O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O
The O
multimodal O
data O
can O
provide O
more O
valuable O
and O
comprehensive O
information O
for O
PQA B-TaskName
systems O
. O

• O
Datasets O
and O
Benchmarks O
. O
Despite O
the O
increasing O
attentions O
on O
developing O
PQA B-TaskName
systems O
, O
the O
publicly O
available O
resources O
for O
PQA B-TaskName
are O
still O
quite O
limited O
. O
Most O
existing O
PQA B-TaskName
studies O
are O
evaluated O
on O
the O
Amazon B-DatasetName
dataset O
( O
McAuley O
and O
Yang O
, O
2016 O
) O
, O
which O
is O
directly O
crawled O
from O
the O
Amazon O
pages O
. O
Some O
researches O
( O
Roy O
et O
al O
. O
, O
2022a O
; O
Shen O
et O
al O
. O
, O
2022a O
) O
have O
discussed O
several O
drawbacks O
of O
evaluating O
PQA B-TaskName
systems O
on O
this O
dataset O
: O
1 O
) O
The O
ground O
- O
truth O
answers O
are O
quite O
noisy O
, O
since O
they O
are O
the O
top O
- O
voted O
community O
answers O
posted O
by O
non O
- O
expert O
users O
. O
2 O
) O
There O
are O
no O
annotations O
for O
assessing O
the O
relevance O
of O
the O
supporting O
documents O
, O
which O
may O
cast O
potential O
risks O
on O
the O
reliability O
of O
the O
PQA B-TaskName
systems O
. O
To O
facilitate O
better O
evaluations O
, O
many O
other O
data O
resources O
for O
PQA B-TaskName
studies O
have O
been O
constructed O
as O
presented O
in O
Table O
2 O
. O
However O
, O
due O
to O
the O
privacy O
or O
the O
commercial O
issues O
, O
some O
of O
the O
datasets O
can O
not O
be O
publicly O
released O
. O
Therefore O
, O
there O
is O
still O
a O
great O
demand O
for O
a O
large O
- O
scale O
, O
high O
- O
quality O
, O
and O
publicly O
available O
benchmark O
dataset O
for O
the O
future O
studies O
on O
PQA B-TaskName
. O

• O
Evaluation O
Protocols O
. O
The O
types O
of O
questions O
vary O
in O
a O
wide O
range O
, O
from O
yes O
- O
no O
questions O
to O
open O
- O
ended O
questions O
( O
McAuley O
and O
Yang O
, O
2016 O
) O
, O
from O
objective O
questions O
to O
subjective O
questions O
( O
Bjerva O
et O
al O
. O
, O
2020 O
) O
, O
from O
factual O
questions O
to O
non O
- O
factual O
questions O
( O
Zhang O
et O
al O
. O
, O
2020d O
) O
. O
Different O
types O
of O
questions O
may O
involve O
different O
specific O
evaluation O
protocol O
. O
For O
example O
, O
it O
is O
necessary O
to O
evaluate O
the O
precision O
of O
opinion O
in O
the O
answers O
for O
subjective O
questions O
( O
Deng O
et O
al O
. O
, O
2020 O
) O
, O
while O
the O
veracity O
or O
factualness O
is O
important O
in O
factual O
questions O
( O
Zhang O
et O
al O
. O
, O
2020d O
) O
. O
Especially O
for O
generation O
- O
based O
PQA B-TaskName
methods O
, O
the O
evaluation O
is O
still O
largely O
using O
lexical O
- O
based O
text O
similarity O
metrics O
, O
which O
are O
not O
correlated O
well O
with O
human O
judgements O
. O

Conclusions O

This O
paper O
makes O
the O
first O
attempt O
to O
overview O
recent O
advances O
on O
PQA B-TaskName
. O
We O
systematically O
categorize O
recent O
PQA B-TaskName
studies O
into O
four O
problem O
settings O
, O
including O
Opinion O
- O
based O
, O
Extraction O
- O
based O
, O
Retrieval O
- O
based O
, O
and O
Generation O
- O
based O
, O
and O
summarize O
the O
existing O
methods O
and O
evaluation O
protocols O
in O
each O
category O
. O
We O
also O
analyze O
the O
typical O
challenges O
that O
distinguish O
PQA B-TaskName
from O
other O
QA B-TaskName
studies O
. O
Finally O
, O
we O
highlight O
several O
potential O
directions O
for O
facilitating O
future O
studies O
on O
PQA B-TaskName
. O

Limitations O

Since O
product B-TaskName
question I-TaskName
answering I-TaskName
( O
PQA B-TaskName
) O
is O
actually O
a O
domain O
- O
specific O
application O
in O
general O
QA B-TaskName
, O
the O
scope O
of O
the O
problem O
may O
be O
limited O
. O
However O
, O
in O
recent O
years O
, O
PQA B-TaskName
has O
received O
increasing O
attention O
in O
both O
academy O
and O
industry O
. O
( O
1 O
) O
From O
the O
research O
perspective O
, O
PQA B-TaskName
exhibits O
some O
unique O
characteristics O
and O
thus O
brings O
some O
interesting O
research O
challenges O
as O
discussed O
in O
Section O
3 O
. O
For O
example O
, O
some O
studies O
use O
PQA B-TaskName
as O
an O
entrypoint O
to O
analyze O
the O
subjectivity O
in O
QA B-TaskName
tasks O
. O

( O
2 O
) O
From O
the O
application O
perspective O
, O
it O
has O
great O
commercial O
value O
. O
Online O
shopping O
is O
playing O
an O
increasingly O
important O
role O
in O
everyone O
's O
daily O
life O
, O
so O
that O
many O
high O
- O
tech O
companies O
develop O
AI O
conversational O
assistants O
for O
promptly O
solving O
customer O
's O
online O
problems O
, O
including O
but O
not O
limited O
to O
Amazon O
, O
eBay O
, O
Alibaba O
, O
JD O
, O
etc O
. O
Regarding O
the O
large O
amount O
of O
research O
efforts O
that O
have O
been O
made O
, O
there O
is O
not O
a O
systematic O
and O
comprehensive O
review O
about O
this O
research O
topic O
. O
Similar O
to O
recent O
surveys O
of O
other O
domain O
- O
specific O
QA B-TaskName
, O
such O
as O
biomedical B-TaskName
QA I-TaskName
( O
Jin O
et O
al O
. O
, O
2023 O
) O
and O
legal B-TaskName
QA I-TaskName
( O
Gil O
, O
2021 O
) O
, O
we O
hope O
that O
this O
paper O
can O
serve O
as O
a O
good O
reference O
for O
people O
working O
on O
PQA B-TaskName
or O
beginning O
to O
work O
on O
PQA B-TaskName
, O
as O
well O
as O
shed O
some O
light O
on O
future O
studies O
on O
PQA B-TaskName
and O
raise O
more O
interests O
from O
the O
community O
for O
this O
topic O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
No O
response O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
No O
response O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
No O
response O
. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
No O
response O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Left O
blank O
. O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
No O
response O
. O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Peer O
- O
Label O
Assisted O
Hierarchical B-TaskName
Text I-TaskName
Classification I-TaskName

Hierarchical B-TaskName
text I-TaskName
classification I-TaskName
( O
HTC B-TaskName
) O
is O
a O
challenging O
task O
, O
in O
which O
the O
labels O
of O
texts O
can O
be O
organized O
into O
a O
category O
hierarchy O
. O
To O
deal O
with O
the O
HTC B-TaskName
problem O
, O
many O
existing O
works O
focus O
on O
utilizing O
the O
parent O
- O
child O
relationships O
that O
are O
explicitly O
shown O
in O
the O
hierarchy O
. O
However O
, O
texts O
with O
a O
category O
hierarchy O
also O
have O
some O
latent O
relevancy O
among O
labels O
in O
the O
same O
level O
of O
the O
hierarchy O
. O
We O
refer O
to O
these O
labels O
as O
peer O
labels O
, O
from O
which O
the O
peer O
effects O
are O
originally O
utilized O
in O
our O
work O
to O
improve O
the O
classification O
performance O
. O
To O
fully O
explore O
the O
peer O
- O
label O
relationship O
, O
we O
develop O
a O
PeerHTC B-MethodName
method O
. O
This O
method O
innovatively O
measures O
the O
latent O
relevancy O
of O
peer O
labels O
through O
several O
metrics O
and O
then O
encodes O
the O
relevancy O
with O
a O
Graph O
Convolutional O
Neural O
Network O
. O
We O
also O
propose O
a O
sample O
importance O
learning O
method O
to O
ameliorate O
the O
side O
effects O
raised O
by O
modelling O
the O
peer O
label O
relevancy O
. O
Our O
experiments O
on O
several O
standard O
datasets O
demonstrate O
the O
evidence O
of O
peer O
labels O
and O
the O
superiority O
of O
PeerHTC B-MethodName
over O
other O
state O
- O
of O
- O
the O
- O
art O
HTC B-TaskName
methods O
in O
terms O
of O
classification B-MetricName
accuracy I-MetricName
. O

Introduction O

Hierarchical B-TaskName
text I-TaskName
classification I-TaskName
( O
HTC B-TaskName
) O
is O
a O
multilabel O
text O
classification O
problem O
which O
aims O
to O
classify O
texts O
into O
categories O
that O
can O
be O
organized O
into O
a O
taxonomic O
hierarchy O
. O
It O
is O
an O
important O
problem O
in O
natural O
language O
processing O
and O
has O
attracted O
increasing O
attention O
in O
both O
industrial O
and O
academic O
fields O
. O
Typical O
HTC B-TaskName
problems O
include O
patent O
categorization O
( O
Gomez O
and O
Moens O
, O
2014 O
) O
, O
medical O
record O
coding O
( O
Cao O
et O
al O
. O
, O
2020 O
) O
, O
and O
product O
categorization O
( O
Cevahir O
and O
Murakami O
, O
2016 O
) O
. O

Due O
to O
the O
complexity O
of O
category O
hierarchy O
, O
the O
problem O
of O
hierarchical B-TaskName
text I-TaskName
classification I-TaskName
is O
more O
challenging O
than O
plain O
text O
classification O
. O
The O
parent O
- O
child O
relationships O
between O
categories O
in O
adjacent O
levels O
of O
the O
hierarchy O
are O
usually O
defined O
in O
advance O
. O
Then O
a O
natural O
way O
to O
solve O
the O
HTC B-TaskName
problem O
is O
to O
incorporate O
this O
prior O
knowledge O
into O
the O
model O
, O
i.e. O
, O
making O
the O
model O
aware O
of O
the O
hierarchy O
. O
Building O
" O
hierarchy O
- O
aware O
" O
models O
is O
beneficial O
for O
HTC B-TaskName
, O
which O
is O
particularly O
true O
for O
categories O
with O
few O
samples O
. O
Therefore O
, O
it O
has O
long O
been O
the O
main O
focus O
in O
HTC B-TaskName
to O
figure O
out O
the O
most O
effective O
way O
of O
utilizing O
the O
category O
hierarchy O
to O
improve O
the O
classification O
performance O
. O

In O
the O
past O
literature O
, O
existing O
approaches O
for O
HTC B-TaskName
can O
be O
generally O
categorized O
into O
three O
groups O
: O
local O
approaches O
, O
global O
approaches O
, O
and O
local O
- O
global O
- O
combined O
ones O
( O
also O
known O
as O
hybrid O
approaches O
) O
. O
The O
local O
approaches O
train O
local O
classifiers O
for O
every O
child O
label O
, O
every O
parent O
label O
or O
every O
level O
in O
the O
hierarchy O
( O
Shimura O
et O
al O
. O
, O
2018 O
; O
Banerjee O
et O
al O
. O
, O
2019 O
) O
. O
The O
parameters O
of O
local O
classifiers O
are O
initialized O
in O
a O
top O
- O
down O
fashion O
according O
to O
the O
category O
hierarchy O
. O
However O
, O
these O
approaches O
usually O
contain O
a O
large O
number O
of O
parameters O
, O
and O
the O
whole O
hierarchy O
can O
not O
be O
fully O
captured O
merely O
by O
parameter O
initialization O
. O
Global O
approaches O
, O
which O
are O
popular O
in O
recent O
years O
, O
aim O
to O
flatten O
HTC B-TaskName
into O
a O
multi B-TaskName
- I-TaskName
label I-TaskName
classification I-TaskName
problem O
, O
and O
then O
incorporate O
the O
information O
of O
category O
hierarchy O
in O
various O
ways O
, O
such O
as O
using O
regularization O
terms O
( O
Gopal O
and O
Yang O
, O
2013 O
) O
, O
modeling O
the O
architecture O
of O
category O
hierarchy O
( O
Zhou O
et O
al O
. O
, O
2020 O
) O
, O
and O
using O
contrastive O
learning O
( O
Wang O
et O
al O
. O
, O
2022 O
) O
. O
The O
local O
- O
global O
- O
combined O
approaches O
can O
be O
seen O
as O
an O
improvement O
on O
local O
approaches O
, O
which O
construct O
the O
information O
flow O
between O
local O
classifiers O
in O
more O
effective O
ways O
, O
and O
meanwhile O
utilize O
a O
global O
classifier O
to O
coordinate O
local O
ones O
( O
Wehrmann O
et O
al O
. O
, O
2018 O
; O
Rojas O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
these O
models O
might O
still O
suffer O
from O
error O
propagation O
( O
Rojas O
et O
al O
. O
, O
2020 O
) O
, O
because O
the O
classification O
of O
child O
layers O
are O
dependent O
on O
that O
of O
their O
parents O
. O

To O
the O
best O
of O
our O
knowledge O
, O
existing O
methods O
only O
exploit O
category O
relevancy O
that O
is O
explicitly O
reflected O
in O
the O
hierarchy O
. O
For O
example O
, O
Gopal O
and O
Yang O
( O
2013 O
) O
used O
a O
recursive O
regularization O
term O
in O
which O
the O
parameters O
of O
parent O
labels O
are O
expected O
to O
be O
similar O
to O
those O
of O
their O
children O
. O
Zhou O
et O
al O
. O
( O
2020 O
) O
used O
a O
structure O
encoder O
for O
labels O
, O
in O
which O
the O
information O
of O
parent O
and O
child O
labels O
are O
integrated O
into O
each O
label O
's O
representation O
. O
However O
, O
there O
could O
still O
exist O
some O
latent O
relevancy O
among O
the O
labels O
in O
the O
same O
level O
, O
which O
could O
also O
be O
beneficial O
to O
the O
HTC B-TaskName
problem O
. O
Take O
the O
Blurb B-DatasetName
- I-DatasetName
GenreCollection I-DatasetName
dataset O
( O
Aly O
et O
al O
. O
, O
2019 O
) O
as O
an O
example O
, O
which O
consists O
of O
descriptions O
and O
genres O
of O
books O
. O
In O
this O
dataset O
, O
two O
third O
- O
level O
book O
categories O
" O
World O
History O
" O
and O
" O
Travel O
: O
Asia O
" O
belong O
to O
different O
second O
- O
level O
categories O
" O
History O
" O
and O
" O
Travel O
" O
, O
respectively O
. O
However O
, O
these O
two O
thirdlevel O
categories O
both O
involve O
geographical O
and O
cultural O
contents O
. O
Therefore O
, O
intuitively O
they O
should O
share O
some O
common O
characteristics O
, O
and O
the O
classification O
of O
one O
category O
could O
benefit O
that O
of O
the O
other O
. O
The O
phenomenon O
that O
labels O
in O
the O
same O
level O
possess O
latent O
relevancy O
is O
similar O
to O
the O
" O
peer O
effect O
" O
existing O
among O
peer O
friends O
. O
Thus O
we O
call O
these O
labels O
as O
" O
peer O
labels O
" O
in O
this O
work O
. O

To O
utilize O
the O
latent O
relevancy O
of O
peer O
labels O
to O
improve O
the O
HTC B-TaskName
performance O
, O
we O
develop O
a O
Peer B-MethodName
- I-MethodName
HTC I-MethodName
method O
. O
It O
incorporates O
two O
types O
of O
label O
relationships O
: O
the O
parent O
- O
child O
relationship O
explicitly O
reflected O
in O
the O
hierarchy O
, O
and O
the O
peer O
- O
label O
relationship O
implicitly O
hidden O
in O
the O
hierarchy O
. O
We O
propose O
several O
measures O
to O
learn O
the O
relevancy O
structure O
among O
peer O
labels O
, O
and O
then O
utilize O
the O
Graph O
Convolutional O
Neural O
Network O
( O
GCN O
) O
to O
realize O
" O
feature O
sharing O
" O
among O
peer O
labels O
. O
To O
address O
the O
possible O
side O
effect O
caused O
by O
modeling O
peer O
labels O
, O
we O
also O
develop O
a O
measure O
to O
evaluate O
the O
degree O
of O
confusion O
between O
labels O
in O
the O
same O
level O
, O
and O
then O
assign O
different O
weights O
to O
training O
samples O
according O
to O
their O
contribution O
in O
alleviating O
label O
confusion O
. O
The O
PeerHTC B-MethodName
method O
is O
realized O
through O
an O
embedded O
two O
- O
stage O
training O
approach O
, O
in O
which O
valuable O
information O
about O
latent O
relevancy O
of O
peer O
labels O
and O
the O
label O
confusion O
can O
be O
harvested O
from O
the O
first O
round O
of O
warm O
- O
up O
training O
and O
then O
enhances O
the O
second O
round O
for O
final O
classification O
. O

The O
rest O
of O
this O
article O
is O
organized O
as O
follows O
: O
Section O
2 O
introduces O
related O
works O
. O
Section O
3 O
defines O
the O
HTC B-TaskName
problem O
. O
Section O
4 O
introduces O
the O
PeerHTC B-MethodName
method O
in O
detail O
. O
Section O
5 O
presents O
the O
experimental O
results O
on O
three O
datasets O
. O
Section O
6 O
concludes O
the O
article O
. O
We O
share O
our O
codes O
on O
GitHub O
1 O
for O
reproducibility O
. O

Related O
Work O

Local O
Approaches O

The O
local O
approaches O
train O
local O
classifiers O
for O
each O
category O
or O
each O
level O
in O
the O
hierarchy O
. O
These O
local O
classifiers O
are O
initialized O
in O
a O
top O
- O
down O
fashion O
according O
to O
the O
category O
hierarchy O
so O
that O
knowledge O
learned O
by O
each O
parent O
classifier O
can O
be O
transferred O
to O
their O
children O
. O
For O
example O
, O
the O
method O
HTrans O
( O
Hierarchical O
Transfer O
Learning O
) O
( O
Banerjee O
et O
al O
. O
, O
2019 O
) O
trained O
a O
binary O
classifier O
for O
each O
label O
, O
and O
then O
initialized O
the O
classifiers O
according O
to O
their O
parents O
. O
The O
method O
HFTCNN O
( O
Hierarchical O
Finetuning O
Based O
CNN O
) O
( O
Shimura O
et O
al O
. O
, O
2018 O
) O
trained O
a O
multi O
- O
label O
classifier O
for O
each O
level O
in O
the O
category O
hierarchy O
, O
and O
then O
followed O
a O
similar O
approach O
for O
parameter O
initialization O
. O
However O
, O
these O
models O
usually O
have O
a O
large O
number O
of O
parameters O
to O
estimate O
and O
also O
suffer O
from O
insufficient O
use O
of O
the O
category O
hierarchy O
. O

Global O
Approaches O

Global O
approaches O
flatten O
HTC B-TaskName
into O
a O
simple O
multilabel B-TaskName
classification I-TaskName
problem O
, O
and O
seek O
to O
incorporate O
the O
information O
of O
category O
hierarchy O
in O
various O
ways O
. O
For O
example O
, O
Gopal O
and O
Yang O
( O
2013 O
) O
imposed O
recursive O
regularization O
on O
parameters O
of O
parent O
and O
child O
nodes O
. O
The O
method O
HiAGM B-MethodName
( O
Zhou O
et O
al O
. O
, O
2020 O
) O
includes O
two O
variants O
, O
i.e. O
, O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
and O
HiAGM O
- O
TP O
. O
In O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
, O
texts O
and O
labels O
are O
encoded O
separately O
, O
and O
multi O
- O
label O
attention O
mechanism O
is O
used O
to O
extract O
label O
- O
wise O
features O
. O
A O
structure O
encoder O
is O
also O
used O
to O
aggregate O
prior O
category O
hierarchy O
information O
into O
label O
embeddings O
. O
In O
HiAGM O
- O
TP O
, O
label O
embeddings O
are O
not O
used O
and O
text O
features O
are O
directly O
propagated O
through O
the O
structure O
encoder O
. O
The O
method O
HTCInfoMax B-MethodName
( O
Hierarchical B-MethodName
Text I-MethodName
Classification I-MethodName
via I-MethodName
Information I-MethodName
Maximization I-MethodName
) O
( O
Deng O
et O
al O
. O
, O
2021 O
) O
seeks O
to O
improve O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
with O
mutual O
information O
maximization O
that O
constrains O
text O
and O
label O
representations O
. O
The O
method O
HiMatch O
( O
Hierarchy O
- O
aware O
Label O
Semantics O
Matching O
Network O
) O
( O
Chen O
et O
al O
. O
, O
2021 O
) O
projects O
the O
representations O
of O
words O
and O
labels O
into O
a O
common O
latent O
space O
and O
utilizes O
hierarchyaware O
matching O
learning O
. O
The O
method O
HGCLR O
( O
Hierarchy O
- O
Guided O
Contrastive O
Learning O
) O
( O
Wang O
et O
al O
. O
, O
2022 O
) O
models O
texts O
and O
labels O
separately O
only O
in O
the O
training O
process O
, O
and O
then O
incorporates O
the O
information O
of O
category O
hierarchy O
into O
the O
text O
encoder O
via O
contrastive O
learning O
. O

Hybrid O
Approaches O

Local O
- O
global O
- O
combined O
approaches O
( O
or O
hybrid O
approaches O
) O
can O
be O
seen O
as O
an O
improvement O
on O
local O
ones O
. O
The O
method O
HMCN B-MethodName
( O
Hierarchical B-MethodName
Multilabel I-MethodName
Classification I-MethodName
Networks I-MethodName
) O
( O
Wehrmann O
et O
al O
. O
, O
2018 O
) O
is O
probably O
the O
first O
hybrid O
model O
. O
In O
HMCN B-MethodName
, O
local O
classifiers O
are O
arranged O
in O
series O
and O
global O
classification O
is O
conducted O
to O
coordinate O
these O
local O
classifiers O
. O
The O
method O
HARNN B-MethodName
( O
Hierarchical B-MethodName
Attention I-MethodName
- I-MethodName
based I-MethodName
Recurrent I-MethodName
Neural I-MethodName
Network I-MethodName
) O
( O
Huang O
et O
al O
. O
, O
2019 O
) O
is O
another O
typical O
hybrid O
model O
. O
It O
shares O
a O
similar O
architecture O
with O
HMCN B-MethodName
, O
but O
uses O
the O
multi O
- O
label O
attention O
mechanism O
to O
extract O
label O
- O
wise O
text O
features O
. O
However O
, O
since O
errors O
in O
the O
prediction O
of O
higher O
- O
level O
categories O
may O
provide O
misleading O
information O
for O
lower O
levels O
, O
these O
hybrid O
approaches O
might O
suffer O
from O
error O
propagation O
( O
Rojas O
et O
al O
. O
, O
2020 O
) O
. O

Problem O
Formulation O

We O
define O
the O
HTC B-TaskName
problem O
in O
this O
section O
. O
Specifically O
, O
we O
first O
give O
the O
definition O
of O
a O
category O
hierarchy O
and O
its O
properties O
, O
and O
then O
define O
the O
HTC B-TaskName
problem O
mathematically O
. O

Definition O
1 O
. O
( O
Category O
Hierarchy O
) O
Assume O
there O
exists O
an O
H O
- O
level O
category O
hierarchy O
γ O
. O
All O
possible O
labels O
in O
γ O
are O
denoted O
by O

C O
= O
{ O
C O
1 O
, O
C O
2 O
, O
• O
• O
• O
, O
C O
H O
} O
, O
where O
C O
i O
= O
{ O
c O
1 O
, O
c O
2 O
, O
• O
• O
• O
} O
∈ O
{ O
0 O
, O
1 O
} O
|C O
i O
| O

is O
the O
label O
set O
in O
the O
i O
- O
th O
level O
, O
and O
|C O
i O
| O
is O
the O
total O
number O
of O
labels O
in O
C O
i O
. O
Consequently O
, O
the O
total O
number O
of O
labels O
in O
C O
is O
K O
= O
H O
i=1 O
|C O
i O
| O
. O
The O
category O
hierarchy O
γ O
is O
then O
defined O
to O
be O
a O
partially O
order O
set O
( O
C O
, O
≺ O
) O
, O
where O
≺ O
represents O
the O
superior O
- O
subordinate O
relationship O
between O
labels O
and O
it O
satisfies O
the O
following O
three O
properties O
: O

• O
asymmetry O
: O
∀c O
x O
∈ O
C O
i O
and O
c O
y O
∈ O
C O
j O
, O
if O
c O
x O
≺ O
c O
y O
, O
then O
we O
have O
c O
y O
⊀ O
c O
x O
. O

• O
anti O
- O
reflexivity O
: O
∀c O
x O
∈ O
C O
i O
, O
we O
have O
c O
x O
⊀ O
c O
x O
. O

• O
transitivity O
: O

∀c O
x O
∈ O
C O
i O
, O
c O
y O
∈ O
C O
j O
and O
c O
z O
∈ O
C O
k O
, O
if O
c O
x O
≺ O
c O
y O
and O
c O
y O
≺ O
c O
z O
, O
then O
we O
have O
c O
x O
≺ O
c O
z O
. O

Definition O
2 O
. O

( O
Hierarchical B-TaskName
Text I-TaskName
Classification I-TaskName
, O
HTC B-TaskName
) O
Given O
a O
category O
hierarchy O
γ O
= O
( O
C O
, O
≺ O
) O
, O
assume O
there O
exist O
a O
total O
number O
of O
M O
documents O
denoted O
by O

D O
= O
{ O
( O
D O
1 O
, O
L O
1 O
) O
, O
( O
D O
2 O
, O
L O
2 O
) O
, O
• O
• O
• O
, O
( O
D O
M O
, O
L O
M O
) O
} O
. O
Here O
D O
d O

denotes O
the O
dth O
text O
document O
, O
which O
is O
typically O
a O
sequence O
of O
words O
, O
i.e. O
, O
D O
d O
= O
{ O
w O
d1 O
, O
w O
d2 O
, O
• O
• O
• O
, O
w O
dN O
d O
} O
, O
where O
N O
d O
is O
the O
total O
number O
of O
words O
in O
document O
D O
d O
. O
Define O
L O
d O
= O
{ O
l O
d1 O
, O
l O
d2 O
, O
• O
• O
• O
, O
l O
dH O
} O
to O
be O
the O
label O
set O
of O
the O
dth O
document O
with O
the O
i O
- O
th O
level O
label O
set O
l O
di O
⊂ O
C O
i O
. O
Then O
the O
goal O
of O
HTC B-TaskName
is O
to O
train O
a O
classification O
model O
Ω O
based O
on O
γ O
and O
D. O
Specifically O
, O
for O
an O
arbitrary O
text O
document O
D O
* O
, O
we O
can O
predict O
its O
label O
set O
L O
* O
through O
the O
classification O
model O
, O
i.e. O
, O

Ω O
( O
D O
* O
, O
γ O
, O
Θ O
) O
→ O
L O
* O
, O

where O
Θ O
is O
the O
parameters O
in O
the O
model O
Ω O
. O

Methodology O

In O
this O
section O
, O
we O
introduce O
the O
PeerHTC B-MethodName
method O
in O
detail O
. O
We O
first O
introduce O
peer O
label O
learning O
and O
sample O
importance O
learning O
, O
and O
then O
propose O
a O
two O
- O
stage O
training O
procedure O
. O
The O
overall O
architecture O
of O
PeerHTC B-MethodName
is O
illustrated O
in O
Figure O
1 O
. O

Multi O
- O
label O
Attention O

Word O
Representations O

Label O
- O
aware O
Text O
Features O

Structure O
Encoder O

Peer O
Label O
Learning O

Text O
Encoder O

Word O
Representations O

Label O
Embeddings O

Peer O
Label O
Learning O

Latent O
relevancy O
encoding O
by O
GCN O

As O
we O
mentioned O
before O
, O
there O
exist O
latent O
relationships O
among O
peer O
labels O
, O
which O
are O
not O
explicitly O
expressed O
in O
the O
category O
hierarchy O
. O
Incorporating O
the O
latent O
relevancy O
structure O
among O
peer O
labels O
could O
also O
benefit O
HTC B-TaskName
. O
Motivated O
by O
this O
idea O
, O
we O
consider O
learning O
label O
relevancy O
from O
two O
perspectives O
. O
First O
, O
we O
follow O
the O
practice O
of O
HiAGM B-MethodName
( O
Zhou O
et O
al O
. O
, O
2020 O
) O
to O
learn O
the O
parent O
- O
child O
relationships O
between O
labels O
in O
adjacent O
levels O
. O
Second O
, O
we O
utilize O
GCN O
to O
incorporate O
the O
latent O
relevancy O
among O
peer O
labels O
into O
the O
label O
encoder O
. O

We O
encode O
labels O
and O
texts O
separately O
in O
Peer B-MethodName
- I-MethodName
HTC I-MethodName
. O
For O
labels O
, O
let O
V O
∈ O
R O
dv×K O
denote O
the O
initial O
label O
embeddings O
, O
where O
d O
v O
is O
the O
embedding O
dimension O
and O
K O
is O
the O
total O
number O
of O
labels O
. O
Then O
following O
HiAGM B-MethodName
( O
Zhou O
et O
al O
. O
, O
2020 O
) O
, O
we O
feed O
the O
initial O
embeddings O
V O
into O
a O
TreeLSTM O
encoder O
to O
learn O
the O
hierarchy O
- O
aware O
embeddings O
H O
↕ O
, O
where O
the O
symbol O
" O
↕ O
" O
stands O
for O
the O
parent O
- O
child O
relationships O
. O
Actually O
, O
H O
↕ O
is O
the O
concatenation O
of O
two O
sets O
of O
embeddings O
derived O
in O
top O
- O
down O
and O
bottom O
- O
up O
fashions O
, O
i.e. O
, O
H O
↕ O
= O
H O
↑ O
⊕H O
↓ O
; O
please O
refer O
to O
Zhou O
et O
al O
. O
( O
2020 O
) O
for O
more O
details O
. O

To O
characterize O
the O
latent O
relationships O
of O
peer O
labels O
, O
we O
use O
H O
↔ O
= O
GCN O
( O
V O
) O
, O
which O
is O
derived O
from O
latent O
label O
connections O
enabled O
by O
GCN O
. O
We O
refer O
to O
H O
↔ O
as O
peer O
- O
aware O
embeddings O
in O
the O
subsequent O
analysis O
. O
To O
fully O
explore O
the O
latent O
relevancy O
of O
peer O
labels O
, O
we O
propose O
GCN O
methods O
using O
two O
strategies O
. O
The O
first O
one O
is O
level O
- O
wise O
GCN O
, O
which O
only O
incorporates O
connections O
of O
labels O
in O
the O
same O
level O
. O
Specifically O
, O
define O
A O
to O
be O
the O
adjacent O
matrix O
that O
tells O
how O
labels O
should O
be O
connected O
. O
Define O
W O
and O
b O
to O
represent O
the O
weight O
matrix O
and O
bias O
term O
, O
which O
are O
both O
trainable O
. O
Let O
σ O
( O
• O
) O
denote O
ReLU O
non O
- O
linear O
activation O
function O
. O
Then O
in O
level O
- O
wise O
GCN O
, O
we O
first O
compute O
H O
↔ O
( O
h O
) O
for O
levels O
1 O
≤ O
h O
≤ O
H O
, O
and O
then O
concatenate O
them O
together O
. O
The O
detailed O
computation O
is O
shown O
below O
. O

H O
↔ O
( O
h O
) O
= O
σ O
( O
A O
( O
h O
) O
V O
⊤ O
( O
h O
) O
W O
( O
h O
) O
+ O
b O
( O
h O
) O
) O
⊤ O
, O
H O
↔ O
= O
concat O
{ O
H O
↔ O
( O
1 O
) O
, O
H O
↔ O
( O
2 O
) O
, O
• O
• O
• O
, O
H O
↔ O
( O
H O
) O
} O
. O
( O
1 O
) O

The O
second O
method O
is O
to O
use O
whole O
- O
hierarchy O
GCN O
, O
which O
is O
a O
single O
GCN O
for O
labels O
in O
the O
whole O
hierarchy O
. O
This O
strategy O
allows O
for O
label O
connections O
throughout O
the O
whole O
hierarchy O
. O
The O
peeraware O
embeddings O
are O
then O
computed O
as O
follows O
: O

H O
↔ O
= O
σ O
( O
AV O
⊤ O
W O
+ O
b O
) O
⊤ O
. O

( O
2 O
) O

After O
computing O
the O
hierarchy O
- O
aware O
embeddings O
H O
↕ O
and O
peer O
- O
aware O
embeddings O
H O
↔ O
, O
we O
concatenate O
them O
together O
. O
Specifically O
, O
we O
have O
H O
↔ O
concatenated O
column O
- O
wise O
with O
H O
↕ O
, O
the O
result O
of O
which O
is O
then O
put O
through O
a O
non O
- O
linear O
projection O
. O
This O
leads O
to O
the O
final O
label O
embeddings O
H O
* O
, O
which O
is O
computed O
as O
follows O
: O

H O
* O
= O
σ O
{ O
W O
* O
• O
( O
H O
↕ O
⊕ O
H O
↔ O
) O
} O
, O

Initialization O
of O
the O
adjacent O
matrix O
A O

By O
using O
GCN O
as O
the O
latent O
relevancy O
encoder O
to O
model O
peer O
labels O
, O
we O
need O
to O
specify O
the O
adjacent O
matrix O
A O
in O
advance O
, O
i.e. O
, O
to O
tell O
how O
labels O
should O
be O
associated O
with O
graph O
edges O
. O
To O
this O
end O
, O
we O
propose O
a O
data O
- O
driven O
approach O
to O
initialize O
A. O
Inspired O
by O
the O
idea O
of O
knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
) O
, O
the O
estimated O
label O
probabilities O
( O
also O
called O
soft O
labels O
in O
knowledge O
distillation O
) O
contain O
extra O
knowledge O
on O
the O
relationships O
among O
different O
labels O
. O
Therefore O
, O
the O
estimated O
label O
probabilities O
can O
be O
regarded O
as O
a O
good O
source O
to O
learn O
the O
latent O
relevancy O
among O
peer O
labels O
. O
Specifically O
, O
if O
two O
labels O
are O
closely O
related O
with O
each O
other O
, O
their O
estimated O
label O
probabilities O
should O
tend O
to O
be O
correlated O
on O
the O
same O
sample O
. O
Therefore O
, O
a O
similarity O
measure O
between O
the O
estimated O
probabilities O
of O
two O
labels O
can O
reflect O
how O
closely O
they O
are O
related O
. O
Besides O
, O
recall O
that O
we O
have O
computed O
the O
label O
embeddings O
H O
* O
, O
the O
similarity O
among O
which O
could O
also O
reflect O
label O
relevancy O
. O

Based O
on O
the O
above O
considerations O
, O
we O
propose O
two O
methods O
for O
initializing O
A. O
In O
the O
first O
method O
, O
we O
adopt O
the O
non O
- O
parametric O
Spearman O
Rank O
Correlation O
Coefficient O
( O
SRCC O
) O
to O
measure O
the O
similarity O
between O
estimated O
probabilities O
. O
Let O
p O
dk O
be O
the O
estimated O
probability O
of O
the O
dth O
document O
associated O
with O
the O
kth O
label O
. O
Recall O
there O
are O
a O
total O
of O
M O
documents O
. O
Hence O
we O
can O
compute O
the O
rank O
of O
p O
dk O
among O
the O
estimated O
probabilities O
of O
the O
M O
documents O
( O
i.e. O
, O
p O
1k O
, O
... O
, O
p O
M O
k O
) O
, O
which O
is O
denoted O
by O
r O
dk O
. O
We O
then O
computer O
k O
, O
which O
is O
the O
average O
of O
r O
dk O
among O
M O
documents O
. O
Then O
, O
we O
can O
compute O
absolute O
SRCC O
for O
any O
two O
labels O
k O
and O
j O
as O
follows O

ρ O
rank O
kj O
= O
M O
d=1 O
( O
r O
dk O
−r O
k O
) O
( O
r O
dj O
−r O
j O
) O
M O
d=1 O
( O
r O
dk O
−r O
k O
) O
2 O
M O
d=1 O
( O
r O
dj O
−r O
j O
) O
2 O
. O

( O
3 O
) O
The O
SRCC O
measure O
can O
be O
computed O
on O
either O
training O
samples O
or O
test O
samples O
, O
since O
it O
does O
not O
require O
true O
labels O
. O

In O
the O
second O
method O
, O
we O
measure O
label O
relevancy O
based O
on O
label O
embeddings O
. O
Specifically O
, O
let O
h O
k O
and O
h O
j O
be O
the O
embeddings O
for O
labels O
k O
and O
j O
, O
which O
are O
extracted O
from O
H O
* O
. O
Then O
we O
can O
use O
the O
absolute O
cosine O
similarity O
between O
them O
to O
measure O
their O
relevancy O
, O
i.e. O
, O

ρ O
emb O
kj O
= O
h O
⊤ O
k O
h O
j O
∥h O
k O
∥ O
• O
∥h O
j O
∥ O
. O
( O
4 O
) O

After O
getting O
the O
similarity O
measures O
( O
ρ O
rank O
kj O
or O
ρ O
emb O
kj O
) O
, O
they O
are O
aligned O
into O
a O
matrix O
and O
then O
normalized O
row O
- O
wise O
( O
except O
for O
the O
diagonal O
entries O
so O
they O
remain O
to O
be O
one O
) O
. O
This O
leads O
to O
two O
matrices O
A O
rank O
and O
A O
emb O
, O
which O
would O
then O
be O
used O
in O
the O
initialization O
of O
GCN O
. O
We O
empirically O
compare O
the O
performance O
of O
different O
initialization O
methods O
of O
A O
; O
see O
Section O
5.2.3 O
for O
the O
detailed O
results O
. O

Multi O
- O
label O
attention O

We O
adopt O
the O
multi O
- O
label O
attention O
mechanism O
to O
extract O
label O
- O
wise O
text O
features O
( O
Huang O
et O
al O
. O
, O
2019 O
; O
Zhou O
et O
al O
. O
, O
2020 O
; O
Deng O
et O
al O
. O
, O
2021 O
) O
. O
For O
the O
dth O
document O
with O
N O
d O
words O
, O
let O
s O
d O
= O
{ O
s O
d1 O
, O
... O
s O
dN O
d O
} O
denote O
the O
word O
representations O
derived O
from O
a O
text O
encoder O
. O
Recall O
that O
h O
k O
is O
the O
embedding O
of O
label O
k O
, O
which O
is O
extracted O
from O
H O
* O
. O
Then O
within O
the O
dth O
document O
, O
we O
can O
compute O
the O
attention O
score O
α O
( O
d O
) O
kn O
between O
the O
representation O
of O
the O
nth O
word O
and O
the O
embedding O
of O
label O
k O
, O
i.e. O
, O

α O
( O
d O
) O
kn O
= O
exp O
{ O
s O
⊤ O
dn O
h O
k O
} O
N O
d O
g=1 O
exp O
{ O
s O
⊤ O
dg O
h O
k O
} O
. O
The O
value O
α O
( O
d O
) O

kn O
indicates O
how O
informative O
the O
nth O
word O
is O
to O
a O
certain O
label O
k O
within O
one O
document O
. O

Note O
that O
in O
PeerHTC B-MethodName
, O
label O
embeddings O
have O
now O
included O
two O
parts O
of O
information O
, O
i.e. O
, O
the O
hierarchical O
relationship O
between O
parent O
and O
child O
labels O
and O
the O
latent O
relationship O
between O
peer O
labels O
. O
Hence O
, O
the O
attention O
score O
α O
( O
d O
) O
kn O
is O
also O
equipped O
with O
the O
ability O
to O
identify O
text O
features O
favoring O
labels O
closely O
related O
to O
label O
k. O
This O
leads O
to O
reinforced O
feature O
sharing O
. O
Finally O
, O
we O
calculate O
a O
weighted O
average O
u O

( O
d O
) O
k O
= O
N O
d O
g=1 O
α O
( O
d O
) O

kn O
s O
dg O
for O
label O
k. O
These O
features O
are O
then O
flattened O
and O
fed O
into O
a O
fully O
- O
connected O
network O
for O
final O
classification O
. O

Sample O
Importance O
Learning O

A O
metric O
for O
label O
confusion O

Assisted O
by O
GCN O
to O
model O
the O
latent O
relevancy O
of O
peer O
labels O
, O
we O
achieve O
reinforced O
feature O
sharing O
that O
would O
enhance O
the O
classification O
of O
one O
category O
with O
the O
help O
of O
text O
features O
extracted O
by O
other O
closely O
related O
categories O
. O
However O
, O
a O
side O
effect O
emerges O
when O
we O
strengthen O
the O
similarity O
between O
the O
embeddings O
of O
peer O
labels O
by O
GCN O
. O
That O
is O
, O
it O
would O
make O
easily O
confused O
labels O
become O
even O
less O
distinguishable O
. O
To O
characterize O
this O
phenomenon O
, O
we O
first O
formalize O
a O
new O
concept O
called O
" O
label O
confusion O
" O
. O
Specifically O
, O
we O
say O
there O
is O
confusion O
between O
two O
labels O
k O
and O
j O
, O
when O
one O
document O
belongs O
to O
label O
k O
but O
gets O
a O
high O
probability O
in O
another O
label O
j O
, O
or O
the O
other O
way O
around O
. O
Take O
two O
book O
categories O
named O
" O
Classics O
" O
and O
" O
Poetry O
" O
for O
example O
. O
They O
are O
prone O
to O
confusion O
since O
both O
of O
them O
involve O
some O
genteel O
expression O
. O
More O
intuitively O
, O
label O
confusion O
is O
pretty O
much O
like O
the O
case O
where O
a O
person O
gets O
confused O
when O
distinguishing O
between O
very O
similar O
objects O
. O

To O
tackle O
this O
potential O
side O
effect O
, O
we O
first O
propose O
a O
metric O
to O
evaluate O
how O
easily O
any O
two O
labels O
can O
be O
confused O
. O
Let O
L O
( O
d O
) O
denote O
the O
true O
label O
set O
of O
the O
dth O
document O
. O
Assume O
we O
have O
label O
k O
∈ O
L O
( O
d O
) O
but O
label O
j O
/ O
∈ O
L O
( O
d O
) O
. O
Then O
the O
estimated O
probability O
of O
label O
j O
measures O
the O
confusion O
between O
these O
two O
labels O
on O
this O
document O
. O
To O
formulate O
this O
idea O
mathematically O
, O
let O
p O
dk O
be O
the O
estimated O
probability O
that O
the O
dth O
document O
belongs O
to O
label O
k. O
Let O
c O
kj O
denote O
the O
degree O
of O
confusion O
between O
labels O
k O
and O
j. O
Denote O
the O
index O
set O

D O
kj O
= O
{ O
d O
: O
1 O
≤ O
d O
≤ O
M O
, O
k O
∈ O
L O
( O
d O
) O
, O
j O
/ O
∈ O
L O
( O
d O
) O
} O
. O

Then O
we O
can O
compute O
c O
kj O
as O
follows O
, O

c O
kj O
= O
1 O
|D O
kj O
| O
d∈D O
kj O
p O
dj O
. O

( O
5 O
) O

Training O
with O
sample O
weighting O

A O
document O
sample O
is O
said O
to O
be O
important O
in O
distinguishing O
labels O
k O
from O
j O
, O
if O
its O
label O
set O
contains O
k O
but O
not O
j. O
With O
the O
metric O
of O
label O
confusion O
c O
kj O
, O
we O
can O
evaluate O
the O
importance O
of O
each O
training O
sample O
. O
Specifically O
, O
define O
β O
dk O
to O
be O
the O
importance O
of O
the O
dth O
document O
with O
respect O
to O
a O
label O
k O
. O

Then O
in O
the O
case O
k O
/ O
∈ O
L O
( O
d O
) O
, O
we O
set O
β O
dk O
= O
1 O
. O
In O
the O
case O
k O
∈ O
L O
( O
d O
) O
, O
we O
specify O
β O
dk O
as O
follows O
: O

β O
dk O
= O
1 O
+ O
j O
/ O
∈L O
( O
d O
) O
{ O
exp O
( O
τ B-HyperparameterName
c O
kj O
) O
− O
1 O
} O
, O
( O
6 O

where O
τ B-HyperparameterName
is O
a O
temperature B-HyperparameterName
hyperparameter O
controlling O
how O
radical O
we O
are O
in O
assigning O
sample O
weights O
. O
We O
then O
plug O
β O
dk O
into O
the O
binary O
cross O
entropy O
loss O
( O
BCE O
) O
function O
, O
which O
is O
popularly O
used O
in O
HTC B-TaskName
( O
Nam O
et O
al O
. O
, O
2014 O
) O
, O
i.e. O
, O

L O
= O
− O
d∈D O
k∈C O
β O
dk O
{ O
y O
k O
log O
( O
p O
dk O
) O
+ O
( O
1 O
− O
y O
k O
) O
log O
( O
1 O
− O
p O
dk O
) O
} O
, O

where O
y O
k O
is O
either O
1 O
or O
0 O
depending O
on O
whether O
k O
∈ O
L O
( O
d O
) O
or O
not O
. O

A O
Two O
- O
Stage O
Training O
Approach O

As O
mentioned O
in O
Section O
4.1 O
and O
Section O
4.2 O
, O
we O
use O
a O
data O
- O
driven O
approach O
to O
identify O
the O
adjacent O
matrix O
A O
and O
sample O
importance O
weights O
β O
dk O
's O
, O
which O
all O
rely O
on O
the O
estimated O
label O
probabilities O
and O
label O
embeddings O
. O
To O
obtain O
the O
adjacent O
matrix O
and O
sample O
importance O
weights O
, O
we O
develop O
a O
two O
- O
stage O
training O
approach O
. O
The O
first O
round O
is O
a O
warm O
- O
up O
training O
stage O
. O
We O
randomly O
initialize O
the O
adjacent O
matrix O
A O
in O
GCN O
, O
and O
assign O
equal O
weights O
to O
all O
training O
samples O
. O
Then O
we O
train O
the O
PeerHTC B-MethodName
model O
for O
the O
first O
time O
. O
The O
obtained O
estimated O
label O
probabilities O
and O
label O
embeddings O
from O
the O
warm O
- O
up O
training O
are O
then O
used O
to O
compute O
the O
adjacent O
matrix O
and O
sample O
importance O
weights O
. O
Then O
we O
re O
- O
train O
the O
PeerHTC B-MethodName
model O
for O
the O
second O
time O
with O
the O
updated O
adjacent O
matrices O
and O
sample O
weights O
. O
This O
leads O
to O
the O
final O
classification O
model O
. O
The O
whole O
procedure O
is O
illustrated O
in O
Algorithm O
1 O
. O

Experiments O

Experimental O
Setup O

Datasets O
. O
We O
use O
three O
datasets O
to O
explore O
the O
classification O
performance O
. O
The O
first O
one O
is O
Webof B-DatasetName
- I-DatasetName
Science I-DatasetName
( O
WOS B-DatasetName
) O
dataset O
( O
Kowsari O
et O
al O
. O
, O
2017 O
) O
, O
which O
consists O
of O
abstracts O
of O
published O
papers O
from O
journals O
in O
Web O
of O
Science O
. O
The O
disciplines O
that O
each O
paper O
belongs O
to O
are O
regarded O
as O
the O
classification O
labels O
. O
The O
second O
dataset O
is O
BlurbGenreCollection B-DatasetName
( O
BGC B-DatasetName
) O
( O
Aly O
et O
al O
. O
, O
2019 O
) O
, O
which O
consists O
of O
advertising O
descriptions O
of O
books O
. O
The O
genres O
of O
books O
are O
regarded O
as O
classification O
labels O
, O
while O
the O
advertising O
descriptions O
are O
regarded O
as O
text O
documents O
. O
The O
last O
dataset O
consists O
of O
the O
textual O
names O
of O
retailing O
products O
( O
we O
refer O
to O
as O
Goods B-DatasetName
) O
, O
which O
are O
collected O
by O
ourselves O
from O
a O
Chinese O
retailing O
company O
. O
In O
this O
dataset O
, O
each O
product O
belongs O
to O
a O
three O
- O
level O
product O
hierarchy O
. O
Among O
the O
three O
datasets O
, O
WOS B-DatasetName
and O
Goods B-DatasetName
are O
both O
for O
singlepath O
HTC B-TaskName
, O
i.e. O
, O
each O
sample O
only O
has O
one O
single O
label O
in O
each O
level O
, O
whereas O
samples O
in O
BGC B-DatasetName
have O
multi O
- O
path O
labels O
, O
i.e. O
, O
each O
sample O
is O
allowed O
to O
have O
multiple O
labels O
in O
the O
same O
level O
. O
Each O
dataset O
is O
randomly O
split O
into O
the O
training O
set O
( O
70 B-HyperparameterValue
% I-HyperparameterValue
) O
, O
validation O
set O
( O
15 B-HyperparameterValue
% I-HyperparameterValue
) O
and O
test O
set O
( O
15 B-HyperparameterValue
% I-HyperparameterValue
) O
. O
The O
descriptive O
statistics O
of O
the O
three O
datasets O
are O
listed O
in O
Table O
1 O
. O

In O
addition O
, O
the O
intended O
use O
of O
public O
datasets O
and O
pre O
- O
trained O
models O
, O
as O
specified O
in O
their O
license O
or O
terms O
, O
was O
strictly O
obeyed O
in O
our O
work O
. O
Baselines O
. O
In O
order O
to O
demonstrate O
the O
effectiveness O
of O
PeerHTC B-MethodName
, O
we O
compare O
it O
with O
three O
naive O
approaches O
that O
treat O
HTC B-TaskName
as O
a O
simple O
multi O
- O
label O
classification O
problem O
, O
along O
with O
four O
state O
- O
of O
- O
theart O
HTC B-TaskName
models O
. O
The O
three O
naive O
approaches O
are O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
Tex B-MethodName
- I-MethodName
tRCNN I-MethodName
( O
Lai O
et O
al O
. O
, O
2015 O
) O
, O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O
The O
four O
state O
- O
of O
- O
the O
- O
art O
HTC O
models O
are O
briefly O
introduced O
as O
follows O
. O

( O
1 O
) O
HMCN B-MethodName
( O
Wehrmann O
et O
al O
. O
, O
2018 O
) O
. O
It O
is O
probably O
the O
first O
hybrid O
approach O
that O
combines O
a O
sequence O
of O
local O
classifiers O
with O
global O
optimization O
. O

( O
2 O
) O
HARNN B-MethodName
( O
Huang O
et O
al O
. O
, O
2019 O
) O
. O
It O
is O
also O
a O
hybrid O
approach O
, O
but O
utilizes O
attention O
mechanism O
and O
refines O
how O
information O
flows O
between O
levels O
. O

( O
3 O
) O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
( O
Zhou O
et O
al O
. O
, O
2020 O
) O
. O
It O
encodes O
labels O
and O
documents O
separately O
, O
and O
utilizes O
multilabel O
attention O
mechanism O
to O
extract O
hierarchy O
- O
aware O
text O
features O
. O

( O
4 O
) O
HTCInfomax B-MethodName
( O
Deng O
et O
al O
. O
, O
2021 O
) O
. O
It O
is O
basically O
an O
improvement O
on O
top O
of O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
via O
mutual O
information O
maximization O
. O

Evaluation O
metrics O
. O
To O
measure O
the O
classification O
performance O
, O
we O
apply O
two O
standard O
evaluation O
metrics O
, O
i.e. O
, O
the O
Micro B-MetricName
- I-MetricName
F1 I-MetricName
and O
Macro B-MetricName
- I-MetricName
F1 I-MetricName
( O
Gopal O
and O
Yang O
, O
2013 O
) O
. O
Micro B-MetricName
- I-MetricName
F1 I-MetricName
computes O
the O
overall O
precision O
and O
recall O
of O
all O
the O
labels O
, O
while O
Macro B-MetricName
- I-MetricName
F1 I-MetricName
computes O
the O
average O
of O
F1 O
scores O
of O
all O
labels O
. O
As O
a O
result O
, O
Micro B-MetricName
- I-MetricName
F1 I-MetricName
assigns O
greater O
weights O
to O
more O
frequent O
labels O
, O
while O
Macro B-MetricName
- I-MetricName
F1 I-MetricName
treats O
all O
the O
labels O
equally O
. O

Implementation O
details O
. O
We O
use O
BERT O
as O
the O
text O
encoder O
in O
PeerHTC B-MethodName
, O
and O
set O
the O
dimension B-HyperparameterName
of I-HyperparameterName
label I-HyperparameterName
embeddings I-HyperparameterName
as O
256 B-HyperparameterValue
. O
The O
BERT O
models O
are O
pretrained O
on O
" O
book_corpus_wiki_en_uncased O
" O
and O
" O
wiki_cn_cased O
" O
for O
English O
and O
Chinese O
datasets O
respectively O
, O
both O
of O
which O
have O
12 O
hidden O
layers O
and O
768 O
hidden O
units O
. O
The O
vocabulary O
is O
created O
with O
words O
that O
appear O
no O
less O
than O
5 O
times O
. O
We O
set O
the O
maximum B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
token I-HyperparameterName
inputs I-HyperparameterName
as O
100 B-HyperparameterValue
. O
The O
threshold B-HyperparameterName
for I-HyperparameterName
tagging I-HyperparameterName
is O
chosen O
to O
be O
0.5 B-HyperparameterValue
. O
Model O
parameters O
are O
initialized O
according O
to O
the O
Xavier O
uniform O
( O
Glorot O
and O
Bengio O
, O
2010 O
) O
when O
random O
initialization O
is O
needed O
. O
We O
use O
the O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
momentum O
parameters O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
, O
a O
learning B-HyperparameterName
rate I-HyperparameterName
α B-HyperparameterName
= O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
and O
a O
mini B-HyperparameterName
- I-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
. O
To O
prevent O
overfitting O
, O
we O
also O
use O
dropout B-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
with O
the O
rate B-HyperparameterName
of O
0.1 B-HyperparameterValue
, O
and O
weight B-HyperparameterName
decay I-HyperparameterName
( O
Loshchilov O
and O
Hutter O
, O
2017 O
) O
with O
the O
tuning O
weight O
equal O
to O
1×10 B-HyperparameterValue
−7 I-HyperparameterValue
. O
The O
parameter O
τ B-HyperparameterName
in O
equation O
( O
6 O
) O
is O
set O
as O
1.2 B-HyperparameterValue
. O

For O
HTC B-TaskName
competitors O
, O
the O
same O
parameter O
settings O
are O
adopted O
. O
We O
follow O
their O
original O
practices O
to O
use O
simple O
text O
encoders O
, O
but O
also O
replace O
them O
with O
BERT O
for O
a O
fair O
comparison O
. O
Specifically O
, O
in O
HARNN B-MethodName
, O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
and O
HTCInfoMax B-MethodName
, O
we O
use O
a O
single O
- O
layer O
bidirectional O
LSTM O
as O
the O
text O
encoder O
; O
in O
HMCN B-MethodName
, O
we O
use O
three O
parallel O
CNN O
layers O
with O
filter O
sizes O
{ O
3 O
, O
4 O
, O
5 O
} O
and O
numbers O
of O
channel O
{ O
100 O
, O
70 O
, O
70 O
} O
as O
the O
text O
encoder O
. O
For O
simple O
text O
encoders O
, O
300 O
- O
dimensional O
pretrained O
word O
embeddings O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
Fasttext O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
are O
used O
on O
English O
and O
Chinese O
datasets O
respectively O
. O
Our O
hyperparameters O
are O
tuned O
on O
the O
validation O
set O
, O
taking O
both O
classification O
performance O
and O
the O
computation O
resources O
available O
into O
consideration O
, O
and O
the O
classification O
performances O
reported O
in O
our O
exper O
- O
imental O
results O
are O
evaluated O
on O
the O
test O
set O
. O
Our O
models O
are O
trained O
on O
two O
Tesla O
P100 O
GPUs O
. O

Experimental O
Results O

Evidence O
of O
peer O
labels O

In O
order O
to O
demonstrate O
the O
existence O
of O
peer O
labels O
, O
we O
take O
the O
BGC B-DatasetName
and O
WOS B-DatasetName
datasets O
as O
examples O
and O
show O
the O
adjacent O
matrices O
of O
their O
first O
- O
level O
labels O
, O
which O
are O
computed O
using O
estimated O
label O
probabilities O
on O
training O
samples O
according O
to O
equation O
( O
3 O
) O
. O
As O
shown O
by O
Figure O
2 O
, O
some O
labels O
have O
higher O
degrees O
of O
relevancy O
with O
others O
, O
which O
can O
serve O
as O
useful O
prior O
knowledge O
for O
classification O
. O
For O
example O
, O
in O
the O
BGC B-DatasetName
dataset O
, O
" O
Classics O
" O
and O
" O
Poetry O
" O
, O
corresponding O
to O
the O
intersection O
of O
the O
third O
row O
and O
fourth O
column O
in O
Figure O
2 O
( O
a O
) O
, O
are O
closely O
related O
. O
In O
the O
WOS B-DatasetName
dataset O
, O
" O
Mechanical O
Aerospace O
Engineering O
( O
MAE O
) O
" O
and O
" O
Civil O
Engineering O
" O
( O
in O
the O
fourth O
row O
and O
fifth O
column O
) O
show O
extremely O
high O
relevancy O
. O
These O
findings O
verify O
the O
existence O
of O
peer O
label O
relevancy O
. O
However O
, O
compared O
with O
BGC B-DatasetName
and O
WOS B-DatasetName
datasets O
, O
the O
latent O
relevancy O
of O
peer O
labels O
is O
relatively O
weak O
for O
the O
Goods B-DatasetName
dataset O
, O
which O
is O
not O
shown O
to O
save O
space O
. O
This O
is O
largely O
due O
to O
the O
fact O
that O
the O
Goods B-DatasetName
dataset O
has O
a O
larger O
number O
of O
categories O
, O
which O
are O
more O
fine O
- O
grained O
and O
thus O
less O
relevant O
with O
each O
another O
. O

Comparison O
results O

To O
explore O
the O
classification O
performance O
of O
Peer B-MethodName
- I-MethodName
HTC I-MethodName
, O
we O
compare O
this O
model O
with O
( O
1 O
) O
naive O
classification O
approaches O
( O
i.e. O
, O
LSTM B-MethodName
, O
TextRCNN B-MethodName
, O
BERT B-MethodName
) O
, O
and O
( O
2 O
) O
state O
- O
of O
- O
the O
- O
art O
HTC B-TaskName
approaches O
( O
i.e. O
, O
HMCN B-MethodName
, O
HARNN B-MethodName
, O
HiAGM B-MethodName
- I-MethodName
LA I-MethodName
, O
HTCInfo B-MethodName
- I-MethodName
Max I-MethodName
) O
. O
For O
the O
HTC B-TaskName
approaches O
, O
we O
also O
replace O
their O
original O
text O
encoders O
with O
BERT O
for O
better O
classification O
performance O
and O
a O
fair O
comparison O
with O
PeerHTC B-MethodName
. O
To O
characterize O
the O
latent O
relevancy O
of O
peer O
labels O
in O
PeerHTC B-MethodName
, O
the O
GCN O
methods O
with O
the O
" O
level O
- O
wise O
" O
and O
" O
whole O
- O
hierarchy O
" O
strategies O
are O
considered O
; O
see O
Equations O
( O
1 O
) O
and O
( O
2 O
) O
for O
details O
. O
To O
decide O
the O
adjacent O
matrix O
A O
in O
GCN O
, O
we O
consider O
three O
methods O
: O
( O
1 O
) O
A B-MethodName
rank I-MethodName
- I-MethodName
train I-MethodName
, O
computed O
by O
the O
estimated O
label O
probabilities O
on O
training O
samples O
; O

( O
2 O
) O
A B-MethodName
rank I-MethodName
- I-MethodName
test I-MethodName
, O
computed O
by O
the O
estimated O
label O
probabilities O
on O
test O
samples O
; O
and O
( O
3 O
) O
A B-MethodName
emb I-MethodName
, O
computed O
by O
label O
embeddings O
. O
This O
results O
in O
a O
total O
of O
2 O
× O
3 O
= O
6 O
choices O
for O
A. O
We O
report O
the O
performances O
of O
all O
these O
different O
choices O
for O
a O
comparison O
. O

Table O
2 O
reports O
the O
classification O
results O
of O
different O
models O
, O
from O
which O
we O
can O
draw O
the O
following O
conclusions O
. O
First O
, O
our O
method O
PeerHTC B-MethodName
has O
achieved O
better O
classification O
performance O
than O
all O
naive O
approaches O
and O
HTC B-TaskName
approaches O
, O
when O
evaluated O
by O
both O
Micro B-MetricName
- I-MetricName
F1 I-MetricName
and O
Macro B-MetricName
- I-MetricName
F1 I-MetricName
in O
the O
three O
datasets O
. O
Second O
, O
when O
replacing O
the O
traditional O
text O
encoders O
in O
HTC B-TaskName
approaches O
by O
BERT O
, O
their O
classification O
performances O
have O
been O
largely O
improved O
. O
This O
reveals O
the O
great O
potential O
of O
pretrained O
models O
in O
HTC B-TaskName
problems O
. O
Even O
so O
, O
our O
method O
PeerHTC B-MethodName
still O
outperforms O
HTC B-TaskName
approaches O
with O
BERT O
on O
the O
BGC B-DatasetName
and O
WOS B-DatasetName
datasets O
. O
On O
the O
Goods B-DatasetName
dataset O
, O
however O
, O
the O
method O
HTCInfoMax B-MethodName
achieves O
the O
best O
performance O
while O
PeerHTC B-MethodName
ranks O
second O
with O
comparable O
results O
. O
We O
believe O
that O
this O
slightly O
poorer O
performance O
of O
PeerHTC B-MethodName
mainly O
results O
from O
the O
weak O
latent O
relevancy O
among O
peer O
labels O
, O
as O
remarked O
in O
Section O
5.2.1 O
. O

Ablation O
study O

To O
further O
demonstrate O
the O
advantages O
of O
incorporating O
peer O
labels O
and O
using O
sample O
weights O
, O
we O
conduct O
the O
following O
ablation O
study O
. O
Specifically O
, O
we O
compare O
the O
following O
three O
models O
. O
The O
first O
one O
, O
denoted O
by O
" O
NA B-MethodName
" O
, O
is O
a O
naive O
HTC B-TaskName
model O
without O
considering O
peer O
labels O
or O
sample O
weights O
. O
The O
second O
one O
, O
denoted O
by O
" O
OPL B-MethodName
" O
, O
is O
a O
variant O
of O
Peer B-MethodName
- I-MethodName
HTC I-MethodName
that O
only O
considers O
the O
latent O
relevancy O
among O
peer O
labels O
, O
but O
does O
not O
utilize O
the O
sample O
weights O
. O
The O
last O
one O
is O
our O
proposed O
PeerHTC B-MethodName
, O
which O
considers O
both O
peer O
labels O
and O
sample O
weights O
. O
In O
addition O
, O
to O
explore O
the O
performances O
of O
using O
different O
adjacent O
matrices O
A O
, O
we O
report O
OPL B-MethodName
with O
six O
different O
adjacent O
matrices O
, O
as O
described O
in O
Section O
5 O
. O
We O
then O
compare O
the O
performances O
of O
using O
different O
adjacent O
matrices O
in O
OPL B-MethodName
. O
As O
shown O
by O
Table O
3 O
, O
on O
the O
dataset O
BGC B-DatasetName
, O
the O
" O
whole O
- O
hierarchy O
" O
strategy O
works O
better O
in O
most O
cases O
. O
On O
the O
dataset O
WOS B-DatasetName
, O
the O
" O
level O
- O
wise O
" O
strategy O
generally O
works O
better O
. O
On O
the O
Goods B-DatasetName
dataset O
, O
the O
performances O
of O
" O
level O
wise O
" O
and O
" O
whole O
hierarchy O
" O
are O
rather O
comparable O
. O
When O
it O
comes O
to O
adjacent O
matrices O
computed O
using O
either O
sample O
probabilities O
or O
label O
embeddings O
, O
there O
is O
no O
obvious O
distinction O
between O
their O
performances O
, O
indicating O
all O
these O
methods O
can O
be O
helpful O
in O
revealing O
latent O
relationships O
among O
peer O
labels O
. O

Finally O
, O
we O
focus O
on O
the O
effect O
of O
using O
sample O
weights O
. O
As O
we O
mentioned O
in O
Section O
4.2 O
, O
characterizing O
the O
latent O
relevancy O
of O
peer O
labels O
would O
create O
shortcuts O
between O
labels O
and O
may O
have O
potential O
side O
effect O
of O
label O
confusion O
. O
To O
cope O
with O
this O
problem O
, O
we O
first O
measure O
the O
degree O
of O
label O
confusion O
, O
then O
identify O
the O
importance O
of O
different O
training O
samples O
in O
alleviating O
label O
confusion O
, O
and O
finally O
plug O
these O
weights O
into O
the O
BCE O
loss O
func O
- O
tion O
. O
As O
shown O
by O
Table O
3 O
, O
the O
PeerHTC B-MethodName
method O
can O
improve O
the O
classification O
performance O
consistently O
on O
the O
three O
datasets O
, O
when O
compared O
with O
the O
OPL B-MethodName
method O
. O
These O
results O
demonstrate O
the O
usefulness O
of O
sample O
weights O
in O
PeerHTC B-MethodName
. O

Conclusion O

In O
this O
work O
, O
we O
originally O
propose O
the O
concept O
of O
" O
peer O
labels O
" O
to O
characterize O
the O
phenomenon O
that O
labels O
in O
the O
same O
level O
have O
latent O
relevancy O
. O
To O
utilize O
these O
peer O
labels O
to O
enhance O
HTC B-TaskName
, O
we O
develop O
the O
PeerHTC B-MethodName
method O
. O
We O
exploit O
GCN O
as O
an O
encoder O
for O
latent O
relevancy O
among O
peer O
labels O
, O
and O
reinforce O
feature O
sharing O
among O
these O
closely O
related O
peer O
labels O
. O
We O
also O
use O
a O
novel O
technique O
to O
assign O
training O
sample O
weights O
based O
on O
their O
importance O
in O
alleviating O
label O
confusion O
. O
The O
above O
procedures O
are O
embedded O
in O
a O
two O
- O
stage O
training O
approach O
. O
Our O
experimental O
results O
demonstrate O
the O
evidence O
of O
peer O
labels O
in O
real O
datasets O
and O
the O
generally O
better O
performance O
of O
PeerHTC B-MethodName
against O
other O
state O
- O
of O
- O
the O
- O
art O
HTC B-TaskName
methods O
. O
In O
terms O
of O
application O
, O
one O
would O
expect O
a O
higher O
lift O
in O
performance O
from O
PeerHTC B-MethodName
when O
the O
granularity O
of O
categorization O
is O
relatively O
low O
, O
as O
demonstrated O
by O
our O
experimental O
results O
. O
We O
also O
suggest O
that O
one O
should O
carry O
out O
exploratory O
analysis O
or O
take O
into O
account O
domain O
knowledge O
, O
in O
order O
to O
decide O
the O
extent O
of O
peer O
- O
label O
relevancy O
for O
a O
specific O
dataset O
. O

Limitations O

In O
this O
work O
, O
we O
adopt O
a O
data O
- O
driven O
approach O
to O
identifying O
latent O
relevancy O
. O
However O
, O
we O
believe O
that O
external O
knowledge O
such O
as O
knowledge O
graphs O
could O
also O
be O
of O
great O
help O
for O
this O
purpose O
, O
and O
is O
thus O
one O
of O
the O
directions O
of O
our O
future O
work O
. O

Acknowledgements O

Feifei O
Wang O
is O
the O
corresponding O
author O
. O
This O
work O
is O
supported O
by O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No.72001205 O
) O
, O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
and O
the O
Research O
Funds O
of O
Renmin O
University O
of O
China O
( O
21XNA026 O
) O
. O
Besides O
, O
we O
thank O
all O
the O
anonymous O
reviewers O
for O
their O
valuable O
suggestions O
. O

ACL O
2023 O
Responsible O
NLP O
Checklist O

A O
For O
every O
submission O
: O
A1 O
. O
Did O
you O
describe O
the O
limitations O
of O
your O
work O
? O

Limitations O
are O
discussed O
in O
Section O
7 O
. O

A2 O
. O
Did O
you O
discuss O
any O
potential O
risks O
of O
your O
work O
? O

To O
the O
best O
of O
our O
knowledge O
, O
our O
work O
does O
not O
involve O
any O
potential O
risk O
. O

A3 O
. O
Do O
the O
abstract O
and O
introduction O
summarize O
the O
paper O
's O
main O
claims O
? O

The O
abstract O
is O
at O
the O
very O
beginning O
of O
the O
paper O
, O
while O
the O
introduction O
is O
in O
section O
1 O
. O

A4 O
. O
Have O
you O
used O
AI O
writing O
assistants O
when O
working O
on O
this O
paper O
? O

We O
did O
not O
use O
any O
AI O
writing O
assistant O
. O

B O
Did O
you O
use O
or O
create O
scientific O
artifacts O
? O
Some O
scientific O
artifacts O
( O
eg O
. O
datasets O
and O
pre O
- O
trained O
models O
) O
were O
used O
in O
our O
experiments O
in O
Section O
5 O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Section O
5.1 O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Not O
applicable O
. O
Data O
used O
in O
our O
work O
do O
not O
involve O
such O
problems O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Introduction O
to O
the O
datasets O
is O
included O
in O
Section O
5.1 O
. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Yes O
. O
In O
Section O
5 O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Section O
5 O
. O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Our O
computing O
infrastructure O
is O
introduced O
in O
Section O
5 O
. O
The O
number O
of O
parameters O
and O
GPU O
hours O
involved O
in O
our O
experiments O
are O
on O
a O
reasonable O
scale O
and O
we O
believe O
they O
will O
not O
cause O
any O
difficulty O
to O
reproduction O
. O
Thereby O
, O
they O
are O
omitted O
to O
save O
space O
. O
D1 O
. O
Did O
you O
report O
the O
full O
text O
of O
instructions O
given O
to O
participants O
, O
including O
e.g. O
, O
screenshots O
, O
disclaimers O
of O
any O
risks O
to O
participants O
or O
annotators O
, O
etc O
. O
? O
Not O
applicable O
. O
Left O
blank O
. O

D2 O
. O
Did O
you O
report O
information O
about O
how O
you O
recruited O
( O
e.g. O
, O
crowdsourcing O
platform O
, O
students O
) O
and O
paid O
participants O
, O
and O
discuss O
if O
such O
payment O
is O
adequate O
given O
the O
participants O
' O
demographic O
( O
e.g. O
, O
country O
of O
residence O
) O
? O
Not O
applicable O
. O
Left O
blank O
. O

D3 O
. O
Did O
you O
discuss O
whether O
and O
how O
consent O
was O
obtained O
from O
people O
whose O
data O
you O
're O
using O
/ O
curating O
? O
For O
example O
, O
if O
you O
collected O
data O
via O
crowdsourcing O
, O
did O
your O
instructions O
to O
crowdworkers O
explain O
how O
the O
data O
would O
be O
used O
? O
Not O
applicable O
. O
Left O
blank O
. O

D4 O
. O
Was O
the O
data O
collection O
protocol O
approved O
( O
or O
determined O
exempt O
) O
by O
an O
ethics O
review O
board O
? O
Not O
applicable O
. O
Left O
blank O
. O

D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
Not O
applicable O
. O
Left O
blank O
. O

Does O
Joint O
Training O
Really O
Help O
Cascaded O
Speech B-TaskName
Translation I-TaskName
? O

Currently O
, O
in O
speech B-TaskName
translation I-TaskName
, O
the O
straightforward O
approach O
-cascading O
a O
recognition O
system O
with O
a O
translation O
system O
-delivers O
state O
- O
of O
- O
theart O
results O
. O
However O
, O
fundamental O
challenges O
such O
as O
error O
propagation O
from O
the O
automatic B-TaskName
speech I-TaskName
recognition I-TaskName
system O
still O
remain O
. O
To O
mitigate O
these O
problems O
, O
recently O
, O
people O
turn O
their O
attention O
to O
direct O
data O
and O
propose O
various O
joint O
training O
methods O
. O
In O
this O
work O
, O
we O
seek O
to O
answer O
the O
question O
of O
whether O
joint O
training O
really O
helps O
cascaded O
speech B-TaskName
translation I-TaskName
. O
We O
review O
recent O
papers O
on O
the O
topic O
and O
also O
investigate O
a O
joint O
training O
criterion O
by O
marginalizing O
the O
transcription O
posterior O
probabilities O
. O
Our O
findings O
show O
that O
a O
strong O
cascaded O
baseline O
can O
diminish O
any O
improvements O
obtained O
using O
joint O
training O
, O
and O
we O
suggest O
alternatives O
to O
joint O
training O
. O
We O
hope O
this O
work O
can O
serve O
as O
a O
refresher O
of O
the O
current O
speech B-TaskName
translation I-TaskName
landscape O
, O
and O
motivate O
research O
in O
finding O
more O
efficient O
and O
creative O
ways O
to O
utilize O
the O
direct O
data O
for O
speech B-TaskName
translation I-TaskName
. O

Introduction O

Speech B-TaskName
translation I-TaskName
( O
ST B-TaskName
) O
is O
the O
task O
of O
automatic O
translation O
of O
speech O
in O
some O
source O
language O
into O
some O
other O
target O
language O
( O
Stentiford O
and O
Steer O
, O
1988 O
; O
Waibel O
et O
al O
. O
, O
1991 O
) O
. O
Traditionally O
, O
a O
cascaded O
approach O
is O
used O
, O
where O
an O
automatic B-TaskName
speech I-TaskName
recognition I-TaskName
( O
ASR B-TaskName
) O
system O
is O
used O
to O
transcribe O
the O
speech O
, O
followed O
by O
a O
machine B-TaskName
translation I-TaskName
( O
MT B-TaskName
) O
system O
, O
to O
translate O
the O
transcripts O
( O
Sperber O
and O
Paulik O
, O
2020 O
) O
. O
The O
problem O
of O
error O
propagation O
has O
been O
the O
center O
of O
discussion O
in O
ST B-TaskName
literature O
( O
Ney O
, O
1999 O
; O
Casacuberta O
et O
al O
. O
, O
2004 O
; O
Matusov O
et O
al O
. O
, O
2005 O
; O
Peitz O
et O
al O
. O
, O
2012 O
; O
Sperber O
et O
al O
. O
, O
2017 O
) O
, O
and O
instead O
of O
using O
the O
discrete O
symbols O
in O
the O
source O
languages O
, O
ideas O
like O
using O
n O
- O
best O
lists O
, O
lattices O
, O
and O
neural O
network O
hidden O
representations O
are O
investigated O
( O
Saleem O
et O
al O
. O
, O
2004 O
; O
Kano O
et O
al O
. O
, O
2017 O
; O
Anastasopoulos O
and O
Chiang O
, O
2018 O
; O
Sperber O
et O
al O
. O
, O
2019 O
) O
. O
For O
a O
more O
sys O
- O
tematic O
review O
of O
the O
ST B-TaskName
development O
, O
we O
refer O
the O
readers O
to O
Sperber O
and O
Paulik O
( O
2020 O
) O
. O

With O
recent O
efforts O
in O
the O
expansion O
of O
the O
ST B-TaskName
data O
collection O
( O
Di O
Gangi O
et O
al O
. O
, O
2019 O
; O
Beilharz O
et O
al O
. O
, O
2020 O
) O
, O
more O
and O
more O
direct O
ST B-TaskName
data O
is O
available O
. O
Such O
direct O
data O
comes O
as O
pairs O
of O
source O
speech O
and O
target O
translation O
, O
and O
often O
as O
triplets O
further O
including O
source O
transcriptions O
. O

Various O
joint O
training O
methods O
are O
proposed O
to O
use O
such O
data O
to O
improve O
cascaded O
systems O
, O
with O
the O
hope O
that O
uncertainties O
during O
transcription O
can O
be O
passed O
on O
to O
translation O
to O
be O
resolved O
. O
Here O
, O
what O
we O
call O
" O
joint O
training O
" O
is O
often O
referred O
to O
as O
" O
end O
- O
to O
- O
end O
training O
" O
in O
the O
literature O
, O
where O
the O
direct O
ST B-TaskName
data O
is O
utilized O
in O
the O
joint O
optimization O
of O
the O
ASR B-TaskName
and O
MT B-TaskName
models O
( O
Kano O
et O
al O
. O
, O
2017 O
; O
Berard O
et O
al O
. O
, O
2018 O
; O
Anastasopoulos O
and O
Chiang O
, O
2018 O
; O
Inaguma O
et O
al O
. O
, O
2019 O
; O
Sperber O
et O
al O
. O
, O
2019 O
; O
Bahar O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Bahar O
et O
al O
. O
, O
2021 O
) O
. O
In O
this O
work O
, O
we O
revisit O
the O
principal O
question O
of O
whether O
or O
not O
joint O
training O
really O
helps O
cascaded O
speech B-TaskName
translation I-TaskName
. O

Cascaded O
Approach O

In O
traditional O
cascaded O
systems O
, O
an O
ASR B-TaskName
model O
p O
( O
f O
J O
1 O
|x O
T O
1 O
) O
and O
an O
MT B-TaskName
model O
p O
( O
e O
I O
1 O
|f O
J O
1 O
) O
are O
trained O
separately O
, O
where O
we O
denote O
speech O
features O
as O
x O
T O
1 O
, O
transcriptions O
as O
f O
J O
1 O
, O
and O
translations O
as O
e O
I O
1 O
. O
The O
decoding O
is O
done O
in O
two O
steps O
: O

f O
J O
1 O
= O
argmax O
[ O
f O
J O
1 O
] O
p O
( O
f O
J O
1 O
|x O
T O
1 O
) O
e O
I O
1 O
= O
argmax O
[ O
e O
I O
1 O
] O
p O
( O
e O
I O
1 O
|f O
J O
1 O
) O

The O
argmax O
is O
approximated O
using O
beam O
search O
for O
computational O
reasons O
, O
and O
we O
will O
assume O
a O
fixed O
beam B-HyperparameterName
size I-HyperparameterName
N B-HyperparameterName
for O
the O
decoding O
of O
both O
transcriptions O
and O
translations O
. O

Joint O
Training O
Approaches O

Top B-MethodName
- I-MethodName
K I-MethodName
Cascaded I-MethodName
Translation I-MethodName

Assume O
we O
have O
pre O
- O
trained O
an O
ASR B-TaskName
and O
an O
MT B-TaskName
model O
, O
and O
some O
direct O
ST B-TaskName
training O
data O
is O
available O
. O
The O
pre O
- O
trained O
ASR B-TaskName
model O
is O
used O
to O
produce O
a O
K B-HyperparameterName
- O
best O
list O
of O
ASR B-TaskName
hypotheses O
F O
1 O
, O
F O
2 O
, O
. O
. O
. O
, O
F O
K O
using O
beam O
search O
with O
beam B-HyperparameterName
size I-HyperparameterName
N B-HyperparameterName
≥ O
K. B-HyperparameterName
While O
there O
is O
no O
unique O
method O
to O
make O
use O
of O
the O
top O
- O
K O
transcript O
, O
we O
describe O
Top B-MethodName
- I-MethodName
K I-MethodName
- I-MethodName
Train I-MethodName
, O
a O
straightforward O
algorithm O
similar O
to O
re O
- O
ranking O
. O
We O
obtain O
the O
scorep O
for O
each O
ASR B-TaskName
hypothesis O
with O
length O
normalization O
and O
normalize O
them O
locally O
within O
the O
top O
- O
K O
hypotheses O
. O

p O
( O
F O
k O
|x O
T O
1 O
) O
= O
p O
( O
F O
k O
|x O
T O
1 O
) O
K O
k O
′ O
= O
1p O
( O
F O
k O
′ O
|x O
T O
1 O
) O
( O
1 O
) O

During O
training O
, O
p O
( O
e O
i O
|F O
K O
; O
e O
i−1 O
0 O
) O
is O
the O
MT B-TaskName
model O
output O
. O
Given O
the O
ASR B-TaskName
hypotheses O
F O
1 O
, O
. O
. O
. O
, O
F O
K O
, O
the O
following O
training O
objective O
is O
maximized O
. O

log O
K O
k=1 O
p O
( O
F O
k O
|x O
T O
1 O
) O
I O
i=1 O
p O
( O
e O
i O
|F O
k O
; O
e O
i−1 O
0 O
) O

We O
hypothesize O
that O
this O
objective O
( O
a O
) O
exposes O
different O
transcriptions O
and O
potential O
ASR B-TaskName
errors O
to O
the O
MT B-TaskName
model O
and O
( O
b O
) O
encourages O
the O
ASR B-TaskName
model O
to O
produce O
hypotheses O
closer O
to O
the O
expectations O
of O
the O
MT B-TaskName
model O
, O
thus O
reducing O
model O
discrepancy O
. O

Since O
discrete O
ASR B-TaskName
hypotheses O
are O
passed O
to O
the O
MT B-TaskName
model O
from O
a O
previous O
beam O
search O
, O
the O
error O
signal O
to O
ASR B-TaskName
is O
passed O
via O
the O
renormalized O
transcript O
scores O
during O
backpropagation O
. O
Similarly O
, O
we O
introduce O
Top B-MethodName
- I-MethodName
K I-MethodName
- I-MethodName
Search I-MethodName
. O
We O
obtain O
an O
MT B-TaskName
hypothesis O
E O
k O
for O
each O
F O
k O
using O
beam O
search O
. O
The O
final O
hypothesis O
is O
obtained O
as O
below O
. O

eÎ O
1 O
= O
argmax O
E O
k O
{ O
p O
( O
F O
k O
|x O
T O
1 O
) O
• O
p O
( O
E O
k O
|F O
k O
) O
} O

Here O
, O
p O
( O
F O
k O
|x O
T O
1 O
) O
is O
obtained O
as O
in O
Equation O
1 O
and O
p O
( O
E O
k O
|F O
k O
) O
is O
the O
length O
normalized O
translation O
score O
from O
the O
MT B-TaskName
model O
. O
Observe O
that O
this O
search O
is O
applicable O
to O
any O
cascade O
architecture O
and O
is O
thus O
independent O
of O
the O
training O
criterion O
. O
In O
our O
experiments O
, O
we O
always O
Top B-MethodName
- I-MethodName
K I-MethodName
- I-MethodName
Search I-MethodName
when O
decoding O
models O
trained O
with O
Top B-MethodName
- I-MethodName
K I-MethodName
- I-MethodName
Train I-MethodName
. O
The O
idea O
of O
generating O
the O
top O
- O
K O
ASR B-TaskName
hypotheses O
during O
search O
has O
also O
been O
explored O
in O
the O
literature O
( O
e.g. O
Section O
3.3 O
) O
. O

Tight B-MethodName
- I-MethodName
Integration I-MethodName

Another O
way O
to O
train O
the O
cascade O
architecture O
using O
direct O
ST B-TaskName
data O
is O
the O
tight O
integrated O
cascade O
approach O
( O
Bahar O
et O
al O
. O
, O
2021 O
) O
. O
We O
introduce O
an O
exponent O
γ B-HyperparameterName
that O
controls O
the O
sharpness O
of O
the O
distribution O
of O
the O
conditional O
probabilities O
. O
Thus O
, O
instead O
of O
passing O
the O
1 O
- O
best O
hypothesis O
of O
the O
ASR B-TaskName
system O
as O
a O
sequence O
of O
1 O
- O
hot O
vectors O
, O
we O
pass O
the O
renormalized O
probabilities O
to O
the O
MT B-TaskName
model O
. O

p O
( O
f O
j O
|f O
j−1 O
1 O
; O
x O
T O
1 O
) O
= O
p O
γ O
( O
f O
j O
|f O
j−1 O
1 O
x O
T O
1 O
) O
f O
′ O
∈|V O
F O
|p O
γ O
( O
f O
′ O
j O
|f O
j−1 O
1 O
x O
T O
1 O
) O

Here O
, O
V O
F O
is O
the O
vocabulary O
of O
the O
ASR B-TaskName
system O
. O
et O
al O
. O
( O
2021 O
) O
propose O
passing O
the O
final O
decoder O
representations O
of O
the O
N O
-best O
ASR B-TaskName
hypotheses O
( O
i.e. O
the O
searchable O
hidden O
intermediates O
) O
directly O
to O
the O
MT B-TaskName
system O
, O
bypassing O
the O
MT B-TaskName
input O
embedding O
. O

Searchable B-MethodName
Hidden I-MethodName
Intermediates I-MethodName

Dalmia O

Additionally O
, O
they O
extend O
the O
multi O
- O
task O
learning O
approach O
by O
allowing O
the O
MT B-TaskName
decoder O
to O
attend O
to O
the O
ASR B-TaskName
encoder O
states O
, O
which O
in O
turn O
are O
optimized O
using O
beam O
search O
in O
training O
. O
They O
show O
that O
during O
decoding O
, O
a O
higher O
ASR B-TaskName
beam B-HyperparameterName
size I-HyperparameterName
indeed O
leads O
to O
a O
better O
ST B-TaskName
performance O
. O

Experimental O
Results O
and O
Analysis O

We O
focus O
on O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
English O
- O
German O
speech O
translation O
task O
( O
Di O
Gangi O
et O
al O
. O
, O
2019 O
) O
in O
the O
domain O
of O
TED O
talks O
and O
evaluate O
on O
test O
- O
HE O
and O
test O
- O
COMMON O
. O
We O
use O
an O
in O
- O
house O
filtered O
subset O
of O
the O
IWSLT B-DatasetName
2021 I-DatasetName
English I-DatasetName
- I-DatasetName
German I-DatasetName
dataset O
as O
in O
Bahar O
et O
al O
. O
( O
2021 O
) O
, O
which O
contains O
1.9 O
M O
segments O
( O
2300 O
hours O
) O
of O
ASR B-TaskName
data O
and O
24 O
M O
parallel O
sentences O
of O
MT B-TaskName
data O
. O
The O
in O
- O
domain O
ASR B-TaskName
data O
comprises O
MUST B-DatasetName
- I-DatasetName
C I-DatasetName
, O
TED B-DatasetName
- I-DatasetName
LIUM I-DatasetName
, O
and O
IWSLT B-DatasetName
TED I-DatasetName
, O
while O
the O
out O
- O
of O
- O
domain O
ASR B-TaskName
data O
consists O
of O
EuroParl B-DatasetName
, O
How2 B-DatasetName
, O
LibriSpeech B-DatasetName
, O
and O
Mozilla B-DatasetName
Common I-DatasetName
Voice I-DatasetName
. O
For O
translation O
, O
the O
dataset O
contains O
24 O
M O
parallel O
sentences O
of O
in O
- O
domain O
translation O
data O
( O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
, O
TED B-DatasetName
- I-DatasetName
LIUM I-DatasetName
, O
and O
IWSLT B-DatasetName
TED I-DatasetName
) O
, O
as O
well O
as O
out O
- O
of O
- O
domain O
translation O
data O
( O
NewsCommentary B-DatasetName
, O
EuroParl B-DatasetName
, O
WikiTitles B-DatasetName
, O
ParaCrawl B-DatasetName
, O
Com B-DatasetName
- I-DatasetName
monCrawl I-DatasetName
, O
Rapid B-DatasetName
, O
OpenSubtitles2018 B-DatasetName
) O
. O
For O
ST B-TaskName
data O
, O
we O
only O
use O
MuST B-DatasetName
- I-DatasetName
C. I-DatasetName
We O
provide O
further O
details O
in O
Appendix O
A. O
Depending O
on O
whether O
or O
not O
fine O
- O
tuned O
on O
in O
- O
domain O
ASR B-TaskName
and O
MT B-TaskName
data O
, O
we O
split O
our O
experiments O
into O
two O
sets O
: O
A1 O
- O
A5 O
and O
B1 O
- O
B5 O
. O
Without O
fine O
- O
tuning O
, O
we O
observe O
that O
both O
the O
tight B-MethodName
integrated I-MethodName
cascade I-MethodName
( O
Bahar O
et O
al O
. O
, O
2021 O
) O
( O
A3 O
) O
and O
our O
marginalization O
approach O
( O
A4 O
) O
outperform O
the O
baseline O
cascade O
( O
A2 O
) O
on O
both O
test O
- O
HE O
and O
test O
- O
COMMON O
( O
Table O
1 O
) O
. O
However O
, O
after O
fine O
- O
tuning O
both O
ASR B-TaskName
and O
MT B-TaskName
models O
, O
we O
do O
not O
observe O
significant O
improvements O
of O
the O
joint O
training O
methods O
( O
B3 O
, O
B4 O
) O
over O
the O
cascade O
baseline O
( O
B2 O
) O
anymore O
. O

Our O
experimental O
results O
suggest O
that O
the O
joint O
training O
of O
cascaded O
speech B-TaskName
translation I-TaskName
models O
does O
not O
seem O
to O
be O
effective O
. O
This O
poses O
the O
questions O
: O
why O
is O
that O
and O
what O
were O
we O
trying O
to O
achieve O
with O
joint O
training O
anyways O
? O
Sperber O
and O
Paulik O
( O
2020 O
) O
highlighted O
three O
main O
issues O
with O
ST B-TaskName
, O
and O
in O
the O
following O
, O
we O
will O
discuss O
these O
issues O
from O
the O
perspective O
of O
joint O
training O
. O

Mismatched O
spoken O
and O
written O
domains O
Transcripts O
and O
translation O
data O
usually O
differ O
in O
e.g. O
linguistic O
style O
and O
punctuation O
. O
This O
mismatch O
poses O
a O
challenge O
for O
cascaded O
models O
, O
as O
translation O
models O
may O
struggle O
to O
handle O
transcript O
- O
style O
text O
. O
As O
Sperber O
and O
Paulik O
( O
2020 O
) O
point O
out O
, O
some O
of O
the O
issues O
such O
as O
differences O
in O
punctuation O
can O
already O
be O
tackled O
by O
plain O
text O
normalization O
. O

More O
generally O
, O
one O
can O
fine O
- O
tune O
the O
models O
on O
in O
- O
domain O
transcript O
- O
like O
ASR B-TaskName
, O
MT B-TaskName
, O
and O
ST B-TaskName
data O
. O
It O
is O
unusual O
to O
find O
ST B-TaskName
datasets O
that O
do O
not O
come O
with O
corresponding O
ASR B-TaskName
and O
MT B-TaskName
data O
, O
as O
ST B-TaskName
data O
acquisition O
usually O
involves O
translating O
from O
transcriptions O
. O
Thus O
, O
we O
can O
simply O
fine O
- O
tune O
the O
ASR B-TaskName
and O
MT B-TaskName
models O
on O
these O
in O
- O
domain O
datasets O
. O

Our O
results O
suggest O
that O
fine O
- O
tuning O
the O
ASR B-TaskName
and O
MT B-TaskName
models O
is O
comparable O
or O
even O
superior O
to O
finetuning O
these O
models O
in O
an O
end O
- O
to O
- O
end O
fashion O
on O
the O
respective O
speech O
translation O
dataset O
. O
However O
, O
Inaguma O
et O
al O
. O
( O
2021 O
) O
and O
Bahar O
et O
al O
. O
( O
2021 O
) O
report O
significant O
improvements O
of O
their O
joint O
cascaded O
approach O
, O
which O
is O
similar O
to O
the O
tight O
integrated O
cascade O
, O
over O
their O
cascaded O
baseline O
( O
Table O
1 O
) O
. O

What O
is O
the O
reason O
for O
this O
disparity O
? O
We O
pin O
it O
down O
to O
one O
major O
difference O
: O
the O
use O
of O
indomain O
ST B-TaskName
data O
, O
or O
more O
precisely O
, O
the O
lack O
thereof O
. O
Inaguma O
et O
al O
. O
( O
2021 O
) O
report O
that O
by O
fine O
- O
tuning O
on O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
and O
ST B-DatasetName
- I-DatasetName
TED I-DatasetName
, O
they O
are O
unable O
to O
significantly O
improve O
their O
MT B-TaskName
baseline O
, O
and O
thus O
, O
the O
MT B-TaskName
component O
in O
their O
cascade O
model O
is O
not O
fine O
- O
tuned O
on O
the O
domain O
of O
TED O
talks O
. O
In O
contrast O
, O
we O
find O
that O
in O
- O
domain O
fine O
- O
tuning O
significantly O
improves O
our O
cascaded O
model O
, O
especially O
if O
applied O
to O
both O
the O
ASR B-TaskName
and O
MT B-TaskName
models O
( O
Table O
2 O
) O
, O
also O
improving O
the O
individual O
components O
, O
as O
we O
observe O
a O
decrease O
in O
WER B-MetricName
[ O
% O
] O
from O
9.3 B-MetricValue
to O
8.1 B-MetricValue
of O
the O
ASR B-TaskName
component O
and O
an O
increase O
in O
BLEU B-MetricName
[ O
% O
] O
from O
32.5 B-MetricValue
to O
33.7 B-MetricValue
of O
the O
MT B-TaskName
component O
on O
tst O
- O
COMMON O
. O
As O
pointed O
out O
earlier O
, O
fine O
- O
tuning O
diminishes O
any O
improvement O
obtained O
using O
any O
of O
the O
joint O
training O
methods O
we O
implement O
, O
as O
the O
cascade O
baseline O
significantly O
gains O
in O
performance O
. O

Thus O
, O
in O
- O
domain O
fine O
- O
tuning O
is O
essential O
in O
order O
to O
tackle O
the O
disparity O
between O
the O
spoken O
and O
the O
written O
domain O
for O
vanilla O
cascaded O
models O
. O
This O
especially O
holds O
true O
for O
the O
MT B-TaskName
model O
, O
which O
is O
trained O
on O
non O
- O
transcript O
- O
like O
data O
, O
but O
we O
want O
it O
to O
adapt O
to O
transcript O
- O
like O
inputs O
and O
transcript O
- O
like O
outputs O
( O
with O
punctuations O
, O
casing O
, O
etc O
. O
) O
. O

Intuitively O
, O
instead O
of O
in O
- O
domain O
fine O
- O
tuning O
, O
a O
simple O
remedy O
is O
to O
only O
use O
in O
- O
domain O
data O
for O
ASR B-TaskName
and O
MT B-TaskName
. O
Xu O
et O
al O
. O
( O
2021 O
) O
observe O
an O
improvement O
of O
their O
multi O
- O
task O
learning O
method O
when O
allowing O
only O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
data O
( O
Table O
1 O
) O
. O
However O
, O
the O
improvement O
vanishes O
as O
they O
introduce O
external O
ASR B-TaskName
and O
MT B-TaskName
data O
. O
Since O
the O
latter O
represents O
a O
more O
realistic O
data O
scenario O
, O
we O
believe O
that O
the O
inclusion O
of O
external O
ASR B-TaskName
and O
MT B-TaskName
data O
is O
necessary O
to O
obtain O
meaningful O
results O
. O

In O
our O
fine O
- O
tuning O
setup O
, O
we O
do O
not O
only O
adapt O
to O
transcript O
- O
like O
data O
, O
but O
also O
to O
the O
specific O
domain O
of O
TED O
talks O
. O
In O
the O
literature O
, O
ST B-TaskName
performance O
is O
commonly O
evaluated O
on O
test O
sets O
in O
the O
same O
domain O
as O
the O
ST B-TaskName
data O
, O
e.g. O
TED O
talks O
( O
i.e. O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
or O
IWSLT B-DatasetName
TED I-DatasetName
) O
, O
LibriSpeech B-DatasetName
and O
CallHome B-DatasetName
( O
Post O
et O
al O
. O
, O
2013 O
) O
. O
However O
, O
this O
poorly O
reflects O
real O
- O
life O
data O
conditions O
, O
because O
large O
amounts O
of O
in O
- O
domain O
ST B-TaskName
data O
are O
not O
always O
available O
, O
while O
in O
- O
domain O
ASR B-TaskName
and O
MT B-TaskName
data O
is O
more O
accessible O
. O
As O
a O
consequence O
, O
we O
believe O
that O
end O
- O
to O
- O
end O
models O
are O
artificially O
favored O
over O
cascade O
models O
in O
these O
setups O
. O
We O
thus O
motivate O
future O
research O
to O
also O
consider O
outof O
- O
domain O
or O
general O
- O
domain O
ST B-TaskName
datasets O
, O
while O
allowing O
in O
- O
domain O
ASR B-TaskName
and O
MT B-TaskName
data O
. O

Error O
propagation O
In O
case O
the O
ASR B-TaskName
produces O
an O
error O
in O
the O
transcript O
or O
intermediate O
representations O
, O
this O
error O
is O
propagated O
to O
the O
translation O
model O
, O
which O
does O
not O
have O
any O
knowledge O
about O
the O
transcription O
process O
. O
Sperber O
and O
Paulik O
( O
2020 O
) O
discuss O
this O
phenomenon O
under O
the O
term O
erroneous O
early O
decisions O
. O

Intuitively O
, O
a O
remedy O
for O
this O
issue O
is O
joint O
training O
, O
as O
we O
allow O
the O
MT B-TaskName
component O
to O
learn O
to O
use O
information O
that O
is O
missing O
or O
erroneous O
in O
the O
intermediate O
representation O
. O
For O
example O
, O
the O
tight B-MethodName
integrated I-MethodName
approach O
( O
Bahar O
et O
al O
. O
, O
2021 O
) O
addresses O
this O
issue O
by O
expressing O
uncertainties O
in O
the O
transcription O
as O
posterior O
probabilities O
, O
while O
propose O
speech B-TaskName
attention I-TaskName
, O
i.e. O
a O
Transformer O
cross O
- O
attention O
sub O
- O
module O
in O
the O
MT B-TaskName
component O
, O
attending O
over O
ASR B-TaskName
encoder O
representations O
. O

However O
, O
we O
make O
the O
case O
that O
joint O
training O
is O
not O
necessarily O
the O
only O
remedy O
to O
error O
propagation O
. O
Therefore O
, O
we O
consider O
a O
cheating O
experiment O
based O
on O
Top B-MethodName
- I-MethodName
K I-MethodName
- I-MethodName
Search I-MethodName
. O
For O
each O
of O
the O
top-4 B-HyperparameterValue
ASR B-TaskName
hypotheses O
, O
we O
pick O
the O
translations O
generated O
by O
the O
MT B-TaskName
model O
based O
on O
the O
sentence O
- O
level O
BLEU B-MetricName
with O
the O
ground O
- O
truth O
target O
. O
In O
these O
experiments O
, O
we O
obtain O
a O
BLEU B-MetricName
[ O
% O
] O
score O
of O
32.4 B-MetricValue
on O
tst B-DatasetName
- I-DatasetName
HE I-DatasetName
and O
34.2 B-MetricValue
on O
tst B-DatasetName
- I-DatasetName
COMMON I-DatasetName
, O
in O
both O
cases O
outperforming O
the O
oracle O
MT B-TaskName
using O
ground O
- O
truth O
transcripts O
. O
Thus O
, O
on O
average O
, O
the O
translation O
of O
one O
of O
the O
top-4 B-HyperparameterValue
transcripts O
generated O
by O
the O
ASR B-TaskName
model O
is O
no O
worse O
than O
using O
the O
ground O
- O
truth O
transcript O
( O
B1 O
) O
. O
Hence O
, O
we O
posit O
that O
error O
propagation O
can O
be O
alleviated O
by O
plain O
ASR B-TaskName
re O
- O
ranking O
. O
A O
possible O
starting O
point O
is O
our O
Top B-MethodName
- I-MethodName
K I-MethodName
- I-MethodName
Search I-MethodName
( O
A5 O
, O
B5 O
) O
, O
giving O
small O
, O
but O
consistent O
improvements O
over O
their O
respective O
cascade O
baselines O
( O
A2 O
, O
B2 O
) O
without O
any O
joint O
training O
. O

Similarly O
, O
instead O
of O
sequence O
- O
level O
reranking O
using O
the O
joint O
ASR B-TaskName
- O
MT B-TaskName
score O
, O
propose O
augmenting O
the O
token O
- O
level O
ASR B-TaskName
probabilities O
with O
either O
the O
CTC O
scores O
from O
the O
ASR B-TaskName
encoder O
or O
an O
external O
LM O
score O
during O
beam O
search O
to O
incorporate O
ASR B-TaskName
uncertainties O
. O

Information O
loss O
Information O
loss O
occurs O
as O
the O
ASR B-TaskName
model O
does O
not O
pass O
on O
auditory O
information O
such O
as O
intonation O
and O
timing O
, O
which O
may O
be O
relevant O
for O
the O
translation O
component O
. O
While O
we O
have O
no O
empirical O
evidence O
on O
the O
significance O
of O
such O
information O
on O
the O
final O
performance O
, O
we O
observe O
that O
our O
MT B-TaskName
model O
, O
given O
the O
ground O
- O
truth O
transcripts O
, O
still O
significantly O
outperforms O
any O
speech B-TaskName
translation I-TaskName
model O
we O
investigate O
, O
doing O
so O
without O
any O
auditory O
information O
. O
Furthermore O
, O
our O
cheating O
experiments O
suggest O
one O
may O
even O
improve O
over O
ground O
- O
truth O
transcriptions O
without O
any O
auditory O
information O
. O
We O
posit O
that O
as O
of O
now O
, O
focusing O
on O
closing O
that O
gap O
is O
of O
higher O
importance O
. O

Conclusion O

In O
this O
work O
, O
we O
analyzed O
several O
reasons O
why O
joint O
training O
does O
not O
appear O
to O
help O
when O
individual O
automatic B-TaskName
speech I-TaskName
recognition I-TaskName
and O
machine B-TaskName
translation I-TaskName
components O
are O
stronger O
. O
We O
point O
out O
that O
cascaded O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
when O
fine O
- O
tuned O
on O
in O
- O
domain O
ST B-TaskName
data O
. O

While O
we O
do O
not O
suggest O
that O
joint O
training O
is O
not O
worth O
exploring O
, O
we O
want O
to O
encourage O
future O
research O
to O
consider O
data O
conditions O
more O
carefully O
and O
produce O
strong O
cascade O
baselines O
. O
Concretely O
, O
we O
suggest O
( O
1 O
) O
the O
inclusion O
of O
external O
ASR B-TaskName
and O
MT B-TaskName
data O
, O
( O
2 O
) O
training O
strong O
cascade O
baselines O
by O
fine O
- O
tuning O
both O
ASR B-TaskName
and O
MT B-TaskName
models O
on O
in O
- O
domain O
transcript O
- O
like O
data O
, O
if O
available O
and O
( O
3 O
) O
the O
investigation O
of O
data O
conditions O
where O
only O
out O
- O
of O
- O
domain O
ST B-TaskName
data O
is O
available O
, O
while O
allowing O
in O
- O
domain O
ASR B-TaskName
and O
MT B-TaskName
data O
. O

Limitations O

In O
our O
experiments O
, O
we O
focus O
only O
on O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
En O
- O
De O
task O
due O
to O
computational O
constraints O
. O
Further O
experiments O
on O
different O
language O
pairs O
could O
e.g. O
show O
differences O
of O
how O
spoken O
- O
written O
domain O
mismatch O
affects O
different O
languages O
. O
Again O
, O
experiments O
in O
different O
domains O
without O
direct O
ST B-TaskName
data O
could O
further O
underline O
or O
refute O
our O
conclusions O
. O

In O
our O
analysis O
, O
we O
include O
experimental O
results O
from O
other O
authors O
as O
we O
do O
not O
have O
the O
resources O
to O
reproduce O
every O
method O
. O
It O
is O
possible O
that O
dif O
- O
ferences O
in O
data O
filtering O
, O
data O
preprocessing O
, O
architectural O
choices O
, O
etc O
. O
could O
affect O
the O
comparability O
of O
these O
results O
. O

We O
have O
only O
analyzed O
a O
subset O
of O
the O
joint O
cascade O
methods O
described O
in O
literature O
. O
A O
systematic O
overview O
of O
such O
methods O
is O
outside O
the O
scope O
of O
our O
work O
. O

In O
order O
to O
be O
comparable O
to O
other O
works O
in O
literature O
, O
we O
mostly O
draw O
our O
conclusions O
using O
BLEU B-MetricName
and O
TER B-MetricName
. O
We O
acknowledge O
that O
using O
other O
automatic O
evaluation O
metrics O
and O
making O
use O
of O
human O
evaluation O
would O
strengthen O
the O
significance O
of O
our O
findings O
. O

warmup B-HyperparameterName
steps I-HyperparameterName
. O
As O
regularization O
, O
we O
use O
a O
dropout B-HyperparameterName
of O
0.1 B-HyperparameterValue
and O
label B-HyperparameterName
smoothing I-HyperparameterName
with O
α B-HyperparameterName
= O
0.1 B-HyperparameterValue
. O

Joint O
Training O

We O
use O
the O
same O
training O
setup O
as O
for O
the O
ASR B-TaskName
model O
, O
but O
with O
a O
learning B-HyperparameterValue
rate I-HyperparameterValue
of O
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−6 I-HyperparameterValue
. O
Furthermore O
, O
as O
we O
are O
now O
working O
on O
a O
smaller O
ST B-TaskName
dataset O
, O
an O
epoch B-HyperparameterName
is O
split O
into O
10 B-HyperparameterValue
sub B-HyperparameterName
- I-HyperparameterName
epochs I-HyperparameterName
. O

Fine O
- O
tuning O
We O
fine O
- O
tune O
the O
ASR B-TaskName
model O
on O
all O
in O
- O
domain O
ASR B-TaskName
data O
( O
600 B-HyperparameterValue
K I-HyperparameterValue
segments O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.0001 B-HyperparameterValue
, O
while O
the O
MT B-TaskName
model O
is O
finetuned O
on O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
dataset O
( O
300 B-HyperparameterValue
K I-HyperparameterValue
parallel O
sentences O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.00001 B-HyperparameterValue
. O

We O
use O
beam O
search O
with O
N B-HyperparameterName
= O
12 B-HyperparameterValue
. O
All O
BLEU B-MetricName
scores O
reported O
are O
calculated O
on O
case O
- O
sensitive O
data O
using O
the O
official O
WMT O
scoring O
script O
. O
TER B-MetricName
scores O
are O
calculated O
using O
TERCom O
. O
For O
top O
- O
K B-HyperparameterName
experiments O
, O
we O
use O
K B-HyperparameterName
= O
4 B-HyperparameterValue
. O

We O
train O
our O
experiments O
on O
NVIDIA O
GTX O
1080 O
Ti O
. O
Training O
the O
ASR B-TaskName
takes O
around O
4 O
weeks O
, O
all O
other O
experiment O
take O
around O
2 O
- O
3 O
weeks O
. O
The O
average O
runtime O
for O
inference O
on O
non O
- O
joint O
experiments O
is O
about O
15 O
minutes O
, O
where O
joint O
experiments O
need O
around O
2 O
hours O
. O

Acknowledgements O

This O
work O
was O
partially O
supported O
by O
the O
project O
HYKIST O
funded O
by O
the O
German O
Federal O
Ministry O
of O
Health O
on O
the O
basis O
of O
a O
decision O
of O
the O
German O
Federal O
Parliament O
( O
Bundestag O
) O
under O
funding O
ID O
ZMVI1 O
- O
2520DAT04A O
, O
and O
by O
NeuroSys O
which O
, O
as O
part O
of O
the O
initiative O
" O
Clusters4Future O
" O
, O
is O
funded O
by O
the O
Federal O
Ministry O
of O
Education O
and O
Research O
BMBF O
( O
03ZU1106DA O
) O
. O

We O
thank O
Parnia O
Bahar O
for O
sharing O
her O
data O
preprocessing O
and O
training O
recipes O
. O

A O
Experimental O
Setup O

For O
the O
ASR B-TaskName
data O
, O
we O
extract O
80 B-HyperparameterValue
- O
dimensional O
MFCC O
features O
every O
10ms O
. O
The O
text O
data O
is O
postprocessed O
by O
lower O
- O
casing O
, O
removing O
punctuation O
and O
transcriber O
tags O
, O
and O
by O
applying O
BPE O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
e O
a O
vocabulary B-HyperparameterName
size I-HyperparameterName
of O
8000 B-HyperparameterValue
. O
We O
post O
- O
process O
the O
source O
text O
to O
be O
transcript O
- O
like O
, O
by O
removing O
punctuation O
and O
lower O
- O
casing O
. O
On O
both O
source O
and O
target O
sentences O
, O
we O
apply O
BPE O
with O
a O
vocabulary B-HyperparameterName
size I-HyperparameterName
of O
32k B-HyperparameterValue
. O
For O
models O
using O
tight B-MethodName
integration I-MethodName
, O
a O
separate O
translation O
model O
is O
trained O
using O
the O
transcription O
vocabulary O
. O
In O
both O
cases O
, O
the O
validation O
set O
is O
the O
concatenation O
of O
the O
validation O
sets O
provided O
by O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
and O
IWSLT B-DatasetName
TED I-DatasetName
. O

Our O
implementation O
is O
based O
on O
fairseq O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
and O
is O
available O
online O
1 O
. O

ASR B-TaskName
model O
For O
speech O
recognition O
, O
we O
use O
a O
Transformer B-MethodName
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
12 B-HyperparameterValue
encoder B-HyperparameterName
layers I-HyperparameterName
and O
6 B-HyperparameterValue
decoder B-HyperparameterName
layers I-HyperparameterName
. O
Instead O
of O
absolute O
positional O
encodings O
, O
we O
use O
relative O
positional O
encodings O
as O
introduced O
by O
( O
Shaw O
et O
al O
. O
, O
2018 O
) O
. O
We O
reduce O
the O
audio O
feature O
length O
using O
a O
two O
- O
layer O
convolutional O
network O
with O
kernel B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
and O
1024 B-HyperparameterValue
channels B-HyperparameterName
. O
Other O
parameters O
are O
adapted O
from O
the O
original O
base O
configuration O
. O
Our O
ASR B-TaskName
model O
consists O
of O
70 B-HyperparameterValue
M I-HyperparameterValue
trained B-HyperparameterName
parameters I-HyperparameterName
. O

Each O
epoch B-HyperparameterName
is O
split O
into O
20 B-HyperparameterValue
sub B-HyperparameterName
- I-HyperparameterName
epochs I-HyperparameterName
. O
We O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
learning B-HyperparameterName
rate I-HyperparameterName
0.0003 B-HyperparameterValue
and O
10 B-HyperparameterValue
warmup B-HyperparameterName
sub I-HyperparameterName
- I-HyperparameterName
epochs I-HyperparameterName
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
scaled O
by O
0.8 B-HyperparameterValue
for O
every O
3 B-HyperparameterValue
sub B-HyperparameterName
- I-HyperparameterName
epochs I-HyperparameterName
without O
improvement O
on O
the O
validation O
set O
. O
As O
regularization O
, O
we O
use O
SpecAugment O
( O
Park O
et O
al O
. O
, O
2019 O
) O
( O
( O
F O
, O
m O
F O
, O
T O
, O
m O
T O
, O
p O
, O
W O
) O
= O
( O
16 O
, O
4 O
, O
40 O
, O
2 O
, O
1.0 O
, O
0 O
) O
) O
, O
a O
dropout B-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
of O
0.1 B-HyperparameterValue
, O
and O
label B-HyperparameterName
smoothing I-HyperparameterName
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
with O
α B-HyperparameterName
= O
0.1 B-HyperparameterValue
. O
Additionally O
, O
we O
train O
a O
CTC O
loss O
as O
an O
additional O
task O
during O
training O
( O
Kim O
et O
al O
. O
, O
2017 O
) O
. O

MT B-TaskName
model O
For O
translation O
, O
we O
also O
use O
a O
Transformer B-MethodName
model O
with O
6 B-HyperparameterValue
encoder B-HyperparameterName
and O
6 B-HyperparameterValue
decoder B-HyperparameterName
layers I-HyperparameterName
. O
Again O
, O
we O
use O
relative O
positional O
encodings O
, O
other O
parameters O
are O
adapted O
from O
the O
original O
base O
configuration O
, O
resulting O
in O
a O
model O
with O
70 B-HyperparameterValue
M I-HyperparameterValue
trained B-HyperparameterName
parameters I-HyperparameterName
. O

We O
use O
the O
same O
optimization O
procedure O
as O
with O
the O
ASR B-TaskName
model O
, O
except O
that O
we O
now O
start O
with O
4000 B-HyperparameterValue

Missing O
Counter O
- O
Evidence O
Renders O
NLP B-TaskName
Fact I-TaskName
- I-TaskName
Checking I-TaskName
Unrealistic O
for O
Misinformation O

Misinformation O
emerges O
in O
times O
of O
uncertainty O
when O
credible O
information O
is O
limited O
. O
This O
is O
challenging O
for O
NLP B-TaskName
- I-TaskName
based I-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
as O
it O
relies O
on O
counter O
- O
evidence O
, O
which O
may O
not O
yet O
be O
available O
. O
Despite O
increasing O
interest O
in O
automatic O
fact O
- O
checking O
, O
it O
is O
still O
unclear O
if O
automated O
approaches O
can O
realistically O
refute O
harmful O
real O
- O
world O
misinformation O
. O
Here O
, O
we O
contrast O
and O
compare O
NLP B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
with O
how O
professional O
fact O
- O
checkers O
combat O
misinformation O
in O
the O
absence O
of O
counter O
- O
evidence O
. O
In O
our O
analysis O
, O
we O
show O
that O
, O
by O
design O
, O
existing O
NLP O
task O
definitions O
for O
fact B-TaskName
- I-TaskName
checking I-TaskName
can O
not O
refute O
misinformation O
as O
professional O
fact O
- O
checkers O
do O
for O
the O
majority O
of O
claims O
. O
We O
then O
define O
two O
requirements O
that O
the O
evidence O
in O
datasets O
must O
fulfill O
for O
realistic O
factchecking B-TaskName
: O
It O
must O
be O
( O
1 O
) O
sufficient O
to O
refute O
the O
claim O
and O
( O
2 O
) O
not O
leaked O
from O
existing O
fact B-TaskName
- I-TaskName
checking I-TaskName
articles O
. O
We O
survey O
existing O
factchecking B-TaskName
datasets O
and O
find O
that O
all O
of O
them O
fail O
to O
satisfy O
both O
criteria O
. O
Finally O
, O
we O
perform O
experiments O
to O
demonstrate O
that O
models O
trained O
on O
a O
large O
- O
scale O
fact B-TaskName
- I-TaskName
checking I-TaskName
dataset O
rely O
on O
leaked O
evidence O
, O
which O
makes O
them O
unsuitable O
in O
real O
- O
world O
scenarios O
. O
Taken O
together O
, O
we O
show O
that O
current O
NLP B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
can O
not O
realistically O
combat O
real O
- O
world O
misinformation O
because O
it O
depends O
on O
unrealistic O
assumptions O
about O
counter O
- O
evidence O
in O
the O
data O
1 O
. O

Introduction O

According O
to O
van O
der O
Linden O
( O
2022 O
) O
, O
misinformation O
is O
" O
false O
or O
misleading O
information O
masquerading O
as O
legitimate O
news O
, O
regardless O
of O
intent O
" O
. O
Misinformation O
is O
dangerous O
as O
it O
can O
directly O
impact O
human O
behavior O
and O
have O
harmful O
real O
- O
world O
consequences O
such O
as O
the O
Pizzagate O
shooting O
( O
Fisher O
et O
al O
. O
, O
2016 O
) O
, O
interfering O
in O
the O
2016 O
democratic O
US O
election O
( O
Bovet O
and O
Makse O
, O
2019 O
) O
, O
or O
the O
promotion O
of O
false O
COVID-19 O
cures O
( O
Aghababaeian O
et O
al O
. O
, O
Figure O
1 O
: O
A O
false O
claim O
from O
PolitiFact O
. O
It O
is O
unlikely O
to O
find O
counter O
- O
evidence O
. O
Fact O
- O
checkers O
refute O
the O
claim O
by O
disproving O
why O
it O
was O
made O
. O
2020 O
) O
. O
Surging O
misinformation O
during O
the O
COVID-19 O
pandemic O
, O
coined O
" O
infodemic O
" O
by O
WHO O
( O
Zarocostas O
, O
2020 O
) O
, O
exemplifies O
the O
danger O
coming O
from O
misinformation O
. O
To O
combat O
misinformation O
, O
journalists O
from O
fact O
- O
checking O
organizations O
( O
e.g. O
, O
Poli O
- O
tiFact O
or O
Snopes O
) O
conduct O
a O
laborious O
manual O
effort O
to O
verify O
claims O
based O
on O
possible O
harms O
and O
their O
prominence O
( O
Arnold O
, O
2020 O
) O
. O
However O
, O
manual O
factchecking O
can O
not O
keep O
pace O
with O
the O
rate O
at O
which O
misinformation O
is O
posted O
and O
circulated O
. O
Automatic B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
has O
gained O
significant O
attention O
within O
the O
NLP O
community O
in O
recent O
years O
, O
with O
the O
goal O
of O
developing O
tools O
to O
assist O
fact O
- O
checkers O
in O
combating O
misinformation O
. O
For O
the O
past O
few O
years O
, O
NLP O
researchers O
have O
created O
a O
wide O
range O
of O
factchecking O
datasets O
with O
claims O
from O
fact O
- O
checking O
organization O
websites O
( O
Vlachos O
and O
Riedel O
, O
2014 O
; O
Wang O
, O
2017 O
; O
Augenstein O
et O
al O
. O
, O
2019 O
; O
Hanselowski O
et O
al O
. O
, O
2019 O
; O
Ostrowski O
et O
al O
. O
, O
2021 O
; O
Gupta O
and O
Srikumar O
, O
2021 O
; O
Khan O
et O
al O
. O
, O
2022 O
) O
. O
The O
fundamental O
goal O
of O
fact O
- O
checking O
is O
, O
given O
a O
claim O
made O
by O
a O
claimant O
, O
to O
find O
a O
collection O
of O
evidence O
and O
provide O
a O
verdict O
about O
the O
claim O
's O
veracity O
based O
on O
the O
evidence O
. O
The O
underlying O
technique O
used O
by O
fact O
- O
checkers O
, O
and O
journalists O
in O
general O
, O
to O
assess O
the O
veracity O
of O
a O
claim O
is O
called O
verification O
( O
Silverman O
, O
2016 O
) O
. O
In O
a O
comprehensive O
survey O
, O
Guo O
et O
al O
. O
( O
2022 O
) O
proposed O
an O
NLP B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
framework B-MethodName
( O
FCNLP B-MethodName
) O
that O
aggregates O
existing O
( O
sub O
) O
tasks O
and O
approaches O
of O
automated B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
. O
FCNLP B-MethodName
reflects O
current O
research O
trends O
on O
automatic B-TaskName
factchecking I-TaskName
in O
NLP O
and O
divides O
the O
aforementioned O
process O
into O
evidence O
retrieval O
, O
verdict O
prediction O
, O
and O
justification O
production O
. O

In O
this O
paper O
, O
we O
focus O
on O
harmful O
misinformation O
claims O
that O
satisfied O
the O
professional O
factcheckers O
' O
selection O
criteria O
and O
refer O
to O
them O
as O
real O
- O
world O
misinformation O
. O
Our O
goal O
is O
to O
answer O
the O
following O
research O
question O
: O
Can O
evidencebased B-TaskName
NLP I-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
approaches O
in O
FC B-MethodName
- I-MethodName
NLP I-MethodName
refute O
novel O
real O
- O
world O
misinformation O
? O
FCNLP B-MethodName
assumes O
a O
system O
has O
access O
to O
counterevidence O
( O
e.g. O
, O
through O
information O
retrieval O
) O
to O
refute O
a O
claim O
. O
Consider O
the O
false O
claim O
" O
Telemundo O
is O
an O
English O
- O
language O
television O
network O
" O
from O
FEVER B-DatasetName
: O
A O
system O
following O
FCNLP B-MethodName
must O
find O
counter O
- O
evidence O
contradicting O
the O
claim O
( O
i.e. O
, O
Telemundo O
is O
a O
Spanish O
company O
) O
to O
refute O
the O
claim O
. O
This O
may O
require O
more O
complex O
reasoning O
over O
multiple O
documents O
. O
We O
contrast O
this O
example O
to O
the O
real O
- O
world O
false O
claim O
that O
" O
Half O
a O
million O
sharks O
could O
be O
killed O
to O
make O
the O
COVID-19 O
vaccine O
" O
( O
Figure O
1 O
) O
. O
If O
true O
, O
credible O
sources O
would O
likely O
report O
this O
incident O
, O
providing O
supporting O
evidence O
. O
As O
it O
is O
not O
, O
before O
being O
factchecked O
, O
there O
is O
no O
refuting O
evidence O
stating O
that O
COVID-19 O
vaccine O
production O
will O
not O
kill O
sharks O
. O
Only O
after O
guaranteeing O
that O
the O
claim O
relies O
on O
the O
false O
premise O
of O
COVID-19 O
vaccines O
using O
squalene O
( O
harvested O
from O
sharks O
) O
, O
it O
can O
be O
refuted O
. O
After O
the O
claim O
's O
verification O
, O
fact O
- O
checkers O
publish O
reports O
explaining O
the O
verdict O
and O
thereby O
produce O
counter O
- O
evidence O
. O
Relying O
on O
counter O
- O
evidence O
leaked O
from O
such O
reports O
is O
unrealistic O
if O
a O
system O
is O
to O
be O
applied O
to O
new O
claims O
. O

In O
this O
work O
, O
we O
identify O
gaps O
between O
current O
research O
on O
FCNLP B-MethodName
and O
the O
verification O
process O
of O
professional O
fact O
- O
checkers O
. O
Via O
analysis O
from O
different O
perspectives O
, O
we O
argue O
that O
the O
assumption O
of O
the O
existence O
of O
counter O
- O
evidence O
in O
FCNLP B-MethodName
is O
unrealistic O
and O
does O
not O
reflect O
real O
- O
world O
requirements O
. O
We O
hope O
our O
analysis O
sheds O
light O
on O
future O
research O
directions O
in O
automatic B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
. O
In O
summary O
, O
our O
major O
contributions O
are O
: O
• O
We O
identify O
two O
criteria O
from O
the O
journalistic O
verification O
process O
, O
which O
allow O
overcoming O
the O
reliance O
on O
counter O
- O
evidence O
( O
Section O
2 O
) O
. O

• O
We O
show O
that O
FCNLP B-MethodName
is O
incapable O
of O
satisfying O
these O
criteria O
, O
preventing O
the O
successful O
verification O
of O
most O
misinformation O
claims O
from O
the O
journalistic O
perspective O
( O
Section O
3 O
) O
. O

• O
We O
identify O
two O
evidence O
criteria O
( O
sufficient O
& O
unleaked O
) O
for O
realistic O
fact O
- O
checking O
. O
We O
find O
that O
all O
existing O
datasets O
in O
FCNLP B-MethodName
containing O
real O
- O
world O
misinformation O
violate O
at O
least O
one O
criterion O
( O
Section O
4 O
) O
and O
are O
hence O
unrealistic O
. O

• O
We O
semi O
- O
automatically O
analyze O
MULTIFC B-DatasetName
, O
a O
large O
- O
scale O
fact O
- O
checking O
dataset O
to O
support O
our O
findings O
, O
and O
show O
that O
models O
trained O
on O
claims O
from O
PolitiFact O
and O
Snopes O
( O
via O
MULTIFC B-DatasetName
) O
rely O
on O
leaked O
evidence O
. O

How O
Humans O
Fact O
- O
check O

To O
motivate O
our O
distinct O
focus O
on O
misinformation O
, O
we O
investigate O
what O
claims O
professional O
factcheckers O
verify O
. O
We O
crawl O
20,274 O
fact O
- O
checked O
claims O
from O
PolitiFact O
2 O
ranging O
from O
2007 O
- O
2021 O
. O

Figure O
2 O
shows O
the O
ratio O
of O
different O
verdicts O
3 O
per O
year O
. O
After O
2016 O
, O
fact O
- O
checkers O
increasingly O
select O
false O
claims O
as O
important O
for O
fact O
- O
checking O
. O
In O
2021 O
less O
than O
10 O
% O
of O
the O
selected O
claims O
were O
correct O
. O
Some O
claims O
can O
be O
refuted O
via O
counter O
- O
evidence O
( O
as O
required O
by O
FCNLP B-MethodName
) O
. O
For O
example O
, O
official O
statistics O
can O
contradict O
the O
false O
claim O
about O
the O
U.S. O
that O
" O
In O
the O
1980s O
, O
the O
lowest O
income O
people O
had O
the O
biggest O
gains O
" O
. O
If O
the O
evidence O
makes O
it O

Claim O
Based O
Upon O

( O
1 O
) O
If O
you O
were O
forced O
to O
use O
a O
Sharpie O
to O
fill O
out O
your O
ballot O
, O
that O
is O
voter O
fraud O
. O

false O
assumption O
( O
2 O
) O
The O
Biden O
administration O
will O
begin O
" O
spying O
" O
on O
bank O
and O
cash O
app O
accounts O
starting O
2022 O
. O

tax O
legislation O
( O
3 O
) O
Barcelona O
terrorist O
is O
cousins O
with O
former O
President O
Barack O
Obama O
. O

satire O
article O
( O
4 O
) O
The O
Democratic O
health O
care O
plan O
is O
a O
government O
takeover O
of O
our O
health O
programs O
. O
health O
care O
plan O
( O
5 O
) O
People O
in O
Holland O
protests O
against O
of O
COVID-19 O
measures O
. O

protests O
event O
impossible O
for O
the O
claim O
to O
be O
true O
( O
e.g. O
, O
because O
of O
mutually O
exclusive O
statistics O
) O
we O
refer O
to O
the O
evidence O
as O
global O
counter O
- O
evidence O
. O
Global O
counterevidence O
attacks O
the O
textual O
claim O
itself O
without O
relying O
on O
reasoning O
and O
sources O
behind O
it O
. O
In O
contrast O
, O
to O
refute O
the O
claim O
that O
" O
COVID-19 O
vaccines O
may O
kill O
sharks O
" O
( O
Figure O
1 O
) O
, O
fact O
- O
checkers O
did O
not O
rely O
on O
global O
counter O
- O
evidence O
specifically O
proofing O
that O
sharks O
will O
not O
be O
killed O
to O
produce O
COVID-19 O
vaccines O
. O
Neither O
is O
it O
plausible O
that O
such O
counter O
- O
evidence O
exists O
. O
Here O
, O
the O
counterevidence O
is O
bound O
to O
the O
claim O
's O
underlying O
( O
false O
) O
reasoning O
. O
The O
claim O
is O
only O
refuted O
because O
it O
follows O
the O
false O
assumption O
, O
not O
because O
it O
was O
disproved O
. O
The O
absence O
of O
global O
counter O
- O
evidence O
is O
not O
an O
exceptional O
problem O
for O
this O
specific O
claim O
but O
is O
common O
among O
misinformation O
: O
Misinformation O
surges O
when O
the O
high O
demand O
for O
information O
can O
not O
be O
met O
with O
a O
sufficient O
supply O
of O
credible O
answers O
( O
Silverman O
, O
2014 O
; O
FullFact O
, O
2020 O
) O
. O
Non O
- O
credible O
and O
possibly O
false O
and O
harmful O
information O
fill O
these O
deficits O
of O
credible O
information O
( O
Golebiewski O
and O
Boyd O
, O
2019 O
; O
Shane O
and O
Noel O
, O
2020 O
) O
. O
The O
very O
existence O
of O
misinformation O
often O
builds O
on O
the O
absence O
of O
credible O
counter O
- O
evidence O
, O
which O
in O
turn O
, O
is O
essential O
for O
FCNLP B-MethodName
. O

Professional O
fact O
- O
checkers O
refute O
misinformation O
even O
if O
no O
global O
counter O
- O
evidence O
exists O
, O
e.g. O
, O
by O
rebutting O
underlying O
assumptions O
( O
Figure O
1 O
) O
. O
Table O
1 O
shows O
a O
few O
false O
claims O
built O
on O
top O
of O
various O
resources O
: O
( O
1 O
) O
relies O
on O
a O
false O
assumption O
that O
sharpies O
invalidate O
election O
ballots O
, O
( O
2 O
& O
4 O
) O
misinterpret O
official O
documents O
or O
laws O
, O
( O
3 O
) O
is O
based O
on O
non O
- O
credible O
sources O
, O
and O
( O
5 O
) O
changes O
a O
topic O
of O
a O
specific O
event O
from O
" O
gas O
extraction O
" O
to O
" O
COVID-19 O
measures O
" O
. O
Fact O
- O
checkers O
use O
the O
reasoning O
for O
the O
claim O
to O
consider O
evidence O
that O
is O
, O
or O
refers O
to O
, O
the O
claimant O
's O
source O
: O
the O
original O
tax O
legislation O
( O
2 O
) O
, O
or O
alternate O
( O
correct O
) O
descriptions O
of O
protests O
against O
gas O
extraction O
( O
5 O
) O
. O
Here O
, O
the O
content O
of O
the O
evidence O
alone O
is O
often O
insufficient O
. O
The O
assertion O
that O
the O
claimant O
's O
source O
and O
the O
used O
counterevidence O
are O
identical O
, O
or O
refer O
to O
the O
same O
event O
is O
crucial O
to O
refute O
the O
claim O
: O
Claim O
( O
2 O
) O
is O
refuted O
because O
the O
tax O
legislation O
it O
relies O
upon O
does O
not O
support O
the O
" O
spying O
" O
claim O
. O
However O
, O
the O
document O
does O
not O
specifically O
refute O
the O
claim O
, O
and O
without O
knowing O
that O
the O
claimant O
relied O
on O
it O
, O
it O
becomes O
useless O
as O
counter O
- O
evidence O
. O
Similarly O
, O
the O
correct O
narrative O
of O
protests O
against O
gas O
extraction O
is O
only O
mutually O
exclusive O
to O
the O
false O
claim O
( O
5 O
) O
of O
protests O
against O
COVID-19 O
measures O
when O
assuring O
both O
refer O
to O
the O
identical O
incident O
. O
For O
similar O
reasons O
, O
the O
co O
- O
reference O
assumption O
is O
critical O
to O
the O
task O
definition O
of O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
. O
After O
this O
assertion O
, O
mutual O
exclusiveness O
is O
not O
required O
to O
refute O
the O
claim O
: O
It O
is O
sufficient O
if O
the O
claim O
is O
not O
entailed O
( O
i.e. O
incorrectly O
derived O
or O
relies O
on O
unverifiable O
speculations O
) O
or O
based O
on O
invalid O
sources O
( O
such O
as O
satire O
) O
to O
refute O
it O
. O
Based O
on O
these O
observations O
we O
identify O
two O
criteria O
to O
refute O
claims O
if O
no O
global O
- O
counter O
evidence O
exists O
. O
We O
validate O
their O
relevance O
in O
Section O
3 O
: O

• O
Source O
Guarantee O
: O
The O
guarantee O
that O
identified O
evidence O
either O
constitutes O
or O
refers O
to O
the O
claimant O
's O
reason O
for O
the O
claim O
. O

• O
Context O
Availability O
: O
We O
broadly O
consider O
context O
as O
the O
claim O
's O
original O
environment O
, O
which O
allows O
us O
to O
unambiguously O
comprehend O
the O
claim O
, O
and O
trace O
the O
claim O
and O
its O
sources O
across O
multiple O
platforms O
if O
required O
. O
It O
is O
a O
logical O
precondition O
for O
the O
source O
guarantee O
. O

Both O
criteria O
are O
challenging O
for O
computers O
but O
naturally O
satisfied O
by O
human O
fact O
- O
checkers O
. O
Buttry O
( O
2014 O
) O
defines O
the O
question O
" O
How O
do O
you O
know O
that O
? O
" O
to O
be O
at O
the O
heart O
of O
verification O
. O
After O
selecting O
a O
claim O
, O
finding O
provenance O
and O
sourcing O
are O
the O
first O
steps O
in O
journalistic O
verification O
. O
Provenance O
provides O
crucial O
information O
about O
context O
and O
motivation O
( O
Urbani O
, O
2020 O
) O
. O
Journalists O
must O
then O
identify O
solid O
sources O
to O
compare O
the O
claim O
with O
( O
Silverman O
, O
2014 O
; O
Borel O
, O
2016 O
) O
. O
Ideally O
, O
the O
claimant O
provides O
sources O
, O
which O
must O
be O
included O
and O
assessed O
in O
the O
verification O
process O
. O
During O
verification O
, O
journalists O
rely O
, O
if O
possible O
, O
on O
relevant O
primary O
sources O
, O
such O
as O
uninterpreted O
and O
original O
legislation O
documents O
( O
for O
claim O
2 O
, O
Table O
1 O
) O
. O
Factchecking O
organisations O
see O
sourcing O
as O
one O
of O
the O
most O
important O
parts O
of O
their O
work O
( O
Arnold O
, O
2020 O
) O
. O

3 O
Can O
FCNLP B-MethodName
Help O
Human O
Verification O
? O

In O
this O
section O
, O
we O
first O
analyze O
human O
verification O
strategies O
based O
on O
an O
analysis O
of O
100 O
misinformation O
claims O
. O
We O
then O
contrast O
human O
verification O
strategies O
with O
FCNLP B-MethodName
. O

Human O
Verification O
Strategies O

We O
manually O
analyze O
100 O
misinformation O
claims O
4 O
from O
two O
well O
- O
known O
fact O
- O
checking O
organizations O
: O
PolitiFact O
and O
Snopes O
. O
We O
randomly O
choose O
50 B-HyperparameterValue
misinformation O
claims O
from O
each O
website O
which O
contains O
25 O
claims O
from O
MULTIFC B-DatasetName
( O
a O
large O
NLP O
fact O
- O
checking O
dataset O
with O
real O
- O
world O
claims O
before O
2019 O
) O
and O
25 O
claims O
from O
2020 O
/ O
2021 O
. O
We O
extract O
the O
URL O
for O
each O
claim O
and O
analyze O
its O
verification O
strategy O
based O
on O
the O
entire O
fact O
- O
checking O
article O
. O
Claims O
that O
require O
the O
identification O
of O
scam O
webpages O
, O
imposter O
messages O
, O
or O
multi O
- O
modal O
reasoning O
5 O
such O
as O
detecting O
misrepresented O
, O
miscaptioned O
or O
manipulated O
images O
( O
Zlatkova O
et O
al O
. O
, O
2019 O
) O
were O
marked O
as O
not O
applicable O
to O
FCNLP B-MethodName
by O
nature O
. O
In O
the O
first O
round O
of O
analysis O
, O
we O
assess O
whether O
humans O
relied O
on O
the O
source O
guarantee O
to O
refute O
the O
claim O
. O
Each O
claim O
( O
and O
its O
verification O
) O
is O
unique O
and O
can O
be O
refuted O
using O
different O
strategies O
. O
In O
the O
second O
round O
of O
analysis O
we O
identify O
the O
primary O
strategy O
to O
refute O
the O
claim O
and O
verify O
that O
it O
is O
based O
on O
the O
source O
guarantee O
. O
This O
led O
us O
to O
identify O
4 O
primary O
human O
- O
verification O
strategies O
: O

1 O
. O
Global O
counter O
- O
evidence O
( O
GCE O
) O
: O
Counterevidence O
via O
arbitrarily O
complex O
reasoning O
but O
without O
the O
source O
guarantee O
. O
In O
one O
case O
( O
other O
) O
, O
fact O
- O
checkers O
relied O
entirely O
on O
expert O
statements O
. O
In O
general O
, O
experts O
supported O
the O
fact O
- O
checkers O
in O
identifying O
and O
discussing O
evidence O
, O
or O
strengthened O
their O
argument O
via O
statements O
but O
did O
not O
affect O
the O
underlying O
verification O
strategy O
. O

NLP B-MethodName
Fact I-MethodName
Verification I-MethodName

Focusing O
on O
evidence O
- O
based O
approaches O
. O
Approaches O
in O
FCNLP B-MethodName
estimate O
the O
claim O
's O
veracity O
based O
on O
surface O
cues O
within O
the O
claim O
( O
Rashkin O
et O
al O
. O
, O
2017 O
; O
Patwa O
et O
al O
. O
, O
2021 O
) O
, O
assisted O
with O
metadata O
( O
Wang O
, O
2017 O
; O
Cui O
and O
Lee O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020 O
; O
Dadgar O
and O
Ghatee O
, O
2021 O
) O
, O
or O
using O
evidence O
documents O
. O
Here O
, O
the O
system O
uses O
the O
stance O
of O
the O
evidence O
towards O
the O
claim O
to O
predict O
the O
verdict O
. O
Verdict O
labels O
are O
often O
non O
- O
binary O
and O
include O
a O
neutral O
stance O
, O
or O
finegrained O
veracity O
labels O
from O
fact O
- O
checking O
organizations O
( O
Augenstein O
et O
al O
. O
, O
2019 O
) O
. O
Evidence O
- O
based O
approaches O
either O
rely O
on O
unverified O
documents O
or O
user O
comments O
( O
Ferreira O
and O
Vlachos O
, O
2016 O
; O
Zubiaga O
et O
al O
. O
, O
2016 O
; O
Pomerleau O
and O
Rao O
, O
2017 O
) O
, O
or O
assume O
access O
to O
a O
presumed O
trusted O
knowledge O
base O
such O
as O
Wikipedia O
, O
scientific O
publications O
( O
Wadden O
et O
al O
. O
, O
2020 O
) O
, O
or O
search O
engine O
results O
( O
Augenstein O
et O
al O
. O
, O
2019 O
) O
. O
In O
this O
paper O
, O
we O
focus O
on O
trusted O
evidence O
- O
based O
verification O
approaches O
which O
can O
deal O
with O
the O
truth O
changing O
over O
time O
( O
Schuster O
et O
al O
. O
, O
2019 O
) O
. O
More O
importantly O
, O
they O
are O
the O
most O
representative O
of O
professional O
fact O
verification O
. O
Effectively O
debunking O
misinformation O
requires O
stating O
the O
corrected O
fact O
and O
explaining O
the O
myth O
's O
fallacy O
( O
Lewandowsky O
et O
al O
. O
, O
2020 O
) O
, O
both O
of O
which O
require O
trusted O
evidence O
. O

Global O
counter O
- O
evidence O
assumption O
in O
FCNLP B-MethodName
. O

In O
FCNLP B-MethodName
, O
evidence O
retrieval O
- O
based O
approaches O
assume O
that O
the O
semantic O
content O
of O
a O
claim O
is O
sufficient O
to O
find O
relevant O
( O
counter- O
) O
evidence O
in O
a O
trusted O
knowledge O
base O
Wadden O
et O
al O
. O
, O
2020 O
; O
Aly O
et O
al O
. O
, O
2021 O
) O
. O
This O
becomes O
problematic O
for O
misinformation O
that O
requires O
the O
source O
guarantee O
to O
refute O
the O
claim O
. O
By O
nature O
, O
in O
this O
case O
, O
the O
claim O
and O
evidence O
content O
are O
distinct O
and O
not O
entailing O
. O
Content O
can O
not O
assert O
that O
two O
different O
narratives O
describe O
the O
same O
protests O
( O
e.g. O
, O
Claim O
5 O
in O
Table O
1 O
) O
, O
or O
that O
a O
non O
- O
entailing O
fact O
( O
squalene O
is O
harvested O
from O
sharks O
) O
serves O
as O
a O
basis O
for O
the O
false O
claim O
( O
e.g. O
, O
Figure O
1 O
) O
. O
The O
consequence O
is O
a O
circular O
reasoning O
problem O
: O
Knowing O
that O
a O
claim O
is O
false O
is O
a O
precondition O
to O
establishing O
the O
source O
guarantee O
, O
which O
in O
turn O
is O
needed O
to O
refute O
the O
claim O
. O
To O
escape O
this O
cycle O
, O
one O
must O
( O
a O
) O
provide O
the O
source O
guarantee O
by O
other O
means O
than O
content O
( O
e.g. O
, O
context O
) O
, O
or O
( O
b O
) O
find O
evidence O
that O
refutes O
the O
claim O
without O
the O
source O
guarantee O
( O
global O
counter O
- O
evidence O
) O
. O
By O
relying O
only O
on O
the O
content O
of O
the O
claim O
, O
FCNLP B-MethodName
can O
not O
provide O
the O
source O
guarantee O
and O
is O
limited O
to O
global O
counter O
- O
evidence O
, O
which O
only O
accounts O
for O
20 O
% O
of O
misinformation O
claims O
analyzed O
in O
the O
previous O
section O
. O

Current O
FCNLP B-MethodName
fails O
to O
provide O
source O
guarantees O
. O
We O
note O
that O
providing O
the O
source O
guarantee O
goes O
beyond O
entity O
disambiguation O
, O
as O
required O
in O
FEVER B-DatasetName
. O
The O
self O
- O
contained O
context O
within O
claims O
in O
FEVER B-DatasetName
is O
typically O
sufficient O
to O
disambiguate O
named O
entities O
if O
required O
. O
6 O
After O
disambiguation O
, O
the O
retrieved O
evidence O
serves O
as O
global O
counter O
- O
evidence O
. O

Recent O
approaches O
further O
add O
context O
snippets O
from O
Wikipedia O
( O
Sathe O
et O
al O
. O
, O
2020 O
) O
or O
dialogues O
( O
Gupta O
et O
al O
. O
, O
2022 O
) O
to O
resolve O
ambiguities O
and O
can O
not O
provide O
the O
source O
guarantee O
to O
break O
the O
circu O
- O
lar O
reasoning O
problem O
. O
These O
snippets O
differ O
from O
the O
context O
used O
by O
professional O
fact O
- O
checkers O
who O
often O
need O
to O
trace O
claims O
and O
their O
sources O
across O
different O
platforms O
. O
Recently O
, O
Thorne O
et O
al O
. O
( O
2021 O
) O
annotate O
more O
realistic O
claims O
w.r.t O
. O
multiple O
evidence O
passages O
. O
They O
found O
supporting O
and O
refuting O
passages O
for O
the O
same O
claim O
, O
which O
prevents O
the O
prediction O
of O
an O
overall O
verdict O
. O
Some O
works O
collect O
evidence O
for O
the O
respective O
claims O
by O
identifying O
scenarios O
where O
the O
claimant O
's O
source O
is O
naturally O
provided O
: O
such O
as O
a O
strictly O
moderated O
forum O
( O
Saakyan O
et O
al O
. O
, O
2021 O
) O
, O
scientific O
publications O
( O
Wadden O
et O
al O
. O
, O
2020 O
) O
, O
or O
Wikipedia O
references O
( O
Sathe O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
such O
source O
evidence O
is O
only O
collected O
for O
true O
claims O
. O
Adhering O
to O
the O
global O
counter O
- O
evidence O
assumptions O
of O
previous O
work O
, O
false O
claims O
in O
these O
works O
are O
generated O
artificially O
and O
do O
not O
reflect O
real O
- O
world O
misinformation O
. O

Human O
and O
NLP O
Comparison O

Our O
analysis O
( O
Table O
2 O
) O
finds O
fact O
- O
checkers O
only O
refuted O
26 O
% O
of O
false O
claims O
with O
global O
counterevidence O
. O
In O
all O
other O
cases O
, O
fact O
- O
checkers O
relied O
on O
source O
guarantees O
( O
LCE O
, O
NCS O
) O
or O
asserted O
that O
no O
supporting O
evidence O
exists O
( O
NEA O
) O
. O
The O
verification O
strategy O
is O
not O
evident O
given O
the O
claim O
alone O
but O
dependent O
on O
existing O
evidence O
. O
The O
claim O
that O
" O
President O
Barack O
Obama O
's O
policies O
have O
forced O
many O
parts O
of O
the O
country O
to O
experience O
rolling O
blackouts O
" O
is O
refuted O
via O
global O
counter O
- O
evidence O
( O
that O
rolling O
blackouts O
had O
natural O
causes O
) O
. O
The O
claim O
that O
" O
90 O
% O
of O
rural O
women O
and O
55 O
% O
of O
all O
women O
are O
illiterate O
in O
Morocco O
" O
seems O
verifiable O
via O
official O
statistics O
. O
Yet O
, O
no O
comparable O
statistics O
exist O
and O
the O
claim O
is O
refuted O
due O
to O
relying O
on O
a O
decade O
- O
old O
USAID O
request O
report O
. O

We O
further O
analyze O
claims O
refuted O
via O
global O
counter O
- O
evidence O
, O
that O
FCNLP B-MethodName
, O
in O
theory O
, O
can O
refute O
. O
Some O
claims O
only O
require O
shallow O
reasoning O
as O
directly O
contradicting O
evidence O
naturally O
exists O
: O
A O
transcript O
of O
an O
interview O
in O
which O
Ron O
DeSantis O
was O
asked O
about O
the O
coronavirus O
can O
easily O
refute O
the O
claim O
" O
Ron O
DeSantis O
was O
never O
asked O
about O
coronavirus O
" O
. O
Another O
case O
is O
when O
information O
about O
the O
claim O
's O
veracity O
already O
exists O
, O
e.g. O
, O
because O
those O
affected O
by O
the O
myth O
already O
corrected O
the O
claim O
. O
Most O
claims O
require O
complex O
reasoning O
like O
legal O
text O
understanding O
or O
comparing O
and O
deriving O
statistics O
. O
Some O
claims O
require O
the O
definition O
of O
some O
terms O
first O
, O
to O
make O
them O
verifiable O
. O
Col- O
lecting O
all O
required O
global O
counter O
- O
evidence O
often O
requires O
aggregating O
and O
comparing O
different O
information O
, O
possibly O
under O
time O
constraints O
. O
Consider O
the O
false O
claim O
that O
" O
Illegal O
immigration O
was O
n't O
a O
subject O
that O
was O
on O
anybody O
's O
mind O
until O
[ O
Trump O
] O
brought O
it O
up O
at O
[ O
his O
] O
announcement O
" O
: O
To O
refute O
this O
claim O
, O
one O
must O
first O
determine O
when O
Trump O
announced O
his O
run O
for O
the O
presidency O
, O
then O
count O
and O
compare O
how O
often O
" O
illegal O
immigration O
" O
was O
mentioned O
before O
and O
after O
the O
announcement O
. O
7 O

NLP B-TaskName
Fact I-TaskName
- I-TaskName
Checking I-TaskName
Datasets O

Based O
on O
our O
observations O
in O
Section O
3 O
and O
FC B-MethodName
- I-MethodName
NLP I-MethodName
's O
reliance O
on O
global O
counter O
- O
evidence O
, O
we O
hypothesize O
that O
evidence O
in O
existing O
fact O
- O
checking O
datasets O
does O
not O
fully O
satisfy O
real O
- O
world O
demands O
. O
We O
, O
hence O
, O
investigate O
how O
FCNLP B-MethodName
's O
assumptions O
affect O
fact O
- O
checking O
datasets O
and O
if O
they O
constitute O
realistic O
test O
beds O
for O
real O
- O
world O
misinformation O
. O

For O
real O
- O
world O
scenarios O
, O
datasets O
must O
contain O
realworld O
misinformation O
claims O
and O
realistic O
counterevidence O
. O
For O
evidence O
, O
we O
define O
the O
following O
two O
requirements O
: O

• O
Sufficient O
: O
Evidence O
must O
be O
sufficient O
to O
justify O
the O
verdict O
from O
a O
human O
perspective O
. O

• O
Unleaked O
: O
Evidence O
must O
not O
contain O
information O
that O
only O
existed O
after O
the O
claim O
was O
verified O
. O

The O
issue O
of O
leaked O
evidence O
was O
also O
mentioned O
very O
recently O
by O
Khan O
et O
al O
. O
( O
2022 O
) O
. O
Unlike O
us O
, O
they O
did O
not O
comprehensively O
analyze O
existing O
datasets O
, O
evaluate O
the O
impact O
on O
trained O
systems O
( O
Section O
5 O
) O
, O
or O
consider O
the O
complementary O
criterion O
of O
sufficient O
( O
counter- O
) O
evidence O
. O
Relying O
on O
leaked O
evidence O
is O
related O
to O
the O
important O
yet O
different O
task O
of O
detecting O
already O
- O
verified O
claims O
( O
Shaar O
et O
al O
. O
, O
2020 O
) O
, O
but O
is O
unrealistic O
for O
novel O
claims O
. O

We O
survey O
NLP B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
datasets O
with O
natural O
input O
claims O
8 O
that O
assume O
access O
to O
trusted O
evidence O
. O
Table O
3 O
summarizes O
our O
survey O
results O
. O
Datasets O
1 O
- O
6 O
contain O
no O
real O
- O
world O
misinformation O
: O
False O
claims O
are O
derived O
from O
true O
real O
- O
world O
claims O
( O
1 O
- O
3 O
) O
or O
within O
a O
gamified O
setting O
( O
4 O
) O
, O
ensuring O
that O
counter O
- O
evidence O
exists O
. O
Other O
works O
( O
5 O
& O
6 O
) O
reformulate O
real O
- O
world O
user O
queries O
, O
which O
are O
linked O
to O
Wikipedia O
articles O
as O
( O
counter- O
) O
evidence O
. O

We O
find O
that O
no O
dataset O
with O
real O
- O
world O
misinformation O
( O
7 O
- O
16 O
) O
satisfies O
both O
evidence O
criteria O
. O
We O
identify O
four O
categories O
: O
First O
, O
datasets O
that O
consider O
( O
parts O
of O
) O
a O
fact O
- O
checking O
article O
as O
evidence O
contain O
sufficient O
, O
yet O
leaked O
evidence O
( O
7 O
& O
8 O
) O
. O
Second O
, O
annotators O
estimate O
claim O
veracity O
based O
on O
evidence O
such O
as O
Wikipedia O
or O
scientific O
publications O
. O
The O
authors O
find O
that O
evidence O
often O
only O
covers O
parts O
of O
these O
realistic O
, O
complex O
claims O
, O
which O
yield O
low O
annotator O
agreement O
( O
9 O
) O
, O
or O
a O
weakened O
task O
definition O
for O
stances O
( O
10 O
) O
. O

Third O
is O
to O
rely O
on O
the O
same O
evidence O
as O
fact- O
checkers O
, O
termed O
premise O
evidence O
by O
Khan O
et O
al O
. O
( O
2022 O
) O
. O
Here O
, O
only O
UKP B-DatasetName
- I-DatasetName
SNOPES I-DatasetName
( O
11 O
) O
provides O
evidence O
annotations O
. O
Hanselowski O
et O
al O
. O
( O
2019 O
) O
collect O
and O
annotate O
original O
evidence O
snippets O
from O
the O
fact O
- O
checking O
article O
. O
They O
find O
the O
stance O
often O
conflicts O
with O
the O
verdict O
: O
Though O
most O
claims O
are O
false O
, O
the O
majority O
of O
evidence O
is O
supporting O
. O

In O
45.5 O
% O
of O
cases O
, O
annotators O
found O
no O
stance O
for O
the O
professionally O
selected O
evidence O
snippets O
even O
though O
professional O
fact O
- O
checkers O
considered O
these O
snippets O
important O
to O
be O
included O
in O
the O
article O
. O
Due O
to O
conflicting O
and O
unexplained O
evidence O
snippets O
, O
we O
rate O
this O
insufficient O
to O
predict O
the O
correct O
verdict O
. O

The O
human O
verification O
process O
( O
Section O
2 O
) O
guides O
the O
creation O
of O
the O
fact O
- O
checking O
article O
and O
can O
serve O
as O
a O
possible O
explanation O
for O
these O
problems O
. O

Articles O
link O
to O
the O
claim O
's O
context O
and O
possibly O
other O
similar O
claims O
( O
likely O
supporting O
) O
. O
Often O
( O
e.g. O
during O
COVID-19 O
( O
Simon O
et O
al O
. O
, O
2020 O
) O
) O
, O
claims O
are O
not O
completely O
fabricated O
. O
Fact O
- O
checkers O
identify O
documents O
and O
their O
interdependence O
when O
investigating O
the O
claimant O
's O
reasoning O
for O
the O
claim O
( O
likely O
not O
refuting O
) O
. O
Documents O
used O
to O
disprove O
the O
claimant O
's O
reasoning O
may O
have O
no O
or O
little O
relevance O
to O
the O
original O
claim O
( O
as O
in O
Figure O
1 O
) O
. O
Each O
step O
is O
non O
- O
trivial O
and O
may O
rely O
on O
numerous O
documents O
( O
or O
expert O
statements O
) O
. O
Relying O
on O
premise O
evidence O
without O
considering O
the O
verification O
process O
and O
how O
these O
documents O
relate O
, O
is O
insufficient O
. O
Both O
other O
datasets O
( O
12 O
& O
13 O
) O
in O
this O
category O
provide O
no O
annotations O
and O
are O
limited O
to O
freely O
available O
evidence O
documents O
( O
as O
opposed O
to O
paywalled O
web O
pages O
or O
e O
- O
mails O
) O
. O

Fourth O
is O
using O
a O
search O
engine O
during O
dataset O
construction O
to O
expand O
the O
accessible O
knowledge O
. O
Even O
when O
excluding O
search O
results O
that O
point O
to O
the O
claim O
's O
fact O
- O
checking O
article O
, O
leaked O
evidence O
persists O
: O
Different O
organizations O
may O
verify O
the O
same O
claims O
, O
or O
disseminate O
the O
fact O
- O
checkers O
verification O
. O
Only O
Baly O
et O
al O
. O
( O
2018 O
) O
provide O
stance O
annotation O
for O
Arabic O
claim O
and O
evidence O
pairs O
. O
For O
false O
claims O
, O
they O
found O
that O
only O
a O
few O
documents O
disagree O
, O
and O
more O
agree O
, O
with O
the O
claim O
. O
A O
possible O
explanation O
is O
that O
misinformation O
often O
emerges O
when O
trusted O
information O
or O
counterevidence O
is O
scarce O
. O
Fact O
- O
checking O
articles O
fill O
this O
deficit O
. O
Partially O
excluding O
them O
during O
dataset O
generation O
reduces O
the O
found O
counter O
- O
evidence O
. O
Lacking O
counter O
- O
evidence O
is O
not O
a O
problem O
of O
the O
dataset O
generation O
, O
but O
the O
underlying O
nature O
of O
misinformation O
, O
and O
should O
be O
considered O
by O
the O
task O
definition O
. O
We O
rate O
evidence O
in O
this O
category O
( O
14 O
- O
16 O
) O
leaked O
and O
insufficient O
, O
and O
back O
it O
up O
in O
Section O
5 O
. O

A O
Case O
Study O
of O
Leaked O
Evidence O

We O
view O
MULTIFC B-DatasetName
( O
Augenstein O
et O
al O
. O
, O
2019 O
) O
, O
the O
largest O
dataset O
of O
its O
group O
, O
as O
an O
instantiation O
of O
FCNLP B-MethodName
applied O
to O
the O
real O
world O
: O
It O
contains O
realworld O
claims O
and O
professionally O
assessed O
verdicts O
as O
labels O
from O
26 O
fact O
- O
checking O
organizations O
( O
like O
Snopes O
or O
PolitiFact O
) O
. O
The O
authors O
use O
the O
Google O
search O
engine O
to O
expand O
evidence O
retrieval O
to O
the O
real O
world O
during O
dataset O
construction O
. O
We O
abstract O
from O
the O
fact O
that O
MULTIFC B-DatasetName
only O
provides O
incomplete O
evidence O
snippets O
and O
consider O
( O
if O
possible O
) O
the O
underlying O
article O
in O
its O
entirety O
. O

Quantification O
of O
Leaked O
Evidence O

We O
focus O
our O
analysis O
on O
16,244 O
misinformation O
claims O
that O
we O
identify O
via O
misinformation O
labels O
( O
listed O
in O
Appendix O
B.1 O
) O
. O
To O
quantify O
how O
many O
claims O
in O
MULTIFC B-DatasetName
contain O
leaked O
evidence O
, O
we O
consider O
all O
evidence O
snippets O
stemming O
from O
a O
fact O
- O
checking O
article O
, O
or O
discussing O
the O
veracity O
of O
a O
claim O
, O
as O
leaked O
. O
Table O
4 O
shows O
examples O
of O
leaked O
evidence O
that O
strongly O
indicates O
the O
claim O
's O
verdict O
. O
The O
first O
snippet O
comes O
directly O
from O
a O
factchecking O
organization O
( O
Truth O
Or O
Fiction O
9 O
) O
. O
Only O
identifying O
leaked O
evidence O
that O
directly O
comes O
from O
fact O
- O
checking O
organizations O
is O
insufficient O
: O
After O
the O
publication O
of O
the O
verification O
report O
, O
its O
content O
is O
disseminated O
via O
other O
publishers O
( O
such O
as O
Pinterest O
in O
the O
second O
example O
) O
. O
We O
identify O
leaked O
evidence O
snippets O
using O
patterns O
for O
their O
source O
URLs O
or O
contained O
phrases O
. O
A O
complete O
list O
of O
all O
used O
patterns O
is O
given O
in O
Appendix O
B.2 O
) O
. O
This O
requires O
the O
evidence O
to O
be O
relevant O
. O
Irrelevant O
articles O
are O
insufficient O
, O
albeit O
not O
leaking O
. O
To O
this O
end O
, O
we O
manually O
analyze O
100 B-HyperparameterValue
claims O
with O
230 O
automatically O
found O
leaking O
evidence O
snippets O
. O
We O
confirm O
that O
83.9 O
% O
of O
the O
snippets O
are O
leaked O
( O
details O
in O
Appendix O
B.3 O
) O
. O
97 O
/ O
100 O
of O
the O
selected O
claims O
contain O
at O
least O
one O
leaked O
evidence O
snippet O
. O
Table O
5 O
lists O
the O
number O
of O
claims O
with O
leaked O
evidence O
identified O
by O
the O
pattern O
- O
based O
approach O
. O
It O
detected O
leaked O
evidence O
for O
69.7 O
% O
of O
misinformation O
claims O
. O
In O
addition O
, O
we O
manually O
analyze O
evidence O
of O
100 B-DatasetName
misinformation O
claims O
for O
which O
this O
approach O
found O
no O
leaked O
evidence O
. O
Misinformation O
verification O
often O
requires O
multiple O
evidence O
documents O
, O
rendering O
a O
single O
sufficient O
evidence O
snippet O
unrealistic O
. O
We O
follow O
Sarrouti O
et O
al O
. O
( O
2021 O
) O
and O
test O
if O
a O
snippet O
supports O
or O
refutes O
parts O
of O
the O
claim O
. O
Table O
6 O
shows O
that O
approximately O
onethird O
of O
the O
claims O
contain O
further O
leaked O
evidence O
. O
15 O
claims O
have O
unleaked O
refuting O
evidence O
. O
In O
10 O
cases O
this O
evidence O
is O
overshadowed O
via O
leaked O
evidence O
for O
the O
same O
claim O
. O
Most O
analyzed O
claims O
only O
have O
non O
- O
refuting O
evidence O
. O
Similar O
to O
Baly O
et O
al O
. O
( O
2018 O
) O
, O
we O
found O
supporting O
evidence O
for O
40 O
misinformation O
claims O
; O
for O
35 O
of O
these O
claims O
, O
the O
evidence O
was O
misinformation O
and O
thus O
supported O
the O
claim O
; O
for O
the O
remaining O
five O
claims O
, O
the O
claim O
became O
accurate O
, O
and O
the O
evidence O
became O
avail- O
able O
at O
a O
date O
later O
than O
the O
claim O
's O
creation O
. O

Impact O
on O
Trained O
Systems O

Hansen O
et O
al O
. O
( O
2021 O
) O
found O
that O
models O
in O
MUL B-DatasetName
- I-DatasetName
TIFC I-DatasetName
can O
predict O
the O
correct O
verdict O
based O
on O
the O
evidence O
snippets O
alone O
. O
To O
test O
if O
leaked O
evidence O
can O
serve O
as O
an O
explanation O
, O
we O
fine O
- O
tune O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
( O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
) O
to O
predict O
the O
veracity O
label O
of O
a O
claim O
given O
the O
evidence O
snippets O
with O
and O
without O
a O
claim O
. O
As O
input O
to O
BERT B-MethodName
, O
we O
separate O
the O
claim O
( O
when O
used O
) O
from O
the O
evidence O
snippets O
using O
a O
[ O
SEP O
] O
token O
and O
predict O
the O
veracity O
label O
based O
on O
a O
linear O
layer O
on O
top O
of O
a O
preceding O
[ O
CLS O
] O
token O
( O
Training O
details O
in O
Appendix O
C.1 O
. O
) O
. O
When O
each O
evidence O
snippet O
is O
represented O
via O
its O
content O
only O
this O
performs O
on O
par O
with O
the O
specialized O
model O
introduced O
by O
Hansen O
et O
al O
. O
( O
2021 O
) O
. O
We O
additionally O
find O
that O
the O
snippet O
's O
title O
carries O
much O
signal O
, O
and O
adding O
it O
to O
the O
input O
improves O
the O
overall O
performance O
on O
Poli O
- O
tiFact O
. O
Snippets O
are O
concatenated O
( O
separated O
by O
" O
; O
" O
in O
the O
order O
provided O
by O
MULTIFC B-DatasetName
and O
truncated O
after O
512 B-HyperparameterValue
tokens O
. O
We O
experiment O
on O
the O
train- O
, O
devand O
test O
- O
splits O
Hansen O
et O
al O
. O
( O
2021 O
) O
extracted O
from O
MULTIFC B-DatasetName
on O
claims O
from O
Snopes O
and O
PolitiFact O
. O
We O
test O
four O
types O
of O
input O
: O
only O
evidence O
( O
only O
title O
, O
only O
text O
, O
both O
) O
, O
or O
the O
complete O
sample O
of O
claim O
and O
evidence O
. O
For O
the O
evaluation O
( O
Table O
7 O
) O
, O
we O
split O
the O
test O
data O
based O
on O
whether O
a O
claim O
contains O
automatically O
identified O
leaked O
evidence O
. O
On O
Snopes O
, O
the O
macro B-MetricName
- I-MetricName
F1 I-MetricName
is O
higher O
on O
the O
unleaked O
than O
on O
the O
leaked O
subset O
. O
Upon O
closer O
inspection O
, O
we O
find O
that O
the O
label O
distribution O
on O
Snopes O
is O
heavily O
skewed O
towards O
" O
false O
" O
, O
which O
worsens O
on O
the O
leaked O
subset O
. O
Models O
seem O
to O
rely O
on O
patterns O
of O
leaked O
evidence O
to O
predict O
the O
majority O
label O
" O
false O
" O
( O
see O
Appendix O
C.2 O
) O
. O
On O
the O
leaked O
subset O
, O
this O
comes O
at O
the O
cost O
of O
incorrect O
predictions O
for O
all O
other O
labels O
, O
yielding O
a O
lower O
F1 B-MetricName
- I-MetricName
macro I-MetricName
. O
On O
the O
larger O
PolitiFact O
subset O
, O
labels O
are O
not O
much O
skewed O
towards O
a O
single O
majority O
label O
. O
Across O
all O
experiments O
, O
the O
performance O
gap O
signals O
the O
reliance O
on O
leaked O
evidence O
. O
We O
confirm O
the O
impact O
of O
leaked O
evidence O
for O
both O
datasets O
by O
evaluating O
the O
model O
on O
the O
same O
instances O
with O
leaked O
or O
unleaked O
evidence O
, O
to O
avoid O
the O
different O
label O
distribution O
distorting O
the O
results O
( O
Appendix O
C.3 O
) O
. O

Related O
Work O

Combat O
Misinformation O
After O
Its O
Verification O
. O
The O
identified O
limitations O
of O
the O
previous O
studies O
on O
NLP B-TaskName
fact I-TaskName
- I-TaskName
checking I-TaskName
datasets O
described O
in O
Section O
4 O
do O
not O
devalue O
the O
surveyed O
datasets O
and O
we O
view O
them O
as O
highly O
important O
and O
useful O
contributions O
. O
These O
limitations O
are O
tied O
to O
our O
specific O
research O
question O
to O
refute O
novel O
real O
- O
world O
misinformation O
. O
We O
strongly O
build O
on O
these O
previous O
works O
and O
view O
them O
as O
crucial O
starting O
points O
to O
fact O
- O
check O
real O
- O
world O
misinformation O
. O
Existing O
fact O
- O
checking O
articles O
are O
highly O
valuable O
and O
automatic O
methods O
should O
utilize O
them O
to O
detect O
and O
combat O
misinformation O
. O
Automatic O
methods O
specifically O
using O
these O
resources O
detect O
misinformation O
by O
matching O
claims O
with O
known O
misconceptions O
( O
Hossain O
et O
al O
. O
, O
2020 O
; O
Weinzierl O
and O
Harabagiu O
, O
2022 O
) O
or O
already O
verified O
claims O
( O
Vo O
and O
Lee O
, O
2020 O
; O
Shaar O
et O
al O
. O
, O
2020 O
; O
Martín O
et O
al O
. O
, O
2022 O
; O
Hardalov O
et O
al O
. O
, O
2022b O
) O
. O

Surveys O
on O
Automatic B-TaskName
Fact I-TaskName
- I-TaskName
Checking I-TaskName
. O
Recent O
work O
surveyed O
( O
aspects O
of O
) O
automated B-TaskName
factchecking I-TaskName
and O
related O
tasks O
, O
including O
explainability O
( O
Kotonya O
and O
Toni O
, O
2020a O
) O
, O
stance O
classification O
( O
Küçük O
and O
Can O
, O
2020 O
; O
Hardalov O
et O
al O
. O
, O
2022a O
) O
, O
propaganda O
detection O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2021 O
) O
, O
rumor O
detection O
on O
social O
media O
( O
Zubiaga O
et O
al O
. O
, O
2018 O
; O
Islam O
et O
al O
. O
, O
2020 O
) O
, O
fake O
- O
news O
detection O
( O
Oshikawa O
et O
al O
. O
, O
2020 O
; O
Zhou O
and O
Zafarani O
, O
2020 O
) O
, O
and O
automated O
fact O
- O
checking O
. O
We O
refer O
interested O
readers O
to O
these O
papers O
. O
Guo O
et O
al O
. O
( O
2022 O
) O
surveyed O
the O
state O
of O
automatic O
fact O
- O
checking O
. O
Based O
on O
their O
work O
, O
we O
zoom O
in O
on O
real O
- O
world O
misinformation O
to O
investigate O
the O
gap O
between O
professional O
fact O
- O
checkers O
and O
FCNLP B-MethodName
. O
Recently O
, O
Nakov O
et O
al O
. O
( O
2021 O
) O
surveyed O
tasks O
to O
assist O
humans O
during O
the O
verification O
. O
Our O
work O
differs O
in O
that O
we O
focus O
solely O
on O
the O
automatic O
verification O
approach O
of O
misinformation O
. O
They O
argue O
for O
the O
need O
for O
automatic O
tools O
to O
support O
humans O
during O
verification O
. O
Similarly O
, O
Graves O
( O
2018 O
) O
interviewed O
expert O
fact O
- O
checkers O
and O
computer O
scientists O
and O
conclude O
, O
that O
automatic O
fact O
- O
checking O
can O
not O
replicate O
professional O
fact O
- O
checkers O
in O
the O
foreseeable O
future O
. O
Our O
results O
confirm O
the O
challenging O
nature O
of O
misinformation O
but O
also O
outline O
why O
current O
models O
have O
unrealistic O
expectations O
, O
and O
how O
humans O
overcome O
these O
problems O
. O
We O
believe O
this O
to O
be O
important O
as O
real O
- O
world O
misinformation O
is O
well O
within O
the O
scope O
of O
current O
NLP O
research O
. O
Towards O
Human O
Verification O
. O
A O
possible O
path O
forward O
is O
to O
align O
automatic O
verification O
with O
journalistic O
verification O
: O
Use O
the O
claimant O
's O
reasoning O
to O
find O
evidence O
and O
verify O
the O
claim O
. O
This O
relies O
on O
the O
complex O
task O
of O
finding O
the O
correct O
sources O
( O
Arnold O
, O
2020 O
) O
. O
A O
fruitful O
but O
understudied O
direction O
may O
be O
automated O
provenance O
detection O
( O
Zhang O
et O
al O
. O
, O
2020 O
. O
Building O
systems O
that O
can O
provide O
source O
guarantees O
paves O
the O
way O
for O
reasoning O
tasks O
, O
such O
as O
the O
detection O
of O
logical O
fallacies O
( O
Jin O
et O
al O
. O
, O
2022 O
) O
, O
implicit O
warrants O
( O
Habernal O
et O
al O
. O
, O
2018 O
) O
, O
or O
propaganda O
techniques O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
; O
Huang O
et O
al O
. O
, O
2022 O
) O
. O
Integrating O
sufficient O
context O
into O
datasets O
is O
nontrivial O
and O
may O
require O
tracing O
a O
claim O
and O
its O
source O
across O
multiple O
platforms O
. O
Existing O
literature O
shows O
the O
heterogeneity O
of O
misinformation O
( O
Borel O
, O
2016 O
; O
Wardle O
et O
al O
. O
, O
2017 O
; O
Cook O
, O
2020 O
) O
and O
can O
help O
to O
identify O
small O
, O
focused O
problems O
that O
can O
realistically O
be O
translated O
into O
NLP O
. O
Approaches O
from O
computer O
vision O
focus O
on O
misinformationspecific O
approaches O
to O
detect O
manipulated O
or O
misrepresented O
images O
( O
Zlatkova O
et O
al O
. O
, O
2019 O
; O
Abdelnabi O
et O
al O
. O
, O
2022 O
; O
Musi O
and O
Rocci O
, O
2022 O
) O
. O

Conclusion O

In O
this O
work O
, O
we O
contrasted O
NLP B-HyperparameterName
fact I-HyperparameterName
- I-HyperparameterName
checking I-HyperparameterName
approaches O
with O
how O
professional O
fact O
- O
checkers O
combat O
misinformation O
. O
We O
identified O
that O
reliance O
on O
counter O
- O
evidence O
hinders O
current O
fact O
- O
checking O
systems O
to O
refute O
real O
- O
world O
misinformation O
. O
Using O
MULTIFC B-DatasetName
we O
find O
that O
most O
evidence O
is O
insufficient O
, O
or O
leaked O
and O
exploited O
by O
trained O
models O
. O
Moving O
forward O
, O
we O
suggest O
to O
align O
NLP O
approaches O
with O
the O
human O
verification O
process O
, O
and O
task O
definitions O
with O
smaller O
and O
well O
- O
defined O
verification O
strategies O
. O

Limitations O

The O
scope O
of O
this O
study O
is O
restricted O
to O
misinformation O
claims O
, O
and O
their O
representation O
as O
textual O
statements O
, O
that O
professional O
fact O
- O
checking O
organizations O
selected O
as O
important O
to O
verify O
. O
This O
only O
represents O
a O
fraction O
of O
all O
existing O
misinformation O
( O
Vinhas O
and O
Bastos O
, O
2022 O
) O
. O
Our O
findings O
can O
not O
be O
generalized O
to O
other O
types O
of O
misinformation O
. O
Process O
definitions O
for O
claim O
selection O
and O
verification O
differ O
amongst O
fact O
- O
checkers O
( O
Arnold O
, O
2020 O
) O
. O
The O
assessed O
claims O
for O
the O
analysis O
and O
experiments O
are O
biased O
to O
the O
claim O
selection O
criteria O
, O
including O
the O
domain O
, O
language O
, O
and O
geographical O
biases O
of O
Snopes O
and O
PolitiFact O
. O
Even O
fact O
- O
checkers O
can O
not O
fully O
eliminate O
subjectivity O
. O
Nieminen O
and O
Sankari O
( O
2021 O
) O
find O
11 O
% O
PolitiFact O
's O
verified O
claims O
uncheckable O
. O
We O
consider O
the O
factcheckers O
assessment O
as O
the O
gold O
standard O
and O
adhere O
to O
the O
introduced O
subjectivity O
. O
PolitiFact O
and O
Snopes O
verify O
claims O
from O
English O
- O
speaking O
countries O
with O
rich O
resources O
and O
trusted O
government O
documents O
. O
Fact O
- O
checking O
organizations O
may O
rely O
on O
different O
strategies O
, O
adapted O
to O
different O
scenarios O
such O
as O
different O
topics O
, O
dissemination O
of O
misinformation O
, O
or O
trust O
and O
availability O
of O
official O
information O
. O
10 O
The O
quantification O
of O
leaked O
evidence O
is O
bound O
to O
the O
time O
- O
frame O
, O
fact O
- O
checking O
organizations O
, O
and O
found O
evidence O
of O
MULTIFC B-DatasetName
. O
We O
did O
not O
investigate O
the O
influence O
of O
different O
factors O
such O
as O
the O
fact O
- O
checkers O
language O
, O
domain O
, O
or O
popularity O
, O
nor O
did O
we O
evaluate O
different O
evidence O
collection O
strategies O
. O
The O
same O
restrictions O
apply O
to O
the O
experimental O
results O
. O
Further O
, O
following O
Hansen O
et O
al O
. O
( O
2021 O
) O
we O
only O
consider O
labels O
on O
a O
veracity O
scale O
for O
the O
experiments O
( O
e.g. O
excluding O
" O
misleading O
" O
) O
. O

Ethics O
Statement O

In O
this O
work O
we O
only O
consider O
publicly O
available O
data O
as O
provided O
by O
fact O
- O
checking O
organizations O
or O
MULTIFC B-DatasetName
, O
but O
do O
not O
publish O
it O
. O
We O
do O
not O
use O
any O
personal O
data O
. O
We O
note O
that O
creating O
more O
realistic O
datasets O
( O
including O
realistic O
context O
) O
, O
as O
suggested O
by O
us O
, O
induces O
ethical O
challenges O
as O
it O
requires O
personalized O
data O
( O
e.g. O
from O
Twitter O
or O
Facebook O
) O
. O
We O
consider O
this O
study O
's O
goal O
to O
reduce O
harmful O
misinformation O
by O
aligning O
automatic O
methods O
with O
best O
- O
practices O
from O
professional O
fact O
- O
checkers O
as O
ethically O
correct O
. O
However O
, O
even O
if O
successfully O
developed O
, O
fact O
- O
checking O
systems O
are O
inevitably O
imperfect O
. O
Malicious O
actors O
may O
design O
claims O
that O
exploit O
the O
system O
's O
weakness O
to O
predict O
the O
opposite O
verdict O
, O
giving O
legitimacy O
to O
false O
claims O
, O
or O
discrediting O
correct O
claims O
. O
Further O
, O
malicious O
actors O
may O
develop O
fact O
- O
checking O
systems O
under O
their O
control O
. O
When O
extended O
with O
triggers O
enabling O
backdoor O
attacks O
( O
Chen O
et O
al O
. O
, O
2021 O
) O
6 O
) O
; O
aggregating O
distinct O
donations O
( O
7 O
) O
; O
finding O
and O
contextualizing O
multiple O
events O
, O
including O
deportation O
and O
imprisonment O
of O
a O
murderer O
, O
and O
aligning O
these O
events O
with O
the O
political O
leadership O
in O
the O
U.S. O
( O
8 O
) O
; O
Claims O
( O
9 O
- O
11 O
) O
require O
source O
guarantees O
to O
resources O
like O
a O
specific O
document O
( O
9 O
) O
, O
event O
( O
10 O
) O
, O
or O
organization O
( O
11 O
) O
. O

A O
Human O
Misinformation O
Verification O
Examples O

B O
Leaked O
Evidence O
Analysis O
B.1 O
Misinformation O
Labels O

We O
consider O
all O
claims O
rated O
with O
strongly O
- O
leaning O
false O
verdicts O
and O
other O
verdicts O
that O
fall O
into O
the O
misinformation O
category O
such O
as O
" O
misleading O
" O
as O
misinformation O
. O
Remaining O
claims O
are O
either O
true O
( O
e.g. O
" O
verified O
" O
, O
" O
mostly O
true O
" O
) O
, O
mixed O
( O
e.g. O
" O
halftrue O
" O
, O
" O
outdated O
" O
) O
or O
not O
clearly O
applicable O
to O
misinformation O
( O
e.g. O
" O
opinion O
! O
" O
, O
" O
scam O
" O
, O
" O
full O
flop O
" O
) O
. O
We O
provide O
all O
considered O
labels O
within O
MULTIFC B-DatasetName
below O
. O

• O
ABC O
: O
in O
- O
the O
- O
red O

• O
Africa O
Check O
: O
incorrect O
, O
misleading O

B.2 O
Automatic O
Identification O
of O
Leaked O
Evidence O

Table O
9 O
shows O
the O
URLs O
used O
to O
automatically O
determine O
leaked O
evidence O
. O
We O
consider O
an O
evidence O
snippet O
as O
leaked O
if O
any O
URLs O
of O
Table O
9 O
is O
a O
substring O
of O
the O
snippet O
's O
URL O
. O
We O
exclude O
URLs O
if O
they O
may O
also O
cover O
URLs O
to O
news O
articles O
. O
Further O
, O
we O
consider O
an O
evidence O
snippet O
as O
leaked O
, O
if O
its O
lowercased O
title O
or O
text O
matches O
any O
of O
the O
regular O
expressions O
in O
Table O
10 O
. O
We O
identified O
two O
commonly O
made O
errors O
: O

• O
Different O
Claim O
: O
The O
approach O
considered O
evidence O
as O
leaked O
if O
it O
is O
not O
relevant O
to O
the O
exact O
same O
claim O
, O
but O
connected O
to O
the O
same O
incident O
( O
" O
President O
Obama O
pushed O
through O
the O
stimulus O
based O
on O
the O
promise O
of O
keeping O
unemployment O
under O
8 O
percent O
. O
" O
and O
" O
The O
president O
promised O
that O
if O
he O
spent O
money O
on O
a O
stimulus O
program O
that O
unemployment O
would O
go O
to O
5.7 O
percent O
or O
6 O
percent O
. O
Those O
were O
his O
words O
" O
) O
, O
or O
thematically O
related O
( O
" O
Cadbury O
chocolate O
eggs O
are O
infected O
with O
HIV O
- O
positive O
blood O
" O
and O
" O
HIV O
& O
AIDS O
infected O
oranges O
coming O
from O
Libya O
" O
) O
. O

• O
Discussing O
Fake O
News O
or O
Fact O
- O
Checking O
: O

The O
approach O
selects O
snippets O
, O
that O
discuss O
fact O
- O
checking O
or O
fake O
news O
from O
a O
different O
perspective O
, O
not O
as O
a O
result O
of O
verification O
. O
This O
includes O
opinions O
or O
reports O
complaining O
about O
" O
fake O
news O
" O
being O
spread O
, O
or O
about O
the O
fact O
- O
checking O
process O
. O

B.3 O
Manual O
Guidelines O

To O
determine O
the O
stance O
of O
evidence O
snippets O
to O
a O
claim O
, O
or O
whether O
it O
is O
leaked O
or O
not O
, O
we O
proceed O
in O
the O
following O
order O
: O
We O
first O
read O
the O
original O
factchecking O
article O
, O
to O
fully O
comprehend O
the O
claim O
and O
how O
fact O
- O
checkers O
refuted O
it O
. O
If O
the O
title O
or O
text O
of O
the O
evidence O
snippets O
provides O
sufficient O
information O
, O
we O
decide O
based O
on O
the O
snippet O
alone O
. O

If O
we O
can O
not O
make O
a O
clear O
decision O
based O
on O
the O
snippet O
, O
we O
consider O
the O
original O
web O
page O
. O
This O
may O
be O
required O
, O
as O
evidence O
snippets O
often O
contain O
incomplete O
sentences O
. O

B.3.1 O
Leaked O
Evidence O
Snippets O

We O
consider O
evidence O
snippets O
as O
leaked O
if O
( O
a O
) O
they O
constitute O
information O
that O
relies O
on O
the O
verification O
of O
the O
same O
claim O
, O
or O
( O
b O
) O
provides O
originally O
unknown O
information O
from O
the O
claim O
's O
future O
. O
When O
relying O
on O
content O
from O
the O
claim O
's O
verification O
, O
we O
do O
not O
require O
the O
information O
to O
contradict O
the O
claim O
from O
a O
human O
perspective O
. O
This O
often O
occurs O
when O
different O
pages O
( O
such O
as O
overview O
pages O
) O
reference O
the O
fact O
- O
checking O
article O
. O
Such O
a O
page O
may O
be O
a O
clear O
indication O
of O
the O
verdict O
in O
some O
cases O
( O
e.g. O
if O
titled O
" O
All O
False O
Claims O
by O
Person O
A O
" O
) O
. O

In O
other O
cases O
, O
different O
interpretations O
are O
valid O
: O

The O
statement O
" O
We O
previously O
fact O
- O
checked O
similar O
claims O
that O
... O
" O
may O
be O
seen O
as O
neutral O
or O
as O
a O
give O
- O
away O
that O
similar O
claims O
were O
refuted O
. O
Further O
, O
humans O
can O
not O
judge O
whether O
models O
may O
rely O
on O
latent O
patterns O
. O
An O
overview O
page O
titled O
" O
All O
claims O
from O
Person O
A O
" O
may O
be O
sufficient O
for O
the O
model O
if O
it O
learned O
that O
most O
claims O
by O
Person O
A O
are O
false O
. O
To O
remove O
this O
ambiguity O
, O
we O
consider O
any O
mention O
/ O
or O
information O
taken O
form O
the O
claim O
's O
verification O
as O
leaked O
. O
We O
do O
not O
strictly O
consider O
all O
evidence O
that O
appeared O
after O
the O
verification O
as O
leaked O
. O
Not O

B.3.2 O
Stance O
of O
Evidence O
Snippets O

For O
most O
claims O
, O
it O
is O
unrealistic O
to O
assume O
a O
single O
evidence O
snippet O
can O
refute O
them O
entirely O
. O
We O
follow O
Sarrouti O
et O
al O
. O
( O
2021 O
) O
to O
allow O
evidence O
to O
support O
or O
refute O
parts O
of O
the O
claim O
only O
. O
We O
separately O
mark O
supporting O
evidence O
from O
the O
claim O
's O
future O
, O
as O
the O
claim O
's O
veracity O
may O
have O
changed O
. O

We O
consider O
correctly O
identified O
counter O
- O
evidence O
as O
refuting O
the O
claim O
, O
even O
when O
it O
requires O
the O
source O
guarantee O
. O

C O
Experiments O
on O
MULTIFC B-DatasetName
C.1 O
Training O
details O

For O
our O
experiments O
we O
use O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
as O
provided O
by O
Wolf O
et O
al O
. O
( O
2020 O
) O
. O
We O
represent O
each O
evidence O
snippet O
e O
as O
the O
title O
, O
the O
text O
body O
, O
or O
the O
concatenation O
of O
both O
( O
depending O
on O
the O
experiment O
) O
. O
We O
concatenate O
all O
evidence O
snippets O
e O
i O
, O
separated O
by O
a O
semicolon O
( O
e O
1 O
; O
e O
2 O
; O
... O
; O
e O
n O
) O
. O
We O
input O
the O
concatenation O
of O
the O
claim O
c O
and O
the O
concatenated O
evidence O
, O
separated O
by O
[ O
SEP O
] O
token O
, O
and O
truncated O
after O
512 O
tokens O
: O
[ O
CLS O
] O
c O
[ O
SEP O
] O
e O
1 O
; O
e O
2 O
; O
... O
; O
e O
n O
[ O
SEP O
] O
. O
We O
predict O
the O
label O
via O
a O
linear O
layer O
on O
the O
[ O
CLS O
] O
token O
. O
We O
train O
all O
models O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
. O
We O
select O
the O
model O
with O
the O
highest O
F1 B-MetricName
score O
on O
the O
development O
dataset O
, O
evaluated O
after O
each O
epoch O
. O
We O
keep O
the O
default O
parameters O
for O
all O
other O
values O
. O
We O
always O
train O
and O
evaluate O
three O
models O
using O
the O
seeds O
( O
1,2,3 O
) O
. O
We O
did O
not O
fine O
- O
tune O
any O
hyperparameter O
. O
We O
provide O
code O
for O
reproduction O
. O
We O
run O
our O
experiments O
on O
a O
DGX O
A100 O
. O

C.2 O
Performance O
per O
Label O

We O
show O
the O
F1 B-MetricName
score O
for O
each O
label O
and O
both O
datasets O
in O
Table O
11 O
( O
Snopes O
) O
and O
Table O
12 O
( O
Politi O
- O
Fact O
) O
. O
The O
dataset O
of O
Snopes O
is O
highly O
imbalanced O
towards O
the O
majority O
class O
" O
false O
" O
. O
The O
class O
imbalance O
is O
amplified O
within O
the O
leaked O
subset O
. O
The O
evidence O
- O
only O
model O
benefits O
from O
leaked O
evidence O
for O
( O
leaning O
) O
false O
claims O
. O
The O
performance O
drops O
on O
samples O
with O
evidence O
exhibiting O
leaked O
characteristics O
when O
the O
gold O
veracity O
tends O
towards O
true O
. O
The O
performance O
on O
the O
unleaked O
subset O
is O
slightly O
more O
balanced O
when O
comparing O
different O
labels O
. O
The O
majority O
label O
" O
false O
" O
is O
still O
dominating O
. O
Yet O
, O
the O
more O
balanced O
predictions O
across O
all O
labels O
yield O
a O
higher O
F1 B-MetricName
- I-MetricName
macro I-MetricName
score O
. O
On O
PolitiFact O
, O
a O
majority O
of O
claims O
across O
all O
labels O
contain O
leaked O
evidence O
. O
Models O
learn O
to O
exploit O
it O
for O
all O
labels O
. O

The O
best O
performance O
gain O
can O
be O
seen O
over O
claims O
misinformation O
claims O
. O
Yet O
, O
the O
more O
balanced O
predictions O
across O
all O
labels O
yield O
a O
higher O
F1 B-MetricName
- I-MetricName
macro I-MetricName
score O
. O

C.3 O
Evaluation O
on O
Identical O
Claims O
with O
Different O
Evidence O

To O
avoid O
the O
label O
distribution O
to O
distort O
the O
impact O
of O
leaked O
evidence O
, O
we O
evaluate O
the O
trained O
model O
only O
on O
claims O
that O
contain O
leaked O
and O
unleaked O
evidence O
snippets O
. O
We O
separately O
measure O
the O
performance O
of O
the O
evidence O
- O
only O
models O
on O
this O
subset O
, O
when O
only O
concatenating O
leaked O
evidence O
to O
the O
claim O
, O
and O
when O
only O
concatenating O
unleaked O
evidence O
to O
the O
same O
claim O
. O
The O
overall O
metrics O
on O
Snopes O
( O
Table O
13 O
) O
do O
not O
change O
much O
. O

The O
performance O
on O
the O
individual O
labels O
differs O
( O
even O
not O
always O
for O
the O
better O
) O
when O
comparing O
the O
prediction O
with O
leaked O
or O
unleaked O
evidence O
. O
We O
find O
that O
leaked O
evidence O
snippets O
are O
strong O
indicators O
for O
the O
model O
to O
predict O
the O
label O
" O
false O
" O
. O
This O
comes O
at O
the O
cost O
of O
a O
lower O
recall O
across O
all O
other O
labels O
. O
On O
PolitiFact O
evidence O
snippets O
show O
great O
improvements O
and O
double O
almost O
every O
metric O
( O
Table O
14 O
) O
. O

C.4 O
Comparison O
with O
a O
Claim O
- O
Only O
Baseline O

We O
show O
the O
detailed O
results O
of O
the O
claim O
- O
only O
baseline O
for O
all O
claims O
that O
contain O
leaked O
and O
unleaked O
evidence O
snippets O
in O
Table O
15 O
. O
All O
these O
samples O
are O
considered O
leaked O
, O
as O
they O
contain O
( O
amongst O
others O
) O
leaked O
evidence O
. O
The O
results O
align O
with O
our O
previous O
observations O
: O
Models O
trained O
on O
the O
Snopes O
subset O
rely O
on O
the O
majority O
class O
" O
false O
" O
, O
yielding O
an O
overall O
high O
performance O
. O
We O
compare O
the O
results O
of O
the O
claim O
- O
only O
model O
( O
which O
by O
default O
can O
not O
benefit O
from O
leaked O
evidence O
) O
with O
the O
results O
of O
the O
evidence O
- O
only O
model O
( O
compare O

Acknowledgements O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
. O
We O
further O
thank O
Luke O
Bates O
, O
Tim O

# O
Year O
Misinformation O
Claim O
Strategy O

( O
1 O
) O
2020 O
Tennis O
star O
Serena O
Williams O
posted O
a O
message O
on O
social O
media O
that O
began O
, O
" O
I O
'm O
sick O
of O
COVID-19 O
. O
I O
'm O
sick O
of O
black O
vs. O
white O
. O
I O
'm O
sick O
of O
Republicans O
vs. O
Democrats O
. O
" O
n O
/ O
a O

( O
2 O
) O
2010 O
You O
can O
look O
up O
anyone O
's O
driver O
's O
license O
for O
free O
through O
the O
' O
National O
Motor O
Vehicle O
Licence O
Organization O
' O
web O
site O
. O
n O
/ O
a O

( O
3 O
) O
2016 O
A O
photograph O
shows O
a O
newly O
hatched O
baby O
dragon O
. O
n O
/ O
a O
( O
4 O
) O
2018 O
Couple O
Arrested O
For O
Selling O
' O
Golden O
Tickets O
To O
Heaven O
. O
n O
/ O
a O

( O
5 O
) O
2021 O
There O
is O
no O
added O
safety O
to O
the O
public O
if O
you O
're O
vaccinated O
. O

GCE O
( O
6 O
) O

2011 O
Limiting O
labor O
negotiations O
to O
only O
wages O
is O
how O
it O
is O
for O
the O
most O
part O
in O
the O
private O
sector O
. O
GCE O
par O
( O
F1 O
- O
micro O
, O
accuracy O
) O
or O
worse O
( O
F1 O
- O
macro O
) O
than O
the O
claim O
- O
only O
model O
. O
Here O
, O
the O
claim O
- O
only O
model O
is O
slightly O
less O
biased O
towards O
predicting O
" O
false O
" O
, O
which O
improves O
the O
performance O
on O
the O
remaining O
labels O
. O
Both O
results O
indicate O
the O
reliance O
of O
the O
model O
on O
leaked O
evidence O
( O
even O
if O
not O
for O
the O
better O
) O
. O
On O
the O
PolitiFact O
subset O
, O
the O
claim O
- O
only O
baseline O
performs O
well O
behind O
the O
evidence O
- O
only O
baseline O
( O
compare O
Table O
14 O
) O
. O

NatLogAttack B-MethodName
: O
A O
Framework O
for O
Attacking O
Natural O
Language O
Inference O
Models O
with O
Natural O
Logic O

Reasoning O
has O
been O
a O
central O
topic O
in O
artificial O
intelligence O
from O
the O
beginning O
. O
The O
recent O
progress O
made O
on O
distributed O
representation O
and O
neural O
networks O
continues O
to O
improve O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
of O
natural O
language O
inference O
. O
However O
, O
it O
remains O
an O
open O
question O
whether O
the O
models O
perform O
real O
reasoning O
to O
reach O
their O
conclusions O
or O
rely O
on O
spurious O
correlations O
. O
Adversarial O
attacks O
have O
proven O
to O
be O
an O
important O
tool O
to O
help O
evaluate O
the O
Achilles O
' O
heel O
of O
the O
victim O
models O
. O
In O
this O
study O
, O
we O
explore O
the O
fundamental O
problem O
of O
developing O
attack O
models O
based O
on O
logic O
formalism O
. O
We O
propose O
NatLogAttack B-MethodName
to O
perform O
systematic O
attacks O
centring O
around O
natural O
logic O
, O
a O
classical O
logic O
formalism O
that O
is O
traceable O
back O
to O
Aristotle O
's O
syllogism O
and O
has O
been O
closely O
developed O
for O
natural O
language O
inference O
. O
The O
proposed O
framework O
renders O
both O
label B-TaskName
- I-TaskName
preserving I-TaskName
and I-TaskName
label I-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
. O
We O
show O
that O
compared O
to O
the O
existing O
attack O
models O
, O
NatLogAttack B-MethodName
generates O
better O
adversarial O
examples O
with O
fewer O
visits O
to O
the O
victim O
models O
. O
The O
victim O
models O
are O
found O
to O
be O
more O
vulnerable O
under O
the O
label O
- O
flipping O
setting O
. O
NatLogAttack B-MethodName
provides O
a O
tool O
to O
probe O
the O
existing O
and O
future O
NLI O
models O
' O
capacity O
from O
a O
key O
viewpoint O
and O
we O
hope O
more O
logicbased O
attacks O
will O
be O
further O
explored O
for O
understanding O
the O
desired O
property O
of O
reasoning O
. O
1 O

Introduction O

While O
deep O
neural O
networks O
have O
achieved O
the O
stateof O
- O
the O
- O
art O
performance O
on O
a O
wide O
range O
of O
tasks O
, O
the O
models O
are O
often O
vulnerable O
and O
easily O
deceived O
by O
imposing O
perturbations O
to O
the O
original O
input O
( O
Goodfellow O
et O
al O
. O
, O
2014 O
; O
Kurakin O
et O
al O
. O
, O
2018 O
) O
, O
which O
seriously O
hurts O
the O
accountability O
of O
the O
systems O
. O
In O
depth O
, O
this O
pertains O
to O
model O
robustness O
, O
capacity O
, O
and O
the O
development O
of O
models O
with O
more O
advanced O
intelligence O
. O

Natural O
language O
inference O
( O
NLI O
) O
, O
also O
known O
as O
textual O
entailment O
( O
Dagan O
et O
al O
. O
, O
2005 O
; O
Iftene O
and O
Balahur O
- O
Dobrescu O
, O
2007 O
; O
MacCartney O
, O
2009 O
; O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
is O
a O
fundamental O
problem O
that O
models O
the O
inferential O
relationships O
between O
a O
premise O
and O
hypothesis O
sentence O
. O
The O
models O
built O
on O
distributed O
representation O
have O
significantly O
improved O
the O
performance O
on O
different O
benchmarks O
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
Chen O
et O
al O
. O
, O
2017 O
; O
Williams O
et O
al O
. O
, O
2018 O
; O
Chen O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Pilault O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
it O
is O
still O
highly O
desirable O
to O
conduct O
research O
to O
probe O
if O
the O
models O
possess O
the O
desired O
reasoning O
ability O
rather O
than O
rely O
on O
spurious O
correlation O
to O
reach O
their O
conclusions O
( O
Glockner O
et O
al O
. O
, O
2018 O
; O
Poliak O
et O
al O
. O
, O
2018 O
; O
Belinkov O
et O
al O
. O
, O
2019 O
; O
McCoy O
et O
al O
. O
, O
2019 O
; O
. O

Adversarial O
attacks O
have O
proven O
to O
be O
an O
important O
tool O
to O
reveal O
the O
Achilles O
' O
heel O
of O
victim O
models O
. O
Specifically O
for O
natural O
language O
inference O
, O
the O
logic O
relations O
are O
easily O
broken O
if O
an O
attack O
model O
does O
not O
properly O
generate O
the O
adversarial O
examples O
following O
the O
logic O
relations O
and O
related O
semantics O
. O
Therefore O
, O
unlike O
other O
textual O
attack O
tasks O
such O
as O
those O
relying O
on O
semantic O
similarity O
and O
relatedness O
, O
it O
is O
more O
challenging O
to O
create O
effective O
attacks O
here O
. O

In O
this O
study O
, O
we O
explore O
the O
basic O
problem O
of O
developing O
adversarial O
attacks O
based O
on O
logic O
formalism O
, O
with O
the O
aim O
to O
probe O
victim O
models O
for O
the O
desired O
reasoning O
capability O
. O
Specifically O
, O
we O
propose O
NatLogAttack B-MethodName
, O
in O
which O
the O
adversarial O
attacks O
are O
generated O
based O
on O
natural O
logic O
( O
Lakoff O
, O
1970 O
; O
Van O
Benthem O
, O
1995 O
; O
MacCartney O
, O
2009 O
; O
Icard O
, O
2012 O
; O
Angeli O
et O
al O
. O
, O
2016 O
; O
Hu O
and O
Moss O
, O
2018 O
; O
, O
a O
classical O
logic O
formalism O
with O
a O
long O
history O
that O
has O
been O
closely O
developed O
with O
natural O
language O
inference O
. O
From O
a O
general O
perspective O
, O
natural O
language O
inference O
provides O
an O
appropriate O
setup O
for O
probing O
the O
development O
of O
distributed O
representation O
and O
the O
models O
based O
on O
that O
. O
A O
robust O
solution O
for O
the O
task O
requires O
manipulation O
of O
discrete O
operations O
and O
adversarial O
attacks O
can O
help O
understand O
whether O
and O
how O
the O
required O
symbols O
and O
inference O
steps O
emerge O
from O
the O
data O
and O
the O
learned O
distributed O
representation O
. O
Our O
work O
has O
also O
been O
inspired O
by O
recent O
research O
on O
exploring O
the O
complementary O
strengths O
of O
neural O
networks O
and O
symbolic O
models O
( O
Garcez O
et O
al O
. O
, O
2015 O
; O
Yang O
et O
al O
. O
, O
2017 O
; O
Rocktäschel O
and O
Riedel O
, O
2017 O
; O
Evans O
and O
Grefenstette O
, O
2018 O
; O
Weber O
et O
al O
. O
, O
2019 O
; O
De O
Raedt O
et O
al O
. O
, O
2019 O
; O
Mao O
et O
al O
. O
, O
2019 O
; O
Feng O
et O
al O
. O
, O
2020Feng O
et O
al O
. O
, O
, O
2022 O
. O

Our O
research O
contributes O
to O
the O
development O
of O
logic B-TaskName
- I-TaskName
based I-TaskName
adversarial I-TaskName
attacks I-TaskName
for O
natural O
language O
understanding O
. O
Specifically O
, O
we O
propose O
a O
novel O
attack O
framework O
, O
NatLogAttack B-MethodName
, O
based O
on O
natural O
logic O
for O
natural O
language O
inference O
. O
Our O
experiments O
with O
both O
human O
and O
automatic O
evaluation O
show O
that O
the O
proposed O
model O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
attack O
methods O
. O
Compared O
to O
the O
existing O
attack O
models O
, O
NatLogAttack B-MethodName
generates O
better O
adversarial O
examples O
with O
fewer O
visits O
to O
the O
victim O
models O
. O
In O
addition O
to O
the O
commonly O
used O
attack O
setting O
where O
the O
labels O
of O
generated O
examples O
remain O
the O
same O
as O
the O
original O
pairs O
, O
we O
also O
propose O
to O
construct O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
. O
The O
victim O
models O
are O
found O
to O
be O
more O
vulnerable O
in O
this O
setup O
and O
NatLogAttack B-MethodName
succeeds O
in O
deceiving O
them O
with O
much O
smaller O
numbers O
of O
queries O
. O
NatLogAttack B-MethodName
provides O
a O
systematic O
approach O
to O
probing O
the O
existing O
and O
future O
NLI O
models O
' O
capacity O
from O
a O
basic O
viewpoint O
that O
has O
a O
traceable O
history O
, O
by O
combining O
it O
with O
the O
recent O
development O
of O
attacking O
models O
. O
The O
proposed O
framework O
is O
constrained O
by O
the O
natural O
logic O
formalism O
and O
we O
hope O
more O
logic O
- O
based O
attacks O
will O
be O
further O
explored O
for O
understanding O
the O
desired O
property O
of O
natural O
language O
reasoning O
. O

Related O
Work O

Adversarial O
Attacks O
in O
NLP O
. O
White O
- O
box O
attacks O
leverage O
the O
architecture O
and O
parameters O
of O
victim O
models O
to O
craft O
adversarial O
examples O
( O
Liang O
et O
al O
. O
, O
2018 O
; O
Wallace O
et O
al O
. O
, O
2019 O
; O
Ebrahimi O
et O
al O
. O
, O
2018 O
) O
. O
Black O
- O
box O
models O
, O
however O
, O
have O
no O
such O
knowledge O
. O
Pioneering O
blind O
models O
( O
Jia O
and O
Liang O
, O
2017 O
) O
, O
for O
example O
, O
create O
adversarial O
examples O
by O
adding O
distracting O
sentences O
to O
the O
input O
. O
More O
recently O
, O
score O
- O
based O
( O
e.g. O
, O
Zhang O
et O
al O
. O
( O
2019 O
) O
; O
) O
and O
decision O
- O
based O
attack O
models O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
also O
query O
the O
prediction O
scores O
or O
the O
final O
decisions O
of O
victim O
models O
. O

In O
terms O
of O
perturbation O
granularities O
, O
characterlevel O
attacks O
modify O
characters O
( O
Ebrahimi O
et O
al O
. O
, O
2018 O
) O
while O
word O
- O
level O
models O
rely O
on O
word O
substitutions O
that O
can O
be O
performed O
based O
on O
word O
embeddings O
( O
Sato O
et O
al O
. O
, O
2018 O
) O
, O
language O
models O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
, O
or O
even O
external O
knowledge O
bases O
( O
Zang O
et O
al O
. O
, O
2020 O
) O
. O
Sentence O
- O
level O
attack O
models O
add O
perturbation O
to O
an O
entire O
sentence O
by O
performing O
paraphrasing O
( O
Iyyer O
et O
al O
. O
, O
2018 O
) O
or O
attaching O
distracting O
sentences O
( O
Jia O
and O
Liang O
, O
2017 O
) O
. O
Kang O
et O
al O
. O
( O
2018 O
) O
generated O
natural O
language O
inference O
examples O
based O
on O
entailment O
label O
composition O
functions O
with O
the O
help O
of O
lexical O
knowledge O
. O
Minervini O
and O
Riedel O
( O
2018 O
) O
utilized O
a O
set O
of O
first O
- O
order O
- O
logic O
constraints O
to O
measure O
the O
degree O
of O
rule O
violation O
for O
natural O
language O
inference O
. O
The O
efforts O
utilized O
the O
generated O
examples O
for O
data O
augmentation O
. O
The O
focus O
is O
not O
on O
adversarial O
attack O
and O
the O
adversarial O
examples O
' O
quality O
, O
e.g. O
, O
the O
attack O
validity O
, O
is O
not O
evaluated O
. O

Natural O
Logic O
. O
Natural O
logic O
has O
a O
long O
history O
and O
has O
been O
closely O
developed O
with O
natural O
language O
inference O
( O
Lakoff O
, O
1970 O
; O
Van O
Benthem O
, O
1995 O
; O
MacCartney O
, O
2009 O
; O
Icard O
, O
2012 O
; O
Angeli O
et O
al O
. O
, O
2016 O
; O
Hu O
and O
Moss O
, O
2018 O
; O
. O
Recently O
, O
some O
efforts O
have O
started O
to O
consider O
monotonicity O
in O
attacks O
, O
including O
creating O
test O
sets O
to O
understand O
NLI O
models O
' O
behaviour O
Yanaka O
et O
al O
. O
, O
2019aYanaka O
et O
al O
. O
, O
, O
b O
, O
2020Geiger O
et O
al O
. O
, O
2020 O
) O
. O
The O
existing O
work O
, O
however O
, O
has O
not O
performed O
systematic O
attacks O
based O
on O
natural O
logic O
. O
The O
core O
idea O
of O
monotonicity O
( O
e.g. O
, O
downward O
monotone O
) O
and O
projection O
has O
not O
been O
systematically O
considered O
. O
The O
models O
have O
not O
been O
combined O
with O
the O
state O
- O
of O
- O
the O
- O
art O
adversarial O
attack O
framework O
and O
search O
strategies O
for O
the O
general O
purpose O
of O
adversarial O
attacks O
. O
For O
example O
, O
and O
Yanaka O
et O
al O
. O
( O
2020 O
) O
generate O
adversarial O
examples O
from O
a O
small O
vocabulary O
and O
pre O
- O
designed O
sentence O
structures O
. O
The O
effort O
of O
Yanaka O
et O
al O
. O
( O
2019b O
) O
is O
limited O
by O
only O
considering O
one O
- O
edit O
distance O
between O
a O
premise O
and O
hypothesis O
. O
We O
aim O
to O
explore O
principled O
approaches O
to O
constructing O
perturbations O
based O
on O
natural O
logic O
, O
and O
the O
control O
of O
the O
quality O
of O
attack O
generation O
can O
leverage O
the O
continuing O
advancement O
of O
language O
models O
. O
The O
proposed O
attack O
settings O
, O
along O
with O
the O
breakdown O
of O
attack O
categories O
, O
help O
reveal O
the O
properties O
of O
victim O
models O
in O
both O
label O
- O
preserving O
and O
label O
- O
flipping O
attacks O
. O

Background O

The O
study O
of O
natural O
logic O
can O
be O
traced O
back O
to O
Aristotle O
's O
syllogisms O
. O
Rather O
than O
performing O
deduction O
over O
an O
abstract O
logical O
form O
, O
natural O
logic O
models O
inference O
in O
natural O
language O
by O
operating O
on O
the O
structure O
or O
surface O
form O
of O
language O
( O
Lakoff O
, O
1970 O
; O
van O
Benthem O
, O
1988 O
; O
Valencia O
, O
1991 O
; O
Van O
Benthem O
, O
1995 O
; O
Nairn O
et O
al O
. O
, O
2006 O
; O
Mac O
- O
Cartney O
, O
2009 O
; O
MacCartney O
and O
Manning O
, O
2009 O
; O
Icard O
, O
2012 O
; O
Angeli O
and O
Manning O
, O
2014 O
; O
Hu O
and O
Moss O
, O
2018 O
; O
. O
It O
allows O
for O
a O
wide O
range O
of O
intuitive O
inferences O
in O
a O
conceptually O
clean O
way O
that O
we O
use O
daily O
and O
provides O
a O
good O
framework O
for O
attacking O
inference O
models O
- O
we O
doubt O
that O
a O
victim O
model O
vulnerable O
to O
such O
natural O
attacks O
indeed O
performs O
reliable O
reasoning O
. O
Our O
work O
uses O
the O
natural O
logic O
variant O
proposed O
by O
MacCartney O
and O
Manning O
( O
2009 O
) O
and O
MacCartney O
( O
2009 O
) O
, O
which O
extends O
the O
prior O
formalism O
to O
model O
the O
entailment O
relations O
between O
two O
spans O
of O
texts O
with O
seven O
relations O
B O
= O
{ O
≡ O
, O
⊏ O
, O
⊐ O
, O
∧ O
, O
| O
, O
⌣ O
, O
# O
} O
, O
representing O
equivalence O
, O
forward O
entailment O
, O
reverse O
entailment O
, O
negation O
, O
alternation O
, O
cover O
, O
and O
independence O
, O
respectively O
. O
Through O
projection O
based O
on O
monotonicity O
in O
context O
, O
local O
lexical O
- O
level O
entailment O
relations O
between O
a O
premise O
and O
hypothesis O
can O
be O
aggregated O
to O
determine O
the O
entailment O
relations O
at O
the O
sentence O
- O
pair O
level O
. O
For O
completeness O
of O
this O
paper O
, O
we O
highlight O
the O
key O
building O
blocks O
in O
Appendix O
A O
. O

Setups O
Label O
yg O
→ O
y O
* O
g O
Strategy O
Nat O
. O
Logic O
Relations O
Label O
- O
preserving O
E O
→ O
E O
H O
⊨ O
H O
* O
H O
≡ O
H O
* O
or O
H O
⊏ O
H O
* O
C O
→ O
C O
H O
* O
⊨ O
H O
H O
≡ O
H O
* O
or O
H O
⊐ O
H O
* O
N O
→ O
N O
H O
* O
⊨ O
H O
H O
≡ O
H O
* O
or O
H O
⊐ O
H O
* O
Label O
- O
flipping O
E O
→ O
C O
H O
⊨ O
¬H O
* O
H O
∧ O
H O
* O
or O
H O
| O
H O
* O
E O
→ O
N O
H O
⊭ O
H O
* O
and O
H O
⊭ O
¬H O
* O
H O
⊐ O
H O
* O
or O
H O
⌣ O
H O
* O
C O
→ O
E O
¬H O
* O
⊨ O
H O
H O
≡ O
¬H O
* O
or O
H O
⊐ O
¬H O
* O

NatLogAttack B-MethodName
Setups O
and O
Principles O

Formally O
, O
given O
a O
premise O
sentence O
P O
, O
its O
n- O

word O
hypothesis O
H O
= O
( O
h O
1 O
, O
h O
2 O
, O
• O
• O
• O
, O
h O
n O
) O

, O
and O
the O
ground O
- O
truth O
natural O
language O
inference O
label O
y O
g O
= O
L O
( O
P O
, O
H O
) O
, O
NatLogAttack B-MethodName
generates O
a O
hypothesis O
H O
* O
that O
satisfies O
a O
desired O
target O
label O
y O
* O
g O
= O
L O
( O
P O
, O
H O
* O
) O
. O
The O
attacking O
pair O
⟨P O
, O
H O
* O
⟩ O
is O
generated O
only O
if O
the O
original O
pair O
⟨P O
, O
H⟩ O
is O
correctly O
classified O
by O
a O
victim O
model O
F. O
Accordingly O
, O
we O
denote O
y O
= O
F O
( O
P O
, O
H O
) O
as O
the O
natural O
language O
inference O
label O
predicated O
by O
the O
victim O
model O
F O
for O
the O
original O
pair O
and O
denote O
y O
* O
= O
F O
( O
P O
, O
H O
* O
) O
as O
the O
predicted O
label O
for O
the O
attacking O
pair O
. O

We O
propose O
to O
perform O
the O
attacks O
in O
two O
setups O
: O
the O
label B-TaskName
- I-TaskName
preserving I-TaskName
and I-TaskName
label I-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
. O
The O
attack O
principles O
and O
setups O
are O
summarized O
in O
Table O
1 O
. O
A O
label B-TaskName
- I-TaskName
preserving I-TaskName
attack I-TaskName
generates O
adversarial O
examples O
with O
y O
* O
g O
= O
y O
g O
, O
aiming O
to O
test O
the O
robustness O
of O
victim O
models O
on O
different O
inputs O
that O
have O
the O
same O
label O
- O
it O
attacks O
victim O
models O
under O
perturbations O
that O
do O
not O
change O
the O
inferential O
labels O
of O
the O
original O
premise O
- O
hypothesis O
pair O
. O

The O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
, O
on O
the O
other O
hand O
, O
aim O
at O
attacking O
victim O
models O
with O
perturbations O
that O
are O
key O
to O
differentiating O
two O
different O
logical O
relations O
where O
y O
* O
g O
̸ O
= O
y O
g O
. O
Note O
that O
natural O
logic O
can O
be O
naturally O
used O
to O
generate O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
, O
and O
our O
work O
here O
is O
among O
the O
first O
to O
explore O
this O
type O
of O
attacks O
for O
natural O
language O
understanding O
, O
although O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
have O
been O
explored O
in O
image O
attacks O
( O
Tramèr O
et O
al O
. O
, O
2020 O
) O
. O

The O
third O
column O
of O
the O
table O
( O
strategy O
) O
lists O
the O
logic O
conditions O
between O
the O
generated O
hypothesis O
H O
* O
and O
the O
original O
hypothesis O
H O
that O
satisfy O
the O
desired O
properties O
of O
preserving O
or O
flipping O
labels O
to O
obtain O
the O
target O
label O
y O
* O
g O
. O
Consider O
the O
second O
row O
of O
the O
label O
- O
preserving O
setup O
( O
i.e. O
, O
C O
→ O
C O
) O
, O
in O
which O
NatLogAttack B-MethodName
generates O
a O
hypothesis O
H O
* O
with O
y O
* O
g O
= O
y O
g O
= O
contradiction O
. O
This O
is O
achieved O
by O
ensuring O
the O
natural O
language O
inference O
label O
between O
H O
* O
and O
H O
to O
obey O
entailment O
: O
H O
* O
⊨ O
H. O
g O
= O
neutral O
) O
using O
the O
premise O
- O
hypothesis O
pairs O
with O
y O
g O
= O
contradiction O
, O
because O
two O
contradictory O
sentences O
may O
refer O
to O
irrelevant O
events O
from O
which O
a O
neutral O
pair O
can O
not O
be O
reliably O
generated O
. O
3 O
Constraint O
3.2 O
NatLogAttack B-MethodName
is O
also O
constrained O
from O
generating O
contradiction O
and O
entailment O
attacks O
( O
y O
* O
g O
= O
contradiction O
or O
y O
* O
g O
= O
entailment O
) O
from O
neutral O
pairs O
( O
y O
g O
= O
neutral O
) O
, O
as O
there O
are O
many O
ways O
two O
sentences O
being O
neutral O
, O
including O
reverse O
entailment O
and O
diverse O
semantic O
relations O
. O

The O
contradiction O
and O
entailment O
pairs O
can O
not O
be O
reliably O
generated O
. O

Generation O
and O
Quality O
Control O

Preparing O
Natural O
Logic O
Relations O

As O
shown O
in O
the O
bottom O
- O
left O
part O
of O
Figure O
1 O
, O
given O
a O
premise O
- O
hypothesis O
pair O
⟨P O
, O
H⟩ O
, O
the O
ground O
- O
truth O
label O
y O
g O
, O
and O
the O
target O
label O
y O
* O
g O
, O
NatLogAttack B-MethodName
retrieves O
natural O
logic O
relations O
from O
the O
last O
column O
of O
Table O
1 O
. O
Consider O
label O
- O
preserving O
attacks O
and O
take O
y O
* O
g O
= O
y O
g O
= O
entailment O
as O
an O
example O
. O
From O
the O
last O
column O
in O
the O
first O
row O
of O
the O
label O
- O
preserving O
setup O
, O
NatLogAttack B-MethodName
finds O
and O
pushes O
the O
relations O
≡ O
and O
⊏ O
into O
the O
natural O
- O
logic O
relations O
set O
, O
R O
* O
g O
= O
{ O
≡ O
, O
⊏ O
} O
, O
where O
R O
* O
g O
includes O
the O
natural O
- O
logic O
relations O
between O
H O
and O
H O
* O
and O
will O
be O
used O
to O
generate O
the O
latter O
. O
Note O
that O
r O
* O
g O
∈ O
R O
* O
g O
is O
one O
of O
relations O
in O
R O
* O
g O
. O
We O
first O
copy O
H O
to O
H O
( O
1 O
) O
, O
denoted O
as O
H O
( O
1 O
) O
← O
H O
for O
the O
convenience O
of O
notation O
, O
because O
the O
generation O
- O
and O
- O
attack O
process O
may O
be O
performed O
multiple O
rounds O
if O
one O
round O
of O
attacks O
fail O
. O
Then O
we O
use O
the O
notation O
H O
( O
1 O
) O
and O
H O
( O
2 O
) O
to O
refer O
to O
the O
original O
and O
a O
generated O
hypothesis O
sentence O
in O
each O
round O
. O
Note O
that O
in O
the O
above O
example O
, O
as O
will O
be O
discussed O
below O
, O
within O
each O
round O
of O
generation O
, O
NatLogAttack B-MethodName
will O
provide O
a O
set O
of O
attacks O
to O
perform O
multiple O
( O
iterative O
) O
attacks O
. O

Candidate O
Generation O

Algorithm O
1 O
: O
Candidate O
Generation O

Input O
: O
Sentence O
H O
( O
1 O
) O
with O
tokens O
( O
h O
( O
1 O
) O
1 O
, O
• O
• O
• O
, O
h O
( O
1 O
) O
n O
) O
, O
target O
natural O
- O
logic O
relation O
set O
R O
* O
g O
Output O
: O
Candidate O
sentence O
set O
H O
1 O
Init O
H O
= O
∅ O
2 O
L O
= O
natlog O
( O
H O
( O
1 O
) O
) O
3 O
foreach O
h O
( O
1 O
) O
i O
∈H O
( O
1 O
) O
and O
r O
* O
g O
∈ O
R O
* O
g O
do O
4 O
R O
* O
local O
= O
L O
B O
[ O
idx O
L O
i O
( O
r O
* O
g O
) O
] O
5 O
if O
≡ O
∈ O
R O
* O
local O
then O
6 O
H O
= O
H O
∪ O
PerturbSyno O
( O
H O
( O
1 O
) O
, O
h O
( O
1 O
) O

i O
) O
7 O
H O
= O
H O
∪ O
DoubleNegation O
( O
H O
( O
1 O
) O
) O
8 O
end O
9 O
if O
⊏ O
∈ O
R O
* O
local O
then O
10 O
H O
= O
H O
∪ O
PerturbHyper O
( O
H O
( O
1 O
) O
, O
h O
( O
1 O
) O

i O
) O
11 O
H O
= O
H O
∪ O
Deletion O
( O
H O
( O
1 O
) O
, O
i O
) O
12 O
end O
13 O
if O
⊐ O
∈ O
R O
* O
local O
then O
14 O
H O
= O
H O
∪ O
PerturbHypo O
( O
H O
( O
1 O
) O
, O
h O
( O
1 O
) O
i O
) O

15 O

H O
= O
H O
∪ O
Insertion O
( O
H O
( O
1 O
) O
, O
i O
) O
16 O
end O
17 O
if O
| O
∈ O
R O
* O
local O
then O
18 O
H O
= O
H O
∪ O
PerturbCoHyper O
( O
H O
( O
1 O
) O
, O
h O
( O
1 O
) O
i O
) O

19 O

H O
= O
H O
∪ O
PerturbAnto O
( O
H O
( O
1 O
) O
, O
h O
( O
1 O
) O

i O
) O
20 O
H O
= O
H O
∪ O
AltLM O
( O
H O
( O
1 O
) O
, O
i O
) O
21 O
end O
22 O
if O
∧ O
∈ O
R O
* O
local O
then O
23 O
H O
= O
H O
∪ O
AddNeg O
( O
H O
( O
1 O
) O
, O
h O
( O
1 O
) O
i O
) O

24 O
end O
25 O
end O
Return O
: O
H O
Our O
candidate O
attack O
generation O
process O
is O
described O
in O
Algorithm O
1 O
. O
Taking O
H O
( O
1 O
) O
and O
R O
* O
g O
as O
the O
input O
, O
the O
algorithm O
aims O
to O
generate O
a O
set O
of O
candidate O
hypotheses O

H O
= O
{ O
H O
( O
2 O
) O
1 O
, O
• O
• O
• O
, O
H O
( O
2 O
) O
m O
} O
with O
each O
pair O
⟨H O
( O
1 O
) O
, O
H O
( O
2 O
) O
i O
⟩ O
following O
a O
target O
re- O
lation O
r O
* O
g O
∈ O
R O
* O

g O
where O
H O

( O
2 O
) O
i O
∈ O
H. O
For O
each O
token O
h O

( O
1 O
) O
i O
∈ O
H O
( O
1 O
) O
and O
r O
* O
g O
∈ O
R O
* O
g O
, O
the O
algorithm O
obtains O
the O
monotonicity O
and O
relation O
projection O
infor O
- O
mation O
using O
the O
Stanford O
natlog O
parser O
4 O
( O
line O
2 O
) O
. O

Specifically O
for O
h O

( O
1 O
) O
i O
, O
suppose O
the O
parser O
outputs O
an O
ordered O
relation O
list O
: O
L O
i O
= O
⟨≡ O
, O
⊐ O
, O
⊏ O
, O
∧ O
, O
| O
, O
⌣ O
, O
# O
⟩ O
, O
this O
returned O
list O
actually O
encodes O
the O
contextualized O
projection O
information O
, O
which O
we O
leverage O
to O
substitute O
h O

( O
1 O
) O
i O
with O
h O
′ O
i O
to O
generate O
H O

( O
2 O
) O
i O
that O
satisfies O
relation O
r O
* O
g O
. O
In O
natural O
logic O
, O
when O
determining O
the O
sentencelevel O
logical O
relation O
between O
a O
premise O
and O
hypothesis O
sentence O
, O
projection O
is O
used O
to O
map O
local O
lexicon O
- O
level O
logical O
relation O
to O
sentence O
- O
level O
relations O
by O
considering O
the O
context O
and O
monotonicity O
. O
However O
, O
in O
adversarial O
attacks O
, O
NatLogAttack B-MethodName
needs O
to O
take O
the O
following O
reverse O
action O
: O

R O
local O
= O
L O
B O
[ O
idx O
L O
i O
( O
r O
* O
g O
) O
] O
( O
1 O
) O

where O
r O
* O
g O
is O
the O
target O
sentence O
- O
level O
natural O
logic O
relation O
( O
in O
our O
above O
example O
, O
suppose O
r O
* O
g O
= O
' O
⊏ O
' O
) O
. O
Then O
idx O
L O
i O
( O
. O
) O
returns O
the O
index O
of O
that O
relation O
in O
L O
i O
. O
For O
' O
⊏ O
' O
, O
the O
index O
is O
3 O
. O
Then O
the O
index O
is O
used O
to O
find O
the O
lexicon O
- O
level O
( O
local O
) O
relation O
from O
the O
predefined O
ordered O
list O
L O
B O
= O
⟨ O
≡ O
, O
⊏ O
, O
⊐ O
, O
∧ O
, O
| O
, O
⌣ O
, O
# O
⟩. O
In O
the O
above O
example O
we O
will O
get O
L O
B O
[ O
3 O
] O
='⊐ O
' O
. O
Again O
, O
Equation O
1 O
presents O
a O
reverse O
process O
of O
the O
regular O
projection O
process O
in O
natural O
logic O
. O
In O
other O
words O
, O
the O
ordered O
relation O
list O
provided O
by O
the O
natlog O
parser O
for O
each O
word O
token O
, O
when O
used O
together O
with O
the O
predefined O
( O
ordered O
) O
relation O
list O
L O
B O
, O
specifies O
a O
mapping O
between O
global O
( O
sentence O
- O
level O
) O
natural O
- O
logic O
relations O
and O
local O
( O
lexicon O
- O
level O
) O
relations O
. O
Note O
also O
that O
the O
output O
R O
local O
is O
a O
set O
, O
because O
L O
i O
is O
an O
ordered O
list O
that O
may O
contain O
the O
same O
relation O
multiple O
times O
. O

Basic O
Word O
Perturbation O
. O
For O
a O
word O
token O
h O
i O
, O
we O
replace O
it O
with O
word O
h O
′ O
i O
to O
ensure O
the O
local O
relation O
⟨h O
i O
, O
h O
′ O
i O
⟩ O
to O
be O
r O
local O
∈ O
R O
local O
. O
NatLogAttack B-MethodName
extracts O
natural O
- O
logic O
relation O
knowledge O
from O
knowledge O
bases O
to O
obtain O
word O
candidates O
for O
the O
desired O
relation O
types O
. O
The O
word O
perturbation O
of O
NatLogAttack B-MethodName
focused O
on O
five O
relations O
in O
Table O
8 O
. O
We O
attack O
the O
victim O
models O
using O
the O
most O
basic O
semantic O
relations O
explicitly O
expressed O
in O
knowledge O
bases O
and O
knowledge O
implicitly O
embedded O
in O
large O
pretrained O
language O
models O
. O
Specifically O
, O
we O
4 O
https O
: O
/ O
/ O
stanfordnlp.github.io O
/ O
CoreNLP O
/ O
natlog.html O
. O

Monotonicity O

Upward O
Downward O
use O
WordNet O
( O
Miller O
, O
1995 O
) O
to O
extract O
the O
desired O
lexical O
relations O
. O
For O
a O
word O
token O
h O
i O
, O
we O
search O
candidate O
words O
h O
′ O
i O
that O
has O
one O
of O
the O
following O
relations O
with O
h O
i O
: O
{ O
≡ O
, O
⊏ O
, O
⊐ O
, O
∧ O
, O
| O
} O
. O
Synonyms O
are O
used O
as O
h O
′ O
i O
to O
substitute O
h O
i O
for O
constructing O
H O
( O
2 O
) O
with O
an O
equivalence O
relation O
to O
H O
( O
1 O
) O
( O
line O
6 O
) O
, O
hypernyms O
are O
used O
for O
forward O
entailment O
( O
line O
10 O
) O
, O
and O
hyponyms O
for O
reverse O
entailment O
( O
line O
14 O
) O
. O
Due O
to O
the O
transitiveness O
of O
forward O
entailment O
( O
⊏ O
) O
and O
reverse O
entailment O
( O
⊐ O
) O
, O
we O
centre O
around O
h O
i O
to O
find O
its O
hypernyms O
and O
hyponyms O
but O
restrict O
the O
distances O
within O
a O
threshold O
to O
avoid O
generating O
sentences O
that O
are O
semantically O
unnatural O
, O
contain O
overgeneralized O
concepts O
, O
or O
are O
semantically O
implausible O
. O
Later O
, O
we O
will O
further O
use O
a O
language O
model O
to O
control O
the O
quality O
. O

Syntax O
adj O
+ O
n O
⊏ O
n O
adj O
+ O
n O
⊐ O
n O
v O
+ O
adv O
⊏ O
v O
v O
+ O
adv O
⊐ O
v O
s O
+ O
PP O
⊏ O
s O
s O
+ O
PP O
⊐ O
s O

For O
alternation O
, O
the O
perturbation O
candidates O
h O
′ O
i O
are O
words O
that O
share O
the O
common O
hypernym O
with O
h O
i O
( O
line O
18 O
) O
. O
Following O
MacCartney O
( O
2009 O
) O
, O
we O
do O
not O
use O
antonyms O
of O
content O
words O
for O
the O
negation O
relation O
but O
instead O
use O
them O
to O
construct O
alternation O
hypotheses O
( O
line O
19 O
) O
. O
For O
the O
negation O
( O
line O
23 O
) O
, O
a O
list O
of O
negation O
words O
and O
phrases O
is O
used O
to O
construct O
new O
hypotheses O
. O
Note O
that O
while O
our O
experiments O
show O
the O
NatLogAttack B-MethodName
has O
been O
very O
effective O
and O
outperforms O
other O
attack O
models O
, O
some O
of O
the O
components O
can O
be O
further O
augmented O
as O
future O
work O
. O

Enhancing O
Alternation O
. O
As O
discussed O
above O
, O
attacks O
may O
run O
multi O
- O
rounds O
if O
the O
prior O
round O
fails O
. O

For O
alternation O
substitution O
, O
NatLogAttack B-MethodName
does O
not O
replace O
the O
word O
token O
that O
has O
been O
substituted O
before O
, O
since O
the O
alternation O
of O
alternation O
does O
not O
guarantee O
to O
be O
the O
alternation O
relation O
. O
In O
addition O
to O
constructing O
alternation O
hypotheses O
using O
Word O
- O
Net O
, O
we O
further O
leverage O
DistilBert O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
to O
obtain O
the O
alternation O
candidates O
using O
the O
function O
AltLM O
( O
line O
20 O
) O
. O
Specifically O
, O
we O
mask O
the O
target O
word O
( O
which O
is O
a O
verb O
, O
noun O
, O
adjective O
or O
adverb O
) O
and O
prompt O
the O
language O
model O
to O
provide O
candidates O
. O
The O
provided O
candidates O
and O
replaced O
words O
are O
required O
to O
have O
the O
same O
POS O
tags O
. O

Insertion O
and O
Deletion O
. O
In O
addition O
to O
substitution O
, O
NatLogAttack B-MethodName
also O
follows O
natural O
logic O
and O
monotonicity O
to O
construct O
examples O
using O
the O
insertion O
and O
deletion O
operations O
. O
As O
shown O
in O
Table O
2 O
, O
adjectives O
, O
adverbs O
and O
prepositional O
phrases O
are O
leveraged O
in O
the O
upward O
and O
downward O
context O
of O
monotonicity O
to O
enhance O
the O
attacks O
for O
entailment O
( O
' O
⊏ O
' O
) O
and O
reverse O
entailment O
( O
' O
⊐ O
' O
) O
. O
We O
include O
the O
details O
in O
Appendix O
B O
, O
which O
is O
built O
on O
Stanford O
CoreNLP O
parser O
and O
pretrained O
language O
models O
. O
Note O
that O
the O
syntactic O
rules O
do O
not O
guarantee O
to O
generate O
sentences O
with O
the O
desired O
NLI O
labels O
( O
e.g. O
, O
see O
( O
Partee O
, O
1995 O
) O
for O
the O
discussion O
on O
the O
semantic O
composition O
of O
adjective O
+ O
noun O
) O
and O
the O
process O
is O
only O
for O
generating O
candidates O
. O
We O
will O
use O
the O
pretrained O
language O
model O
to O
further O
identify O
good O
adversarial O
examples O
at O
a O
later O
stage O
. O
Both O
the O
insertion O
and O
deletion O
operations O
are O
used O
with O
monotonicity O
and O
projection O
context O
to O
generate O
different O
relations O
. O

Attack O
Quality O
Control O

NatLogAttack B-MethodName
uses O
DistilBert O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
to O
calculate O
the O
pseudo O
- O
perplexity O
scores O
( O
Salazar O
et O
al O
. O
, O
2020 O
) O
for O
all O
generated O
hypotheses O
H O
= O

{ O
H O
( O
2 O
) O
1 O
, O
H O
( O
2 O
) O
2 O
, O
• O
• O
• O
, O
H O
( O
2 O
) O

m O
} O
, O
and O
keeps O
only O
a O
maximum O
of O
100 B-HyperparameterValue
candidates B-HyperparameterName
with I-HyperparameterName
the I-HyperparameterName
lowest I-HyperparameterName
perplexity I-HyperparameterName
values I-HyperparameterName
. O
In O
our O
development O
, O
we O
found O
that O
the O
quality O
control O
stage O
is O
important O
for O
ensuring O
the O
quality O
of O
attack O
examples O
, O
particularly O
for O
reducing O
word O
perturbation O
mistakes O
resulting O
from O
incorrect O
interpretation O
of O
the O
words O
being O
substituted O
, O
which O
often O
results O
in O
unnatural O
hypothesis O
sentences O
, O
as O
well O
as O
reducing O
other O
sources O
of O
low O
- O
quality O
attacks O
including O
over O
- O
generalization O
of O
concepts O
and O
implausible O
semantics O
caused O
by O
insertion O
and O
deletion O
. O
The O
output O
of O
this O
stage O
is O
an O
ordered O
list O
of O
candidate O
attacks O

H O
sqc O
= O
⟨H O
( O
2 O
) O
r O
1 O
, O
H O
( O
2 O
) O
r O
2 O
, O
• O
• O
• O
, O
H O
( O
2 O
) O
r O
k O
⟩ O
. O

Iterative O
and O
Multi O
- O
rounds O
Attacking O

As O
discussed O
above O
, O
NatLogAttack B-MethodName
performs O
iterative O
attacking O
within O
each O
round O
of O
generation O
and O
then O
multi O
- O
round O
attacks O
if O
the O
current O
round O
fails O
. O
Within O
each O
round O
, O
the O
original O
premise O
P O
and O
each O
hypothesis O
in O
the O
ranked O
hypotheses O
list O
H O
sqc O
form O
an O
attack O
list O
⟨⟨P O
, O
H O

2 O
) O
r O
1 O
⟩ O
, O
• O
• O
• O
, O
⟨P O
, O
H O
( O
2 O
) O
r O
k O
⟩⟩. O
( O

As O
shown O
in O
Figure O
1 O
, O
when O
an O
attack O
succeeds O
, O
we O
output O
the O
corresponding O
hypothesis O
as O
H O
* O
, O
which O
is O
sent O
for O
evaluation O
. O
If O
an O
attack O
fails O
, O
the O
next O
pair O
in O
the O
ranked O
attack O
list O
will O
be O
tried O
until O
the O
list O
is O
exhausted O
. O
Then O
NatLogAttack B-MethodName
organizes O
the O
next O
round O
of O
attacks O
. O
In O
total O
NatLogAttack B-MethodName
generates O
a O
maximum O
of O
500 O
attacks O
for O
each O
⟨P O
, O
H⟩ O
pair O
. O
When O
generating O
the O
next O
round O
attacks O
, O
we O
identify O
the O
adversarial O
pair O
for O
which O
the O
victim O
model O
has O
the O
lowest O
confidence O
( O
indexed O
as O
j O
lc O
) O
over O
the O
ground O
- O
truth O
class O
y O
* O
g O
: O

j O
lc O
= O
arg O
min O
j∈ O
{ O
r O
1 O
, O
... O
, O
r O
k O
} O
{ O
s O
r O
1 O
, O
. O
. O
. O
, O
s O
r O
k O
} O
( O
2 O
) O
s O
r O
j O
= O
o O
( O
y O
* O
g O
| O
( O
P O
, O
H O
( O
2 O
) O
r O
j O
) O
) O
( O
3 O
) O

where O
o O
( O
* O
) O
returns O
the O
corresponding O
softmax O
probabilities O
of O
the O
output O
layer O
. O
We O
then O
copy O
H O

( O
2 O
) O

j O
lc O
to O
H O
( O
1 O
) O
, O
denoted O
as O
H O
( O
1 O
) O
← O
H O

( O
2 O
) O
j O
lc O
. O
The O
attack O
continues O
until O
the O
victim O
model O
is O
deceived O
to O
make O
a O
wrong O
prediction O
y O
* O
that O
is O
different O
from O
the O
ground O
truth O
y O
* O
g O
or O
the O
maximum O
number O
of O
attacks O
is O
reached O
. O

Experiments O
and O
Results O

Experimental O
Setup O

Dataset O
Our O
study O
uses O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
MED B-DatasetName
( O
Yanaka O
et O
al O
. O
, O
2019a O
) O
, O
HELP B-DatasetName
( O
Yanaka O
et O
al O
. O
, O
2019b O
) O
, O
and O
SICK B-DatasetName
( O
Marelli O
et O
al O
. O
, O
2014 O
; O
datasets O
. O
The O
MED B-DatasetName
upward O
and O
downward O
subsets O
are O
denoted O
as O
MED B-DatasetName
up I-DatasetName
and O
MED B-DatasetName
down I-DatasetName
, O
respectively O
. O
Details O
of O
the O
datasets O
and O
the O
setup O
for O
training O
can O
be O
found O
in O
Appendix O
C. O
( O
Zang O
et O
al O
. O
, O
2020 O
) O
. O
Specifically O
, O
we O
used O
the O
implementation O
made O
publicly O
available O
in O
TextAttack O
. O
5 O
For O
victim O
models O
, O
we O
used O
uncased O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa O
base O
models O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
The O
accuracy B-MetricName
of O
victim O
models O
is O
included O
in O
Table O
3 O
, O
which O
is O
comparable O
to O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

Attack O
and O
Victim O
Models O

Evaluation O
Metrics O
Three O
metrics O
are O
used O
to O
evaluate O
the O
models O
from O
different O
perspectives O
. O
The O
sign O
↑ O
( O
↓ O
) O
indicates O
that O
the O
higher O
( O
lower O
) O
the O
values O
are O
, O
the O
better O
the O
performance O
is O
. O
to O
validate O
if O
the O
generated O
attack O
examples O
belong O
to O
the O
desired O
relations O
. O
Each O
example O
was O
annotated O
by O
at O
least O
three O
workers O
and O
the O
label O
is O
determined O
by O
the O
majority O
voting O
. O
HVASR B-MetricName
is O
the O
percentage O
of O
successful O
- O
and O
- O
valid O
adversarial O
examples O
that O
successfully O
deceived O
the O
victim O
models O
to O
make O
the O
wrong O
prediction O
and O
at O
the O
same O
time O
the O
majority O
of O
the O
annotators O
think O
their O
NLI O
labels O
are O
the O
desired O
target O
labels O
y O
* O
g O
. O
While O
HVASR B-MetricName
is O
our O
major O
evaluation O
metric O
, O
we O
also O
use O
query O
numbers O
and O
perplexity O
to O
provide O
additional O
perspectives O
for O
observations O
. O
• O
Query B-MetricName
number I-MetricName
( O
QN B-MetricName
↓ O
) O
refers O
to O
the O
average O
number O
of O
times O
that O
a O
successful O
attack O
needs O
to O
query O
the O
victim O
model O
( O
Zang O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020 O
) O
. O
QN B-MetricName
can O
reflect O
the O
efficiency O
( O
but O
6 O
https O
: O
/ O
/ O
www.mturk.com O
/ O
not O
effectiveness O
) O
of O
an O
attack O
model O
. O
• O
Perplexity B-MetricName
( O
PPL B-MetricName
↓ O
) O
reflects O
the O
fluency O
and O
quality O
of O
generated O
examples O
. O
Same O
as O
in O
( O
Zang O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2021 O
) O
, O
it O
is O
computed O
with O
GPT-2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
during O
evaluation O
. O

Results O
and O
Analysis O

Results O
on O
Label B-TaskName
Preserving I-TaskName
Attacks I-TaskName
Table O
4 O
shows O
the O
performance O
of O
different O
models O
on O
labelpreserving B-TaskName
attacks I-TaskName
. O
We O
can O
see O
that O
NatLogAttack B-MethodName
consistently O
achieves O
the O
best O
performance O
on O
HVASR B-MetricName
. O
The O
detailed O
results O
on O
MED B-DatasetName
also O
show O
that O
NatLogAttack B-MethodName
has O
a O
better O
ability O
to O
construct O
adversarial O
examples O
in O
both O
upward O
and O
downward O
monotone O
. O
NatLogAttack B-MethodName
also O
shows O
superior O
performance O
on O
average O
QN B-MetricName
and O
PPL B-MetricName
in O
nearly O
all O
setups O
. O
We O
can O
see O
that O
NatLogAttack O
has O
a O
large B-MetricValue
HVASR B-MetricName
and O
small B-MetricValue
QN B-MetricName
value O
in O
MED B-DatasetName
up I-DatasetName
, O
suggesting O
that O
NatLogAttack B-MethodName
can O
easily O
generate O
attacks O
in O
the O
upward O
monotone O
. O
However O
, O
in O
MED B-DatasetName
down I-DatasetName
, O
NatLogAttack B-MethodName
needs O
more O
efforts O
( O
QN B-MetricName
) O
. O
Our O
further O
analysis O
reveals O
that O
this O
is O
because O
in O
the O
downward O
monotone O
, O
the O
attack O
model O
relies O
more O
on O
the O
insertion O
operation O
than O
deletion O
, O
and O
the O
former O
is O
more O
likely O
to O
result O
in O
unsuccessful O
attempts O
. O

Figure O
2 O
further O
compares O
the O
query B-MetricName
numbers I-MetricName
( O
QNs B-MetricName
) O
of O
different O
attack O
models O
on O
BERT O
and O
RoBERTa O
in O
terms O
of O
the O
medians O
( O
instead O
of O
means O
) O
and O
density O
of O
QN B-MetricName
. O
We O
can O
see O
that O
the O
majority O
of O
query O
numbers O
of O
NatLogAttack B-MethodName
are O
rather O
small O
and O
medians O
are O
less B-MetricValue
than I-MetricValue
12 I-MetricValue
for O
on O
both O
SNLI B-DatasetName
and O
MED B-DatasetName
, O
showing O
that O
NatLogAttack B-MethodName
could O
attack O
successfully O
with O
very O
limited O
attempts O
in O
most O
cases O
. O
For O
each O
attack O
model O
, O
the O
density O
of O
QN B-MetricName
on O
BERT O
and O
RoBERTa O
is O
close O
to O
each O
other O
and O
the O
medians O
are O
indiscernible O
and O
are O
represented O
by O
the O
same O
red O
dot O
in O
the O
figure O
. O

Results O
on O
Label B-TaskName
Flipping I-TaskName
Attacks I-TaskName
Table O
5 O
shows O
the O
performance O
of O
NatLogAttack B-MethodName
on O
the O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
. O
Note O
that O
there O
has O
been O
little O
prior O
work O
providing O
systematic O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
for O
NLP O
tasks O
. O
This O
new O
angle O
of O
evaluation O
is O
more O
easily O
implemented O
with O
logic O
- O
based O
attacks O
and O
provides O
additional O
insights O
. O
Specifically O
, O
the O
table O
shows O
that O
the O
numbers O
of O
queries O
that O
NatLogAttack B-MethodName
sent O
to O
the O
victim O
models O
are O
much O
smaller O
than O
those O
in O
the O
label O
- O
preserving O
setting O
presented O
in O
Table O
4 O
, O
suggesting O
that O
the O
victim O
models O
are O
more O
vulnerable O
in O
label O
- O
flipping O
setting O
. O
For O
example O
, O
we O
can O
see O
that O
most O
of O
the O
query O
numbers O
are O
within O
1 O
- O
5 O
in O
Table O
5 O
. O
The O
pretrained O
victim O
models O
are O
capable O
of O
memorizing O
the O
superficial O
features O
related O
to O
the O
original O
label O
and O
have O
difficulty O
in O
capturing O
the O
logical O
relationship O
when O
we O
alter O
them O
between O
sentences O
by O
keeping O
the O
majority O
of O
words O
untouched O
. O

In O
both O
the O
label O
- O
preserving O
and O
label O
- O
flipping O
setup O
, O
the O
HVASR B-MetricName
may O
still O
be O
further O
improved O
, O
although O
the O
proposed O
models O
have O
substantially O
outperformed O
the O
off O
- O
the O
- O
shelf O
state O
- O
of O
- O
the O
- O
art O
attack O
models O
and O
cautions O
have O
been O
exercised O
in O
all O
attack O
generation O
steps O
, O
which O
leaves O
room O
for O
more O
research O
on O
improving O
logic O
- O
based O
attacks O
as O
future O
work O
. O

Examples O
and O
Analysis O
. O
Table O
6 O
provides O
the O
generated O
attack O
examples O
in O
the O
label O
- O
preserving O
setup O
( O
E O
→ O
E O
) O
, O
in O
which O
we O
can O
see O
the O
quality O
of O
attacks O
generated O
by O
NatLogAttack B-MethodName
is O
clearly O
higher O
. O
The O
baseline O
attacking O
models O
generate O
adversarial O
examples O
by O
replacing O
words O
based O
on O
word O
embedding O
or O
language O
models O
, O
which O
can O
easily O
break O
the O
logic O
relationships O
. O
Some O
examples O
in O
Table O
6 O
show O
that O
the O
baselines O
often O
rely O
on O
semantic O
relatedness O
to O
construct O
adversarial O
examples O
, O
which O
is O
not O
detailed O
enough O
for O
NLI O
and O
hence O
break O
the O
logic O
relations O
( O
e.g. O
, O
the O
last O
BertAttack B-MethodName
example O
) O
. O
Also O
, O
the O
last O
example O
of O
Clare B-MethodName
shows O
that O
the O
model O
deletes O
words O
without O
considering O
the O
context O
( O
downward O
) O
monotonicity O
, O
resulting O
in O
an O
invalid O
attack O
. O
Note O
that O
the O
baseline O
models O
modify O
both O
premises O
and O
hypotheses O
and O
NatLagAttack B-MethodName
focuses O
only O
on O
modifying O
hypotheses O
- O
it O
is O
straightforward O
to O
copy O
or O
adapt O
the O
operations O
of O
NatLagAttack B-MethodName
to O
modify O
premises O
- O
in O
many O
applications O
, O
it O
is O
more O
natural O
to O
modify O
the O
hypotheses O
and O
keep O
the O
premises O
( O
evidences O
) O
untouched O
. O

Table O
7 O
shows O
more O
adversarial O
examples O
generated O
by O
NatLogAttack B-MethodName
in O
the O
label O
- O
flipping O
setup O
. O
For O
all O
the O
six O
examples O
, O
the O
prediction O
of O
the O
victim O
model O
RoBERTa O
remains O
unchanged O
( O
i.e. O
, O
entailment O
, O
entailment O
and O
contradiction O
for O
the O
first O
, O
middle O
, O
and O
last O
two O
examples O
, O
respectively O
) O
, O
while O
the O
ground O
- O
truth O
labels O
are O
now O
contradiction O
, O
neutral O
, O
and O
entailment O
, O
respectively O
. O
The O
victim O
model O
had O
difficulty O
in O
telling O
the O
difference O
, O
which O
renders O
an O
angle O
to O
challenge O
the O
models O
' O
ability O
of O
understanding O
and O
perform O
reasoning O
. O

Conclusion O

Towards O
developing O
logic O
- O
based O
attack O
models O
, O
we O
introduce O
a O
framework O
NatLogAttack B-MethodName
, O
which O
centres O
around O
the O
classical O
natural O
logic O
formalism O
. O
The O
experiments O
with O
human O
and O
automatic O
evaluation O
show O
that O
the O
proposed O
framework O
outperforms O
the O
existing O
attack O
methods O
. O
Compared O
to O
these O
models O
, O
NatLogAttack B-MethodName
generates O
better O
adversarial O
examples O
with O
fewer O
visits O
to O
the O
victim O
models O
. O
In O
addition O
to O
the O
widely O
used O
labelpreserving O
attacks O
, O
NatLogAttack B-MethodName
also O
provides O
label B-TaskName
- I-TaskName
flipping I-TaskName
attacks I-TaskName
. O
The O
victim O
models O
are O
found O
to O
be O
more O
vulnerable O
in O
this O
setup O
and O
NatLogAttack B-MethodName
succeeds O
in O
deceiving O
them O
with O
much O
smaller O
numbers O
of O
queries O
. O
NatLogAttack B-MethodName
provides O
an O
approach O
to O
probing O
the O
existing O
and O
future O
NLI O
models O
' O
capacity O
from O
a O
key O
viewpoint O
and O
we O
hope O
more O
logic O
- O
based O
attacks O
will O
be O
further O
explored O
for O
understanding O
the O
desired O
property O
of O
reasoning O
. O

Limitations O

Our O
research O
focuses O
on O
the O
adversarial O
attack O
itself O
and O
provides O
a O
framework O
that O
can O
be O
potentially O
used O
in O
different O
adversarial O
training O
strategies O
. O
We O
limit O
ourselves O
on O
attacks O
in O
this O
work O
, O
but O
it O
would O
be O
interesting O
to O
investigate O
logic O
- O
based O
attacks O
in O
adversarial O
training O
. O
We O
will O
leave O
that O
as O
future O
work O
. O
The O
proposed O
attack O
approach O
is O
also O
limited O
by O
the O
limitations O
of O
natural O
logic O
, O
while O
the O
latter O
has O
been O
a O
classical O
logic O
mechanism O
. O
For O
example O
, O
our O
proposed O
framework O
has O
less O
deductive O
power O
than O
first O
- O
order O
logic O
. O
It O
can O
not O
construct O
attacks O
building O
on O
inference O
rules O
like O
modus O
ponens O
, O
modus O
tollens O
, O
and O
disjunction O
elimination O
. O

As O
discussed O
in O
the O
paper O
, O
some O
components O
of O
the O
generation O
and O
quality O
control O
process O
can O
be O
further O
enhanced O
. O

Relation O

Relation O
Name O
Example O
Set O
Theoretic O
Definition O
( O
2009 O
) O
. O
parser O
7 O
, O
which O
are O
then O
used O
with O
a O
state O
- O
of O
- O
the O
- O
art O
pretrained O
model O
to O
perform O
insertion O
. O
To O
insert O
an O
adjective O
before O
a O
noun O
or O
an O
adverb O
after O
a O
verb O
, O
NatLogAttack B-MethodName
leverages O
DistilBert O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
to O
obtain O
the O
candidates O
in O
the O
corresponding O
locations O
. O
The O
syntactic O
rules O
do O
not O
guarantee O
to O
generate O
sentences O
with O
the O
desired O
NLI O
labels O
( O
e.g. O
, O
see O
( O
Partee O
, O
1995 O
) O
for O
discussion O
on O
the O
semantic O
composition O
of O
adjective O
+ O
noun O
) O
. O
The O
above O
process O
is O
only O
for O
generating O
candidates O
, O
and O
we O
will O
use O
pretrained O
language O
models O
to O
find O
good O
adversarial O
examples O
. O

x O
≡ O
y O
equivalence O
mommy O
≡ O
mother O
x O
= O
y O
x O
⊏ O
y O
forward O
entailment O
bird O
⊏ O
animal O
x O
⊂ O
y O
x O
⊐ O
y O
reverse O
entailment O
animal O
⊐ O
bird O
x O
⊃ O
y O
x O
∧ O
y O
negation O
human O
∧ O
nonhuman O
x O
∩ O
y O
= O
∅ O
∧ O
x O
∪ O
y O
= O
U O
x O
| O
y O
alternation O
bird O
| O
dog O
x O
∩ O
y O
= O
∅ O
∧ O
x O
∪ O
y O
̸ O
= O
U O
x O
⌣ O
y O
cover O
animal O
⌣ O
nonhuman O
x O
∩ O
y O
̸ O
= O
∅ O
∧ O
x O
∪ O
y O
= O
U O
x O
# O
y O
independence O
red O
# O
weak O
all O
other O
cases O

1 O
≡ O
⊏ O
⊐ O
∧ O
| O
⌣ O
# O
≡ O
≡ O
⊏ O
⊐ O
∧ O
| O
⌣ O
# O
⊏ O
⊏ O
⊏ O
# O
| O
| O
# O
# O
⊐ O
⊐ O
# O
⊐ O
⌣ O
# O
⌣ O
# O
∧ O
∧ O
⌣ O
| O
≡ O
⊐ O
⊏ O
# O
| O
| O
# O
| O
⊏ O
# O
⊏ O
# O
⌣ O
⌣ O
⌣ O
# O
⊐ O
⊐ O
# O
# O
# O
# O
# O
# O
# O
# O
# O
# O

In O
order O
to O
insert O
a O
prepositional O
phrase O
( O
PP O
) O
, O
we O
first O
collected O
from O
the O
SNLI B-DatasetName
training O
dataset O
all O
the O
PPs O
that O
are O
the O
constitutes O
of O
other O
noun O
phrases O
( O
NPs O
) O
for O
more O
than O
100 O
times O
. O
We O
also O
collected O
PPs O
that O
appear O
in O
other O
verb O
phrases O
( O
VPs O
) O
at O
least O
100 O
times O
. O
During O
insertion O
, O
these O
PPs O
will O
be O
added O
as O
modifiers O
to O
a O
noun O
or O
a O
verb O
, O
respectively O
. O
We O
also O
insert O
assertion O
phrases O
such O
as O
" O
It O
is O
not O
true O
that O
" O
to O
deceive O
the O
victim O
models O
. O
For O
the O
deletion O
operation O
, O
we O
delete O
the O
corresponding O
constituents O
based O
on O
the O
parse O
tree O
and O
POS O
tags O
. O

C O
Details O
of O
Datasets O
and O
Baselines O

As O
discussed O
in O
Section O
4.1 O
, O
our O
study O
uses O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
MED B-DatasetName
( O
Yanaka O
et O
al O
. O
, O
2019a O
) O
, O
HELP B-DatasetName
( O
Yanaka O
et O
al O
. O
, O
2019b O
) O
, O
and O
SICK B-DatasetName
( O
Marelli O
et O
al O
. O
, O
2014 O
; O
to O
evaluate O
the O
models O
. O
SNLI B-DatasetName
and O
MNLI B-DatasetName
are O
widely O
- O
used O
general O
- O
purpose O
NLI O
datasets O
. O
Following O
Li O
et O
al O
. O
( O
2021 O
) O
, O
for O
MNLI B-DatasetName
, O
we O
evaluate O
the O
performance O
on O
the O
matched O
set O
. O
MED B-DatasetName
and O
HELP B-DatasetName
are O
designed O
for O
monotonicitybased O
reasoning O
and O
hence O
suit O
for O
probing O
models O
' O
capacity O
in O
natural O
logic O
- O
related O
behaviour O
. O
SICK B-DatasetName
is O
rich O
in O
lexical O
, O
syntactic O
and O
semantic O
phenomena O
designed O
for O
distributional O
semantic O
models O
including O
those O
recognizing O
textual O
entailment O
. O
For O
SICK B-DatasetName
, O
we O
use O
the O
corrected O
labels O
proposed O
by O
. O
The O
pretrained O
victim O
models O
tested O
on O
the O
SNLI B-DatasetName
, O
MNLI B-DatasetName
, O
and O
SICK B-DatasetName
test O
set O
were O
finetuned O
on O
their O
own O
training O
set O
and O
the O
performances O
are O
comparable O
to O
the O
state O
- O
of O
- O
the O
- O
art O
performances O
as O
well O
as O
those O
used O
in O
the O
previous O
attack O
models O
. O
Following O
Yanaka O
et O
al O
. O
( O
2019a O
) O
, O
the O
models O
tested O
on O
MED B-DatasetName
are O
finetuned O
on O
both O
the O
SNLI B-DatasetName
training O
set O
and O
the O
entire O
HELP B-DatasetName
dataset O
. O
Since O
HELP O
is O
not O
manually O
annotated O
, O
we O
do O
not O
use O
it O
as O
the O
test O
set O
. O
The O
MED B-DatasetName
upward O
subset O
is O
denoted O
as O
MED B-DatasetName
up I-DatasetName
and O
downward O
subset O
as O
MED B-DatasetName
down I-DatasetName
. O
Following O
( O
Alzantot O
et O
al O
. O
, O
2018 O
; O
Zang O
et O
al O
. O
, O
2020 O
) O
, O
each O
test O
set O
has O
1,000 O
sentence O
pairs O
. O
Also O
following O
Zeng O
et O
al O
. O
( O
2021 O
) O
, O
we O
set O
the O
maximum O
query O
number O
to O
be O
500 O
. O

For O
all O
the O
attack O
models O
in O
comparison O
, O
we O
used O
the O
implementation O
made O
available O
by O
Morris O
et O
al O
. O
( O
2020 O
) O
. O
Details O
of O
these O
attack O
models O
are O
as O
follows O
. O

• O
PWWS B-MethodName
( O
Ren O
et O
al O
. O
, O
2019 O
) O
makes O
use O
of O
the O
synonyms O
in O
WordNet O
( O
Miller O
, O
1995 O
) O
for O
word O
substitutions O
and O
designs O
a O
greedy O
search O
algorithm O
based O
on O
the O
probability O
- O
weighted O
word O
saliency O
to O
generate O
adversarial O
samples O
. O

• O
TextFooler B-MethodName
utilizes O
counterfitting O
word O
embeddings O
to O
obtain O
synonyms O
and O
then O
performs O
substitution O
based O
on O
that O
. O

• O
PSO B-MethodName
( O
Zang O
et O
al O
. O
, O
2020 O
) O
utilizes O
the O
knowledge O
base O
HowNet O
( O
Dong O
et O
al O
. O
, O
2010 O
) O
to O
generate O
word O
substitutions O
. O
It O
adopts O
particle O
swarm O
optimization O
, O
another O
popular O
metaheuristic O
population O
- O
based O
search O
algorithm O
, O
as O
its O
search O
strategy O
. O

• O
BertAttack B-MethodName
( O
Li O
et O
al O
. O
, O
2020 O
) O
leverages O
the O
superior O
performance O
of O
pretrained O
language O
model O
and O
greedily O
replaces O
tokens O
with O
the O
predictions O
from O
BERT O
. O

• O
Clare B-MethodName
( O
Li O
et O
al O
. O
, O
2021 O
) O
adds O
two O
more O
types O
of O
perturbations O
, O
insert O
and O
merge O
, O
building O
on O
BertAttack B-MethodName
. O
Since O
Clare B-MethodName
has O
a O
very O
high O
query O
number O
to O
the O
victim O
models O
, O
we O
reduce O
the O
number B-HyperparameterName
of I-HyperparameterName
each I-HyperparameterName
type I-HyperparameterName
of I-HyperparameterName
perturbation I-HyperparameterName
to O
10 B-HyperparameterValue
in O
order O
to O
make O
sure O
that O
Clare B-MethodName
can O
attack O
the O
victim O
model O
successfully O
within O
the O
maximum O
query O
number O
in O
most O
cases O
. O

9974 O

ACL O
2023 O
Responsible O
NLP O
Checklist O
A O
For O
every O
submission O
: O

A1 O
. O
Did O
you O
describe O
the O
limitations O
of O
your O
work O
? O

They O
are O
in O
the O
Limitation O
Section O
A2 O
. O
Did O
you O
discuss O
any O
potential O
risks O
of O
your O
work O
? O

We O
do O
not O
envision O
there O
are O
potential O
risks O
. O

A3 O
. O
Do O
the O
abstract O
and O
introduction O
summarize O
the O
paper O
's O
main O
claims O
? O

The O
abstract O
and O
introduction O
summarize O
the O
main O
claims O
presented O
in O
section O
3 O
- O
5 O
. O

A4 O
. O
Have O
you O
used O
AI O
writing O
assistants O
when O
working O
on O
this O
paper O
? O

Left O
blank O
. O

B O
Did O
you O
use O
or O
create O
scientific O
artifacts O
? O

Section O
3 O
and O
4 O
. O

B1 O
. O
Did O
you O
cite O
the O
creators O
of O
artifacts O
you O
used O
? O

Section O
3 O
an O
4 O
B2 O
. O
Did O
you O
discuss O
the O
license O
or O
terms O
for O
use O
and O
/ O
or O
distribution O
of O
any O
artifacts O
? O

We O
used O
the O
mostly O
widely O
use O
NLP O
tools O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Not O
relevant O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
we O
do O
not O
think O
the O
data O
we O
used O
contain O
such O
information O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
not O
relevant O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O

Left O
blank O
. O

C O
Did O
you O
run O
computational O
experiments O
? O
section O
4 O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
we O
did O
not O
collect O
the O
information O
. O
D2 O
. O
Did O
you O
report O
information O
about O
how O
you O
recruited O
( O
e.g. O
, O
crowdsourcing O
platform O
, O
students O
) O

and O
paid O
participants O
, O
and O
discuss O
if O
such O
payment O
is O
adequate O
given O
the O
participants O
' O
demographic O
( O
e.g. O
, O
country O
of O
residence O
) O
? O
we O
follow O
MTurk O
standards O
. O

D3 O
. O
Did O
you O
discuss O
whether O
and O
how O
consent O
was O
obtained O
from O
people O
whose O
data O
you O
're O
using O
/ O
curating O
? O
For O
example O
, O
if O
you O
collected O
data O
via O
crowdsourcing O
, O
did O
your O
instructions O
to O
crowdworkers O
explain O
how O
the O
data O
would O
be O
used O
? O
we O
follow O
MTurk O
standards O
. O

D4 O
. O
Was O
the O
data O
collection O
protocol O
approved O
( O
or O
determined O
exempt O
) O
by O
an O
ethics O
review O
board O
? O

We O
use O
MTurk O
and O
the O
collection O
is O
not O
qualified O
for O
ethics O
review O
. O

D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
not O
relevant O

Acknowledgements O

The O
research O
is O
supported O
by O
the O
NSERC O
Discovery O
Grants O
and O
the O
Discovery O
Accelerator O
Supplements O
. O
We O
thank O
Bairu O
Hou O
for O
his O
contributions O
to O
an O
early O
version O
of O
the O
proposed O
model O
. O

A O
Background O

Our O
work O
is O
based O
on O
the O
specific O
natural O
logic O
formalism O
proposed O
by O
MacCartney O
and O
Manning O
( O
2009 O
) O
and O
MacCartney O
( O
2009 O
) O
. O
To O
model O
the O
entailment O
relations O
between O
two O
spans O
of O
texts O
, O
Mac O
- O
Cartney O
and O
Manning O
( O
2009 O
) O
introduced O
seven O
relations O
inspired O
by O
the O
set O
theory O
: O
8 O
for O
some O
examples O
) O
. O
The O
inference O
of O
natural O
logic O
is O
built O
on O
monotonicity O
, O
which O
is O
a O
pervasive O
feature O
of O
natural O
language O
that O
explains O
the O
impact O
of O
semantic O
composition O
on O
entailment O
relations O
( O
Van O
Benthem O
et O
al O
. O
, O
1986 O
; O
Valencia O
, O
1991 O
; O
Icard O
and O
Moss O
, O
2014 O
) O
. O
Suppose O
dog O
⊏ O
animal O
, O
the O
upward O
monotone O
context O
keeps O
the O
entailment O
relation O
when O
the O
argument O
" O
increases O
" O
( O
e.g. O
, O
dog O
⊏ O
animal O
) O
. O
Downward O
monotone O
keeps O
the O
entailment O
relation O
when O
the O
argument O
" O
decreases O
" O
( O
e.g. O
, O
in O
all O
animals O
⊏ O
all O
dogs O
) O
. O
The O
system O
performs O
monotonicity O
inference O
through O
a O
projection O
ρ O
: O
B O
→ O
B O
, O
which O
is O
determined O
by O
the O
context O
and O
projection O
rules O
. O
As O
will O
be O
detailed O
, O
a O
monotonicity O
- O
based O
parser O
can O
provide O
monotonicity O
information O
for O
each O
word O
token O
in O
a O
sentence O
and O
the O
projection O
information O
. O
For O
example O
, O
consider O
the O
sentence O
All↑ O
the↓ O
kids↓ O
run↑ O
, O
where O
↑ O
denoted O
upward O
polarity O
and O
↓ O
downward O
polarity O
. O
If O
we O
mutate O
the O
word O
kids O
with O
boys O
, O
where O
kids O
⊐ O
boys O
, O
the O
system O
projects O
the O
reverse O
entailment O
( O
' O
⊐ O
' O
) O
into O
forward O
entailment O
( O
' O
⊏ O
' O
) O
due O
to O
its O
downward O
polarity O
, O
i.e. O
, O
ρ O
( O
' O
⊐ O
' O
) O
= O
' O
⊏ O
' O
, O
and O
thus O
All O
the O
kids O
run O
⊏ O
All O
the O
boys O
run O
. O

With O
these O
components O
ready O
, O
the O
system O
aggregates O
the O
projected O
local O
relations O
to O
obtain O
the O
inferential O
relation O
between O
a O
premise O
and O
hypothesis O
sentence O
. O
Specifically O
, O
Table O
9 O
( O
MacCartney O
, O
2009 O
; O
MacCartney O
and O
Manning O
, O
2009 O
; O
Angeli O
and O
Manning O
, O
2014 O
) O
shows O
the O
composition O
function O
when O
a O
relation O
in O
the O
first O
column O
is O
joined O
with O
a O
relation O
listed O
in O
the O
first O
row O
, O
yielding O
the O
relations O
in O
the O
corresponding O
table O
cell O
. O
MacCartney O
( O
2009 O
) O
shows O
that O
different O
orders O
of O
compositions O
yield O
consistent O
results O
except O
in O
some O
rare O
artificial O
cases O
. O
Therefore O
, O
many O
works O
, O
including O
ours O
, O
perform O
a O
sequential O
( O
left O
- O
to O
- O
right O
) O
composition O
. O
Consider O
two O
edits O
from O
the O
premise O
sentence O
, O
All O
the O
kids O
run O
, O
to O
the O
hypothesis O
, O
All O
the O
boys O
sleep O
. O
The O
first O
edit O
that O
replaces O
kids O
in O
the O
premise O
with O
boys O
yields O
All O
the O
kids O
run O
⊏ O
All O
the O
boys O
run O
. O
The O
second O
edit O
of O
replacing O
run O
with O
sleep O
yields O
All O
the O
boys O
run O
| O
All O
the O
boys O
sleep O
. O
Based O
on O
Table O
9 O
, O
the O
union O
of O
the O
relations O
resulted O
from O
these O
two O
edits O
( O
i.e. O
, O
' O
⊏ O
' O
1 O
' O
| O
' O
) O
is O
' O
| O
' O
, O
where O
1 O
is O
the O
union O
operator O
. O
As O
a O
result O
, O
we O
obtain O
All O
the O
kids O
run O
| O
All O
the O
boys O
sleep O
. O

The O
seven O
natural O
logic O
relations O
at O
the O
sentencepair O
level O
can O
then O
be O
mapped O
to O
the O
typical O
threeway O
NLI O
labels O
( O
entailment O
, O
contradiction O
, O
and O
neutral O
) O
, O
where O
the O
' O
≡ O
' O
or O
' O
⊏ O
' O
relation O
can O
be O
mapped O
to O
entailment O
; O
the O
' O
∧ O
' O
or O
' O
| O
' O
relation O
to O
contradiction O
; O
the O
' O
⊐ O
' O
, O
' O
⌣ O
' O
, O
and O
' O
# O
' O
to O
neutral O
. O

B O
Insertion O
and O
Deletion O

For O
both O
insertion O
and O
deletion O
, O
the O
part O
- O
ofspeech O
( O
POS O
) O
tags O
and O
constituency O
parse O
tree O
for O
H O
( O
1 O
) O
are O
first O
obtained O
using O
Stanford O
CoreNLP O

Evidence O
> O
Intuition O
: O
Transferability B-TaskName
Estimation I-TaskName
for I-TaskName
Encoder I-TaskName
Selection I-TaskName

With O
the O
increase O
in O
availability O
of O
large O
pre O
- O
trained O
language O
models O
( O
LMs O
) O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
, O
it O
becomes O
critical O
to O
assess O
their O
fit O
for O
a O
specific O
target O
task O
a O
priori O
- O
as O
fine O
- O
tuning O
the O
entire O
space O
of O
available O
LMs O
is O
computationally O
prohibitive O
and O
unsustainable O
. O
However O
, O
encoder B-TaskName
transferability I-TaskName
estimation I-TaskName
has O
received O
little O
to O
no O
attention O
in O
NLP O
. O
In O
this O
paper O
, O
we O
propose O
to O
generate O
quantitative O
evidence O
to O
predict O
which O
LM O
, O
out O
of O
a O
pool O
of O
models O
, O
will O
perform O
best O
on O
a O
target O
task O
without O
having O
to O
fine O
- O
tune O
all O
candidates O
. O
We O
provide O
a O
comprehensive O
study O
on O
LM O
ranking O
for O
10 O
NLP O
tasks O
spanning O
the O
two O
fundamental O
problem O
types O
of O
classification O
and O
structured O
prediction O
. O
We O
adopt O
the O
state O
- O
of O
- O
the O
- O
art O
Logarithm B-MetricName
of I-MetricName
Maximum I-MetricName
Evidence I-MetricName
( O
LogME B-MetricName
) O
measure O
from O
Computer O
Vision O
( O
CV O
) O
and O
find O
that O
it O
positively O
correlates O
with O
final O
LM O
performance O
in O
94 B-MetricValue
% I-MetricValue
of O
the O
setups O
. O
In O
the O
first O
study O
of O
its O
kind O
, O
we O
further O
compare O
transferability O
measures O
with O
the O
de O
facto O
standard O
of O
human O
practitioner O
ranking O
, O
finding O
that O
evidence O
from O
quantitative O
metrics O
is O
more O
robust O
than O
pure O
intuition O
and O
can O
help O
identify O
unexpected O
LM O
candidates O
. O

Introduction O

Advances O
in O
Deep O
Learning O
- O
based O
NLP O
and O
CV O
build O
on O
expressive O
representations O
from O
encoder O
models O
pre O
- O
trained O
on O
massive O
corpora O
. O
Downstream O
models O
make O
use O
of O
latent O
information O
in O
these O
representations O
to O
extract O
relevant O
features O
for O
the O
task O
at O
hand O
. O
Within O
this O
paradigm O
, O
deciding O
which O
pre O
- O
trained O
encoder O
to O
use O
in O
any O
taskspecific O
architecture O
is O
crucial O
, O
however O
training O
a O
model O
using O
each O
encoder O
candidate O
is O
infeasible O
. O
In O
absence O
of O
prior O
heuristics O
( O
e.g. O
, O
via O
related O
work O
) O
, O
the O
choice O
of O
encoder O
has O
therefore O
prevailingly O
been O
based O
on O
practitioner O
intuition O
rather O
than O
quantitative O
evidence O
. O

The O
authors O
contributed O
equally O
to O
this O
work O
. O

In O
NLP O
, O
prior O
work O
has O
examined O
the O
different O
yet O
related O
task O
of O
performance O
prediction O
( O
Xia O
et O
al O
. O
, O
2020a O
; O
Ye O
et O
al O
. O
, O
2021 O
) O
, O
surveyed O
and O
categorized O
LMs O
( O
Xia O
et O
al O
. O
, O
2020b O
) O
, O
and O
used O
probing O
to O
predict O
LM O
performance O
specifically O
for O
dependency O
parsing O
( O
Müller O
- O
Eberstein O
et O
al O
. O
, O
2022b O
) O
, O
but O
has O
yet O
to O
extensively O
investigate O
how O
to O
rank O
the O
increasingly O
large O
number O
of O
pre O
- O
trained O
LM O
encoders O
across O
various O
tasks O
and O
domains O
. O
Preliminary O
work O
by O
You O
et O
al O
. O
( O
2021 O
) O
shows O
that O
the O
LogME B-MetricName
estimator O
holds O
promise O
, O
including O
the O
first O
steps O
for O
encoder O
selection O
in O
NLP O
. O
With O
their O
main O
focus O
being O
on O
CV O
, O
however O
, O
they O
evaluate O
only O
a O
limited O
set O
of O
tasks O
and O
models O
for O
NLP O
and O
use O
self O
- O
reported O
benchmark O
scores O
instead O
of O
running O
controlled O
experiments O
which O
should O
include O
, O
e.g. O
, O
the O
variance O
across O
initializations O
, O
domains O
, O
and O
fine O
- O
tuning O
strategies O
( O
Section O
2 O
) O
. O
As O
such O
, O
we O
seek O
to O
answer O
: O
How O
well O
can O
we O
estimate B-TaskName
the I-TaskName
transferability I-TaskName
of I-TaskName
pre I-TaskName
- I-TaskName
trained I-TaskName
LMs I-TaskName
to O
specific O
NLP O
tasks O
? O
To O
do O
so O
, O
we O
contribute O
: O

• O
The O
broadest O
encoder O
selection O
study O
in O
NLP O
to O
date O
, O
on O
10 O
domain O
- O
diverse O
classification O
and O
structured O
prediction O
tasks O
( O
Section O
3 O
) O
; O

• O
An O
extensive O
evaluation O
and O
analysis O
across O
multiple O
dimensions O
of O
variation O
, O
including O
seven O
general O
vs. O
domain O
- O
specific O
LMs O
, O

[ O
CLS O
] O
vs. O
mean O
representations O
, O
and O
head O
vs. O
full O
model O
fine O
- O
tuning O
( O
Section O
4 O
) O
; O

• O
A O
study O
with O
NLP O
experts O
, O
comparing O
the O
prevailing O
ranking O
of O
LMs O
by O
human O
intuition O
with O
LogME B-MetricName
's O
empirical O
evidence O
( O
Section O
5 O
) O
; O

• O
Guidelines O
for O
applying O
and O
interpreting O
transferability O
measures O
to O
NLP O
( O
Section O
6 O
) O
, O
and O
an O
open O
- O
source O
toolkit O
for O
efficient O
, O
task O
- O
adaptive O
LM O
pre O
- O
selection O
. O
1 O
DATASET O
TASK O
TRAIN O
/ O
DEV O
|Y| O
METRIC O
CLASSIFICATION O
AGNews B-DatasetName
( O
Zhang O
et O
al O
. O
, O
2015 O
) O
Topic B-TaskName
Classification I-TaskName
84 B-HyperparameterValue
K I-HyperparameterValue
/ O
12 B-HyperparameterValue
K I-HyperparameterValue
4 O
micro B-MetricName
- I-MetricName
F1 I-MetricName
Airline B-DatasetName
( O
Crowdflower O
, O
2020 O
) O
Sentiment B-TaskName
Analysis I-TaskName
10 B-HyperparameterValue
K I-HyperparameterValue
/ O
1.5 B-HyperparameterValue
K I-HyperparameterValue
3 O
micro B-MetricName
- I-MetricName
F1 I-MetricName
SciERC B-DatasetName
( O
Luan O
et O
al O
. O
, O
2018 O
) O
Relation B-TaskName
Classification I-TaskName
1.9 B-HyperparameterValue
K I-HyperparameterValue
/ O
275 B-HyperparameterValue
7 O
macro B-MetricName
- I-MetricName
F1 I-MetricName
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
393 B-HyperparameterValue
K I-HyperparameterValue
/ O
20 B-HyperparameterValue
K I-HyperparameterValue
3 O
micro B-MetricName
- I-MetricName
F1 I-MetricName
QNLI B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
Q B-TaskName
& I-TaskName
A I-TaskName
/ O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
105 B-HyperparameterValue
K I-HyperparameterValue
/ O
5.4 B-HyperparameterValue
K I-HyperparameterValue
2 O
micro B-MetricName
- I-MetricName
F1 I-MetricName
RTE B-DatasetName
( O
Giampiccolo O
et O
al O
. O
, O
2007 O
) O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
2.5 B-HyperparameterValue
K I-HyperparameterValue
/ O
3 B-HyperparameterValue
K I-HyperparameterValue
3 O
micro B-MetricName
- I-MetricName
F1 I-MetricName
STR O
. O
PRED O
. O

EWT B-DatasetName
( O
Silveira O
et O
al O
. O
, O
2014 O
) O
Dependency B-TaskName
Labeling I-TaskName
12.5k B-HyperparameterValue
/ O
2k B-HyperparameterValue
36 O
micro B-MetricName
- I-MetricName
F1 I-MetricName
CrossNER B-DatasetName
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
15 B-HyperparameterValue
K I-HyperparameterValue
/ O
3.5 B-HyperparameterValue
K I-HyperparameterValue
4 O
span B-MetricName
- I-MetricName
F1 I-MetricName
CrossNER B-DatasetName
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
200 B-HyperparameterValue
/ O
450 B-HyperparameterValue
17 O
span B-MetricName
- I-MetricName
F1 I-MetricName
JobStack B-DatasetName
( O
Jensen O
et O
al O
. O
, O
2021 O
) O
De B-TaskName
- I-TaskName
identification I-TaskName
18 B-HyperparameterValue
K I-HyperparameterValue
/ O
2 B-HyperparameterValue
K I-HyperparameterValue
11 O
span B-MetricName
- I-MetricName
F1 I-MetricName

Transferability B-TaskName
Estimation I-TaskName

Transferability B-TaskName
estimation I-TaskName
aims O
to O
quantify O
the O
ability O
of O
a O
model O
to O
transfer O
knowledge O
learned O
from O
one O
task O
to O
another O
( O
Eaton O
et O
al O
. O
, O
2008 O
; O
Sinapov O
et O
al O
. O
, O
2015 O
) O
. O
Formally O
, O
given O
a O
pool O
of O
L O
pretrained O
LMs O
{ O
ϕ O
l O
} O
L O
l=1 O
and O
a O
dataset O
D O
, O
we O
calculate O
a O
predictive O
score O
S O
l O
( O
D O
) O
for O
each O
ϕ O
l O
which O
ideally O
correlates O
with O
the O
model O
's O
final O
performance O
P O
l O
( O
D O
) O
. O
S O
l O
( O
D O
) O
is O
computed O
without O
fine O
- O
tuning O
ϕ O
l O
on O
D O
such O
that O
the O
optimal O
ϕ O
* O
l O
can O
be O
chosen O
from O
a O
large O
model O
pool O
at O
a O
low O
computational O
cost O
. O

The O
CV O
community O
has O
begun O
to O
explore O
methods O
for O
encoder O
pre O
- O
selection O
and O
ranking O
through O
metrics O
such O
as O
LogME B-MetricName
and O
the O
Log B-MetricName
Expected I-MetricName
Empirical I-MetricName
Prediction I-MetricName
( O
LEEP B-MetricName
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
. O
These O
are O
widely O
- O
used O
state O
- O
of O
- O
the O
- O
art O
methods O
in O
CV O
. O
Recent O
work O
introduced O
the O
Gaussian B-MetricName
Bhattacharyya I-MetricName
Coefficient I-MetricName
( O
GBC B-MetricName
; O
Pándy O
et O
al O
. O
, O
2021 O
) O
and O
Optimal B-MetricName
Transport I-MetricName
based I-MetricName
Conditional I-MetricName
Entropy I-MetricName
( O
OTCE B-MetricName
; O
Tan O
et O
al O
. O
, O
2021 O
) O
, O
the O
exploration O
of O
which O
we O
leave O
for O
future O
work O
. O
However O
, O
in O
the O
NLP O
field O
, O
related O
work O
focus O
on O
choosing O
a O
task O
and O
not O
an O
LM O
encoder O
for O
transferability O
( O
Vu O
et O
al O
. O
, O
2020 O
; O
Padmakumar O
et O
al O
. O
, O
2022 O
) O
, O
leaving O
the O
ranking O
of O
encoders O
an O
unexplored O
question O
. O

LogME B-MetricName
LogME B-MetricName
measures O
the O
suitability O
of O
all O
encoded O
dataset O
features O
F O
∈ O
R O
|D|×h O
( O
e.g. O
, O
embeddings O
with O
dimensionality B-HyperparameterName
h B-HyperparameterName
) O
to O
predict O
all O
scalar O
labels O
y O
∈ O
R O
|D| O
via O
the O
probability O
density O
p O
( O
y|F O
) O
. O
As O
this O
density O
is O
intractable O
, O
it O
is O
estimated O
by O
mapping O
F O
→ O
y O
using O
a O
linear O
transformation O
w O
; O
this O
is O
akin O
to O
training O
a O
linear O
probe O
with O
optimal O
param O
- O
eters O
w O
* O
and O
using O
the O
likelihood O
p O
( O
y|F O
, O
w O
* O
) O
as O
a O
proxy O
for O
feature O
suitability O
. O
Because O
a O
simple O
linear O
model O
will O
overfit O
on O
the O
training O
data O
, O
it O
would O
be O
beneficial O
to O
obtain O
the O
marginal O
likelihood O
, O
or O
evidence O
, O
by O
integrating O
over O
all O
possible O
values O
of O
w O
: O
p O
( O
y|F O
) O
= O
p O
( O
y|F O
, O
w O
) O
p O
( O
w O
) O
dw O
. O
To O
once O
again O
make O
this O
computation O
tractable O
, O
You O
et O
al O
. O
( O
2021 O
) O
reformulate O
it O
as O
an O
efficient O
, O
iterative O
evidence O
maximization O
problem O
where O
both O
w O
as O
well O
as O
y O
are O
drawn O
from O
lightly O
parametrized O
, O
isotropic O
Gaussian O
distributions O
. O
The O
normalized O
logarithm O
of O
the O
maximized O
evidence O
( O
LogME B-MetricName
) O
can O
then O
be O
used O
as O
S O
l O
( O
D O
) O
to O
rank O
encoder O
models O
directly O
. O

NLP O
Setting O
LogME B-MetricName
has O
shown O
promise O
for O
CV O
, O
and O
an O
initial O
study O
on O
the O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
indicate O
the O
same O
for O
NLP O
( O
You O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
for O
NLP O
, O
there O
are O
notable O
differences O
in O
setups O
across O
tasks O
. O
We O
adapt O
and O
apply O
LogME B-MetricName
extensively O
to O
a O
wide O
range O
of O
NLP O
settings O
to O
identify O
empirically O
grounded O
guidelines O
. O

In O
particular O
, O
we O
investigate O
variations O
concerning O
the O
task O
, O
instance O
granularity O
, O
domain O
, O
and O
tuning O
strategy O
. O
First O
, O
compared O
to O
most O
image O
classification O
tasks O
, O
NLP O
tasks O
are O
subject O
to O
differences O
in O
granularity O
, O
i.e. O
, O
classification B-TaskName
( O
C B-TaskName
) O
and O
structured B-TaskName
prediction I-TaskName
( O
SP B-TaskName
) O
. O
Furthermore O
, O
there O
is O
less O
clarity O
than O
for O
individual O
images O
as O
to O
which O
representation O
best O
captures O
the O
full O
language O
input O
( O
Mosbach O
et O
al O
. O
, O
2020 O
) O
. O
Therefore O
, O
for O
C B-TaskName
setups O
we O
experiment O
with O
two O
representations O
: O
i.e. O
, O
using O
[ O
CLS O
] O
/ O
< O
s O
> O
versus O
mean O
over O
sequence O
/ O
subwords O
. O

Second O
, O
depending O
on O
differences O
in O
the O
data O
domain O
, O
NLP O
practitioners O
are O
often O
faced O
with O
a O
pool O
of O
domain O
- O
adapted O
LMs O
in O
addition O
to O
more O
general O
- O
purpose O
encoders O
- O
the O
correct O
choice O
of O
which O
may O
not O
be O
immediately O
apparent O
. O
Finally O
, O
the O
best O
performance O
in O
NLP O
is O
often O
achieved O
using O
full O
fine O
- O
tuning O
, O
while O
CV O
models O
usually O
do O
not O
fine O
- O
tune O
the O
encoder O
( O
Peters O
et O
al O
. O
, O
2019 O
) O
. O
It O
will O
therefore O
be O
crucial O
to O
investigate O
whether O
the O
predictive O
performance O
of O
S O
l O
( O
D O
) O
holds O
when O
it O
is O
computed O
based O
on O
untuned O
F O
while O
P O
l O
( O
D O
) O
is O
based O
on O
fully O
fine O
- O
tuned O
representations O
. O

Experimental O
Setup O

Applying O
seven O
architecturally O
and O
domain O
- O
diverse O
pre O
- O
trained O
LMs O
with O
up O
to O
four O
configurations O
each O
to O
10 O
datasets O
and O
a O
wide O
variety O
of O
tasks O
, O
we O
investigate O
LogME B-MetricName
's O
predictive O
power O
for O
transferability B-TaskName
estimation I-TaskName
in O
NLP O
- O
for O
a O
total O
of O
280 O
setups O
. O
We O
refer O
to O
Table O
1 O
for O
our O
detailed O
set O
of O
tasks O
. O

Language O
Models O
We O
pick O
seven O
pre O
- O
trained O
LMs O
with O
a O
wide O
domain O
and O
architectural O
variety O
from O
the O
Transformers O
library O
's O
model O
hub O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
Three O
are O
" O
generalpurpose O
" O
models O
, O
namely O
BERT B-MethodName
base I-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
RoBERTa B-MethodName
base I-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
and O
DistilBERT B-MethodName
base I-MethodName
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
. O
Four O
models O
are O
pre O
- O
trained O
on O
domain O
- O
specific O
corpora O
, O
namely O
Clinical B-MethodName
- I-MethodName
BioBERT I-MethodName
( O
Alsentzer O
et O
al O
. O
, O
2019 O
) O
, O
BioBERT B-MethodName
( O
Lee O
et O
al O
. O
, O
2020 O
) O
, O
Twitter B-MethodName
- I-MethodName
RoBERTa I-MethodName
base I-MethodName
( O
Barbieri O
et O
al O
. O
, O
2020 O
) O
, O
and O
SciBERT B-MethodName
base I-MethodName
( O
Beltagy O
et O
al O
. O
, O
2019 O
) O
. O
Note O
that O
for O
BioBERT B-MethodName
variants O
domain B-MethodName
- I-MethodName
adaptive I-MethodName
pre I-MethodName
- I-MethodName
training I-MethodName
has O
been O
applied O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
. O

Model O
Setups O

The O
model O
setup O
follows O
the O
same O
structure O
for O
each O
task O
: O
A B-MethodName
pre I-MethodName
- I-MethodName
trained I-MethodName
LM I-MethodName
encoder I-MethodName
and I-MethodName
a I-MethodName
3 I-MethodName
- I-MethodName
layer I-MethodName
perceptron I-MethodName
head I-MethodName
, O
following O
Tenney O
et O
al O
. O
( O
2019 O
) O
. O
The O
input O
to O
the O
latter O
is O
either O
the O
[ O
CLS O
] O
token O
or O
mean O
over O
sequence O
subwords O
for O
C B-TaskName
tasks O
or O
mean O
over O
token O
subwords O
for O
SP B-TaskName
tasks O
. O
While O
it O
is O
common O
in O
CV O
to O
keep O
the O
encoder O
frozen O
and O
only O
fine O
- O
tune O
the O
task O
- O
specific O
head O
, O
we O
also O
evaluate O
the O
practice O
of O
full O
model O
finetuning O
, O
as O
is O
more O
common O
in O
NLP O
( O
Peters O
et O
al O
. O
, O
2019 O
) O
. O
Considering O
these O
variations O
( O
frozen O
vs. O
finetuning O
, O
and O
[ O
CLS O
] O
vs. O
mean O
) O
, O
we O
obtain O
up O
to O
four O
setups O
per O
C B-TaskName
task O
and O
two O
setups O
per O
SP B-TaskName
task O
. O
Each O
experiment O
is O
run O
with O
five O
random B-HyperparameterName
seeds I-HyperparameterName
. O
Details O
for O
reproducibility O
can O
be O
found O
in O
Appendix O
A O
. O

Evaluation O
Following O
You O
et O
al O
. O
( O
2021 O
) O
, O
we O
evaluate O
LogME B-MetricName
's O
predictive O
power O
for O
ranking O
LMs O
according O
to O
their O
final O
performance O
by O
using O
the O
two O
correlation O
coefficients O
Pearson B-MetricName
's I-MetricName
ρ I-MetricName
and O
weighted B-MetricName
Kendall I-MetricName
's I-MetricName
τ I-MetricName
w I-MetricName
( O
Vigna O
, O
2015 O
) O
, O
both O
in O
[ O
−1 B-MetricValue
, O
1 B-MetricValue
] O
. O
Kendall B-MetricName
's I-MetricName
τ I-MetricName
w I-MetricName
further O
allows O
us O
to O
estimate O
the O
probability O
of O
a O
higher O
- O
ranked O
LM O
actually O
performing O
better O
by O
computing O
τw+1 B-MetricName
2 I-MetricName
. O

Analysis O
of O
Results O

Our O
results O
across O
all O
setups O
are O
consolidated O
in O
Figure O
1 O
and O
Figure O
2 O
( O
C B-TaskName
: O
blue O
, O
SP B-TaskName
: O
beige O
) O
. O
2,3 O
The O
left O
of O
each O
figure O
plots O
the O
performance O
using O
frozen O
LM O
embeddings O
( O
) O
against O
LogME B-MetricName
scores O
, O
while O
on O
the O
right O
, O
full O
LM O
fine O
- O
tuning O
is O
applied O
( O
) O
. O
4 O
Figure O
1 O
shows O
the O
results O
of O
using O
mean O
- O
pooled O
embeddings O
in O
both O
C B-TaskName
/ O
SP B-TaskName
settings O
. O
For O
, O
we O
obtain O
ρ B-MetricName
> O
0.8 B-MetricValue
on O
8 O
/ O
10 O
tasks O
and O
τ B-MetricName
w I-MetricName
> O
0.7 B-MetricValue
on O
6 O
/ O
10 O
tasks O
, O
indicating O
a O
strong O
relationship O
between O
model O
performance O
and O
LogME B-MetricName
. O
After O
fine O
- O
tuning O
( O
) O
, O
we O
observe O
a O
general O
reduction O
in O
ρ B-MetricName
and O
τ B-MetricName
w I-MetricName
( O
most O
on O
CrossNER B-DatasetName
, O
EN B-DatasetName
- I-DatasetName
EWT I-DatasetName
) O
, O
however O
overall O
correlations O
remain O
positive O
to O
a O
significant O
degree O
. O

For O
C B-TaskName
setups O
using O
the O
alternative O
[ O
CLS O
] O
/ O
< O
s O
> O
representations O
( O
Figure O
2 O
) O
, O
LogME B-MetricName
correlates O
highly O
at O
ρ B-MetricName
> O
0.95 B-MetricValue
on O
5 O
/ O
6 O
tasks O
and O
τ B-MetricName
w I-MetricName
> O
0.7 B-MetricValue
on O
4 O
/ O
6 O
tasks O
when O
using O
head O
- O
only O
tuning O
( O
) O
. O
After O
full O
fine O
- O
tuning O
( O
) O
, O
SciERC B-DatasetName
, O
RTE B-DatasetName
and O
AGNews B-DatasetName
have O
lower O
correlations O
, O
particularly O
with O
the O
high O
- O
variance O
RoBERTa B-MethodName
model O
. O
However O
, O
the O
remaining O
tasks O
maintain O
a O
stable O
correlation O
, O
with O
ρ B-MetricName
> O
0.6 B-MetricValue
and O
τ B-MetricName
w I-MetricName
> O
0.3 B-MetricValue
across O
5 O
/ O
6 O
tasks O
. O

Overall O
, O
LogME B-MetricName
has O
a O
positive O
correlation O
with O
final O
performance O
in O
30 O
/ O
32 O
cases O
. O
In O
more O
detail O
, O
LogME B-MetricName
has O
a O
τ B-MetricName
w I-MetricName
> O
0.41 B-MetricValue
in O
20 O
/ O
32 O
setups O
, O
meaning O
that O
selecting O
a O
higher O
ranked O
model O
is O
the O
better O
choice O
71 O
% O
of O
the O
time O
. O
LogME B-MetricName
both O
identifies O
intuitive O
, O
domain O
- O
specific O
scenarios O
( O
e.g. O
, O
Twitter B-MethodName
- I-MethodName
RoBERTa I-MethodName
performing O
well O
on O
Airline B-DatasetName
Twitter I-DatasetName
) O
, O
but O
also O
finds O
cases O
that O
may O
be O
unintuitive O
, O
such O
as O
DistilBERT B-MethodName
's O
occasionally O
high O
performance O
for O
CrossNER B-DatasetName
and O
JobStack B-DatasetName
. O
This O
finding O
holds O
across O
C B-TaskName
, O
SP B-TaskName
, O
domains O
as O
well O
as O
different O
input O
representations O
. O
For O
the O
latter O
, O
we O
note O
that O
, O
surprisingly O
, O
even O
the O
untuned O
representation O
of O
[ O
CLS O
] O
/ O
< O
s O
> O
seems O
to O
contain O
useful O
information O
with O
comparable O
performance O
to O
mean O
pooling O
. O

Comparing O
versus O
, O
we O
notice O
that O
, O
as O
expected O
, O
model O
performance O
improves O
, O
but O
in O
general O
, O
LogME B-MetricName
's O
predictive O
power O
decreases O
. O

The O
fully O
fine O
- O
tuned O
model O
makes O
predictions O
on O
updated O
representations O
such O
that O
decreases O
in O
predictive O
performance O
are O
inevitable O
unless O
the O
initial O
LM O
already O
represents O
a O
local O
optimum O
for O
the O
task O
at O
hand O
. O
This O
fact O
is O
crucial O
for O
NLP O
practitioners O
where O
full O
fine O
- O
tuning O
is O
the O
standard O
practice O
. O
Taking O
these O
factors O
into O
account O
, O
LogME B-MetricName
's O
efficiency O
is O
especially O
beneficial O
, O
as O
it O
offers O
an O
86× O
speedup O
over O
full O
model O
fine O
- O
tuning O
( O
You O
et O
al O
. O
, O
2021 O
) O
, O
and O
its O
positive O
correlation O
in O
94 B-MetricValue
% I-MetricValue
of O
our O
evaluated O
setups O
indicates O
that O
it O
is O
an O
effective O
score O
for O
transferability B-TaskName
estimation I-TaskName
in O
NLP O
. O

Human O
Performance O

Given O
the O
lack O
of O
prior O
work O
examining B-TaskName
transferability I-TaskName
estimation I-TaskName
of O
pre O
- O
trained O
LM O
encoders O
, O
the O
most O
common O
method O
for O
encoder O
selection O
employed O
today O
is O
practitioner O
intuition O
. O
As O
such O
, O
we O
conduct O
a O
small O
- O
scale O
study O
with O
12 O
NLP O
practitioners O
and O
ask O
them O
to O
perform O
the O
same O
ranking O
as O
in O
Section O
3 O
. O
Despite O
having O
access O
to O
model O
details O
and O
associated O
papers O
, O
this O
task O
is O
difficult O
even O
for O
experts O
. O
While O
for O
LogME B-MetricName
, O
the O
range O
of O
τ B-MetricName
w I-MetricName
is O
in O
[ O
−0.20 B-MetricValue
; O
1.00 B-MetricValue
] O
, O
human O
rankings O
fall O
into O
a O
wider O
range O
of O
[ O
−0.54 B-MetricValue
; O
1.00 B-MetricValue
] O
, O
indicating O
higher O
uncertainty O
. O
Similarly O
, O
we O
observe O
that O
human O
correlations O
are O
negative O
thrice O
as O
often O
as O
for O
LogME B-MetricName
. O
Additionally O
, O
LogME B-MetricName
provides O
a O
continuous O
scale O
for O
comparing O
models O
, O
while O
human O
rankings O
offer O
no O
indication O
of O
relative O
performance O
differences O
. O
At O
the O
same O
time O
, O
they O
are O
more O
inaccurate O
for O
tasks O
without O
an O
associated O
domain O
- O
specific O
model O
( O
e.g. O
, O
news O
, O
mixture O
of O
genres O
in O
EWT B-DatasetName
) O
. O
Moreover O
, O
even O
when O
domains O
are O
clear O
( O
e.g. O
, O
Twitter O
, O
science O
) O
, O
LogME B-MetricName
tends O
to O
be O
more O
accurate O
than O
the O
predictions O
of O
most O
human O
participants O
. O
Finally O
, O
the O
high O
variance O
between O
practitioners O
and O
the O
fact O
that O
no O
single O
person O
was O
an O
expert O
in O
all O
setups O
further O
reinforces O
the O
necessity O
of O
quantitative O
transferability O
scores O
. O

Conclusion O

We O
show O
the O
value O
of O
transferability B-TaskName
estimation I-TaskName
for O
selecting O
high O
- O
performing O
LMs O
before O
full O
model O
fine O
- O
tuning O
in O
experiments O
, O
covering O
the O
two O
fundamental O
NLP O
settings O
of O
classification B-TaskName
and O
structured B-DatasetName
prediction I-DatasetName
. O
By O
adopting O
the O
stateof O
- O
the O
- O
art O
LogME B-MetricName
scoring O
method O
, O
we O
are O
able O
to O
rank O
LMs O
on O
a O
continuous O
scale O
which O
correlates O
with O
final O
performance O
- O
with O
the O
better O
encoder O
being O
chosen O
in O
71 O
% O
of O
cases O
. O
Additionally O
, O
we O
identify O
NLP O
- O
specific O
guidelines O
for O
transferability B-TaskName
estimation I-TaskName
: O
In O
particular O
, O
predicting O
the O
best O
LM O
for O
tasks O
/ O
domains O
which O
greatly O
deviate O
from O
an O
encoder O
's O
pre O
- O
training O
setup O
and O
require O
large O
amounts O
of O
full O
fine O
- O
tuning O
may O
require O
larger O
pre O
- O
selections O
of O
LMs O
due O
to O
the O
higher O
uncertainty O
of O
the O
scoring O
methods O
. O
Finally O
, O
our O
human O
study O
showed O
that O
practitioners O
frequently O
misconstrue O
the O
performance O
of O
LMs O
even O
on O
domain O
- O
specific O
tasks O
. O
As O
such O
transferability O
quantification O
methods O
provide O
valuable O
evidence O
over O
intuition O
. O

Limitations O

A O
key O
limitation O
that O
practitioners O
should O
consider O
is O
that O
, O
while O
LogME B-MetricName
is O
viable O
for O
the O
quantitative B-TaskName
transferability I-TaskName
estimation I-TaskName
of O
LM O
encoders O
, O
there O
is O
a O
noticeable O
drop O
in O
predictive O
accuracy O
after O
full O
model O
fine O
- O
tuning O
. O
We O
attribute O
this O
to O
the O
misalignment O
between O
the O
frozen O
representations O
of O
the O
encoder O
, O
which O
LogME B-MetricName
is O
applied O
to O
, O
and O
the O
representations O
after O
fine O
tuning O
. O
As O
stated O
in O
Section O
4 O
, O
unless O
the O
untuned O
LM O
already O
constitutes O
a O
local O
optimum O
for O
the O
task O
at O
hand O
, O
task O
- O
specific O
shifts O
in O
its O
parameters O
and O
representations O
are O
inevitable O
. O
This O
similarly O
applies O
to O
cases O
where O
the O
untuned O
representations O
differ O
substantially O
from O
what O
a O
fully O
fine O
- O
tuned O
model O
uses O
during O
training O
. O
Specifically O
, O
for O
the O
relation O
classification O
task O
of O
SciERC B-DatasetName
, O
it O
is O
important O
to O
note O
that O
the O
input O
given O
to O
the O
model O
is O
augmented O
with O
special O
tokens O
delimiting O
the O
entities O
involved O
in O
the O
relation O
( O
Baldini O
Soares O
et O
al O
. O
, O
2019 O
) O
which O
are O
unknown O
to O
the O
untuned O
model O
and O
thus O
the O
representations O
that O
LogME B-MetricName
is O
computed O
on O
. O
Furthermore O
, O
for O
EN B-DatasetName
- I-DatasetName
EWT I-DatasetName
we O
suspect O
that O
dependency B-TaskName
labeling I-TaskName
is O
a O
more O
fundamental O
task O
solvable O
with O
high O
accuracy O
by O
most O
LMs O
, O
especially O
after O
fine O
- O
tuning O
as O
reflected O
in O
micro B-MetricName
- I-MetricName
F1 I-MetricName
scores O
between O
93 B-MetricValue
- O
95 B-MetricValue
. O
This O
is O
mirrored O
by O
work O
on O
probing O
untuned O
LMs O
which O
identifies O
high O
levels O
of O
inherent O
dependency O
information O
( O
Tenney O
et O
al O
. O
, O
2019 O
; O
Müller O
- O
Eberstein O
et O
al O
. O
, O
2022a O
) O
. O
Such O
sensitivity O
to O
representational O
shifts O
is O
not O
exclusive O
to O
LogME B-MetricName
: O
In O
preliminary O
experiments O
, O
we O
examined O
LEEP B-MetricName
( O
Nguyen O
et O
al O
. O
, O
2020 O
) O
as O
an O
alternative O
predictive O
score O
S O
l O
( O
D O
) O
. O
Its O
original O
use O
was O
to O
rank O
the O
transferability O
of O
a O
classifier O
trained O
on O
one O
dataset O
, O
to O
a O
new O
task O
- O
leaving O
the O
ranking O
of O
pre O
- O
trained O
LMs O
for O
future O
work O
. O
LEEP B-MetricName
has O
so O
far O
only O
been O
applied O
to O
CV O
tasks O
, O
but O
we O
apply O
it O
to O
LM O
ranking O
on O
the O
collection O
of O
NLP O
tasks O
above O
. O
Our O
initial O
experiments O
achieved O
low O
and O
unintuitive O
correlations O
between O
LEEP B-MetricName
's O
S O
l O
( O
D O
) O
and O
P O
l O
( O
D O
) O
. O
We O
speculate O
that O
this O
is O
due O
to O
the O
absence O
of O
a O
normalizing O
factor O
over O
the O
number O
of O
source O
classes O
, O
i.e. O
, O
the O
high O
number O
of O
embedding O
dimensions O
in O
our O
case O
( O
see O
Equation O
2 O
in O
Nguyen O
et O
al O
. O
, O
2020 O
) O
. O
While O
it O
would O
further O
be O
valuable O
to O
investigate O
methods O
beyond O
LEEP B-MetricName
and O
LogME B-MetricName
, O
as O
mentioned O
in O
Section O
2 O
, O
we O
leave O
their O
evaluation O
on O
NLP O
to O
future O
work O
. O
At O
the O
time O
of O
writing O
, O
the O
former O
two O
were O
the O
most O
extensively O
explored O
in O
CV O
, O
in O
addition O
to O
the O
original O
LogME B-MetricName
work O
containing O
an O
initial O
study O
showing O
promise O
for O
NLP O
. O

Finally O
, O
our O
human O
ranking O
study O
in O
Section O
5 O
was O
limited O
by O
the O
number O
of O
practitioners O
with O
a O
publication O
record O
which O
we O
could O
contact O
confidentially O
. O
However O
, O
the O
group O
still O
constituted O
a O
diverse O
set O
over O
seniority O
, O
gender O
, O
and O
cultural O
background O
. O
A O
larger O
group O
would O
cover O
a O
broader O
range O
of O
backgrounds O
and O
may O
produce O
different O
rankings O
. O
However O
, O
as O
the O
surveyed O
group O
already O
displayed O
high O
variance O
, O
overall O
predictive O
performance O
is O
unlikely O
to O
be O
significantly O
higher O
. O

Keeping O
these O
limitations O
in O
mind O
, O
correlations O
do O
remain O
mostly O
positive O
for O
LogME B-MetricName
and O
scores O
are O
well O
suited O
to O
be O
applied O
to O
high O
- O
dimensional O
embedding O
spaces O
, O
such O
that O
it O
offers O
a O
predictive O
and O
efficient O
measure O
for O
quantifying O
transferability O
compared O
to O
human O
practitioner O
intuition O
. O

Ethics O
Statement O

It O
is O
difficult O
to O
foresee O
ethical O
issues O
for O
this O
work O
due O
to O
the O
broad O
applicability O
of O
LM O
encoder O
pre O
- O
selection O
. O
To O
the O
best O
of O
our O
knowledge O
, O
in O
the O
CV O
community O
from O
which O
our O
evaluated O
scoring O
methods O
originate O
, O
there O
have O
been O
no O
harmful O
applications O
thus O
far O
. O
In O
fact O
, O
as O
fine O
- O
tuning O
the O
entire O
space O
of O
available O
language O
models O
is O
unsustainable O
and O
unethical O
in O
terms O
of O
climate O
sustainability O
, O
efficient O
encoder O
pre O
- O
selection O
methods O
such O
as O
LogME B-MetricName
provide O
a O
positive O
first O
step O
towards O
tackling O
this O
problem O
. O
MODEL O
) O
and O
its O
performance O
on O
a O
wide O
variety O
of O
datasets O
( O
DATASET O
) O
in O
different O
settings O
( O
FROZEN O
, O
TUNED O
) O
by O
taking O
the O
representations O
of O
the O
tokens O
and O
apply O
mean O
pooling O
( O
µ O
) O
. O
Here O
we O
do O
not O
take O
the O
representation O
of O
the O
[ O
CLS O
] O
token O
as O
this O
has O
no O
meaning O
for O
the O
structured O
prediction O
task O
. O
Given O
the O
LogME B-MetricName
scores O
and O
the O
performance O
metrics O
, O
we O
can O
calculate O
the O
Pearson B-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
ρ B-MetricName
) O
and O
the O
weighted B-MetricName
Kendall I-MetricName
's I-MetricName
tau I-MetricName
( O
τ B-MetricName
w I-MetricName
) O
. O

Acknowledgements O

We O
would O
like O
to O
wholeheartedly O
thank O
the O
members O
NLPnorth O
group O
at O
the O
IT O
University O
of O
Copenhagen O
, O
The O
Center O
for O
Information O
and O
Language O
Processing O
at O
the O
Ludwig O
Maximilian O
University O
of O
Munich O
, O
and O
the O
Dialogue O
Modeling O
Group O
at O
the O
University O
of O
Amsterdam O
for O
comments O
on O
earlier O
versions O
of O
this O
work O
and O
participating O
in O
the O
human O
practitioner O
ranking O
study O
. O

MZ O
and O
BP O
are O
supported O
by O
the O
Independent O
Research O
Fund O
Denmark O
( O
DFF O
) O
grant O
9131 O
- O
00019B. O
EB O
, O
MME O
, O
and O
BP O
are O
supported O
by O
the O
Independent O
Research O
Fund O
Denmark O
( O
DFF O
) O
Sapere O
Aude O
grant O
9063 O
- O
00077B. O
BP O
is O
supported O
by O
the O
ERC O
Consolidator O
Grant O
DIALECT O
101043235 O
. O

Appendix O
A O
Reproducibility O

Each O
model O
is O
trained O
on O
an O
NVIDIA O
A100 O
GPU O
with O
40GBs O
of O
VRAM O
and O
an O
AMD O
Epyc O
7662 O
CPU O
. O
The O
seed B-HyperparameterName
numbers I-HyperparameterName
the O
models O
are O
initialized O
with O
are O
4012 B-HyperparameterValue
, O
5060 B-HyperparameterValue
, O
9908 B-HyperparameterValue
, O
8857 B-HyperparameterValue
, O
8823 B-HyperparameterValue
. O
We O
run O
the O
models O
for O
30 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
patience B-HyperparameterName
of O
3 B-HyperparameterValue
on O
each O
respective O
dev O
. O
data O
. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue
, O
or O
64 B-HyperparameterValue
depending O
on O
the O
size O
of O
the O
dataset O
. O
When O
keeping O
the O
language O
model O
weights O
frozen O
, O
we O
use O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-3 B-MethodName
. O
For O
full O
model O
fine O
- O
tuning O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
at O
5e-5 B-MetricValue
. O
On O
GLUE B-DatasetName
, O
JobStack B-DatasetName
, O
and O
CrossNER B-DatasetName
( O
News O
) O
, O
we O
observed O
training O
instability O
and O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
5e-7 B-HyperparameterValue
. O
The O
evaluated O
LMs O
have O
between O
66 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
( O
DistilBERT B-MethodName
base I-MethodName
) O
and O
125 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
( O
RoBERTa B-MethodName
base I-MethodName
) O
, O
taking O
between O
10 O
minutes O
( O
Sci B-DatasetName
- I-DatasetName
ERC I-DatasetName
) O
and O
3 O
days O
( O
e.g. O
, O
AGNews B-DatasetName
, O
MNLI B-DatasetName
) O
to O
fully O
fine O
- O
tune O
. O
Keeping O
the O
LM O
frozen O
and O
only O
finetuning O
the O
task O
- O
specific O
head O
is O
around O
70 O
% O
more O
time O
- O
efficient O
. O
Computing O
LogME B-MetricName
requires O
one O
forward O
pass O
to O
embed O
the O
dataset O
instances O
, O
before O
completing O
the O
score O
calculation O
in O
under O
1 O
minute O
. O

B O
Exact O
Results O

In O
Table O
2 O
and O
Table O
3 O
, O
we O
present O
the O
exact O
performance O
numbers O
shown O
in O
Figure O
1 O
and O
Figure O
2 O
. O
The O
results O
here O
are O
separated O
by O
task O
. O

Parameter B-MethodName
- I-MethodName
efficient I-MethodName
Continual I-MethodName
Learning I-MethodName
Framework I-MethodName
in O
Industrial O
Real O
- O
time O
Text O
Classification O
System O

Catastrophic O
forgetting O
is O
a O
challenge O
for O
model O
deployment O
in O
industrial O
real O
- O
time O
systems O
, O
which O
requires O
the O
model O
to O
quickly O
master O
a O
new O
task O
without O
forgetting O
the O
old O
one O
. O
Continual B-TaskName
learning I-TaskName
aims O
to O
solve O
this O
problem O
; O
however O
, O
it O
usually O
updates O
all O
the O
model O
parameters O
, O
resulting O
in O
extensive O
training O
times O
and O
the O
inability O
to O
deploy O
quickly O
. O
To O
address O
this O
challenge O
, O
we O
propose O
a O
parameter O
- O
efficient O
continual O
learning O
framework O
, O
in O
which O
efficient O
parameters O
are O
selected O
through O
an O
offline O
parameter O
selection O
strategy O
and O
then O
trained O
using O
an O
online O
regularization O
method O
. O
In O
our O
framework O
, O
only O
a O
few O
parameters O
need O
to O
be O
updated O
, O
which O
not O
only O
alleviates O
catastrophic O
forgetting O
, O
but O
also O
allows O
the O
model O
to O
be O
saved O
with O
the O
changed O
parameters O
instead O
of O
all O
parameters O
. O
Extensive O
experiments O
are O
conducted O
to O
examine O
the O
effectiveness O
of O
our O
proposal O
. O
We O
believe O
this O
paper O
will O
provide O
useful O
insights O
and O
experiences O
on O
developing O
deep O
learning O
- O
based O
online O
real O
- O
time O
systems O
. O

Introduction O

In O
industry O
, O
many O
text O
- O
related O
applications O
have O
enjoyed O
a O
superior O
performance O
boost O
from O
the O
emerging O
of O
pre O
- O
trained O
language O
models O
, O
such O
as O
word2vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
; O
Zhao O
et O
al O
. O
, O
2017 O
) O
, O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
( O
Radford O
et O
al O
. O
, O
, O
2019 O
, O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
when O
a O
fine O
- O
tuned O
model O
needs O
to O
be O
updated O
to O
master O
a O
new O
task O
swiftly O
, O
it O
usually O
loses O
the O
ability O
to O
handle O
previous O
tasks O
. O
This O
phenomenon O
is O
known O
as O
catastrophic O
forgetting O
( O
French O
, O
1999 O
) O
, O
and O
it O
poses O
a O
significant O
issue O
in O
industrial O
settings O
. O

Continual B-TaskName
learning I-TaskName
aims O
to O
incrementally O
expand O
acquired O
knowledge O
for O
future O
learning O
( O
Chen O
and O
Liu O
, O
2018 O
) O
, O
and O
mitigate O
the O
impact O
of O
catastrophic O
forgetting O
in O
the O
meantime O
. O
Existing O
continual B-TaskName
learning I-TaskName
methods O
usually O
use O
data O
replay O
* O
* O
Corresponding O
author O
. O
( O
Rebuffi O
et O
al O
. O
, O
2017b O
) O
, O
parameter O
isolation O
( O
Rusu O
et O
al O
. O
, O
2016 O
; O
Fernando O
et O
al O
. O
, O
2017 O
) O
, O
and O
regularization O
( O
Kirkpatrick O
et O
al O
. O
, O
2017 O
; O
Li O
and O
Hoiem O
, O
2017 O
) O
to O
make O
models O
adapt O
to O
new O
tasks O
without O
catastrophic O
forgetting O
. O
However O
, O
these O
approaches O
lack O
research O
on O
implementing O
continual B-TaskName
learning I-TaskName
in O
industrial O
scenarios O
, O
where O
endowing O
models O
with O
continual B-TaskName
learning I-TaskName
capabilities O
meets O
numerous O
practical O
constraints O
. O
For O
time O
constraints O
, O
when O
new O
data O
or O
tasks O
arrive O
, O
the O
model O
should O
be O
launched O
in O
minutes O
or O
even O
seconds O
, O
which O
is O
common O
in O
time O
- O
sensitive O
scenarios O
, O
e.g. O
, O
blocking O
certain O
rumors O
content O
. O
For O
space O
constraints O
, O
the O
strong O
demand O
for O
tracing O
tasks O
makes O
it O
necessary O
to O
save O
every O
model O
once O
it O
is O
changed O
. O
So O
for O
the O
current O
large O
- O
scale O
pre O
- O
trained O
models O
, O
storage O
becomes O
an O
industrial O
challenge O
with O
the O
increase O
of O
new O
tasks O
. O

To O
solve O
these O
industrial O
challenges O
, O
we O
propose O
a O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
continual I-MethodName
learning I-MethodName
framework I-MethodName
based O
on O
an O
offline O
parameter O
selection O
strategy O
. O
The O
framework O
consists O
of O
two O
parts O
, O
i.e. O
, O
offline O
calculation O
and O
online O
training O
. O
In O
the O
offline O
calculation O
part O
, O
all O
the O
parameters O
that O
are O
important O
to O
the O
old O
task O
are O
selected O
to O
be O
fixed O
, O
while O
the O
remaining O
parameters O
are O
employed O
to O
learn O
the O
new O
task O
. O
Since O
in O
a O
real O
industrial O
scenario O
, O
the O
arrival O
of O
a O
new O
task O
will O
have O
an O
interval O
of O
hours O
or O
even O
days O
, O
we O
can O
make O
full O
use O
of O
this O
interval O
to O
advance O
the O
selection O
of O
parameters O
. O
During O
the O
online O
training O
phase O
, O
the O
model O
is O
parameterefficiently O
trained O
on O
a O
new O
task O
within O
a O
small O
set O
of O
parameters O
and O
further O
combines O
multiple O
regularization O
- O
based O
methods O
( O
Kirkpatrick O
et O
al O
. O
, O
2017 O
; O
Li O
and O
Hoiem O
, O
2017 O
) O
to O
overcome O
catastrophic O
forgetting O
. O
To O
alleviate O
storage O
costs O
, O
we O
only O
save O
the O
modified O
parameters O
for O
each O
snapshot O
. O
Extensive O
experiments O
demonstrate O
that O
our O
framework O
can O
maintain O
the O
old O
task O
performance O
while O
learning O
a O
new O
task O
quickly O
. O
Our O
implementa O
- O
tion O
is O
based O
on O
UER O
- O
py O
pre O
- O
training O
toolkit O
1 O
( O
Zhao O
et O
al O
. O
, O
2019 O
) O
. O

The O
main O
contributions O
of O
this O
paper O
can O
be O
summarized O
as O
follows O
: O

• O
We O
are O
the O
first O
to O
explore O
continual B-TaskName
learning I-TaskName
with O
only O
a O
few O
model O
parameters O
, O
and O
show O
that O
updating O
0.1 O
% O
parameters O
of O
BERT B-MethodName
can O
achieve O
competitive O
performance O
. O

• O
We O
propose O
a O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
continual I-MethodName
learning I-MethodName
framework I-MethodName
that O
solves O
issues O
in O
real O
- O
world O
industrial O
settings O
by O
utilizing O
parameter O
- O
efficient O
- O
based O
offline O
parameter O
selection O
strategies O
and O
regularization O
- O
based O
online O
training O
methods O
. O

• O
Extensive O
experiments O
on O
a O
real O
- O
world O
domain O
incremental O
text O
classification O
task O
verify O
the O
effectiveness O
of O
our O
proposed O
framework O
. O

Related O
Work O

Continual B-TaskName
Learning I-TaskName

The O
major O
challenge O
of O
continual B-TaskName
learning I-TaskName
is O
catastrophic O
forgetting O
( O
McCloskey O
and O
Cohen O
, O
1989 O
; O
French O
, O
1999 O
) O
, O
which O
occurs O
when O
optimizing O
for O
a O
new O
task O
causes O
performance O
degradation O
on O
a O
task O
learned O
previously O
. O
Methods O
designed O
to O
mitigate O
catastrophic O
forgetting O
mainly O
fall O
into O
three O
categories O
: O
replay B-MethodName
methods I-MethodName
, O
parameter B-MethodName
isolation I-MethodName
methods I-MethodName
, O
and O
regularization B-MethodName
- I-MethodName
based I-MethodName
methods I-MethodName
( O
Delange O
et O
al O
. O
, O
2021 O
) O
. O

Replay B-MethodName
methods I-MethodName
explicitly O
retrain O
on O
a O
subset O
of O
stored O
old O
task O
samples O
while O
training O
on O
new O
tasks O
. O
Instead O
of O
selecting O
samples O
at O
random O
, O
Rebuffi O
et O
al O
. O
( O
2017b O
) O
incorporated O
the O
Herding O
technique O
( O
Welling O
, O
2009 O
) O
to O
choose O
samples O
that O
best O
approximate O
the O
mean O
feature O
vector O
of O
a O
class O
, O
and O
it O
is O
widely O
used O
in O
Castro O
et O
al O
. O
( O
2018 O
) O
, O
, O
Hou O
et O
al O
. O
( O
2019 O
) O
, O
Zhao O
et O
al O
. O
( O
2020 O
) O
, O
Mi O
et O
al O
. O
( O
2020a O
, O
b O
) O
. O
Ramalho O
and O
Garnelo O
( O
2019 O
) O
proposed O
to O
store O
samples O
that O
the O
model O
is O
least O
confident O
. O
However O
, O
replay O
methods O
exploit O
samples O
from O
old O
tasks O
, O
which O
will O
slow O
down O
the O
online O
training O
. O
To O
meet O
the O
time O
constraint O
, O
they O
are O
not O
used O
in O
our O
framework O
. O

Parameter B-MethodName
isolation I-MethodName
methods I-MethodName
dedicate O
different O
model O
parameters O
to O
each O
task O
, O
which O
are O
divided O
in O
two O
directions O
. O
One O
is O
growing O
a O
new O
branch O
network O
for O
a O
new O
task O
, O
while O
freezing O
previous O
task O
parameters O
( O
Rusu O
et O
al O
. O
, O
2016 O
; O
Xu O
and O
Zhu O
, O
2018 O
) O
. O
The O
other O
one O
is O
masking O
out O
parameters O
of O
previous O
task O
during O
new O
task O
training O
, O
which O
is O
imposed O
either O
at O
parameters O
level O
( O
Fernando O
et O
al O
. O
, O
2017 O
; O
Mallya O
and O
Lazebnik O
, O
2018 O
) O
, O
or O
unit O
level O
( O
Serra O
et O
al O
. O
, O
2018 O
) O
. O
Parameter O
isolation O
is O
unsuitable O
for O
usage O
in O
industrial O
scenario O
. O
It O
is O
difficult O
to O
keep O
track O
of O
the O
model O
's O
scale O
if O
the O
number O
of O
used O
parameters O
is O
continually O
accumulated O
as O
the O
number O
of O
tasks O
increases O
. O

Regularization B-MethodName
- I-MethodName
based I-MethodName
methods I-MethodName
add O
an O
additional O
regularization O
term O
in O
the O
loss O
function O
, O
which O
will O
consolidate O
previous O
knowledge O
when O
learning O
on O
new O
data O
( O
Delange O
et O
al O
. O
, O
2021 O
) O
. O
Elastic B-MethodName
weight I-MethodName
consolidation I-MethodName
( O
EWC B-MethodName
) O
( O
Kirkpatrick O
et O
al O
. O
, O
2017 O
) O
is O
a O
well O
- O
known O
regularization B-MethodName
- I-MethodName
based I-MethodName
method I-MethodName
, O
which O
introduces O
network O
parameter O
uncertainty O
in O
the O
Bayesian O
framework O
. O
LwF O
( O
Li O
and O
Hoiem O
, O
2017 O
) O
is O
another O
regularization O
method O
, O
using O
the O
previous O
model O
to O
infer O
current O
data O
and O
taking O
the O
outputs O
as O
soft O
labels O
to O
mitigate O
forgetting O
and O
transfer O
knowledge O
. O

Parameter O
- O
efficient O
Training O

Training O
a O
model O
with O
a O
few O
parameters O
is O
useful O
in O
many O
applications O
. O
Not O
only O
does O
the O
model O
have O
the O
potential O
to O
achieve O
better O
performance O
, O
but O
also O
disk O
space O
can O
be O
saved O
by O
only O
saving O
the O
updated O
parameters O
for O
each O
task O
. O
Recent O
work O
has O
shown O
that O
it O
is O
possible O
to O
update O
only O
a O
small O
subset O
of O
the O
model O
's O
parameters O
during O
training O
. O
This O
kind O
of O
work O
could O
heavily O
alleviate O
storage O
and O
deployment O
communication O
requirements O
. O
For O
example O
, O
Adapters O
( O
Houlsby O
et O
al O
. O
, O
2019 O
; O
Rebuffi O
et O
al O
. O
, O
2017a O
; O
Bapna O
et O
al O
. O
, O
2019 O
) O
introduce O
additional O
trainable O
parameters O
into O
a O
pre O
- O
trained O
model O
in O
the O
form O
of O
small O
task O
- O
specific O
modules O
while O
the O
rest O
of O
the O
model O
's O
parameters O
are O
kept O
fixed O
. O
Many O
works O
like O
Diff O
Pruning O
( O
Guo O
et O
al O
. O
, O
2020 O
) O
and O
BitFit O
( O
Ben O
Zaken O
et O
al O
. O
, O
2021 O
) O
have O
shown O
that O
it O
is O
possible O
to O
update O
only O
a O
small O
subset O
of O
the O
model O
's O
parameters O
during O
training O
, O
which O
can O
alleviate O
storage O
and O
communication O
requirements O
. O
Xu O
et O
al O
. O
( O
2021 O
) O
and O
Sung O
et O
al O
. O
( O
2021 O
) O
even O
show O
a O
acceptable O
performance O
on O
random O
selection O
of O
parameters O
. O
Therefore O
, O
we O
choose O
to O
perform O
parameter O
- O
efficient O
training O
on O
continual O
learning O
to O
quickly O
master O
a O
new O
task O
while O
avoid O
catastrophic O
forgetting O
. O

Methodology O

We O
introduce O
a O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
continual I-MethodName
learning I-MethodName
framework I-MethodName
, O
as O
shown O
in O
figure O
1 O
. O
Our O
frame- O

Mask O
Fisher O
information O
matrix O

Calculation O
Selection O

Parameter O
Inheritance O

BERT B-MethodName
parameters O
After O
Selection O

EWC O
Loss O

Lwf O
Loss O

CE O
Loss O

Regularization B-MethodName
- I-MethodName
based I-MethodName
methods I-MethodName

Offline O
Calculation O
Online O
Training O

Task O
A O
Task O
B O

Figure O
1 O
: O
The O
overall O
architecture O
of O
the O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
continual I-MethodName
learning I-MethodName
framework I-MethodName
. O

work O
is O
divided O
into O
two O
components O
. O
The O
first O
is O
offline O
computation O
, O
which O
makes O
use O
of O
the O
interval O
between O
tasks O
to O
evaluate O
the O
data O
and O
select O
the O
parameters O
that O
are O
crucial O
to O
old O
tasks O
. O
These O
parameters O
are O
kept O
fixed O
in O
the O
new O
task O
. O
The O
other O
part O
is O
online O
training O
. O
In O
this O
stage O
, O
we O
utilize O
parameter O
- O
efficient O
training O
to O
perform O
new O
task O
training O
on O
the O
parameters O
that O
are O
not O
fixed O
. O
In O
addition O
, O
we O
introduce O
some O
well O
- O
known O
regularization O
- O
based O
methods O
in O
our O
framework O
for O
further O
improvement O
. O

Offline O
calculation O

The O
goal O
of O
offline O
calculation O
is O
to O
select O
the O
subset O
of O
parameters O
that O
are O
( O
in O
some O
sense O
) O
the O
most O
important O
to O
all O
of O
the O
old O
tasks O
, O
and O
fix O
them O
. O
Therefore O
, O
we O
make O
full O
use O
of O
the O
interval O
between O
tasks O
to O
review O
the O
previous O
training O
data O
, O
and O
calculate O
the O
parameters O
that O
are O
important O
to O
the O
previous O
tasks O
in O
the O
latest O
model O
( O
snapshot O
) O
. O
These O
parameters O
will O
be O
fixed O
in O
the O
new O
task O
, O
and O
the O
remaining O
parameters O
will O
participate O
in O
the O
training O
. O

As O
for O
the O
method O
of O
measuring O
the O
importance O
of O
parameters O
, O
we O
consider O
the O
indicator O
of O
how O
much O
changing O
the O
parameter O
will O
impact O
the O
model O
's O
output O
. O
The O
Fisher O
information O
is O
particularly O
well O
suited O
to O
identifying O
the O
highly O
relevant O
subset O
of O
parameters O
for O
previous O
tasks O
. O
It O
serves O
as O
an O
useful O
tool O
for O
estimating O
how O
much O
information O
a O
random O
variable O
contains O
about O
a O
parameter O
of O
the O
distribution O
( O
Tu O
et O
al O
. O
, O
2016 O
) O
. O
The O
Fisher O
information O
assumes O
that O
the O
more O
important O
the O
parameter O
towards O
the O
target O
task O
, O
the O
higher O
value O
it O
conveys O
. O
Formally O
, O
the O
Fisher O
information O
for O
the O
parameter O
is O
as O
follows O
: O
where O
denotes O
the O
task O
- O
specific O
training O
data O
, O
and O
denote O
the O
input O
and O
the O
output O
respectively O
. O

( O
) O
= O
1 O
| O
| O
| O
| O
∑︁ O
= O
1 O
log O
| O
; O
2 O
( O
1 O
) O

Parameter O
- O
efficient O
Online O
training O

Given O
the O
important O
parameters O
selected O
in O
offline O
stage O
, O
we O
use O
the O
remaining O
parameters O
for O
online O
training O
. O
Updating O
subset O
of O
the O
parameters O
can O
avoid O
catastrophic O
forgetting O
to O
some O
extent O
and O
largely O
decrease O
the O
storage O
space O
required O
by O
the O
snapshot O
. O
In O
addition O
, O
we O
combine O
the O
parameterefficient O
training O
with O
two O
typical O
regularizationbased O
continual O
learning O
methods O
. O
The O
combination O
of O
multiple O
orthogonal O
techniques O
can O
further O
improve O
the O
performance O
of O
our O
system O
. O
The O
overview O
optimization O
objective O
is O
as O
follows O
: O
denotes O
the O
cross O
- O
entropy O
loss O
, O
and O
− O
denotes O
regularization O
- O
based O
method O
loss O
. O

Parameter O
- O
efficient O
learning O

As O
shown O
in O
figure O
2 O
, O
in O
traditional O
training O
setting O
( O
left O
) O
, O
all O
of O
the O
model O
's O
parameters O
are O
updated O
. O
But O
in O
our O
online O
training O
( O
right O
) O
, O
we O
only O
train O
a O
few O
parameters O
, O
and O
most O
of O
the O
parameters O
are O
fixed O
according O
to O
the O
result O
of O
offline O
calculation O
to O
avoid O
forgetting O
old O
tasks O
. O
In O
general O
, O
the O
model O
will O
be O
easy O
to O
forget O
old O
tasks O
while O
learning O
new O
tasks O
, O
if O
more O
parameters O
are O
updated O
. O
Therefore O
, O
in O
our O
framework O
, O
we O
choose O
to O
perform O
parameterefficient O
training O
on O
parameters O
that O
are O
not O
important O
to O
previous O
tasks O
, O
which O
are O
not O
fixed O
. O
Refer O
to O
previous O
experience O
( O
Ben O
Zaken O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2021 O
) O
, O
we O
chose O
a O
layer O
- O
wise O
strategy O
to O
select O
important O
parameters O
. O
We O
fixed O
the O
parameters O
from O
large O
to O
small O
( O
Fisher O
information O
) O
in O
a O
certain O
proportion O
at O
each O
layer O
. O

On O
the O
other O
hand O
, O
updating O
a O
small O
number O
of O
parameters O
is O
beneficial O
for O
storage O
purpose O
and O
rapid O
deployment O
. O
Sometimes O
we O
need O
to O
deploy O
our O
model O
on O
thousands O
of O
servers O
. O
So O
the O
model O
size O
needs O
to O
be O
as O
small O
as O
possible O
( O
around O
1.2 O
GB O
for O
BERT B-MethodName
- I-MethodName
Large I-MethodName
400 O
MB O
for O
BERT B-MethodName
- I-MethodName
Base I-MethodName
) O
. O
Our O
framework O
only O
needs O
to O
store O
the O
values O
and O
indices O
denoting O
the O
position O
of O
the O
updated O
parameters O
. O
Our O
experimental O
results O
demonstrate O
that O
only O
0.1 O
% O
trainable O
parameters O
of O
the O
original O
model O
can O
achieve O
competitive O
performance O
. O

Regularization B-MethodName
- I-MethodName
based I-MethodName
method I-MethodName

EWC B-MethodName
and O
LwF B-MethodName
are O
two O
representative O
approaches O
for O
preventing O
catastrophic O
forgetting O
in O
neural O
networks O
. O
They O
respectively O
add O
restrictions O
on O
model O
parameters O
and O
output O
activation O
. O
The O
two O
methods O
are O
orthogonal O
and O
we O
combine O
them O
as O
follows O
: O

− O
( O
) O
= O
1 O
( O
) O
+ O
2 O
( O
) O
( O
3 O

) O
Elastic B-MethodName
Weight I-MethodName
Consolidation I-MethodName
( O
EWC B-MethodName
) O
( O
Kirkpatrick O
et O
al O
. O
, O
2017 O
) O
introduces O
network O
parameter O
uncertainty O
in O
the O
Bayesian O
framework O
. O
Intuitively O
, O
this O
approach O
consists O
of O
a O
2 O
penalty O
on O
the O
difference O
between O
the O
parameters O
for O
the O
old O
* O
( O
denotes O
the O
indexes O
of O
the O
parameters O
) O
and O
the O
new O
. O
It O
uses O
the O
diagonal O
of O
the O
Fisher O
information O
matrix O
( O
2 O
) O
to O
weight O
different O
parameters O
. O
The O
EWC O
loss O
( O
4 O
) O
slows O
down O
the O
learning O
process O
of O
task O
- O
relevant O
parameters O
, O
which O
contains O
knowledge O
learned O
previously O
. O

( O
) O
= O
∑︁ O
( O
− O
* O
) O
2 O
( O
4 O
) O

From O
formula O
( O
4 O
) O
, O
we O
can O
see O
that O
if O
most O
of O
the O
parameters O
are O
fixed O
, O
it O
is O
equivalent O
to O
reducing O
the O
EWC O
loss O
, O
which O
is O
beneficial O
to O
preventing O
catastrophic O
forgetting O
. O

Learning B-MethodName
without I-MethodName
forgetting I-MethodName
( O
LwF B-MethodName
) O
( O
Li O
and O
Hoiem O
, O
2017 O
) O
is O
another O
method O
for O
continual O
learning O
. O
Before O
training O
the O
new O
task O
, O
network O
outputs O
for O
the O
new O
task O
data O
are O
recorded O
, O
which O
is O
denoted O
by O
′ O
. O
It O
will O
be O
subsequently O
used O
during O
training O
to O
distill O
prior O
task O
knowledge O
. O
LwF B-MethodName
employs O
a O
variant O
of O
knowledge O
distillation O
. O
In O
our O
framework O
, O
we O
use O
2 O
( O
5 O
) O
loss O
to O
regulate O
the O
outputs O
: O

( O
) O
= O
∑︁ O
( O
− O
′ O
) O
2 O
( O
5 O
) O

Experiment O

In O
this O
section O
, O
we O
empirically O
verify O
the O
effectiveness O
and O
efficiency O
of O
our O
framework O
under O
the O
setting O
of O
incremental O
text O
classification O
tasks O
in O
the O
industrial O
scenarios O
. O

Dataset O
& O
Implementation O
Details O

In O
the O
experiments O
, O
we O
utilize O
the O
Amazon B-DatasetName
Reviews I-DatasetName
dataset O
( O
He O
and O
McAuley O
, O
2016 O
) O

Models O

We O
compare O
our O
proposed O
models O
with O
the O
a O
series O
of O
baseline O
methods O
in O
our O
experiments O
: O

• O
lower O
- O
bound O
: O
a O
standard O
classification O
model O
is O
fine O
- O
tuned O
on O
the O
individual O
task O
without O
any O
continual O
learning O
strategy O
, O
which O
can O
be O
considered O
as O
the O
lower O
- O
bound O
method O
. O

• O
upper O
- O
bound O
: O
a O
model O
is O
trained O
on O
all O
tasks O
simultaneously O
, O
which O
can O
be O
considered O
as O
the O
upper O
- O
bound O
method O
since O
it O
has O
access O
to O
the O
whole O
dataset O
. O

• O
EWC B-MethodName
& O
LwF B-MethodName
: O
Two O
classical O
regularizationbased O
methods O
for O
continual O
learning O
. O

• O
PE O
- O
rand O
: O
Our O
proposal O
model O
is O
trained O
by O
randomly O
choosing O
some O
parameters O
and O
keeping O
them O
unchanged O
during O
online O
training O
stage O
, O
instead O
of O
using O
the O
offline O
calculation O
strategy O
. O

• O
PE- O
* O
: O
Our O
continual O
learning O
framework O
, O
including O
offline O
calculation O
and O
parameterefficient O
online O
training O
. O

Results O

The O
models O
are O
trained O
on O
the O
current O
training O
set O
and O
evaluated O
on O
the O
union O
of O
all O
the O
test O
sets O
. O
To O
ensure O
the O
robustness O
of O
the O
task O
ordering O
, O
we O
evaluate O
our O
methods O
on O
the O
four O
different O
orderings O
( O
chosen O
randomly O
) O
, O
which O
are O
shown O
in O
Appendix O
A O
. O

Table O
1 O
provides O
a O
summary O
of O
our O
main O
results O
. O
We O
report O
the O
micro O
- O
averaged O
accuracy O
for O
the O
classification O
task O
. O
The O
lower O
bound O
is O
trained O
in O
the O
current O
task O
without O
using O
any O
continual O
learning O
strategy O
to O
overcome O
catastrophic O
forgetting O
, O
while O
the O
upper O
bound O
is O
trained O
on O
all O
data O
after O
the O
new O
task O
comes O
, O
which O
can O
be O
considered O
multi O
- O
task O
method O
. O
There O
is O
a O
significant O
gap O
between O
the O
lower O
bound O
and O
the O
upper O
bound O
, O
which O
illustrates O
the O
need O
for O
continual O
learning O
. O
As O
the O
classical O
CL B-TaskName
methods O
, O
EWC B-MethodName
and O
LwF B-MethodName
outperform O
the O
standard O
model O
without O
any O
specific O
continual O
learning O
, O
but O
still O
suffer O
from O
catastrophic O
forgetting O
in O
the O
order O
IV O
. O
It O
can O
be O
seen O
that O
our O
proposed O
PE- O
* O
achieves O
a O
better O
performance O
than O
EWC B-MethodName
, O
LwF B-MethodName
and O
their O
combination O
. O
This O
is O
because O
most O
of O
the O
parameters O
in O
BERT B-MethodName
are O
fixed O
, O
which O
is O
equivalent O
to O
posing O
a O
strict O
regularization O
to O
the O
parameters O
to O
prevent O
catastrophic O
forgetting O
while O
using O
the O
remaining O
parameters O
to O
learn O
new O
tasks O
. O
Compared O
to O
PErand O
, O
PE- O
* O
has O
a O
better O
average O
accuracy O
, O
which O
verifies O
the O
importance O
of O
parameter O
selections O
. O
Although O
the O
random O
selection O
method O
outperforms O
PE- O
* O
in O
order O
I O
, O
it O
is O
difficult O
to O
obtain O
a O
suitable O
set O
of O
parameters O
in O
most O
cases O
for O
models O
to O
learn O
new O
tasks O
while O
maintaining O
previous O
knowledge O
. O

Moreover O
, O
according O
to O
the O
principles O
of O
EWC B-MethodName
and O
LwF B-MethodName
, O
the O
former O
records O
the O
initial O
model O
parameters O
, O
and O
the O
latter O
records O
the O
data O
features O
of O
new O
tasks O
. O
As O
the O
training O
progresses O
, O
their O
regular O
loss O
terms O
especially O
( O
) O
will O
get O
bigger O
and O
bigger O
in O
model O
like O
BERT B-MethodName
- I-MethodName
Base I-MethodName
with O
110 O
M O
parameters O
. O
What O
's O
more O
, O
in O
real O
industrial O
scenarios O
, O
each O
new O
task O
may O
have O
different O
suitable O
hyper O
parameters O
. O
We O
do O
not O
have O
time O
to O
do O
grid O
search O
of O
the O
best O
hyper O
parameters O
, O
so O
and O
may O
not O
be O
optimal O
solutions O
. O
This O
results O
in O
an O
unbalanced O
ratio O
of O
( O
) O
to O
( O
) O
and O
( O
) O
, O
where O
( O
) O
may O
much O
smaller O
than O
( O
) O
and O
( O
) O
. O
Therefore O
, O
as O
the O
number O
of O
tasks O
increases O
, O
the O
training O
of O
new O
tasks O
will O
become O
more O
and O
more O
difficult O
with O
the O
same O
set O
of O
hyperparameters O
. O
However O
, O
the O
previous O
tasks O
have O
not O
been O
fully O
learned O
. O
This O
problem O
accumulates O
gradually O
in O
regularization O
- O
based O
method O
and O
leads O
to O
results O
that O
are O
not O
as O
good O
as O
our O
method O
( O
PE- O
* O
) O
which O
just O
handles O
a O
very O
small O
amount O
parameters O
. O

Figure O
3 O
shows O
the O
accuracy O
of O
model O
on O
the O
first O
task O
test O
set O
as O
the O
model O
are O
trained O
on O
more O
tasks O
. O
The O
figure O
illustrates O
how O
well O
each O
model O
retains O
its O
previously O
acquired O
knowledge O
as O
it O
learns O
new O
knowledge O
. O
We O
can O
see O
that O
our O
framework O
is O
con- O
sistently O
better O
and O
more O
stable O
compared O
to O
other O
methods O
. O

Parameter O
- O
efficient O
Strategy O

Figure O
4 O
shows O
the O
weight O
map O
of O
Fisher O
information O
. O
It O
can O
be O
seen O
that O
in O
the O
BERT B-MethodName
model O
, O
the O
parameters O
of O
the O
embedding O
- O
layer O
have O
little O
effect O
on O
our O
classification O
task O
. O
Most O
of O
the O
important O
parameters O
are O
concentrated O
in O
the O
transformer O
block O
layer O
, O
and O
the O
importance O
of O
the O
attention O
layer O
is O
higher O
than O
that O
of O
the O
feed O
forward O
layer O
. O
In O
addition O
, O
according O
to O
our O
statistics O
, O
we O
found O
that O
13 O
% O
( O
about O
14 O
M O
) O
of O
the O
parameters O
' O
Fisher O
information O
is O
0 O
, O
and O
most O
of O
them O
are O
in O
embedding O
- O
layer O
. O

In O
our O
experiments O
, O
we O
found O
that O
the O
parameters O
to O
be O
fixed O
can O
not O
be O
determined O
simply O
in O
order O
of O
magnitude O
. O
According O
to O
( O
Jawahar O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
encodes O
rich O
linguistic O
information O
in O
different O
transformer O
blocks O
. O
Therefore O
, O
refer O
to O
previous O
experience O
( O
Ben O
Zaken O
et O
al O
. O
, O
2021 O
; O
Xu O
et O
al O
. O
, O
2021 O
) O
, O
we O
chose O
a O
layer O
- O
wise O
strategy O
to O
select O
important O
parameters O
. O
We O
fixed O
the O
parameters O
from O
large O
to O
small O
in O
a O
certain O
proportion O
at O
each O
layer O
of O
the O
model O
. O
To O
this O
end O
, O
each O
layer O
has O
parameters O
for O
new O
tasks O
to O
learn O
. O

Model O
Size O

We O
set O
layer O
- O
wise O
parameter O
reserved O
for O
new O
task O
to O
different O
values O
, O
0.5 O
% O
, O
1 O
% O
, O
5 O
% O
and O
10 O
% O
, O
and O
the O
percent O
of O
parameters O
fixed O
for O
old O
task O
are O
99.5 O
% O
, O
99 O
% O
, O
95 O
% O
and O
90 O
% O
. O
The O
advantage O
of O
our O
parameter O
- O
efficient O
continual O
learning O
becomes O
more O
pronounced O
at O
extreme O
sparsity O
rates O
. O
In O
Table O
2 O
, O
we O
report O
the O
accuracy B-MetricName
across O
different O
task O
orders O
and O
reserved O
rates O
. O
We O
can O
observe O
that O
the O
more O
parameters O
fixed O
, O
the O
better O
the O
effect O
on O
alleviating O
catastrophic O
forgetting O
. O

In O
the O
above O
experiments O
, O
we O
only O
trained O
0.1 O
% O
of O
the O
parameters O
in O
the O
BERT B-MethodName
model O
. O
We O
use O
the O
sparse O
- O
matrix O
method O
to O
store O
the O
model O
, O
and O
only O
store O
the O
index O
and O
value O
each O
time O
, O
occupying O
about O
1.2 O
Mb O
of O
space O
, O
which O
is O
0.3 O
% O
of O
the O
entire O
model O
, O
as O
shown O
in O
Table O
3 O
. O
Parameter O
- O
efficient O
strategy O
greatly O
saves O
network O
bandwidth O
and O
storage O
requirements O
. O

Conclusion O

This O
paper O
introduces O
a O
parameter B-MethodName
- I-MethodName
efficient I-MethodName
continual I-MethodName
learning I-MethodName
framework I-MethodName
, O
which O
is O
designed O
for O
realtime O
incremental O
learning O
system O
. O
In O
offline O
stage O
, O
the O
framework O
identifies O
the O
parameters O
that O
are O
less O
important O
to O
the O
old O
tasks O
. O
By O
updating O
these O
parameters O
in O
online O
training O
stage O
, O
the O
model O
is O
able O
to O
learn O
new O
tasks O
in O
short O
time O
without O
forgetting O
the O
old O
ones O
. O
Furthermore O
, O
we O
surprisingly O
find O
that O
decent O
results O
can O
be O
achieved O
by O
only O
training O
a O
small O
subset O
of O
parameters O
( O
e.g. O
0.1 O
% O
) O
. O
This O
observation O
enables O
us O
to O
largely O
decrease O
the O
storage O
of O
the O
snapshot O
, O
which O
is O
important O
for O
the O
system O
requiring O
frequent O
update O
. O

A O
Task O
order O

We O
use O
the O
following O
task O
orders O
( O
chosen O
randomly O
) O
for O
text O
classification O
: O

StoryER B-TaskName
: O
Automatic B-TaskName
Story I-TaskName
Evaluation I-TaskName
via O
Ranking B-TaskName
, O
Rating B-TaskName
and O
Reasoning B-TaskName

Existing O
automatic B-TaskName
story I-TaskName
evaluation I-TaskName
methods O
place O
a O
premium O
on O
story O
lexical O
level O
coherence O
, O
deviating O
from O
human O
preference O
. O
We O
go O
beyond O
this O
limitation O
by O
considering O
a O
novel O
Story B-TaskName
Evaluation I-TaskName
method O
that O
mimics O
human O
preference O
when O
judging O
a O
story O
, O
namely O
StoryER B-TaskName
, O
which O
consists O
of O
three O
sub O
- O
tasks O
: O
Ranking B-TaskName
, O
Rating B-TaskName
and O
Reasoning B-TaskName
. O
Given O
either O
a O
machine O
- O
generated O
or O
a O
human O
- O
written O
story O
, O
StoryER B-TaskName
requires O
the O
machine O
to O
output O
1 O
) O
a O
preference O
score O
that O
corresponds O
to O
human O
preference O
, O
2 O
) O
specific O
ratings O
and O
their O
corresponding O
confidences O
and O
3 O
) O
comments O
for O
various O
aspects O
( O
e.g. O
, O
opening O
, O
character O
- O
shaping O
) O
. O
To O
support O
these O
tasks O
, O
we O
introduce O
a O
wellannotated O
dataset O
comprising O
( O
i O
) O
100k B-HyperparameterValue
ranked B-HyperparameterName
story I-HyperparameterName
pairs I-HyperparameterName
; O
and O
( O
ii O
) O
a O
set O
of O
46k B-HyperparameterValue
ratings B-HyperparameterName
and O
comments O
on O
various O
aspects O
of O
the O
story O
. O
We O
finetune O
Longformer B-MethodName
- I-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
( O
LED B-MethodName
) O
on O
the O
collected O
dataset O
, O
with O
the O
encoder O
responsible O
for O
preference O
score O
and O
aspect O
prediction O
and O
the O
decoder O
for O
comment O
generation O
. O
Our O
comprehensive O
experiments O
result O
in O
a O
competitive O
benchmark O
for O
each O
task O
, O
showing O
the O
high O
correlation O
to O
human O
preference O
. O
In O
addition O
, O
we O
have O
witnessed O
the O
joint O
learning O
of O
the O
preference O
scores O
, O
the O
aspect O
ratings O
, O
and O
the O
comments O
brings O
gain O
in O
each O
single O
task O
. O
Our O
dataset O
and O
benchmarks O
are O
publicly O
available O
to O
advance O
the O
research O
of O
story O
evaluation O
tasks O
. O
1 O

Introduction O

Even O
for O
humans O
, O
evaluating O
story O
quality O
is O
a O
challenging O
task O
. O
Although O
many O
literature O
criteria O
have O
been O
proposed O
, O
the O
most O
straightforward O
way O
is O
to O
count O
how O
many O
readers O
like O
the O
story O
which O
is O
referred O
as O
to O
human O
preference O
. O
Bearing O
it O
in O
mind O
, O
story O
writing O
community O
usually O
uses O
upvote O
count O
1 O
Dataset O
and O
pre O
- O
trained O
model O
demo O
are O
available O
at O
anonymous O
website O
http O
: O
/ O
/ O
storytelling-lab.com O
/ O
eval O
and O
https O
: O
/ O
/ O
github.com O
/ O
sairin1202 O
/ O
StoryER O
Figure O
1 O
: O
The O
existing O
story O
evaluation O
method O
( O
UNION B-MethodName
) O
outputs O
a O
score O
for O
estimating O
the O
coherence B-MetricName
of O
the O
stories O
, O
while O
human O
- O
written O
stories O
rarely O
suffer O
from O
this O
problem O
. O
Our O
model O
( O
Ours O
) O
which O
is O
trained O
by O
comparing O
two O
stories O
( O
Ranking B-TaskName
) O
, O
evaluates O
the O
story O
based O
on O
human O
preference O
( O
i.e. O
, O
upvote O
counts O
) O
, O
produces O
scores O
for O
various O
aspects O
( O
Rating B-TaskName
) O
, O
and O
leaves O
comments O
( O
Reasoning B-TaskName
) O
. O
Our O
model O
is O
applicable O
to O
both O
machine O
- O
generated O
and O
human O
- O
written O
stories O
. O

as O
a O
story O
quality O
criterion O
. O
As O
shown O
in O
Fig O
. O
1 O
, O
more O
readers O
like O
the O
left O
story O
( O
upvote O
count O
= O
1.8k O
) O
rather O
than O
the O
right O
one O
( O
upvote O
count O
= O
1 O
) O
. O

Existing O
methods O
which O
use O
referenced O
metrics O
( O
e.g. O
, O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
, O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
) O
and O
unreferenced O
metrics O
( O
e.g. O
, O
UNION B-MetricName
( O
Guan O
and O
Huang O
, O
2020 O
) O
, O
MANPLTS B-MetricName
( O
Ghazarian O
et O
al O
. O
, O
2021 O
) O
) O
, O
deviate O
from O
human O
preference O
( O
Fig O
. O
1 O
) O
. O
On O
the O
contrary O
, O
we O
aim O
to O
explicitly O
evaluate O
a O
story O
, O
introducing O
a O
human O
preference O
- O
liked O
system O
consisting O
of O
three O
subtasks O
: O
Ranking B-TaskName
, O
Rating B-TaskName
and O
Reasoning B-TaskName
. O

We O
build O
a O
model O
upon O
Longformer B-MethodName
- I-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
( O
LED B-MethodName
) O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
, O
where O
the O
encoder O
predicts O
the O
preference O
score O
( O
Ranking B-TaskName
) O
, O
aspect O
ratings O
and O
confidences O
( O
Rating B-TaskName
) O
while O
the O
decoder O
generates O
the O
comments O
( O
Reasoning B-TaskName
) O
. O
Inspired O
by O
widely O
- O
used O
pairwise O
comparison O
in O
story O
evaluation O
, O
we O
train O
our O
model O
with O
the O
ranking O
objectives O
. O
In O
this O
way O
, O
the O
score O
margin O
between O
Figure O
2 O
: O
The O
Writing O
Prompt O
Dataset O
with O
metadata O
( O
left O
) O
contains O
prompt O
, O
story O
, O
upvotes O
, O
and O
comments O
from O
readers O
. O
Our O
dataset O
collection O
pipeline O
( O
right O
) O
shows O
the O
template O
for O
data O
collection O
. O
We O
ask O
the O
workers O
to O
select O
3 O
- O
5 O
aspects O
, O
score O
each O
aspect O
1 O
- O
5 O
from O
poor O
to O
good O
and O
leave O
the O
comments O
that O
shows O
the O
reason O
for O
the O
score O
they O
rated O
. O

good O
and O
poor O
stories O
are O
enlarged O
, O
resulting O
in O
high O
correlation O
between O
human O
preference O
and O
our O
predicted O
preference O
score O
( O
Fig O
. O
1 O
) O
. O
We O
also O
witness O
that O
our O
performance O
is O
improved O
when O
we O
conduct O
joint O
training O
on O
three O
subtasks O
. O

In O
aid O
of O
the O
proposed O
task O
, O
we O
present O
a O
wellannotated O
crowd O
- O
sourcing O
dataset O
, O
consisting O
of O
two O
parts O
. O
( O
i O
) O
One O
is O
built O
from O
63,929 B-HyperparameterValue
stories O
and O
their O
corresponding O
upvote O
counts O
provided O
in O
Writ B-DatasetName
- I-DatasetName
ingPrompt I-DatasetName
dataset O
( O
WP B-DatasetName
) O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
( O
Figure O
2 O
( O
left O
) O
) O
by O
pairing O
one O
highly O
- O
upvoted O
story O
( O
upvotes B-HyperparameterName
≥ O
50 B-HyperparameterValue
) O
and O
one O
lowly O
- O
upvoted O
story O
( O
upvotes B-HyperparameterName
≤ O
0 B-HyperparameterValue
) O
under O
the O
same O
prompt O
. O
As O
a O
result O
, O
we O
obtain O
100k B-HyperparameterValue
pairs B-HyperparameterName
of I-HyperparameterName
stories I-HyperparameterName
, O
namely O
100k B-HyperparameterValue
story O
ranking O
data O
, O
used O
to O
train O
and O
evaluate O
the O
preference O
score O
prediction O
. O
( O
ii O
) O
The O
other O
part O
is O
made O
up O
of O
45,948 B-HyperparameterValue
aspect O
comments O
and O
their O
respective O
rating O
scores O
( O
1 O
- O
5 O
) O
by O
Amazon O
Mechanical O
Turk O
( O
AMT O
) O
and O
augmented O
data O
( O
Section O
3.2 O
) O
, O
namely O
46k B-HyperparameterValue
aspect O
rating O
and O
reasoning O
data O
, O
used O
for O
model O
explanation O
. O
Our O
contributions O
are O
three O
- O
fold O
: O

• O
This O
study O
addresses O
a O
novel O
task O
StoryER B-TaskName
, O
that O
consists O
of O
preference B-TaskName
score I-TaskName
prediction I-TaskName
, O
aspect B-TaskName
rating I-TaskName
and O
comment B-TaskName
generation I-TaskName
. O
• O
We O
introduce O
a O
new O
dataset O
for O
StoryER B-TaskName
task O
and O
create O
benchmarks O
to O
promote O
the O
story O
evaluation O
research O
. O
• O
Comprehensive O
experiments O
and O
intensive O
analysis O
indicate O
our O
preference O
score O
prediction O
outperforms O
previous O
metrics O
, O
and O
more O
accurately O
reflects O
human O
preference O
. O
Aspect B-TaskName
rating I-TaskName
and O
comment B-TaskName
generation I-TaskName
also O
helps O
in O
the O
evaluation O
and O
provide O
explanations O
. O
Moreover O
, O
we O
point O
out O
the O
remaining O
challenges O
under O
various O
scenarios O
in O
the O
hope O
that O
facilitates O
future O
research O
. O

Related O
work O

Overlap O
- O
based O
metrics O
such O
as O
BLEU B-MetricName
( O
Sulem O
et O
al O
. O
, O
2018 O
) O
and O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
calculate O
lexical O
matches O
( O
i.e. O
, O
n O
- O
gram O
matching O
) O
and O
reward O
the O
words O
that O
resemble O
the O
reference O
in O
their O
surface O
form O
, O
even O
if O
they O
do O
not O
accurately O
capture O
meaning O
, O
and O
penalize O
other O
paraphrases O
. O
Recent O
research O
( O
Edunov O
et O
al O
. O
, O
2020 O
) O
indicates O
that O
these O
metrics O
do O
not O
reflect O
human O
preferences O
, O
particularly O
for O
open O
- O
ended O
text O
generation O
tasks O
. O

Neural O
- O
based O
metrics O
are O
motivated O
by O
the O
success O
of O
transformers O
as O
multitask O
learners O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
and O
adapt O
them O
for O
the O
task O
of O
neural O
language O
evaluation O
. O
When O
compared O
to O
overlapbased O
metrics O
, O
BERTScore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
, O
MoverScore B-MetricName
( O
Zhao O
et O
al O
. O
, O
2019 O
) O
, O
BLEURT B-MetricName
( O
Sellam O
et O
al O
. O
, O
2020 O
) O
report O
stronger O
correlations O
with O
human O
judgment O
. O
For O
specific O
use O
, O
in O
open O
dialogue O
generation O
, O
Adem B-MetricName
( O
Lowe O
et O
al O
. O
, O
2017 O
) O
captures O
semantic O
similarity O
beyond O
word O
overlap O
statistics O
, O
and O
exploits O
both O
the O
context O
and O
the O
reference O
response O
to O
calculate O
its O
score O
for O
the O
model O
response O
. O
RUBER B-MethodName
( O
Tao O
et O
al O
. O
, O
2018 O
) O
and O
its O
variant O
, O
RUBER B-MethodName
- I-MethodName
BERT I-MethodName
( O
Ghazarian O
et O
al O
. O
, O
2019 O
) O
evaluates O
a O
reply O
by O
taking O
into O
consideration O
both O
a O
ground O
- O
truth O
reply O
and O
a O
query O
without O
requiring O
labels O
of O
human O
satisfaction O
and O
can O
be O
extended O
to O
different O
datasets O
and O
languages O
. O

Neural O
discriminator O
is O
proposed O
particularly O
for O
story O
evaluation O
. O
The O
metrics O
mentioned O
above O
show O
limited O
performance O
in O
story O
evaluation O
as O
demonstrated O
in O
Guan O
et O
al O
. O
( O
2021 O
) O
. O
UNION B-MethodName
( O
Guan O
and O
Huang O
, O
2020 O
) O
and O
MANPLTS B-MethodName
( O
Ghazarian O
et O
al O
. O
, O
2021 O
) O
analyze O
the O
problem O
from O
machinegenerated O
stories O
and O
generate O
negative O
data O
by O
heuristics O
and O
plot O
manipulation O
, O
and O
then O
distin- O
guish O
by O
a O
BERT O
- O
based O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

The O
coherence B-MetricName
score O
they O
produce O
can O
be O
expressed O
as O
the O
probability O
of O
the O
story O
being O
identified O
as O
human O
- O
written O
story O
. O
In O
this O
paper O
, O
we O
require O
our O
model O
to O
follow O
human O
preference O
, O
not O
only O
the O
coherence B-MetricName
, O
which O
we O
believe O
is O
a O
more O
general O
way O
of O
story O
evaluation O
. O

Dataset O

Our O
dataset O
comprises O
of O
two O
parts O
: O
100k B-HyperparameterValue
story B-TaskName
ranking I-TaskName
, O
and O
46k B-HyperparameterValue
aspect B-TaskName
rating I-TaskName
and O
reasoning B-TaskName
. O
2 O

100k B-HyperparameterValue
Story B-TaskName
Ranking I-TaskName
Data O

As O
we O
mentioned O
above O
, O
ranking O
method O
is O
more O
flexible O
and O
better O
than O
discrimination O
when O
evaluating O
the O
story O
( O
we O
also O
experimentally O
compare O
them O
in O
Sec O
. O
F.1 O
) O
. O
We O
thus O
prepare O
100k B-HyperparameterValue
pairwise O
ranking O
data O
for O
training O
the O
model O
. O
To O
this O
end O
, O
we O
first O
collect O
193,842 B-HyperparameterValue
stories O
prior O
to O
03 O
/ O
2020 O
from O
WP B-DatasetName
3 O
along O
with O
their O
prompt O
, O
the O
number O
of O
upvotes O
and O
uncategorized O
comments O
. O
We O
remove O
the O
stories O
updated O
from O
12 O
/ O
2019 O
to O
03 O
/ O
2020 O
, O
since O
newly O
- O
updated O
stories O
usually O
have O
few O
upvotes O
regardless O
of O
whether O
they O
are O
good O
or O
bad O
. O
Then O
, O
we O
exclusively O
keep O
stories O
with O
the O
word B-HyperparameterName
count I-HyperparameterName
between O
200 B-HyperparameterValue
and O
800 B-HyperparameterValue
. O
Finally O
, O
we O
pick O
two O
stories O
from O
the O
same O
prompt O
, O
one O
highly O
upvoted O
( O
i.e. O
, O
upvotes B-HyperparameterName
≥ O
50 B-HyperparameterValue
4 O
) O
and O
one O
lowly O
upvoted O
( O
i.e. O
, O
upvotes B-HyperparameterName
≤ O
0 B-HyperparameterValue
) O
, O
resulting O
in O
a O
total O
of O
63,929 B-HyperparameterValue
unique O
stories O
and O
116,971 B-HyperparameterValue
story O
pairs O
. O
We O
split O
the O
story O
pairs O
based O
on O
the O
prompts O
into O
training O
, O
validation O
and O
testing O
( O
Table O
1 O
) O
, O
to O
ensure O
that O
each O
division O
receives O
a O
unique O
set O
of O
prompts O
. O

2 O
All O
data O
collection O
follows O
the O
same O
procedure O
as O
described O
in O
the O
previous O
work O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
on O
Reddit O
, O
which O
comply O
with O
ACL O
Code O
of O
Ethics O
. O
3 O
https O
: O
/ O
/ O
huggingface.co O
/ O
datasets O
/ O
rewardsignal O
/ O
reddit_writing_prompts O
4 O
we O
notice O
that O
some O
stories O
that O
receive O
upvotes B-HyperparameterName
≥ O
50 B-HyperparameterValue
can O
be O
listed O
in O
/ O
r O
/ O
bestofWritingPrompts O
/ O

46k B-HyperparameterValue
Aspect B-TaskName
Rating I-TaskName
and O
Reasoning B-TaskName
Data O

Apart O
from O
the O
preference O
score O
, O
we O
require O
our O
model O
to O
provide O
ratings O
and O
comments O
on O
predefined O
aspects O
to O
aid O
in O
the O
explanation O
of O
the O
predicted O
preference O
score O
. O
Aspect O
category O
extraction O
. O
To O
begin O
with O
, O
we O
must O
determine O
which O
aspects O
in O
the O
content O
should O
be O
measured O
. O
As O
some O
readers O
leave O
comments O
to O
explain O
why O
they O
upvote O
or O
downvote O
the O
stories O
, O
a O
straightforward O
way O
is O
to O
extract O
aspect O
categories O
based O
on O
those O
uncategorized O
comments O
. O
We O
therefore O
adopt O
latent O
Dirichlet O
allocation O
( O
LDA O
) O
, O
which O
models O
the O
documents O
with O
a O
certain O
number O
of O
topics O
, O
based O
upon O
the O
co O
- O
occurrence O
of O
individual O
words O
. O
More O
precisely O
, O
we O
follow O
Brody O
and O
Elhadad O
( O
2010 O
) O
to O
treat O
each O
comment O
as O
a O
separate O
document O
. O
LDA O
can O
produce O
a O
distribution O
of O
frequency O
of O
occurrence O
for O
each O
word O
in O
the O
topics O
. O
We O
optimize O
LDA O
through O
a O
cluster O
validation O
scheme O
, O
and O
obtain O
the O
optimal O
number O
of O
aspects O
10 B-HyperparameterValue
. O
Based O
on O
the O
most O
representative O
words O
in O
each O
topic O
, O
we O
manually O
name O
each O
topic O
as O
the O
aspect O
category O
. O
These O
aspect O
categories O
are O
defined O
using O
some O
widely O
used O
aspects O
inspired O
from O
the O
websites O
. O
5 O
Comment O
and O
aspect O
collection O
. O
Comments O
in O
WP B-DatasetName
meta O
data O
are O
neither O
categorized O
with O
aspect O
categories O
, O
nor O
labeled O
with O
ratings O
, O
and O
some O
of O
them O
are O
totally O
irrelevant O
to O
the O
content O
. O
More O
importantly O
, O
there O
is O
a O
bias O
towards O
positive O
comments O
, O
which O
implies O
that O
not O
too O
many O
readers O
are O
willing O
to O
leave O
comments O
on O
poor O
stories O
. O
Therefore O
, O
we O
collect O
new O
comments O
via O
crowd O
- O
sourcing O
. O
By O
learning O
from O
these O
well O
- O
annotated O
comment O
data O
, O
we O
train O
neural O
models O
to O
filter O
out O
noisy O
data O
from O
comments O
in O
WP B-DatasetName
meta O
data O
. O
To O
collect O
the O
data O
, O
we O
ask O
workers O
from O
AMT O
to O
select O
aspects O
, O
rate O
sentiment O
and O
leave O
comments O
on O
5,964 B-HyperparameterValue
unique O
stories O
from O
WP B-DatasetName
. O
For O
increasing O
the O
diversity O
of O
comments O
, O
some O
stories O
are O
allocated O
to O
two O
different O
annotators O
, O
resulting O
in O
a O
total O
of O
9,112 O
submissions O
( O
i.e. O
, O
1.53 O
annotations O
/ O
story O
) O
. O
As O
shown O
in O
Figure O
2 O
( O
right O
) O
, O
each O
story O
requires O
the O
annotators O
to O
rate O
( O
normalized O
to O
0 O
- O
1 O
) O
and O
leave O
comments O
on O
3 O
to O
5 O
aspects O
that O
are O
most O
confident O
by O
the O
workers O
. O
The O
final O
statistics O
of O
the O
comments O
is O
listed O
in O
We O
list O
the O
number O
of O
comments O
with O
rating O
scores O
( O
2nd O
and O
3rd O
columns O
) O
, O
averaged O
rating O
scores O
( O
4th O
and O
5th O
columns O
) O
and O
averaged O
word O
count O
( O
6th O
and O
7th O
columns O
) O
. O

trained O
with O
our O
collected O
data O
. O
The O
training O
details O
can O
be O
found O
in O
the O
supplementary O
material O
. O
We O
filter O
out O
irrelevant O
comments O
by O
eliminating O
those O
with O
no O
values O
in O
aspect O
categories O
that O
exceeds O
0.9 B-HyperparameterValue
after O
softmax O
and O
retain O
the O
comments O
with O
the O
word B-HyperparameterName
count I-HyperparameterName
ranged O
from O
15 B-HyperparameterValue
to O
50 B-HyperparameterValue
. O
The O
remaining O
comments O
are O
then O
rated O
by O
the O
their O
sentiments O
. O
Finally O
, O
we O
obtain O
17,849 B-HyperparameterValue
valuable O
comments O
for O
6,705 B-HyperparameterValue
additional O
unique O
stories O
and O
merge O
them O
into O
our O
collected O
data O
, O
resulting O
in O
a O
total O
number O
of O
45,948 B-HyperparameterValue
for O
comments O
and O
12,669 O
for O
unique O
stories O
. O
We O
split O
the O
collected O
data O
into O
training O
, O
validation O
, O
and O
test O
data O
in O
the O
ratio B-HyperparameterName
of O
8:1:1 B-HyperparameterValue
and O
put O
the O
augmented O
data O
into O
the O
training O
data O
( O
Table O
2 O
) O
. O

StoryER B-TaskName

Task O
Definition O

Given O
a O
story O
s O
, O
the O
task O
is O
to O
output O
a O
set O
{ O
p O
s O
, O
a O
c O
, O
a O
r O
, O
c O
} O
where O
p O
s O
denotes O
the O
preference O
score O
of O
the O
story O
s O
, O
which O
is O
used O
for O
comparing O
story O
quality O
. O
For O
more O
explicit O
explanation O
, O
we O
further O
output O
confidence O
scores O
a O
c O
= O
{ O
a O
c O
k O
} O
K O
k=1 O
, O
aspect O
ratings O
a O
r O
= O
{ O
a O
r O
k O
} O
K O
k=1 O
, O
and O
comments O
c O
= O
{ O
c O
k O
} O
K O
k=1 O
for O
K B-HyperparameterName
aspects B-HyperparameterName
( O
K B-HyperparameterName
= O
10 B-HyperparameterValue
in O
our O
experiments O
) O
, O
respectively O
. O
Confidence O
scores O
a O
c O
reflect O
the O
likelihood O
of O
utilizing O
the O
specific O
aspects O
as O
measures O
, O
as O
some O
aspects O
( O
e.g. O
, O
horror O
) O
are O
not O
applicable O
in O
some O
stories O
( O
e.g. O
, O
comic O
story O
) O
. O
Aspect O
ratings O
a O
r O
k O
are O
considered O
as O
the O
scores O
of O
each O
aspect O
. O
Comments O
c O
demonstrate O
the O
reason O
that O
the O
reader O
upvotes O
/ O
downvotes O
the O
story O
, O
producing O
a O
more O
explicit O
explanation O
for O
the O
aspect O
rating O
. O
We O
assume O
K O
k=1 O
a O
c O
k O
= O
1 O
for O
aspect O
confidence O
, O
and O
a O
r O
k O
∈ O
[ O
0 O
, O
1 O
] O
for O
aspect O
rating O
, O
which O
is O
calculated O
separately O
during O
the O
training O
. O

Please O
note O
that O
aspect O
rating O
and O
comment O
gen- O
eration O
results O
are O
not O
used O
as O
metrics O
in O
this O
work O
, O
while O
they O
are O
used O
for O
1 O
) O
improving O
preference B-TaskName
score I-TaskName
prediction I-TaskName
by O
joint O
learning O
, O
and O
2 O
) O
producing O
explanation O
. O
Investigating O
how O
to O
include O
them O
into O
metrics O
is O
a O
future O
direction O
for O
this O
research O
. O

Learning O
a O
Story O
Evaluator O

Following O
Ghazarian O
et O
al O
. O
( O
2021 O
) O
, O
we O
use O
Longformer B-MethodName
- I-MethodName
Encoder I-MethodName
- I-MethodName
Decoder I-MethodName
( O
LED B-MethodName
) O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
to O
produce O
a O
preference O
score O
, O
as O
well O
as O
ratings O
and O
comments O
for O
the O
pre O
- O
defined O
aspects O
. O
As O
shown O
in O
Figure O
3 O
, O
we O
encode O
the O
story O
s O
, O
and O
use O
its O
feature O
on O
the O
special O
token O
( O
i.e. O
, O
[ O
CLS O
] O
) O
to O
predict O
the O
preference O
score O
p O
s O
, O
aspect O
confidence O
a O
c O
and O
rating O
a O
r O
by O
additional O
layers O
. O
For O
generating O
comments O
, O
we O
concatenate O
the O
story O
with O
aspect O
category O
name O
with O
a O
special O
token O
( O
i.e. O
, O
< O
sep O
> O
) O
, O
and O
send O
it O
into O
the O
same O
encoder O
. O
The O
decoder O
outputs O
the O
comment O
c O
that O
implies O
the O
performance O
of O
the O
story O
on O
the O
given O
aspect O
. O

Task O
1 O
: O
Preference B-TaskName
Score I-TaskName
Prediction I-TaskName
( O
Ranking B-TaskName
) O

Our O
model O
learns O
to O
predict O
the O
preference O
score O
by O
ranking O
two O
stories O
from O
the O
same O
prompt O
. O
As O
shown O
in O
Figure O
3 O
, O
we O
use O
the O
feature O
of O
[ O
CLS O
] O
in O
the O
story O
, O
following O
a O
linear O
layer O
with O
sigmoid O
activation O
and O
finally O
turning O
it O
into O
a O
scalar O
score O
. O

We O
take O
Margin O
Ranking O
Loss O
to O
enlarge O
the O
margin O
gap O
m O
of O
the O
scores O
between O
stories O
with O
high O
and O
low O
upvotes B-HyperparameterName
: O

L O
ps O
= O
max O
( O
0 O
, O
σ O
( O
W O
ps O
v O
s O
low O
) O
− O
σ O
( O
W O
ps O
v O
s O
high O
) O
+ O
m O
) O
, O
( O
1 O
) O

where O
W O
ps O
denotes O
a O
linear O
layer O
for O
the O
feature O
of O
the O
story O
v O
s O
. O
σ O
( O
• O
) O
is O
the O
sigmoid O
activation O
function O
. O
s O
high O
and O
s O
low O
represent O
the O
highly O
- O
upvoted O
and O
lowly O
- O
upvoted O
stories O
. O

Negative O
sample O
. O
Machine O
- O
generated O
stories O
often O
suffer O
from O
the O
coherence O
and O
consistency O
problem O
, O
while O
human O
- O
written O
stories O
usually O
do O
not O
. O
Therefore O
our O
model O
trained O
on O
human O
- O
written O
stories O
can O
hardly O
evaluate O
story O
coherence O
. O
To O
enable O
our O
model O
to O
evaluate O
story O
considering O
coherence O
issues O
, O
we O
further O
train O
our O
model O
( O
Ours O
( O
N O
) O
) O
with O
negative O
stories O
that O
are O
generated O
by O
the O
methods O
in O
the O
previous O
works O
( O
Guan O
and O
Huang O
, O
2020 O
; O
Ghazarian O
et O
al O
. O
, O
2021 O
) O
. O
We O
change O
the O
margin O
ranking O
loss O
as O
follow O
: O

L O
pref O
= O
max O
( O
0 O
, O
σ O
( O
W O
ps O
v O
s O
low O
) O
− O
σ O
( O
W O
ps O
v O
s O
high O
) O
+ O
m O
) O
, O
L O
coh O
= O
max O
( O
0 O
, O
σ O
( O
W O
ps O
v O
sneg O
) O
− O
σ O
( O
W O
ps O
v O
s O
low O
) O
+ O
m O
) O
, O
L O
ps O
= O
L O
pref O
+ O
L O
coh O
, O
( O
2 O
) O

where O
s O
neg O
denotes O
the O
negative O
stories O
derived O
from O
the O
previous O
works O
. O
In O
each O
iteration O
, O
we O
takes O
two O
pairs O
as O
training O
data O
: O
s O
high O
and O
s O
low O
, O
s O
low O
and O
s O
neg O
. O

Task O
2 O
: O
Aspect B-TaskName
Confidence I-TaskName
and I-TaskName
Rating I-TaskName
Prediction I-TaskName
( O
Rating B-TaskName
) O
. O

We O
adopt O
two O
additional O
linear O
layers O
on O
the O
same O
feature O
v O
s O
used O
in O
the O
story O
ranking O
. O
One O
is O
with O
learnable O
parameters O
W O
a O
c O
, O
outputting O
confidence B-MetricName
scores I-MetricName
a O
c B-MetricName
= O
softmax O
( O
W O
a O
c O
v O
s O
) O
. O
The O
other O
one O
has O
W O
a O
r O
, O
producing O
aspect O
rating O
a O
r O
= O
σ O
( O
W O
a O
r O
v O
s O
) O
. O

Let O
y O
a O
c B-MetricName
∈ O
{ O
0 B-MetricValue
, O
1 B-MetricValue
} O
K B-HyperparameterName
, O
y O
a O
r O
∈ O
[ O
0 B-MetricValue
, O
1 B-MetricValue
] O
K B-HyperparameterName
be O
the O
groundtruth O
confidence O
and O
rating O
, O
we O
define O
the O
confi O
- O
dence O
and O
rating O
loss O
functions O
as O
follows O
: O

L O
a O
c B-MetricName
= O
− O
K B-HyperparameterName
k=1 O
y O
a O
c B-MetricName
[ O
k O
] O
log O
a O
c O
[ O
k O
] O
, O
( O
3 O
) O

L O
a O
r O
= O
− O
k∈Ms O
y O
a O
r O
[ O
k O
] O
log O
a O
r O
[ O
k O
] O
( O
4 O
) O
+ O
( O
1 O
− O
y O
a O
r O
[ O
k O
] O
) O
* O
log O
( O
1 O
− O
a O
r O
[ O
k O
] O
) O
. O

We O
calculate O
the O
multi O
- O
class O
cross O
- O
entropy O
loss O
for O
the O
aspect O
confidence O
. O
y O
a O
c O
[ O
k O
] O
= O
1 O
if O
the O
k B-HyperparameterName
- O
th O
aspect O
is O
selected O
, O
otherwise O
y O
a O
c O
[ O
k O
] O
= O
0 O
. O
For O
aspect O
rating O
, O
binary O
cross O
- O
entropy O
loss O
is O
calculated O
separately O
for O
each O
selected O
aspects O
. O
M O
s O
denotes O
the O
set O
of O
aspects O
that O
are O
selected O
for O
story O
s. O
y O
a O
r O
[ O
k O
] O
denotes O
the O
normalized O
rating O
score O
for O
the O
k O
- O
th O
aspect O
. O

4.5 O
Task O
3 O
: O
Comment B-TaskName
Generation I-TaskName
( O
Reasoning B-TaskName
) O
. O

The O
comments O
are O
generated O
conditioned O
on O
the O
aspect O
a O
and O
the O
story O
s. O
We O
input O
the O
concatenation O
of O
the O
aspect O
category O
name O
, O
special O
token O
, O
story O
, O
and O
train O
the O
LED B-MethodName
under O
Maximum O
Likelihood O
Estimation O
( O
MLE O
) O
with O
the O
comment O
as O
target O
: O

L O
c O
( O
p O
θ O
) O
= O
− O
|c| O
t=1 O
log O
p O
θ O
( O
c O
t O
| O
a O
, O
s O
, O
c O
< O
t O
) O
, O
( O
5 O
) O

where O
the O
c O
t O
denotes O
the O
t O
- O
th O
token O
in O
the O
comment O
. O

For O
joint O
training O
three O
tasks O
, O
our O
final O
loss O
is O
the O
summation O
of O
all O
above O
loss O
functions O
: O

L O
= O
L O
ps O
+ O
L O
a O
c O
+ O
L O
a O
r O
+ O
L O
c O
. O
( O
6 O
) O

Hyperparameters O

We O
conduct O
a O
comprehensive O
set O
of O
experiments O
to O
examine O
the O
effectiveness O
under O
different O
scenarios O
. O

We O
fine O
- O
tune O
pre O
- O
trained O
LED B-MethodName
from O
Huggingface O
6 O
with O
the O
batch B-HyperparameterName
size I-HyperparameterName
16 B-HyperparameterValue
, O
the O
margin B-HyperparameterName
0.3 B-HyperparameterValue
and O
run O
20k B-HyperparameterValue
iterations B-HyperparameterName
for O
training O
( O
10 O
hours O
) O
. O
We O
adopt O
AdamW O
optimizer O
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
with O
an O
initial B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
4e-6 B-HyperparameterValue
, O
warming O
up O
in O
the O
first O
epoch O
and O
decreasing O
by O
a O
linear O
schedule O
. O
The O
reported O
results O
are O
averaged O
by O
the O
best O
results O
from O
three O
models O
with O
the O
same O
structure O
but O
initialized O
with O
three O
different O
seeds O
. O
More O
details O
and O
code O
can O
be O
found O
in O
the O
Appendix O
. O
3 O
: O
Evaluation O
on O
preference O
score O
prediction O
. O
Compared O
with O
previous O
works O
, O
our O
predict O
scores O
more O
correctly O
match O
the O
human O
judgement O
. O
We O
conduct O
hypothesis O
test O
( O
Diedenhofen O
and O
Musch O
, O
2015 O
) O
, O
and O
* O
denotes O
that O
p O
≤ O
0.01 O
. O

and O
Huang O
, O
2020 O
) O
, O
and O
MANPLTS B-MethodName
( O
Ghazarian O
et O
al O
. O
, O
2021 O
) O
. O

Preference B-MetricName
Score I-MetricName
Evaluation O

Accuracy B-MetricName
and O
Score O
Distance O

We O
evaluate O
the O
predicted O
preference O
scores O
obtained O
by O
all O
compared O
methods O
on O
100k B-HyperparameterName
Story O
Ranking O
test O
data O
. O
Pairwise B-MetricName
Ranking I-MetricName
Accuracy I-MetricName
( O
Acc B-MetricName
) O
is O
calculated O
as O
the O
percentage O
of O
the O
story O
with O
higher O
upvotes B-HyperparameterName
getting O
a O
higher O
score O
than O
the O
one O
with O
lower O
upvotes B-HyperparameterName
. O
We O
also O
compute O
the O
averaged B-MetricName
score I-MetricName
gap I-MetricName
( O
Dis B-MetricName
) O
between O
two O
stories O
in O
pairs O
. O
Table O
3 O
( O
Human O
( O
Ranking O
) O
) O
indicates O
that O
existing O
methods O
on O
preference O
- O
aware O
story O
evaluation O
on O
human O
- O
written O
stories O
are O
close O
to O
random O
selection O
( O
i.e. O
, O
Acc=0.5 B-MetricName
, O
Dis=0 B-MetricName
) O
. O
In O
contrast O
, O
our O
method O
can O
successfully O
compare O
two O
stories O
and O
achieve O
an O
acceptable O
score O
gap O
between O
two O
stories O
. O

Correlation O
with O
Human O
Judgments O

We O
calculate O
the O
correlation O
between O
our O
predicted O
preference B-MetricName
scores I-MetricName
and O
human O
judgment O
for O
stories O
. O
We O
use O
the O
correlation O
metrics O
Spearman B-MetricName
( O
ρ B-MetricName
) O
( O
Zar O
, O
1972 O
) O
and O
Kendall B-MetricName
( O
τ B-MetricName
) O
( O
Schaeffer O
and O
Levitt O
, O
1956 O
) O
, O
which O
are O
known O
to O
be O
beneficial O
in O
estimating O
monotonic O
associations O
for O
not O
normally O
distributed O
and O
ranked O
scores O
. O
We O
collect O
and O
annotate O
both O
human O
- O
written O
and O
machine O
- O
generated O
stories O
as O
our O
test O
data O
: O
WP B-DatasetName
200 I-DatasetName
. O
We O
collect O
human O
judgments O
for O
the O
stories O
in O
WP B-DatasetName
( O
sampled O
from O
test O
data O
in O
Table O
2 O
) O
, O
where O
each O
story O
is O
assigned O
to O
8 O
annotators O
. O
Annotators O
are O
asked O
to O
rate O
each O
story O
on O
a O
scale O
of O
1 O
to O
5 O
( O
from O
poor O
to O
good O
) O
. O
To O
ensure O
correctness O
, O
we O
follow O
Clark O
et O
al O
. O
( O
2021 O
) O
to O
ask O
the O
annotators O
to O
compare O
the O
stories O
and O
write O
down O
the O
reason O
for O
clarification O
. O
We O
carefully O
detect O
the O
worker O
behavior O
and O
set O
traps O
inside O
the O
annotation O
( O
see O
Appendix O
for O
details O
) O
. O
Finally O
, O
we O
obtain O
100 O
highly O
- O
upvoted O
and O
100 O
lowly O
- O
upvoted O
stories O
and O
average O
the O
human O
rates O
as O
the O
target O
scores O
in O
this O
test O
data O
, O
namely O
, O
WP B-DatasetName
200 I-DatasetName
in O
the O
following O
experiments O
. O
Inside O
, O
we O
witness O
a O
higher O
score O
for O
highly O
- O
voted O
stories O
, O
proving O
our O
hypothesis O
that O
upvote B-HyperparameterName
counts O
reflect O
human O
preference O
. O
SCARY B-DatasetName
200 I-DatasetName
. O
We O
crawled O
scary O
stories O
from O
Reddit O
( O
r O
/ O
shortscarystories O
7 O
) O
, O
which O
are O
similar O
to O
the O
stories O
in O
WP B-DatasetName
but O
in O
a O
constrained O
story O
type O
. O
We O
use O
the O
same O
procedure O
for O
WP B-DatasetName
200 I-DatasetName
to O
create O
another O
human O
- O
annotated O
test O
dataset O
, O
namely O
SCARY B-DatasetName
200 I-DatasetName
. O
PREF B-DatasetName
200 I-DatasetName
. O
The O
same O
procedure O
is O
also O
used O
for O
collecting O
human O
annotation O
for O
machine O
- O
generated O
stories O
. O
We O
select O
100 O
generated O
stories O
by O
LED B-MethodName
trained O
with O
highly O
- O
voted O
stories O
in O
WP B-DatasetName
and O
100 O
stories O
by O
another O
LED B-MethodName
trained O
with O
lowly O
- O
voted O
stories O
. O
We O
manually O
ensure O
that O
the O
selected O
stories O
do O
not O
contain O
severe O
coherence O
issues O
, O
and O
ask O
the O
annotators O
to O
rate O
the O
stories O
based O
on O
whether O
they O
enjoy O
the O
stories O
. O
COH B-DatasetName
200 I-DatasetName
. O
We O
use O
the O
same O
human O
collected O
data O
in O
the O
previous O
work O
( O
Ghazarian O
et O
al O
. O
, O
2021 O
) O
8 O
, O
which O
focused O
on O
recognizing O
coherence O
issues O
in O
the O
machine O
- O
generated O
stories O
( O
e.g. O
, O
repeat O
plots O
, O
conflict O
logic O
) O
. O

Results O
. O
( O
PREF B-DatasetName
200 I-DatasetName
) O
and O
coherence O
- O
based O
judgments O
( O
COH B-DatasetName
200 I-DatasetName
) O
are O
distinct O
. O
Metrics O
that O
perform O
well O
in O
terms O
of O
coherence O
may O
perform O
poorly O
in O
terms O
of O
preference O
, O
and O
vice O
versa O
. O
To O
mitigate O
the O
gap O
between O
preference O
and O
coherence O
, O
we O
train O
our O
model O
using O
negative O
stories O
created O
by O
UNION B-MethodName
and O
MANPLTS B-MethodName
. O
As O
a O
result O
, O
Ours O
( O
N O
) O
shows O
rapidly O
increasing O
performance O
on O
the O
evaluation O
in O
terms O
of O
coherence O
with O
a O
bit O
of O
performance O
drop O
on O
the O
preference O
- O
aware O
evaluation O
, O
indicating O
a O
potential O
to O
take O
into O
account O
both O
coherence O
and O
human O
preference O
when O
evaluating O
a O
story O
. O

6 O
Ablation O
Study O

Preference B-TaskName
Score I-TaskName
Prediction I-TaskName

In O
this O
section O
, O
we O
further O
test O
the O
performance O
of O
preference B-TaskName
score I-TaskName
prediction I-TaskName
combined O
with O
other O
components O
: O
aspects O
a O
, O
comments O
c O
and O
negative O
stories O
N. O
Table O
4 O
summarizes O
the O
results O
by O
joint O
training O
. O
When O
aspects O
are O
used O
, O
performance O
decreases O
in O
the O
WP B-DatasetName
200 I-DatasetName
but O
increases O
in O
the O
SCARY B-DatasetName
200 I-DatasetName
, O
and O
the O
pattern O
is O
reversed O
when O
comments O
are O
used O
. O
We O
also O
test O
the O
model O
performance O
trained O
with O
the O
dataset O
without O
data O
augmentation O
△ O
, O
and O
we O
can O
see O
that O
our O
model O
trained O
with O
augmented O
data O
outperforms O
that O
with O
the O
original O
data O
, O
which O
shows O
the O
significance O
of O
data O
augmentation O
. O

Aspect B-TaskName
Evaluation I-TaskName

We O
evaluate O
our O
model O
for O
predicting O
confidence O
scores O
and O
ratings O
for O
the O
aspects O
. O
For O
confidence O
scores O
, O
we O
calculate O
the O
recall B-MetricName
performance I-MetricName
on I-MetricName
topk I-MetricName
( O
i.e. O
, O
k=1,3,5 B-HyperparameterName
) O
on O
the O
test B-HyperparameterName
split I-HyperparameterName
of O
46 B-HyperparameterValue
K I-HyperparameterValue
Aspect O
Rating O
and O
Reasoning O
data O
to O
show O
the O
percentage O
of O
human O
selected O
aspects O
that O
can O
be O
involved O
within O
the O
aspects O
with O
top B-MetricName
- I-MetricName
k I-MetricName
confidence I-MetricName
. O
For O
ratings O
, O
we O
calculate O
the O
correlation O
between O
human O
annotation O
and O
our O
model O
prediction O
. O
6 O
: O
Comment O
generation O
evaluation O
on O
automatic O
scores O
and O
human O
evaluation O
. O
In O
human O
evaluation O
, O
the O
kappa B-MetricName
coefficient I-MetricName
κ B-MetricName
for O
each O
score O
are O
located O
in O
0.4 B-MetricValue
- O
0.6 B-MetricValue
, O
indicating O
a O
moderate O
agreement O
between O
annotators O
. O

Story O
ranking O
and O
reasoning O
help O
the O
model O
output O
more O
correct O
confidence O
and O
ratings O
. O

Comment O
Evaluation O

We O
evaluate O
the O
comment O
generation O
with O
automatic O
metrics O
and O
human O
evaluation O
. O
For O
automatic O
scores O
, O
we O
apply O
Perplexity B-MetricName
( O
PPL B-MetricName
) O
, O
Averaged O
BLEU1 B-MetricName
- O
4 O
( O
B B-MetricName
) O
, O
ROUGE B-MetricName
( O
R B-MetricName
) O
. O
For O
human O
evaluation O
, O
we O
mainly O
measure O
the O
relativeness O
between O
comments O
with O
the O
given O
story O
Rel O
( O
s O
) O
, O
aspect O
category O
Rel O
( O
a O
) O
and O
rating B-MetricName
score I-MetricName
( O
0 B-MetricValue
- O
1 B-MetricValue
negative O
- O
positive O
) O
Rel O
( O
r O
) O
. O
We O
also O
measure O
Overall B-MetricName
( O
O B-MetricName
) O
quality B-MetricName
by O
calculating O
the O
percentage O
of O
the O
comments O
that O
are O
agreed O
upon O
by O
annotators O
. O
Each O
comment O
is O
assigned O
to O
5 O
annotators O
with O
a O
binary O
choice O
( O
i.e. O
, O
related O
or O
not O
related O
, O
agree O
or O
not O
agree O
) O
. O
From O
the O
result O
in O
Table O
10 O
, O
our O
generated O
comments O
are O
highly O
related O
to O
the O
given O
stories O
and O
the O
aspects O
. O
Together O
with O
the O
training O
on O
preference O
score O
prediction O
and O
aspect O
rating O
further O
improve O
the O
comment O
generation O
performance O
. O
The O
results O
so O
far O
show O
that O
the O
preference B-MetricName
score I-MetricName
, O
aspects O
, O
and O
comments O
all O
benefit O
one O
another O
, O
illustrating O
the O
significance O
of O
incorporating O
aspects O
and O
comments O
into O
our O
task O
. O

Discussion O

Pairwise O
Evaluation O
with O
StoryER B-TaskName

Given O
a O
set O
of O
prompts O
, O
two O
story O
generation O
models O
can O
generate O
stories O
based O
on O
the O
given O
prompt O
. O
We O
have O
two O
straightforward O
ways O
to O
compare O
two O
models O
using O
our O
proposed O
preference B-MetricName
scores I-MetricName
: O
1 O
) O
average O
the O
preference B-MetricName
scores I-MetricName
for O
stories O
on O
each O
model O
and O
compare O
the O
mean O
average O
scores O
. O
2 O
) O
perform O
pairwise O
comparisons O
for O
stories O
from O
the O
same O
prompt O
and O
get O
the O
preference O
percentage O
. O
We O
recommend O
the O
second O
method O
as O
it O
strictly O
follows O
our O
pairwise O
ranking O
strategy O
. O

Domain O
Transfer O
in O
Preference B-MetricName
Score I-MetricName

To O
show O
the O
generalization O
of O
evaluation O
metrics O
, O
we O
calculate O
the O
averaged O
predicted O
preference B-MetricName
scores I-MetricName
for O
data O
from O
different O
domains O
( O
see O
Table O
7 O
) O
. O
We O
compute O
average O
scores O
on O
1 O
) O
lowlyvoted O
( O
low O
) O
and O
highly O
- O
voted O
stories O
( O
high O
) O
on O
both O
WP B-DatasetName
200 I-DatasetName
and O
SCARY B-DatasetName
200 I-DatasetName
, O
2 O
) O
machine O
- O
generated O
stories O
by O
LED B-MethodName
( O
LED B-MethodName
) O
, O
and O
with O
Plan B-MethodName
- I-MethodName
and I-MethodName
- I-MethodName
Write I-MethodName
strategy I-MethodName
( O
Yao O
et O
al O
. O
, O
2019 O
) O
( O
P B-MethodName
& I-MethodName
W I-MethodName
) O
trained O
separately O
on O
the O
highly O
- O
upvoted O
and O
lowly O
- O
upvoted O
stories O
, O
3 O
) O
negative O
stories O
generated O
from O
previous O
works O
( O
Guan O
and O
Huang O
, O
2020 O
; O
Ghazarian O
et O
al O
. O
, O
2021 O
) O
, O
4 O
) O
stories O
from O
other O
datasets O
: O
fairy B-DatasetName
tales I-DatasetName
( O
short O
stories O
) O
, O
childbook B-DatasetName
dataset I-DatasetName
( O
Hill O
et O
al O
. O
, O
2015 O
) O
and O
bookcorpus B-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
. O

As O
shown O
in O
Table O
7 O
, O
UNION B-MethodName
and O
MANPLTS B-MethodName
consistently O
produce O
higher O
scores O
for O
humanwritten O
stories O
( O
Human O
and O
Other O
blocks O
) O
while O
producing O
lower O
scores O
for O
machine O
- O
generated O
stories O
( O
Machine O
and O
N O
blocks O
) O
. O
While O
looking O
into O
more O
details O
, O
we O
can O
see O
that O
they O
can O
not O
successfully O
distinguish O
the O
story O
quality O
, O
e.g. O
, O
SCARY B-DatasetName
200 I-DatasetName
( O
low O
) O
and O
SCARY B-DatasetName
200 I-DatasetName
( O
high O
) O
receive O
identical O
scores O
. O
These O
observations O
strongly O
indicate O
that O
UNION B-MethodName
and O
MANPLTS B-MethodName
work O
well O
on O
evaluating O
coherence O
but O
deviate O
from O
human O
preference O
when O
evaluating O
human O
- O
written O
stories O
. O

Our O
method O
, O
on O
the O
other O
hand O
, O
is O
capable O
of O
following O
human O
preference O
( O
Human O
and O
Machine O
block O
) O
( O
also O
see O
SCARY B-DatasetName
200 I-DatasetName
( O
low O
) O
and O
SCARY B-DatasetName
200 I-DatasetName
( O
high O
) O
as O
an O
example O
) O
. O
The O
model O
trained O
with O
highly O
- O
voted O
stories O
can O
generate O
better O
stories O
than O
that O
trained O
with O
lowly O
- O
voted O
stories O
, O
and O
P B-MethodName
& I-MethodName
W I-MethodName
strategy O
performs O
even O
better O
as O
proved O
in O
many O
previous O
works O
( O
Fan O
et O
al O
. O
, O
2019 O
; O
Tan O
et O
al O
. O
, O
2021 O
) O
. O
From O
the O
results O
, O
our O
model O
produces O
higher O
scores O
for O
LED B-MethodName
( O
high O
) O
compared O
with O
LED B-MethodName
( O
low O
) O
and O
even O
higher O
scores O
for O
LED B-MethodName
P B-MethodName
& I-MethodName
W I-MethodName
( O
high O
) O
, O
which O
indicates O
that O
our O
model O
still O
follows O
the O
human O
preference O
on O
machine O
- O
generated O
stories O
. O
As O
serious O
coherence O
problems O
do O
not O
commonly O
occur O
in O
our O
training O
data O
, O
our O
method O
show O
failure O
in O
recognizing O
manually O
created O
incoherent O
stories O
( O
N O
block O
) O
. O
However O
, O
our O
model O
( O
Ours O
( O
N O
) O
) O
works O
after O
we O
incorporate O
these O
stories O
into O
our O
training O
data O
, O
leading O
to O
a O
future O
direction O
that O
unifies O
the O
coherence O
- O
based O
and O
preference O
- O
aware O
metrics O
. O
Surprisingly O
, O
our O
model O
gives O
relatively O
low O
scores O
when O
adopting O
stories O
from O
other O
domains O
( O
Other O
block O
) O
. O
We O
think O
this O
is O
because O
the O
writing O
style O
changes O
the O
criterion O
of O
human O
preference O
, O
which O
misleads O
our O
model O
to O
predict O
a O
not O
reasonable O
score O
, O
thus O
leading O
us O
to O
a O
big O
challenge O
in O
generalizing O
preference O
- O
aware O
story O
evaluation O
. O

More O
Analysis O

Due O
to O
the O
page O
limit O
, O
we O
put O
more O
analysis O
in O
the O
ablation O
studies O
. O
In O
Appendix O
Sec O
. O
D O
, O
we O
witness O
high O
correlation O
scores O
between O
preference B-MetricName
score I-MetricName
and O
each O
aspect B-MetricName
rating I-MetricName
, O
indicating O
the O
effectiveness O
of O
all O
pre O
- O
defined O
aspects O
in O
the O
evaluation O
. O
We O
also O
analyze O
the O
confidence O
and O
rating O
scores O
of O
the O
horror O
aspect O
with O
the O
preference O
score O
on O
scary O
stories O
in O
Appendix O
Sec O
. O
E. O
The O
result O
follows O
the O
human O
intuition O
that O
evaluation O
on O
scary O
stories O
shows O
a O
tendency O
to O
rely O
on O
the O
horror O
aspect O
. O

Conclusion O

In O
this O
paper O
, O
we O
investigate O
a O
novel O
task O
of O
preference B-TaskName
- I-TaskName
aware I-TaskName
story I-TaskName
evaluation I-TaskName
, O
StoryER B-TaskName
, O
which O
produce O
a O
score O
with O
explanation O
through O
various O
aspects O
and O
comments O
, O
bringing O
gains O
on O
both O
machine O
- O
generated O
and O
human O
- O
written O
stories O
evaluation O
. O
To O
support O
the O
task O
, O
we O
present O
a O
new O
dataset O
consisting O
of O
paired O
ranked O
stories O
and O
more O
explicit O
annotation O
( O
i.e. O
, O
rating O
and O
reasons O
) O
for O
predefined O
aspects O
. O
Our O
comprehensive O
ablation O
studies O
and O
intensive O
analysis O
show O
the O
effectiveness O
of O
using O
aspect O
rating O
and O
reasoning O
on O
preference O
score O
prediction O
. O
With O
the O
development O
of O
story O
generation O
, O
we O
believe O
that O
preference B-TaskName
- I-TaskName
aware I-TaskName
story I-TaskName
evaluation I-TaskName
will O
be O
the O
mainstream O
research O
when O
machine O
- O
generated O
stories O
do O
not O
suffer O
from O
serious O
coherence O
problems O
. O
Further O
studies O
on O
our O
dataset O
can O
also O
be O
conducted O
to O
reveal O
the O
point O
that O
influence O
the O
readers O
to O
upvote O
the O
stories O
. O

Limitations O

Our O
work O
( O
currently O
) O
has O
the O
following O
limitations O
: O

( O
1 O
) O
As O
indicated O
in O
Section O
7.2 O
, O
our O
proposed O
metrics O
are O
negatively O
affected O
by O
the O
significant O
domain O
shift O
, O
since O
we O
only O
take O
stories O
from O
one O
platform O
to O
train O
our O
model O
. O
Idealistically O
, O
a O
more O
general O
model O
can O
be O
trained O
with O
all O
types O
of O
stories O
, O
but O
it O
needs O
massive O
annotations O
on O
human O
preference O
( O
i.e. O
, O
upvote O
counts O
) O
. O

( O
2 O
) O
Since O
the O
upvote O
counts O
in O
the O
original O
dataset O
will O
be O
influenced O
by O
the O
prompt O
's O
topic O
, O
typically O
, O
fantastic O
stories O
get O
more O
upvotes O
than O
others O
. O
Our O
model O
is O
only O
trained O
by O
story O
pairs O
within O
the O
same O
topic O
, O
thus O
if O
a O
user O
inputs O
two O
unrelated O
stories O
, O
our O
system O
will O
provide O
unpredictable O
results O
. O
Therefore O
, O
we O
propose O
using O
pairwise O
evaluation O
with O
the O
same O
given O
prompt O
to O
avoid O
comparing O
stories O
with O
diverse O
topics O
. O

( O
3 O
) O
In O
this O
work O
, O
we O
propose O
to O
implicitly O
joint O
training O
to O
increase O
the O
performance O
of O
each O
task O
without O
explicitly O
addressing O
the O
connection O
of O
three O
subtasks O
. O
Although O
we O
have O
aspect O
rating O
and O
comment O
generation O
, O
preference B-MetricName
score I-MetricName
is O
still O
the O
most O
effective O
approach O
to O
assess O
the O
quality O
of O
the O
story O
. O
How O
to O
use O
these O
comments O
and O
aspect O
ratings O
is O
a O
challenge O
that O
will O
be O
addressed O
in O
the O
future O
work O
. O

Ethics O
and O
Broader O
Impacts O

We O
hereby O
acknowledge O
that O
all O
of O
the O
co O
- O
authors O
of O
this O
work O
are O
aware O
of O
the O
provided O
ACM O
Code O
of O
Ethics O
and O
honor O
the O
code O
of O
conduct O
. O
This O
work O
is O
mainly O
about O
propose O
a O
novel O
method O
in O
automatic O
story O
evaluation O
. O
The O
followings O
give O
the O
aspects O
of O
both O
our O
ethical O
considerations O
and O
our O
potential O
impacts O
to O
the O
community O
. O

Dataset O
. O
We O
collect O
the O
human O
annotation O
of O
the O
aspect O
rating O
and O
comments O
via O
Amazon O
Mechanical O
Turk O
( O
MTurk O
) O
and O
ensure O
that O
all O
the O
personal O
information O
of O
the O
workers O
involved O
( O
e.g. O
, O
usernames O
, O
emails O
, O
urls O
, O
demographic O
information O
, O
etc O
. O
) O
is O
discarded O
in O
our O
dataset O
. O
All O
the O
stories O
in O
our O
dataset O
are O
collected O
from O
a O
public O
dataset O
, O
namely O
WritingPrompt O
. O
Although O
we O
aim O
at O
providing O
a O
dataset O
that O
agreed O
upon O
from O
various O
people O
, O
there O
might O
still O
be O
unintended O
biases O
within O
the O
judgements O
, O
we O
make O
efforts O
on O
reducing O
these O
biases O
by O
collecting O
diverse O
comments O
and O
replacing O
the O
annotators O
who O
tends O
to O
be O
racist O
. O
The O
detailed O
annotation O
process O
( O
pay O
per O
amount O
of O
work O
, O
guidelines O
) O
is O
included O
in O
the O
appendix O
and O
our O
public O
website O
; O
We O
primarily O
consider O
English O
speaking O
regions O
for O
our O
annotations O
as O
the O
task O
requires O
certain O
level O
of O
English O
proficiency O
. O

Techniques O
. O
We O
benchmark O
the O
story O
evaluation O
task O
with O
conventional O
metrics O
and O
our O
proposed O
metric O
. O
As O
the O
story O
evaluation O
are O
of O
our O
main O
focus O
, O
we O
do O
not O
anticipate O
production O
of O
harmful O
outputs O
on O
our O
proposed O
task O
. O

Acknowledgments O

A O
Website O
Demo O

We O
display O
the O
collected O
data O
, O
AMT O
template O
and O
models O
on O
our O
website O
9 O
. O
The O
users O
can O
input O
their O
own O
stories O
or O
randomly O
select O
one O
story O
. O
The O
server O
then O
runs O
our O
model O
and O
output O
a O
preference B-MetricName
score I-MetricName
, O
and O
comments O
for O
each O
aspect O
. O
Figure O
4 O
shows O
an O
example O
. O

B O
Code O

We O
also O
put O
our O
source O
codes O
into O
the O
supplementary O
materials O
. O
Due O
to O
the O
upload O
size O
limitation O
. O
We O
truncate O
our O
100k B-HyperparameterValue
Story O
Ranking O
data O
into O
a O
size O
of O
1000 B-HyperparameterValue
, O
as O
well O
as O
the O
46k B-HyperparameterValue
Aspect O
Rating O
and O
Reasoning O
data O
. O
Please O
kindly O
follow O
the O
README O
to O
run O
the O
experiment O
. O
Our O
human O
annotation O
results O
can O
be O
also O
found O
under O
the O
folder O
" O
data O
" O
. O
Additionally O
, O
we O
put O
some O
examples O
for O
machine O
- O
generated O
stories O
introduced O
in O
our O
paper O
. O

C O
Correlation O
Between O
Story O
Quality O
and O
Aspect O
Rating O

We O
calculate O
the O
correlation O
between O
human O
ratings O
on O
each O
aspect O
with O
the O
upvote B-HyperparameterName
number I-HyperparameterName
, O
and O
the O
predicted O
aspect O
rating O
with O
the O
predicted O
preference B-MetricName
score I-MetricName
, O
to O
figure O
out O
the O
correlation O
between O
the O
aspect O
rating O
and O
the O
preference O
score O
. O
The O
results O
are O
listed O
in O
Figure O
5 O
. O
We O
can O
see O
the O
results O
from O
our O
model O
greatly O
match O
the O
distribution O
of O
the O
correlation O
between O
human O
aspect O
rating O
and O
human O
upvote B-HyperparameterName
number I-HyperparameterName
. O
None O
of O
these O
shows O
domination O
, O
which O
proves O
that O
all O
pre O
- O
defined O
aspects O
affect O
the O
final O
preference B-MetricName
score I-MetricName
prediction O
. O

D O
Horror O
/ O
Scary O
Aspect O
with O
SCARY B-DatasetName
200 I-DatasetName

To O
show O
how O
aspect O
ratings O
and O
confidence O
are O
related O
to O
the O
story O
, O
we O
further O
analyze O
their O
performance O
on O
WP B-DatasetName
200 I-DatasetName
and O
SCARY B-DatasetName
200 I-DatasetName
. O
We O
calculate O
the O
recall O
performance O
and O
rating O
correlation O
on O
" O
horror O
/ O
scary O
" O
aspect O
only O
to O
detect O
how O
this O
aspect O
works O
in O
both O
data O
. O
Table O
8 O
depicts O
that O
horror O
aspect O
can O
achieve O
36 O
% O
probability O
to O
be O
the O
top O
confident O
aspect O
in O
SCARY B-DatasetName
200 I-DatasetName
, O
while O
the O
number O
is O
only O
0.5 O
% O
in O
the O
original O
WP B-DatasetName
200 I-DatasetName
. O
On O
the O
other O
hand O
, O
the O
preference B-MetricName
score I-MetricName
also O
has O
a O
higher O
correlation O
with O
the O
rating O
from O
" O
horror O
/ O
scary O
" O
aspect O
. O
These O
results O
prove O
that O
the O
predicted O
aspects O
show O
high O
connection O
to O
the O
preference B-MetricName
score I-MetricName
prediction O
. O

E.2 O
Model O
for O
Aspect B-TaskName
Category I-TaskName
Classification I-TaskName

Our O
model O
for O
aspect O
category O
classification O
is O
based O
on O
RoBERTa B-MethodName
large I-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
Same O
as O
the O
model O
we O
used O
in O
preference O
score O
prediction O
, O
we O
apply O
a O
linear O
projection O
on O
the O
feature O
of O
[ O
CLS O
] O
, O
the O
first O
token O
of O
the O
input O
comments O
. O
We O
then O
train O
the O
model O
with O
cross O
- O
entropy O
loss O
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
4e-5 B-HyperparameterValue
. O

E.3 O
Model O
for O
Comment B-TaskName
Sentiment I-TaskName
Analysis I-TaskName

Our O
model O
for O
comment B-TaskName
sentiment I-TaskName
analysis I-TaskName
uses O
the O
same O
model O
structure O
for O
aspect O
category O
classification O
. O
We O
also O
use O
the O
same O
epochs B-HyperparameterName
number O
and O
learning B-HyperparameterName
rate I-HyperparameterName
during O
the O
training O
. O
The O
only O
difference O
is O
that O
the O
targets O
in O
training O
are O
the O
sentiment O
rate O
with O
a O
scale O
of O
1 O
- O
5 O
( O
from O
definitely O
negative O
to O
definitely O
positive O
) O

F O
More O
Results O

F.1 O
Ranking O
vs O
Discrimination O

Given O
two O
types O
of O
stories O
, O
highly O
and O
lowly O
upvoted O
, O
a O
straightforward O
method O
to O
build O
the O
model O
is O
through O
discrimination O
; O
use O
0 O
and O
1 O
as O
target O
with O
cross O
- O
entropy O
loss O
. O
We O
compare O
the O
results O
by O
using O
ranking O
and O
discrimination O
. O
The O
result O
is O
shown O
Table O
9 O
. O
From O
the O
result O
, O
we O
see O
that O
ranking O
strategy O
achieves O
better O
scores O
than O
discrimination O
and O
that O
with O
label O
smoothing O
. O
We O
believe O
it O
is O
because O
when O
we O
conduct O
ranking O
, O
we O
only O
enlarge O
the O
preference O
scores O
between O
stories O
written O
from O
the O
same O
prompt O
. O
The O
encoder O
can O
learn O
better O
how O
human O
preference O
works O
by O
comparing O
stories O
with O
the O
same O
topic O
. O
On O
the O
other O
hand O
, O
the O
ranking O
loss O
is O
more O
flexible O
compared O
with O
binary O
classification O
, O
which O
can O
be O
easily O
extended O
to O
rank O
more O
than O
two O
types O
of O
stories O
as O
shown O
in O
Equation O
2 O
. O

F.2 O
PPL B-MetricName
in O
automatic B-TaskName
story I-TaskName
evaluation I-TaskName

For O
an O
interesting O
finding O
, O
Perplexity B-MetricName
( O
PPL B-MetricName
) O
shows O
positively O
correlated O
to O
the O
score O
of O
WP B-DatasetName
and O
more O
highly O
correlated O
to O
the O
score O
of O
SCARY B-DatasetName
200 I-DatasetName
, O
while O
showing O
substantially O
negatively O
correlated O
to O
the O
score O
of O
coherence O
on O
machine O
- O
generated O
stories O
, O
which O
reveals O
a O
potential O
for O
story O
evaluation O
using O
pre O
- O
trained O
language O
models O
. O

F.3 O
Results O
of O
Comment O
Evaluation O

Due O
to O
the O
page O
limitation O
, O
we O
put O
the O
results O
of O
comment O
evaluation O
with O
more O
metrics O
in O
Table O
10 O
. O

We O
see O
that O
our O
model O
achieves O
higher O
performance O
on O
most O
of O
the O
metrics O
. O

F.4 O
Results O
of O
Aspect B-TaskName
Category I-TaskName
Classification I-TaskName

We O
use O
aspect B-TaskName
category I-TaskName
classification I-TaskName
model O
, O
introduced O
in O
Sec O
. O
E.2 O
, O
for O
filtering O
out O
noisy O
comments O
. O

Figure O
6 O
shows O
the O
classification O
results O
. O
Except O
for O
" O
ending O
" O
and O
" O
heartwarming O
" O
, O
all O
aspect O
classes O
can O
achieve O
an O
average O
of O
around O
80 B-MetricValue
% I-MetricValue
accuracy B-MetricName
, O
showing O
high O
performance O
on O
classification O
. O
We O
filter O
out O
the O
comments O
, O
with O
no O
aspect B-MetricName
category I-MetricName
score I-MetricName
exceeding O
0.9 B-MetricValue
after O
softmax O
function O
. O

F.5 O
Results O
of O
Comment O
Sentiment O
Analysis O

Comment B-TaskName
Sentiment I-TaskName
Analysis I-TaskName
model O
, O
introduced O
in O
Sec O
. O
E.3 O
, O
is O
used O
to O
rate O
comments O
by O
their O
sentiments O
. O
Table O
11 O
shows O
the O
results O
. O
Our O
output O
is O
the O
rates O
from O
1 O
to O
5 O
. O
In O
the O
evaluation O
, O
we O
simply O
group O
1 O
and O
2 O
as O
the O
negative O
, O
3 O
as O
the O
neutral O
, O
4 O
and O
5 O
as O
the O
positive O
. O
The O
results O
show O
that O
our O
sentiment B-TaskName
analysis I-TaskName
model O
can O
correctly O
predict O
the O
sentiment O
, O
especially O
on O
positive O
and O
negative O
. O

F.6 O
Comment O
Data O
Augmentation O

We O
collect O
over O
150k B-HyperparameterName
uncategorized O
comments O
from O
metadata O
in O
WP B-DatasetName
. O
We O
use O
the O
aspect B-TaskName
category I-TaskName
classification I-TaskName
model O
and O
filter O
out O
the O
irrelevent O
comments O
. O
However O
, O
we O
found O
bias O
inside O
the O
comments O
. O
For O
example O
, O
we O
get O
almost O
9000 O
comments O
about O
" O
ending O
" O
, O
while O
only O
1200 O
for O
" O
sad O
" O
. O

To O
mitigate O
the O
bias O
that O
would O
be O
inducted O
into O
our O
story O
evaluation O
model O
, O
we O
sample O
about O
2000 B-HyperparameterValue
comments O
for O
each O
aspect O
, O
and O
use O
all O
comments O
for O
the O
aspect O
which O
contains O
less O
than O
2000 B-HyperparameterValue
comments O
. O
The O
final O
data O
statistics O
of O
comments O
can O
be O
referred O
to O
our O
website O
. O

G O
Human O
Annotation O

G.1 O
Human O
Annotation O
on O
Test O
Data O

For O
evaluation O
, O
we O
collect O
human O
judgments O
through O
AMT O
for O
200 B-HyperparameterValue
highly O
- O
upvoted O
stories O
and O
200 B-HyperparameterValue
lowly O
- O
upvoted O
stories O
from O
WP B-DatasetName
( O
sampled O
from O
test O
data O
in O
100k B-HyperparameterValue
Story O
Ranking O
data O
) O
, O
where O
each O
story O
is O
assigned O
to O
8 O
annotators O
. O
Annotators O
are O
asked O
to O
rate O
each O
story O
on O
a O
scale O
of O
1 O
to O
5 O
( O
from O
poor O
to O
good O
) O
. O
Following O
Clark O
et O
al O
. O
( O
2021 O
) O
, O
we O
asked O
the O
annotators O
to O
compare O
the O
stories O
before O
rating O
and O
write O
down O
a O
very O
brief O
reason O
for O
clarification O
. O
To O
further O
ensure O
the O
correctness O
of O
the O
annotation O
, O
we O
calculate O
the O
statistics O
of O
the O
annotator O
behavior O
( O
i.e. O
, O
working O
time O
per O
hit O
) O
and O
set O
traps O
in O
the O
batch O
( O
i.e. O
, O
insert O
extremely O
poor O
story O
, O
duplicate O
stories O
for O
one O
annotator O
to O
test O
their O
consistency O
) O
. O
The O
submissions O
from O
annotators O
with O
poor O
quality O
are O
all O
rejected O
and O
then O
recollected O
from O
new O
annotators O
. O
Finally O
, O
we O
exclusively O
keep O
the O
100 B-HyperparameterValue
highly O
- O
upvoted O
and O
100 B-HyperparameterValue
lowly O
- O
upvoted O
stories O
with O
the O
lowest O
variance O
from O
8 O
annotators O
and O
average O
the O
human O
rates O
as O
the O
target O
scores O
in O
this O
test O
data O
, O
namely O
, O
WP B-DatasetName
200 I-DatasetName
in O
the O
following O
experiments O
. O
Annotators O
get O
$ O
0.2 O
as O
the O
reward O
for O
each O
submission O
. O
Besides O
, O
we O
crawled O
scary O
stories O
from O
Reddit O
( O
r O
/ O
shortscarystories O
11 O
) O
, O
which O
have O
a O
similar O
writing O
style O
to O
the O
stories O
in O
WP B-DatasetName
but O
in O
a O
constrained O
story O
type O
. O
We O
repeat O
the O
procedure O
for O
WP B-DatasetName
200 I-DatasetName
and O
create O
another O
humanannotated O
test O
data O
, O
namely O
SCARY B-DatasetName
200 I-DatasetName
. O
The O
same O
procedure O
is O
also O
used O
for O
collecting O
human O
annotation O
on O
machine O
- O
generated O
stories O
. O
We O
generate O
200 B-HyperparameterValue
stories O
using O
LED B-MethodName
trained O
with O
highly O
- O
voted O
stories O
and O
another O
200 B-HyperparameterValue
stories O
using O
LED B-MethodName
trained O
with O
lowly O
- O
voted O
stories O
for O
annotation O
. O
We O
ask O
the O
annotators O
to O
rate O
the O
stories O
based O
on O
human O
preference O
and O
also O
ask O
them O
to O
distinguish O
whether O
the O
given O
stories O
are O
human O
- O
written O
or O
machinegenerated O
. O
We O
exclusively O
keep O
the O
stories O
that O
10 O
: O
Comment O
generation O
evaluation O
on O
automatic O
scores O
and O
human O
evaluation O
. O
In O
human O
evaluation O
, O
the O
kappa B-MetricName
coefficient I-MetricName
κ B-MetricName
for O
each O
score O
are O
located O
in O
0.4 B-MetricValue
- O
0.6 B-MetricValue
, O
indicating O
a O
moderate O
agreement O
between O
annotators O
. O

positive O
neutral O
negative O
average O
Acc B-MetricName
89.70 B-MetricValue
% I-MetricValue
50.93 B-MetricValue
% I-MetricValue
85.20 B-MetricValue
% I-MetricValue
83.03 B-MetricValue
% I-MetricValue
deceive O
the O
annotators O
, O
as O
these O
stories O
do O
not O
contain O
serious O
coherence O
problems O
. O

G.2 O
Data O
Collection O

In O
this O
paper O
, O
we O
mainly O
collect O
data O
for O
two O
different O
uses O
. O
Annotators O
get O
$ O
1 O
as O
the O
reward O
for O
each O
submission O
. O
The O
total O
data O
collection O
takes O
2 O
months O
. O
To O
assess O
the O
quality O
of O
each O
annotator O
, O
we O
randomly O
sample O
the O
submissions O
from O
each O
annotator O
every O
two O
days O
, O
bonus O
the O
one O
with O
good O
quality O
and O
warn O
the O
annotators O
who O
give O
nonsense O
comments O
. O

G.3 O
Human O
Annotation O
Inner O
- O
Agreement O

As O
we O
assign O
one O
story O
for O
more O
than O
one O
annotator O
, O
we O
calculate O
the O
inner O
- O
agreement O
from O
different O
annotators O
on O
aspect O
selection O
. O
As O
a O
result O
, O
65.80 O
% O
aspects O
are O
selected O
by O
more O
than O
one O
annotator O
, O
and O
the O
correlation O
coefficient O
of O
multi O
- O
annotation O
on O
aspect O
ratings O
are O
0.913 O
and O
0.811 O
, O
corresponding O
to O
the O
Spearman O
( O
Zar O
, O
1972 O
) O
and O
Kendall O
( O
Schaeffer O
and O
Levitt O
, O
1956 O
) O
respectively O
. O

H O
Aspect O
Category O
Name O
Definition O

As O
no O
standard O
criterion O
exists O
for O
story O
evaluation O
, O
we O
collect O
some O
well O
- O
used O
aspects O
that O
used O
in O
the O
Internet O
. O
We O
mainly O
refer O
to O
the O
websites O
12 O
13 O
14 O
. O

Faking O
Fake O
News O
for O
Real O
Fake B-TaskName
News I-TaskName
Detection I-TaskName
: O
Propaganda O
- O
Loaded O
Training O
Data O
Generation O

Despite O
recent O
advances O
in O
detecting O
fake O
news O
generated O
by O
neural O
models O
, O
their O
results O
are O
not O
readily O
applicable O
to O
effective O
detection O
of O
human O
- O
written O
disinformation O
. O
What O
limits O
the O
successful O
transfer O
between O
them O
is O
the O
sizable O
gap O
between O
machine O
- O
generated O
fake O
news O
and O
human O
- O
authored O
ones O
, O
including O
the O
notable O
differences O
in O
terms O
of O
style O
and O
underlying O
intent O
. O
With O
this O
in O
mind O
, O
we O
propose O
a O
novel O
framework O
for O
generating O
training O
examples O
that O
are O
informed O
by O
the O
known O
styles O
and O
strategies O
of O
human O
- O
authored O
propaganda O
. O
Specifically O
, O
we O
perform O
self O
- O
critical O
sequence O
training O
guided O
by O
natural O
language O
inference O
to O
ensure O
the O
validity O
of O
the O
generated O
articles O
, O
while O
also O
incorporating O
propaganda O
techniques O
, O
such O
as O
appeal O
to O
authority O
and O
loaded O
language O
. O
In O
particular O
, O
we O
create O
a O
new O
training O
dataset O
, O
PROPANEWS B-DatasetName
, O
with O
2,256 O
examples O
, O
which O
we O
release O
for O
future O
use O
. O
Our O
experimental O
results O
show O
that O
fake O
news O
detectors O
trained O
on O
PROPANEWS B-DatasetName
are O
better O
at O
detecting O
human O
- O
written O
disinformation O
by O
3.62 B-MetricValue
- I-MetricValue
7.69 I-MetricValue
% I-MetricValue
F1 B-MetricName
score O
on O
two O
public O
datasets O
. O

Introduction O

The O
dissemination O
of O
false O
information O
online O
can O
cause O
chaos O
, O
hatred O
, O
and O
trust O
issues O
, O
and O
can O
eventually O
hinder O
the O
development O
of O
society O
as O
a O
whole O
( O
Dewatana O
and O
Adillah O
, O
2021 O
; O
Wasserman O
and O
Madrid O
- O
Morales O
, O
2019 O
) O
. O
In O
particular O
, O
humanwritten O
disinformation O
2 O
is O
often O
used O
to O
manipulate O
certain O
populations O
and O
reportedly O
already O
had O
a O
catastrophic O
impact O
on O
multiple O
events O
, O
such O
as O
Brexit O
( O
Bastos O
and O
Mercea O
, O
2019 O
) O
, O
the O
COVID-19 O
pandemic O
( O
van O
Der O
Linden O
et O
al O
. O
, O
2020 O
) O
, O
and O
the O
2022 O
Russian O
assault O
on O
Ukraine O
. O

Hence O
, O
there O
is O
an O
urgent O
need O
for O
a O
defense O
mechanism O
against O
human O
- O
written O
disinformation O
. O

3 O
For O
this O
, O
we O
need O
a O
substantial O
amount O
of O
training O
data O
to O
build O
detectors O
. O
A O
naïve O
solution O
is O
to O
collect O
human O
- O
written O
news O
articles O
that O
contain O
inaccurate O
information O
by O
crawling O
untrustworthy O
news O
media O
. O
However O
, O
news O
articles O
published O
by O
suspicious O
sources O
do O
not O
necessarily O
contain O
false O
information O
, O
which O
means O
that O
annotators O
are O
required O
to O
fact O
- O
check O
every O
claim O
in O
each O
untrustworthy O
article O
. O
Moreover O
, O
articles O
containing O
false O
claims O
are O
often O
removed O
shortly O
after O
posting O
. O
While O
some O
work O
collected O
human O
- O
written O
fake O
news O
from O
factchecking O
websites O
( O
Shu O
et O
al O
. O
, O
2018 O
; O
Nguyen O
et O
al O
. O
, O
2020 O
) O
, O
the O
size O
of O
these O
datasets O
is O
limited O
. O
The O
curation O
process O
of O
these O
websites O
also O
requires O
a O
lot O
of O
manual O
efforts O
. O
Hence O
, O
such O
a O
solution O
is O
neither O
scalable O
nor O
reliable O
. O
Thus O
, O
an O
alternative O
direction O
complementing O
the O
existing O
efforts O
would O
be O
to O
generate O
training O
data O
automatically O
in O
a O
way O
that O
avoids O
these O
issues O
. O

Our O
goal O
is O
to O
enhance O
disinformation B-TaskName
detection I-TaskName
by O
generating O
training O
examples O
that O
are O
better O
informed O
by O
the O
known O
styles O
and O
strategies O
of O
human O
- O
authored O
disinformation O
. O
We O
started O
by O
collecting O
human O
- O
written O
articles O
from O
untrustworthy O
sites O
4 O
, O
and O
we O
analyzed O
around O
40 O
of O
them O
that O
spread O
false O
claims O
. O
Throughout O
our O
analysis O
, O
we O
found O
two O
characteristics O
of O
this O
human O
- O
written O
disinformation O
. O
First O
, O
about O
33 O
% O
of O
the O
articles O
used O
propaganda O
techniques O
to O
convince O
the O
audience O
that O
the O
fake O
information O
was O
actually O
authentic O
, O
and O
these O
techniques O
often O
involve O
the O
use O
of O
emotion O
- O
triggering O
language O
or O
logical O
fallacies O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
) O
to O
increase O
the O
impact O
on O
the O
reader O
. O
Statistics O
about O
the O
propaganda O
techniques O
are O
given O
in O
Appendix O
A O
. O

AJDABIYAH O
, O
Libya O
| O
Thu O
Apr O
7 O
, O
2011 O
6:34 O
pm O
EDT O
AJDABIYAH O
, O
Libya O
-LRB O
- O
Reuters O
-RRB O
-- O
Rebels O
fighting O
to O
overthrow O
Muammar O
Gaddafi O
said O
five O
of O
their O
fighters O
were O
killed O
... O
" O
In O
rebel O
- O
held O
eastern O
Libya O
, O
wounded O
rebels O
being O
brought O
to O
a O
hospital O
Ajdabiyah O
said O
their O
trucks O
and O
tanks O
were O
hit O
on O
Thursday O
by O
a O
NATO O
air O
strike O
outside O
Brega O
. O
NATO O
said O
it O
was O
investigating O
an O
attack O
by O
its O
aircraft O
on O
a O
tank O
column O
in O
the O
area O
along O
the O
Mediterranean O
coast O
on O
Thursday O
, O
saying O
the O
situation O
was O
" O
unclear O
and O
fluid O
. O
" O
Rebels O
said O
at O
least O
five O
of O
their O
fighters O
were O
killed O
when O
NATO O
planes O
mistakenly O
bombed O
a O
rebel O
tank O
column O
near O
the O
contested O
port O
. O
" O
A O
number O
of O
vehicles O
were O
hit O
by O
a O
NATO O
strike O
" O
, O
officers O
from O
UN O
concluded O
. O
The O
fighting O
for O
Brega O
, O
the O
only O
active O
front O
, O
has O
dragged O
on O
for O
a O
week O
... O
Table O
1 O
: O
An O
example O
of O
our O
generated O
fake O
news O
. O
Given O
an O
authentic O
news O
article O
, O
our O
approach O
first O
identifies O
a O
salient O
sentence O
, O
which O
it O
then O
replaces O
with O
a O
plausible O
but O
disinformative O
sentence O
that O
is O
coherent O
to O
the O
context O
. O
Finally O
, O
it O
generates O
a O
propaganda O
sentence O
to O
make O
the O
article O
resemble O
human O
- O
written O
fake O
news O
. O

Second O
, O
more O
than O
55 O
% O
of O
the O
articles O
that O
we O
analyzed O
contained O
inaccurate O
information O
mixed O
with O
correct O
information O
: O
in O
fact O
, O
all O
claims O
, O
except O
for O
one O
or O
two O
, O
in O
these O
disinformation O
articles O
were O
factual O
, O
which O
makes O
the O
few O
false O
claims O
in O
these O
articles O
even O
more O
believable O
. O

Prior O
work O
has O
made O
significant O
progress O
in O
generating O
fake O
news O
using O
large O
pre O
- O
trained O
sequenceto O
- O
sequence O
( O
seq2seq O
) O
models O
( O
Zellers O
et O
al O
. O
, O
2019 O
; O
Fung O
et O
al O
. O
, O
2021 O
; O
. O
However O
, O
the O
articles O
generated O
by O
these O
approaches O
contain O
an O
overwhelmingly O
large O
proportion O
of O
false O
information O
and O
do O
not O
explicitly O
use O
propaganda O
. O

To O
address O
these O
issues O
, O
here O
we O
propose O
a O
novel O
generation O
method O
. O
Given O
an O
authentic O
news O
article O
, O
we O
replace O
a O
salient O
sentence O
with O
a O
plausible O
but O
fake O
piece O
of O
information O
using O
a O
seq2seq O
model O
. O
As O
the O
generated O
texts O
can O
often O
be O
entailed O
by O
the O
original O
contexts O
, O
we O
incorporate O
a O
self O
- O
critical O
sequence O
training O
objective O
( O
Rennie O
et O
al O
. O
, O
2017 O
) O
that O
incorporates O
a O
natural O
language O
inference O
( O
NLI O
) O
model O
into O
the O
loss O
function O
. O
Additionally O
, O
we O
use O
the O
NLI O
model O
to O
filter O
out O
generated O
sentences O
that O
can O
be O
inferred O
from O
the O
replaced O
ones O
. O
Then O
, O
we O
add O
propaganda O
techniques O
to O
mimic O
how O
humans O
craft O
disinformation O
. O
In O
particular O
, O
we O
automate O
two O
commonly O
used O
propaganda O
techniques O
, O
appeal O
to O
authority O
and O
loaded O
language O
, O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
) O
to O
add O
propaganda O
into O
the O
faked O
sentences O
. O

Subsequently O
, O
we O
use O
the O
silver O
- O
standard O
training O
data O
generated O
from O
these O
two O
steps O
to O
train O
a O
detector O
. O
An O
example O
is O
shown O
in O
Table O
1 O
. O
We O
further O
recruited O
crowdsourcing O
workers O
to O
validate O
that O
some O
generated O
texts O
were O
indeed O
fake O
, O
so O
that O
we O
could O
construct O
a O
gold O
- O
standard O
training O
dataset O
. O

Comparing O
our O
method O
to O
state O
- O
of O
- O
the O
- O
art O
fake B-TaskName
news I-TaskName
generation I-TaskName
approaches O
, O
the O
evaluation O
results O
on O
two O
human O
- O
written O
fake O
news O
datasets O
show O
that O
detectors O
are O
substantially O
better O
at O
spotting O
human O
- O
written O
disinformation O
when O
trained O
on O
our O
generated O
fake O
news O
dataset O
. O

Our O
ablation O
studies O
confirm O
the O
effectiveness O
of O
incorporating O
propaganda O
into O
the O
generated O
articles O
for O
producing O
better O
training O
data O
. O

Our O
contributions O
can O
be O
summarized O
as O
follows O
: O

• O
We O
propose O
an O
effective O
method O
to O
automatically O
generate O
more O
realistic O
disinformation O
compared O
to O
previous O
work O
. O
• O
We O
develop O
the O
first O
automatic O
methods O
to O
generate O
specific O
propaganda O
techniques O
such O
that O
the O
generated O
articles O
are O
closer O
to O
disinformation O
written O
by O
humans O
. O
• O
We O
demonstrate O
that O
detectors O
trained O
on O
our O
generated O
data O
, O
compared O
to O
generated O
articles O
using O
other O
methods O
, O
are O
better O
at O
detecting O
human O
- O
written O
disinformation O
. O
• O
We O
release O
PROPANEWS B-DatasetName
, O
a O
dataset O
for O
disinformation O
detection O
containing O
2.2 O
K O
articles O
generated O
by O
our O
approach O
and O
validated O
by O
humans O
. O

Training O
Data O
Generation O

Our O
process O
of O
generating O
training O
data O
for O
propaganda O
- O
loaded O
disinformation O
consists O
of O
two O
main O
steps O
: O
disinformation B-TaskName
generation I-TaskName
( O
§ O
2.1 O
) O
and O
propaganda B-TaskName
generation I-TaskName
( O
§ O
2.2 O
) O
. O
Below O
, O
we O
describe O
each O
of O
these O
steps O
in O
detail O
. O

Disinformation B-TaskName
Generation I-TaskName

Our O
disinformation B-TaskName
generation I-TaskName
approach O
aims O
at O
two O
sub O
- O
goals O
: O
( O
i O
) O
replacing O
a O
salient O
sentence O
in O
the O
given O
article O
with O
a O
sequence O
of O
generated O
coherent O
texts O
that O
looks O
plausible O
, O
and O
( O
ii O
) O
ensuring O
that O
the O
generated O
information O
can O
not O
be O
entailed O
by O
the O
original O
masked O
- O
out O
sentence O
; O
otherwise O
, O
the O
generated O
texts O
will O
not O
be O
disinformative O
. O
To O
achieve O
the O
first O
sub O
- O
goal O
, O
we O
first O
identify O
salient O
sentences O
using O
extractive O
summarization O
, O
and O
we O
then O
perform O
mask O
- O
infilling O
with O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
. O
We O
achieve O
the O
second O
sub O
- O
goal O
using O
self O
- O
critical O
sequence O
training O
( O
Rennie O
et O
al O
. O
, O
2017 O
) O
with O
an O
NLI O
component O
, O
which O
we O
use O
as O
a O
reward O
function O
for O
generation O
. O

Figure O
1 O
: O
Illustration O
of O
our O
self O
- O
critical O
sequence O
training O
. O
Given O
a O
corrupted O
input O
articlex O
, O
BART O
generates O
two O
sequences O
with O
nucleus O
sampling O
and O
greedy O
decoding O
, O
respectively O
. O
The O
reward O
for O
each O
sequence O
is O
computed O
as O
the O
negative O
entailment O
probability O
−P O
ent O
as O
output O
from O
the O
NLI O
model O
. O
Salient B-TaskName
Sentence I-TaskName
Identification I-TaskName
A O
salient O
sentence O
is O
critical O
for O
the O
overall O
semantics O
of O
the O
article O
. O
When O
it O
is O
manipulated O
or O
replaced O
, O
the O
complex O
events O
described O
in O
the O
article O
may O
be O
drastically O
changed O
. O
Yet O
, O
there O
is O
no O
salient B-TaskName
sentence I-TaskName
identification I-TaskName
dataset O
publicly O
available O
. O
Motivated O
by O
the O
fact O
that O
sentences O
included O
in O
an O
extractive O
summary O
are O
often O
of O
higher O
importance O
, O
we O
take O
the O
scores O
computed O
by O
an O
extractive O
summarization O
model O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
, O
which O
predicts O
how O
likely O
each O
sentence O
is O
to O
belong O
to O
the O
summary O
, O
to O
estimate O
its O
saliency O
. O
We O
found O
that O
this O
yields O
reasonably O
good O
sentence O
saliency O
estimation O
. O
For O
each O
news O
outlet O
, O
we O
replaced O
one O
sentence O
that O
had O
the O
highest O
extractive O
summarization O
score O
with O
our O
generated O
disinformation O
. O

Mask O
Infilling O
with O
BART O
To O
perform O
infilling O
, O
we O
took O
an O
approach O
similar O
to O
that O
of O
Donahue O
et O
al O
. O
( O
2020 O
) O
, O
but O
we O
used O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
. O
At O
training O
time O
, O
we O
randomly O
masked O
out O
a O
sentence O
y O
* O
from O
a O
given O
article O
x. O
The O
bidirectional O
encoder O
first O
produces O
contextualized O
representations O
h O
e O
= O
Encoder O
( O
x O
) O
given O
the O
article O
with O
a O
masked O
- O
out O
sentencex O
= O
x O
− O
y O
* O
. O
Then O
, O
the O
autoregressive O
decoder O
learns O
a O
maximum O
likelihood O
estimation O
that O
aims O
to O
maximize O
the O
probability O
of O
generating O
the O
next O
token O
y O
* O
t O
at O
time O
step O
t O
given O
all O
tokens O
in O
previous O
time O
steps O
{ O
y O
* O
0 O
, O
... O
, O
y O
* O
t−1 O
} O
and O
the O
encoder O
hidden O
states O
h O
e O
by O
minimizing O
the O
negative O
log O
probability O
of O
generating O
y O
* O
t O
as O
follows O
: O

L O
m O
= O
− O
T O
t=1 O
log O
P O
( O
y O
* O
t O
|y O
* O
0 O
, O
... O
, O
y O
* O
t−1 O
, O
h O
e O
) O
. O
( O
1 O
) O

During O
inference O
time O
, O
rather O
than O
using O
random O
masking O
, O
x O
is O
formed O
by O
masking O
out O
the O
sentence O
with O
the O
highest O
score O
computed O
by O
the O
extractive O
summarization O
model O
given O
the O
original O
document O
x O
, O
as O
discussed O
in O
the O
previous O
paragraph O
. O

′ O
) O
= O
−P O
nli O
( O
y O
* O
, O
y O
′ O
) O
, O
( O
2 O
) O

where O
r O
( O
y O

) O
is O
the O
reward O
of O
the O
sequence O
sampled O
from O
the O
current O
policy O
y O
′ O
, O
and O
P O
nli O
( O
y O
* O
, O
y O
′ O
) O
is O
the O
probability O
that O
y O
* O
entails O
y O
′ O
. O
To O
generate O
y O
′ O
, O
we O
use O
Nucleus O
Sampling O
( O
Holtzman O
et O
al O
. O
, O
2020 O
) O
with O
p O
= O
0.96 O
, O
as O
this O
sampling O
method O
has O
shown O
advantages O
in O
open O
- O
ended O
generation O
( O
Holtzman O
et O
al O
. O
, O
2020 O
; O
Zellers O
et O
al O
. O
, O
2019 O
) O
. O
We O
generate O
the O
baseline O
output O
y O
′′ O
using O
greedy O
decoding O
, O
then O
obtain O
the O
entailment O
probabilities O
between O
y O
′ O
and O
y O
′′ O
from O
the O
NLI O
model O
. O
We O
then O
compute O
the O
self O
- O
critical O
sequence O
training O
loss O
: O

L O
s O
= O
− O
( O
r O
( O
y O
′ O
) O
− O
r O
( O
y O
′′ O
) O
) O
T O
t=1 O
log O
P O
( O
y O
′ O
t O
|y O
′ O
0 O
, O
.. O
, O
y O
′ O
t−1 O
, O
h O
e O
) O
. O
( O
3 O

Here O
r O
( O
y O

) O
is O
a O
baseline O
reward O
, O
and O
r O
( O
y O

′ O
) O
− O
r O
( O
y O
′′ O
) O

is O
a O
normalized O
reward O
. O
This O
loss O
function O
encourages O
BART O
to O
generate O
y O
′ O
when O
r O
( O
y O

′ O
) O
> O
r O
( O
y O
′′ O
) O
, O

whereas O
it O
suppresses O
the O
probability O
of O
decoding O
y O
′ O
when O
r O
( O
y O

′ O
) O
< O
r O
( O
y O
′′ O

) O
. O
An O
overview O
of O
SCST O
is O
shown O
in O
Figure O
1 O
. O
The O
final O
objective O
function O
to O
minimize O
is O
a O
weighted O
sum O
of O
Equation O
( O
1 O
) O
and O
Equation O
( O
3 O
) O
, O

L O
f O
inal O
= O
αL O
m O
+ O
βL O
s O
, O
( O
4 O
) O

where O
α O
and O
β O
are O
the O
weights O
for O
each O
loss O
. O

6 O

Post O
- O
processing O
To O
further O
ensure O
the O
quality O
of O
the O
disinformation O
generated O
, O
we O
reuse O
the O
NLI O
model O
discussed O
in O
the O
previous O
paragraph O
to O
filter O
out O
invalid O
outputs O
y O
′ O
that O
can O
be O
entailed O
from O
the O
masked O
- O
out O
sentence O
y O
* O
, O
as O
demonstrated O
in O
Fig- O

ure O
2 O
. O
We O
found O
that O
incorporating O
the O
SCST O
loss O
( O
Equation O
( O
3 O
) O
) O
into O
the O
training O
objective O
successfully O
reduces O
the O
invalid B-MetricName
rate I-MetricName
from O
7.8 B-MetricValue
% I-MetricValue
to O
3.2 B-MetricValue
% I-MetricValue
. O

Propaganda B-TaskName
Generation I-TaskName

After O
generating O
inaccurate O
information O
, O
we O
incorporate O
propaganda O
into O
each O
generated O
article O
. O
We O
chose O
two O
representative O
propaganda O
techniques O
of O
each O
type O
: O
emotional O
versus O
non O
- O
emotional O
. O

Loaded O
language O
is O
an O
emotional O
technique O
and O
it O
is O
also O
by O
far O
the O
most O
frequent O
propaganda O
technique O
as O
shown O
in O
Table O
5 O
of O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
) O
and O
Table O
2 O
of O
( O
Dimitrov O
et O
al O
. O
, O
2021 O
) O
. O

Based O
on O
these O
two O
tables O
, O
we O
also O
see O
that O
appeal O
to O
authority O
is O
among O
the O
most O
frequent O
nonemotional O
techniques O
. O

Appeal O
to O
Authority O
Appeal O
to O
authority O
is O
a O
propaganda O
technique O
that O
aims O
to O
strengthen O
or O
to O
invalidate O
an O
argument O
by O
referring O
to O
a O
statement O
made O
by O
authorities O
or O
experts O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
) O
. O
We O
first O
collect O
experts O
from O
various O
domains O
, O
such O
as O
economics O
and O
immunology O
, O
from O
Wikidata O
. O
7 O
In O
particular O
, O
we O
specify O
the O
occupation O
( O
P108 O
) O
of O
each O
expert O
and O
we O
filter O
out O
entities O
that O
were O
born O
before O
1940 O
to O
ensure O
recency O
. O
To O
consider O
only O
impactful O
entities O
, O
we O
rank O
all O
candidates O
based O
on O
the O
number O
of O
corresponding O
outcoming O
statements O
( O
i.e. O
, O
connected O
concepts O
in O
Wikidata O
) O
, O
inspired O
by O
PageRank O
( O
Page O
et O
al O
. O
, O
1999 O
) O
, O
and O
we O
add O
the O
top O
100 O
entities O
for O
each O
occupation O
into O
the O
candidate O
list O
Z. O
Then O
, O
we O
include O
the O
person O
named O
entities O
extracted O
by O
a O
name O
tagger O
, O
8 O
which O
are O
more O
relevant O
to O
the O
local O
context O
. O
This O
makes O
sense O
as O
we O
found O
that O
more O
than O
73 O
% O
of O
the O
news O
articles O
contain O
authorities O
. O
More O
details O
on O
how O
authority O
candidates O
Z O
are O
collected O
can O
be O
found O
in O
Appendix O
E. O
Once O
we O
collect O
a O
candidate O
list O
Z O
, O
we O
then O
generate O
fake O
arguments O
made O
by O
each O
z O
i O
∈ O
Z O
with O
the O
BART O
model O
that O
has O
already O
been O
fine O
- O
tuned O
in O
§ O
2.1 O
. O
In O
particular O
, O
a O
< O
mask O
> O
token O
is O
inserted O
right O
after O
the O
filled O
- O
in O
sentence O
y O
′ O
in O
the O
input O
article O
to O
BART O
so O
that O
it O
knows O
where O
to O
perform O
infilling O
. O
To O
inform O
BART O
that O
it O
should O
generate O
a O
statement O
made O
by O
an O
authority O
, O
we O
prefix O
the O
decoder O
with O
the O
template O
[ O
z O
i O
confirmed O
that O
" O
] O
, O
where O
z O
i O
∈ O
Z O
is O
the O
name O
of O
the O
authority O
. O
The O
prefix O
ends O
with O
an O
opening O
quotation O
mark O
to O
indicate O
that O
it O
should O
be O
followed O
by O
a O
statement O
by O
authority O
z O
i O
. O
To O
increase O
the O
diversity O
of O
the O
generated O
statements O
, O
we O
devise O
a O
variety O
of O
templates O
, O
as O
detailed O
in O
Appendix O
E. O
Finally O
, O
the O
best O
sequence O
s O
* O
is O
selected O
with O
the O
lowest O
perplexity O

s O
* O
= O
argmin O
s O
i O
Perplexity O
( O
s O
i O
) O
, O

where O
s O
i O
denotes O
the O
generated O
sequence O
using O
z O
i O
as O
the O
authority O
. O

Loaded O
Language O
Loaded O
language O
is O
another O
propaganda O
technique O
that O
uses O
emotion O
- O
triggering O
terms O
or O
phrases O
to O
influence O
the O
opinions O
of O
the O
audience O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
; O
Dimitrov O
et O
al O
. O
, O
2021 O
) O
. O
Often O
, O
loaded O
language O
uses O
sensational O
adverbs O
or O
adjectives O
to O
exaggerate O
a O
statement O
. O

Technique O
Generated O
Disinformation O
and O
Propaganda O

Appeal O
to O
Authority O
Cairo O
's O
Tahrir O
Square O
was O
the O
scene O
of O
clashes O
between O
protesters O
and O
police O
on O
Wednesday O
. O
" O
At O
least O
three O
people O
were O
killed O
and O
more O
than O
600 O
were O
injured O
in O
the O
clashes O
, O
" O
said O
Egypt O
's O
President O
. O

Loaded O
Language O

Cairo O
's O
Tahrir O
Square O
was O
the O
scene O
of O
deadly O
clashes O
between O
protesters O
and O
police O
on O
Wednesday O
. O

Table O
2 O
: O
Examples O
of O
the O
two O
generated O
propaganda O
techniques O
, O
as O
shown O
by O
texts O
in O
blue O
. O
The O
first O
row O
shows O
how O
the O
argument O
is O
strengthened O
by O
appealing O
to O
an O
authority O
's O
statement O
, O
while O
the O
second O
row O
demonstrates O
how O
loaded O
language O
is O
introduced O
with O
an O
emotion O
- O
triggering O
term O
. O

Based O
on O
this O
observation O
, O
we O
utilize O
the O
propaganda O
dataset O
released O
by O
Da O
San O
Martino O
et O
al O
. O
( O
2019 O
) O
where O
propaganda O
techniques O
are O
annotated O
at O
the O
fragment O
level O
( O
i.e. O
span O
level O
) O
. O
The O
dataset O
contains O
2,547 O
loaded O
language O
instances O
. O
Yet O
, O
not O
every O
instance O
contains O
adjectives O
or O
adverbs O
that O
are O
emotion O
- O
triggering O
. O
To O
create O
valid O
training O
data O
for O
loaded O
language O
generation O
, O
we O
first O
use O
SpaCy O
to O
perform O
part O
- O
of O
- O
speech O
tagging O
and O
dependency O
parsing O
, O
and O
then O
keep O
the O
examples O
where O
there O
exists O
an O
adverb O
pointing O
to O
a O
verb O
or O
an O
adjective O
pointing O
to O
a O
noun O
through O
dependency O
parsing O
edges O
. O
This O
results O
in O
1,017 O
samples O
of O
valid O
loaded O
language O
instances O
. O
Examples O
of O
the O
generated O
appeal O
to O
authority O
and O
loaded O
language O
are O
shown O
in O
Table O
2 O
. O

Upon O
collecting O
the O
training O
data O
to O
generate O
loaded O
language O
, O
we O
fine O
- O
tune O
another O
BART O
on O
this O
dataset O
. O
Naïvely O
, O
we O
can O
take O
the O
articles O
with O
emotion O
- O
triggering O
adverbs O
or O
adjectives O
removed O
as O
input O
to O
BART O
and O
using O
the O
original O
article O
as O
the O
decoding O
target O
. O
However O
, O
we O
found O
that O
around B-MetricValue
25 I-MetricValue
% I-MetricValue
of O
the O
time O
BART O
does O
not O
exactly O
reproduce O
the O
unmasked O
texts O
due O
to O
hallucination O
. O
This O
observation O
is O
consistent O
with O
Donahue O
et O
al O
. O
( O
2020 O
) O
's O
findings O
. O
To O
this O
end O
, O
we O
propose O
a O
twostep O
generation O
approach O
. O
First O
, O
we O
train O
BART O
to O
insert O
a O
< O
mask O
> O
token O
into O
the O
target O
sentence O
in O
the O
input O
document O
marked O
with O
special O
tokens O
. O
Then O
, O
BART O
learns O
to O
infill O
the O
< O
mask O
> O
with O
an O
approach O
similar O
to O
what O
is O
discussed O
in O
§ O
2.1 O
but O
without O
the O
SCST O
objective O
. O
Empirically O
, O
we O
found O
that O
this O
approach O
successfully O
reduces O
the O
chance B-MetricName
of I-MetricName
failure I-MetricName
in O
generating O
the O
exact O
unmasked O
contexts O
to O
around B-MetricValue
2 I-MetricValue
% I-MetricValue
. O

Intermediate O
Pre O
- O
training O

As O
the O
size O
of O
TIMELINE17 B-DatasetName
( O
Tran O
et O
al O
. O
, O
2013 O
) O
and O
the O
propaganda B-DatasetName
dataset O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
) O
are O
relatively O
small O
, O
we O
perform O
intermediate O
pre O
- O
training O
( O
IPT O
) O
on O
the O
news O
articles O
from O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
, O
a O
large O
news O
summarization O
dataset O
( O
Hermann O
et O
al O
. O
, O
2015 O
) O
, O
for O
domain O
adaptation O
. O
Details O
of O
IPT O
can O
be O
found O
in O
Appendix O
F O
. O

Our O
PROPANEWS B-DatasetName
Dataset O

Data O
Source O

When O
selecting O
the O
source O
of O
data O
, O
we O
considered O
two O
criteria O
. O
First O
, O
the O
news O
articles O
must O
have O
high O
trustworthiness O
. O
This O
ensures O
that O
, O
except O
for O
our O
manipulated O
sentences O
, O
the O
rest O
is O
genuine O
. O
Second O
, O
the O
news O
events O
described O
in O
the O
articles O
must O
be O
important O
. O
Motivated O
by O
these O
two O
criteria O
, O
we O
repurposed O
the O
TIMELINE17 B-DatasetName
dataset O
( O
Tran O
et O
al O
. O
, O
2013 O
) O
as O
our O
source O
of O
data O
. O
It O
contains O
17 O
timelines O
, O
each O
of O
which O
corresponds O
to O
a O
news O
event O
. O
Each O
timeline O
is O
associated O
with O
a O
series O
of O
news O
articles O
that O
span O
across O
a O
wide O
time O
span O
, O
implying O
the O
high O
importance O
and O
impact O
of O
these O
events O
. O
Moreover O
, O
the O
articles O
come O
from O
trustworthy O
media O
. O
In O
total O
, O
there O
are O
4,535 O
news O
articles O
in O
TIMELINE17 B-DatasetName
. O

Crowdsourcing O
for O
Data O
Curation O

We O
use O
Amazon O
's O
Mechanical O
Turk O
( O
AMT O
) O
to O
verify O
the O
quality B-MetricName
and O
the O
correctness B-MetricName
of O
the O
generated O
disinformation O
. O
In O
total O
, O
there O
are O
around O
400 O
unique O
crowdsourcing O
workers O
contributing O
to O
approximately O
2,000 O
Human O
Intelligence O
Tasks O
( O
HITs O
) O
. O
For O
each O
HIT O
, O
the O
annotators O
were O
tasked O
to O
look O
for O
supporting O
evidence O
from O
trustworthy O
news O
media O
to O
determine O
whether O
the O
sentences O
generated O
are O
indeed O
inaccurate O
. O
Only O
those O
labeled O
as O
inaccurate O
were O
included O
in O
PROPANEWS B-DatasetName
, O
while O
the O
accurate O
counterparts O
were O
discarded O
. O
Appendix O
H O
gives O
additional O
details O
. O

To O
measure O
the O
inter O
- O
annotator O
agreement O
( O
IAA O
) O
, O
we O
use O
the O
Worker B-MetricName
Agreement I-MetricName
With I-MetricName
Aggregate I-MetricName
( O
WAWA B-MetricName
) O
score O
( O
Ning O
et O
al O
. O
, O
2020 O
; O
Sheng O
et O
al O
. O
, O
2021 O
) O
, O
which O
compares O
each O
annotator O
's O
answer O
to O
the O
aggregated O
answer O
obtained O
via O
majority O
votes O
and O
micro O
- O
averages O
the O
results O
across O
all O
samples O
. O

9 O

The O
resulting O
WAWA B-MetricName
precision B-MetricName
, O
recall B-MetricName
, O
and O
F B-MetricName
1 I-MetricName
are O
80.01 B-MetricValue
% I-MetricValue
, O
78.94 B-MetricValue
% I-MetricValue
, O
and O
79.47 B-MetricValue
% I-MetricValue
, O
which O
indicates O
moderate O
to O
high O
agreement O
. O

Disinformation B-TaskName
Detection I-TaskName

The O
disinformation B-TaskName
detection I-TaskName
task O
challenges O
detectors O
to O
determine O
whether O
a O
given O
input O
article O
contains O
inaccurate O
information O
or O
not O
. O
We O
experiment O
on O
four O
detectors O
, O
including O
HDSF B-MethodName
( O
Karimi O
and O
Tang O
, O
2019 O
) O
, O
GROVER B-MethodName
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
ROBERTA B-MethodName
. O
HDSF B-MethodName
leverages O
the O
hierarchical O
structures O
of O
discourse O
- O
level O
features O
, O
such O
as O
dependency O
trees O
, O
to O
predict O
the O
veracity O
of O
a O
news O
article O
. O
GROVER B-MethodName
is O
an O
unidirectional O
seq2seq O
model O
pre O
- O
trained O
on O
news O
documents O
. O
We O
use O
the O
discriminative O
version O
for O
detection O
which O
is O
adapted O
from O
its O
generative O
version O
by O
feeding O
the O
[ O
CLS O
] O
token O
representations O
to O
a O
multi O
- O
layer O
perceptron O
. O
Similarly O
, O
BERT B-MethodName
and O
ROBERTA B-MethodName
take O
in O
the O
entire O
article O
as O
input O
and O
feed O
the O
representations O
of O
the O
first O
token O
to O
a O
classification O
head O
to O
determine O
the O
veracity O
of O
each O
article O
. O
In O
addition O
, O
all O
models O
are O
optimized O
using O
cross O
entropy O
. O
For O
fair O
comparison O
, O
we O
set O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
to O
512 B-HyperparameterValue
and O
we O
use O
the O
LARGE B-HyperparameterValue
variants B-HyperparameterName
for O
all O
models O
. O
More O
details O
can O
be O
found O
in O
Appendix O
J O
. O

Experiments O

In O
our O
experiments O
, O
we O
aim O
( O
1 O
) O
to O
analyze O
the O
performance O
of O
different O
models O
on O
our O
new O
PROPANEWS B-DatasetName
dataset O
, O
( O
2 O
) O
to O
examine O
the O
effect O
of O
various O
training O
data O
sets O
, O
and O
( O
3 O
) O
to O
investigate O
how O
much O
silver O
- O
standard O
data O
is O
equivalent O
to O
goldstandard O
data O
. O

Data O

PROPANEWS B-DatasetName
The O
PROPANEWS B-DatasetName
dataset O
consists O
of O
2,256 O
distinct O
articles O
, O
with O
a O
balanced O
number O
of O
fake O
and O
real O
documents O
. O
Within O
the O
fake O
articles O
, O
30 O
% O
use O
appeal O
to O
authority O
, O
another O
30 O
% O
include O
loaded O
language O
, O
and O
the O
remaining O
40 O
% O
simply O
contains O
inaccurate O
information O
. O
We O
split O
the O
data O
into O
1,256:500:500 B-HyperparameterValue
for O
training B-HyperparameterName
, I-HyperparameterName
validation I-HyperparameterName
, I-HyperparameterName
and I-HyperparameterName
testing I-HyperparameterName
. O

Evaluation O
Data O
We O
use O
two O
sets O
of O
humanwritten O
articles O
released O
by O
Nguyen O
et O
al O
. O
( O
2020 O
) O
and O
Shu O
et O
al O
. O
( O
2018 O
) O
to O
evaluate O
the O
effectiveness O
of O
our O
approach O
. O
The O
articles O
in O
each O
dataset O
are O
collected O
from O
two O
fact O
- O
checking O
websites O
, O
SNOPES B-DatasetName
and O
POLITIFACT B-DatasetName
, O
respectively O
. O
Articles O
no O
longer O
accessible O
via O
the O
given O
URL O
are O
removed O
. O
Statistics O
about O
both O
datasets O
are O
shown O
in O
Appendix O
I O
. O

Other O
generated O
training O
data O
We O
compare O
PROPANEWS B-DatasetName
to O
the O
following O
approaches O
. O
GROVER B-MethodName
- I-MethodName
GEN I-MethodName
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
generates O
headlines O
which O
condition O
on O
the O
original O
body O
texts O
, O
followed O
by O
body O
text O
generation O
conditioning O
on O
the O
generated O
headlines O
. O
FACTGEN B-MethodName
enhances O
the O
factual O
consistency O
of O
the O
generated O
article O
with O
a O
fact O
retriever O
that O
fetches O
supporting O
information O
from O
external O
corpora O
. O
FA B-MethodName
- I-MethodName
KEEVENT I-MethodName
( O
Wu O
et O
al O
. O
, O
2022 O
) O
generates O
sentences O
sequentially O
with O
condition O
on O
the O
manipulated O
knowledge O
elements O
of O
each O
sentence O
. O
Also O
, O
we O
form O
the O
PN B-MethodName
- I-MethodName
SILVER I-MethodName
dataset O
by O
resampling O
our O
generated O
data O
but O
disregarding O
the O
annotator O
validation O
. O
Furthermore O
, O
we O
construct O
additional O
training O
sets O
by O
replacing O
the O
salient O
sentence O
in O
each O
article O
with O
one O
sentence O
generated O
by O
each O
baseline O
method O
, O
as O
indicated O
by O
-1SENT B-MethodName
. O
To O
ensure O
fair O
comparisons O
, O
all O
generators O
take O
in O
the O
same O
set O
of O
authentic O
articles O
as O
inputs O
. O

Results O
and O
Discussion O

Human B-TaskName
- I-TaskName
written I-TaskName
disinformation I-TaskName
detection I-TaskName
To O
study O
the O
effectiveness O
of O
human B-TaskName
- I-TaskName
written I-TaskName
disinformation I-TaskName
detection I-TaskName
, O
we O
train O
GROVER B-MethodName
- I-MethodName
LARGE I-MethodName
and O
ROBERTA B-MethodName
- I-MethodName
LARGE I-MethodName
on O
different O
training O
datasets O
and O
evaluate O
them O
on O
the O
SNOPES B-DatasetName
and O
POLITIFACT B-DatasetName
datasets O
, O
as O
shown O
in O
Table O
3 O
. O
Both O
models O
perform O
best O
when O
trained O
on O
PROPANEWS B-DatasetName
, O
compared O
to O
training O
on O
other O
datasets O
. O
Consider O
ablating O
human B-MetricName
validation I-MetricName
, O
detectors O
trained O
on O
PN B-DatasetName
- I-DatasetName
SILVER I-DatasetName
still O
outperform O
their O
counterparts O
trained O
on O
other O
datasets O
. O
This O
shows O
that O
our O
generative O
method O
produces O
articles O
that O
are O
more O
similar O
to O
humanwritten O
disinformation O
. O
To O
further O
verify O
this O
finding O
, O
we O
measure O
the O
similarity O
between O
articles O
generated O
by O
different O
approaches O
and O
disinformative O
articles O
in O
the O
POLITIFACT B-DatasetName
dataset O
using O
the O
MAUVE B-MetricName
metric O
( O
Pillutla O
et O
al O
. O
, O
2021 O
) O
. O
MAUVE B-MetricName
computes O
the O
similarity O
between O
two O
text O
distributions O
by O
adding O
the O
areas O
under O
a O
divergence O
curve O
, O
and O
has O
been O
shown O
to O
produce O
better O
approximations O
than O
other O
metrics O
such O
as O
JS O
divergence O
( O
Martins O
et O
al O
. O
, O
2020 O
) O
. O
We O
find O
that O
the O
MAUVE B-MetricName
score O
with O
POLITIFACT B-DatasetName
for O
PROPANEWS B-DatasetName
and O
GROVER B-DatasetName
- I-DatasetName
GEN I-DatasetName
is O
17.1 B-MetricName
% I-MetricName
and O
13.7 B-MetricName
% I-MetricName
, O
respectively O
, O
suggesting O
that O
the O
generated O
documents O
in O
PROPANEWS B-DatasetName
are O
closer O
to O
human O
- O
written O
disinformation O
. O
These O
results O
confirm O
that O
the O
advantage O
of O
our O
generated O
articles O
in O
defending O
against O
human O
- O
written O
disinformation O
is O
resulted O
from O
the O
closer O
gap O
between O
them O
. O
Comparing O
each O
baseline O
method O
and O
its O
counterpart O
that O
only O
generates O
one O
sentence O
to O
be O
substituted O
for O
the O
salient O
sentence O
( O
i.e. O
, O
-1SENT B-MethodName
) O
, O
we O
found O
significant O
performance O
drops O
on O
GROVER B-MethodName
- I-MethodName
GEN I-MethodName
and O
FACTGEN B-MethodName
when O
only O
generating O
one O
sentence O
. O
This O
is O
likely O
caused O
by O
the O
incoherence O
between O
the O
right O
context O
and O
the O
sentence O
generated O
by O
these O
approaches O
due O
to O
the O
left O
- O
to O
- O
right O
fashion O
of O
text O
generation O
. O
While O
FAKEEVENT B-MethodName
does O
not O
see O
the O
right O
context O
, O
it O
additionally O
conditions O
on O
knowledge O
elements O
corresponding O
to O
the O
sentence O
, O
which O
discourages O
it O
from O
producing O
topically O
irrelevant O
content O
and O
thus O
does O
not O
lead O
to O
huge O
performance O
drop O
. O

In O
Table O
4 O
, O
we O
show O
two O
examples O
of O
disinformative O
articles O
from O
POLITIFACT B-DatasetName
where O
ROBERTA B-MethodName
is O
able O
to O
classify O
them O
as O
inaccurate O
when O
trained O
on O
PN B-DatasetName
- I-DatasetName
SILVER I-DatasetName
, O
but O
fails O
when O
trained O
on O
GROVER O
- O
GEN O
. O
Both O
articles O
contain O
propaganda O
, O
which O
is O
incorporated O
into O
PN B-DatasetName
- I-DatasetName
SILVER I-DatasetName
but O
not O
into O
GROVER B-DatasetName
- I-DatasetName
GEN I-DatasetName
. O
This O
demonstrates O
that O
detectors O
trained O
on O
our O
generated O
data O
perform O
better O
at O
detecting O
human O
- O
written O
disinformation O
that O
has O
such O
properties O
. O

Is O
propaganda O
generation O
helpful O
for O
disinformation O
detection O
? O
We O
further O
conduct O
an O
ablation O
study O
to O
analyze O
the O
contributions O
of O
each O
propaganda O
technique O
. O
As O
shown O
in O
the O
bottom O
of O
Table O
3 O
, O
both O
appeal O
to O
authority O
and O
loaded O
language O
prove O
beneficial O
in O
enhancing O
models O
' O
abilities O
to O
detect O
human O
- O
written O
disinformation O
. O

We O
can O
further O
see O
in O
Table O
3 O
, O
when O
comparing O
PROPANEWS B-MethodName
WITHOUT I-MethodName
AA I-MethodName
& I-MethodName
LL I-MethodName
to O
other O
generation O
approaches O
, O
that O
both O
models O
trained O
on O
our O
generated O
data O
, O
even O
without O
the O
incorporation O
of O
propaganda O
techniques O
, O
still O
outperform O
their O
counterparts O
trained O
on O
other O
datasets O
. O
This O
illustrates O
that O
our O
generated O
disinformation O
texts O
are O
closer O
to O
news O
articles O
written O
by O
humans O
. O

How O
good O
is O
the O
generation O
quality O
? O
To O
evaluate O
the O
quality O
of O
our O
generation O
approach O
, O
we O
asked O
Amazon O
Mechanical O
Turk O
( O
AMT O
) O
workers O
to O
rate O
the O
plausibility O
of O
100 O
generated O
articles O
from O
PROPANEWS B-DatasetName
and O
to O
determine O
the O
degree O
by O
which O
their O
answer O
to O
this O
question O
is O
influenced O
by O
the O
generated O
propaganda O
. O
Each O
article O
was O
rated O
by O
three O
different O
AMT O
workers O
. O
For O
comparison O
, O
we O
also O
asked O
the O
AMT O
workers O
to O
rate O
the O
plausibility O
of O
100 O
generated O
articles O
from O
GROVER B-DatasetName
- I-DatasetName
GEN I-DatasetName
. O
The O
average B-MetricName
plausibility I-MetricName
scores I-MetricName
for O
PROPANEWS B-DatasetName
and O
GROVER B-DatasetName
- I-DatasetName
GEN I-DatasetName
were O
2.25 B-MetricName
and O
2.15 B-MetricName
( O
out O
of O
3 O
) O
, O
respectively O
. O
indicating O
that O
our O
generation O
approach O
has O
a O
slight O
advantage O
over O
GROVER B-DatasetName
- I-DatasetName
GEN I-DatasetName
in O
terms O
of O
plausibility O
. O
Moreover O
, O
among O
the O
articles O
in O
PROPANEWS B-DatasetName
that O
are O
rated O
highly O
plausible O
, O
29.2 B-MetricValue
% I-MetricValue
of O
the O
workers O
think O
that O
the O
generated O
propaganda O
highly O
affects O
their O
response O
( O
i.e. O
rated O
3 O
out O
of O
3 O
) O
that O
the O
generated B-MetricName
article I-MetricName
is I-MetricName
plausible I-MetricName
. O
This O
demonstrates O
the O
effectiveness O
of O
our O
propaganda O
techniques O
in O
increasing O
the O
plausibility O
of O
generated O
articles O
. O
Survey O
details O
and O
score O
distributions O
are O
discussed O
in O
Appendix O
K O
. O

Article O
and O
Analysis O

Article O
: O
... O
Statement O
from O
FDA O
Commissioner O
Scott O
Gottlieb O
, O
M.D. O
, O
on O
FDA O
's O
ongoing O
efforts O
to O
help O
improve O
effectiveness O
of O
influenza O
vaccinesFor O
Immediate O
Release O
: O
... O
Analysis O
: O
Appealing O
to O
authority O
is O
common O
in O
human O
- O
written O
fake O
news O
. O

Article O
: O
... O
Regardless O
of O
how O
much O
we O
hate O
Nacy O
Pelosi O
, O
she O
represents O
a O
Congressional O
District O
that O
saw O
a O
million O
fraudulent O
votes O
from O
illegal O
immigrants O
... O
Analysis O
: O
The O
use O
of O
loaded O
language O
often O
indicates O
disinformation O
. O

Related O
Work O

Fake B-TaskName
News I-TaskName
Generation I-TaskName
and I-TaskName
Detection I-TaskName
There O
has O
been O
a O
focus O
in O
prior O
research O
on O
using O
neural O
networks O
to O
automatically O
generate B-TaskName
fake I-TaskName
news I-TaskName
as O
a O
means O
of O
defending O
against O
the O
proliferation O
of O
machine O
- O
generated O
fake O
news O
. O
Zellers O
et O
al O
. O
( O
2019 O
) O
pre O
- O
trained O
a O
generator O
with O
the O
GPT-2 O
architecture O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
on O
a O
large O
- O
scale O
news O
corpus O
and O
demonstrated O
that O
it O
was O
effective O
in O
detecting O
neural O
fake O
news O
. O
More O
recently O
, O
Fung O
et O
al O
. O
( O
2021 O
) O
improved O
the O
controllability O
of O
the O
generated O
fake O
news O
by O
conditioning O
the O
generator O
on O
knowledge O
elements O
, O
such O
as O
entities O
, O
relations O
and O
events O
, O
extracted O
from O
the O
original O
news O
article O
. O
enhanced O
the O
factuality O
of O
the O
generated O
article O
by O
introducing O
a O
fact O
retriever O
that O
fetches O
relevant O
information O
from O
external O
corpora O
. O
Mosallanezhad O
et O
al O
. O
( O
2021 O
) O
used O
adversarial O
reinforcement O
learning O
to O
generate O
topic O
- O
preserving O
articles O
. O
These O
studies O
developed O
methods O
for O
generating O
fake O
news O
that O
is O
hard O
to O
distinguish O
from O
real O
news O
to O
humans O
. O
Nevertheless O
, O
due O
to O
the O
overwhelming O
amount O
of O
inaccurate O
information O
introduced O
and O
the O
lack O
of O
propaganda O
techniques O
in O
the O
generated O
texts O
, O
these O
approaches O
are O
suboptimal O
for O
detecting O
human O
- O
written O
fake O
news O
, O
as O
shown O
in O
§ O
5.2 O
. O
In O
contrast O
, O
we O
generate O
fake O
news O
by O
incorporating O
propaganda O
techniques O
and O
preserving O
the O
majority O
of O
the O
correct O
information O
. O
Hence O
, O
our O
approach O
is O
more O
suitable O
for O
studying O
defense O
against O
human O
- O
written O
fake O
news O
. O
Also O
, O
since O
our O
dataset O
is O
annotated O
with O
the O
exact O
offset O
of O
the O
disinformative O
passages O
, O
it O
enables O
research O
on O
interpretable B-TaskName
detection I-TaskName
of I-TaskName
fake I-TaskName
news I-TaskName
. O

Propaganda B-TaskName
Generation I-TaskName
and I-TaskName
Detection I-TaskName

There O
is O
little O
previous O
work O
on O
propaganda B-TaskName
generation I-TaskName
. O
Zellers O
et O
al O
. O
( O
2019 O
) O
is O
the O
only O
relevant O
work O
, O
and O
it O
studied O
the O
generation O
of O
propaganda O
to O
communicate O
targeted O
disinformation O
. O
In O
contrast O
, O
we O
generate O
propaganda O
techniques O
to O
bring O
the O
generated O
articles O
closer O
to O
human O
- O
written O
fake O
news O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
study O
the O
incorporation O
of O
specific O
propaganda O
techniques O
into O
generated O
articles O
. O
Prior O
work O
on O
propaganda O
detection O
mainly O
focused O
on O
documentlevel O
detection O
. O
Early O
work O
collected O
propaganda O
datasets O
using O
distant O
supervision O
( O
Rashkin O
et O
al O
. O
, O
2017 O
) O
by O
assigning O
the O
same O
propaganda O
label O
to O
each O
news O
outlet O
under O
the O
same O
source O
based O
on O
the O
news O
- O
media O
- O
level O
label O
of O
corresponding O
news O
source O
listed O
on O
trustworthy O
sites O
. O
However O
, O
classifiers O
trained O
on O
such O
datasets O
may O
only O
learn O
to O
recognize O
the O
bias O
of O
each O
news O
source O
instead O
of O
propaganda O
( O
Martino O
et O
al O
. O
, O
2020 O
) O
. O
Our O
dataset O
avoids O
such O
issues O
by O
explicitly O
incorporating O
propaganda O
into O
each O
generated O
article O
. O
Furthermore O
, O
Da O
San O
Martino O
et O
al O
. O
( O
2019 O
) O
presented O
a O
fragmentlevel O
propaganda O
detection O
dataset O
, O
where O
specific O
propaganda O
techniques O
were O
labeled O
onto O
spans O
of O
text O
instead O
of O
each O
document O
. O
Recent O
approaches O
for O
detecting O
these O
propaganda O
techniques O
rely O
on O
pre O
- O
trained O
transformers O
( O
Morishita O
et O
al O
. O
, O
2020 O
; O
Feng O
et O
al O
. O
, O
2021 O
) O
. O
In O
contrast O
, O
we O
focus O
on O
detecting O
disinformative O
articles O
with O
propaganda O
signals O
. O

Conclusions O
and O
Future O
Work O

We O
have O
proposed O
a O
novel O
method O
for O
generating O
disinformation O
that O
is O
closer O
to O
human O
- O
written O
fake O
news O
. O
Evaluation O
on O
two O
human O
- O
written O
fake O
news O
datasets O
, O
POLITIFACT B-DatasetName
and O
SNOPES B-DatasetName
, O
demonstrated O
the O
effectiveness O
of O
our O
generated O
data O
PROPANEWS B-DatasetName
in O
enabling O
better O
detection O
performance O
on O
human O
- O
written O
fake O
news O
. O
We O
hope O
that O
the O
dataset O
presented O
in O
this O
work O
, O
PROPANEWS B-DatasetName
, O
can O
serve O
as O
an O
enabling O
resource O
for O
detecting O
human O
- O
written O
fake O
news O
and O
encouraging O
future O
research O
in O
this O
direction O
. O

In O
future O
work O
, O
we O
plan O
to O
extend O
our O
approach O
to O
other O
languages O
and O
to O
cover O
more O
propaganda O
techniques O
. O
We O
are O
also O
interested O
in O
studying O
other O
aspects O
of O
fake O
news O
generation O
, O
such O
as O
novelty O
and O
elaboration O
, O
as O
well O
as O
engaging O
linguistic O
style O
. O

Limitations O

To O
understand O
the O
gap O
between O
our O
automatic O
data O
generation O
method O
and O
fake O
news O
written O
by O
humans O
, O
we O
expanded O
PN B-DatasetName
- I-DatasetName
SILVER I-DatasetName
to O
different O
sizes O
and O
compared O
the O
performance O
of O
ROBERTA B-MethodName
- I-MethodName
LARGE I-MethodName
when O
trained O
on O
these O
generated O
datasets O
and O
the O
human O
- O
written O
fake O
news O
dataset O
, O
SNOPES B-DatasetName
. O

Note O
that O
since O
the O
TIMELINE17 B-DatasetName
dataset O
only O
contains O
around O
4 O
K O
samples O
, O
we O
additionally O
crawled O
New O
York O
Times O
news O
articles O
as O
an O
input O
to O
our O
generator O
for O
the O
" O
5 O
times O
" O
to O
" O
10 O
times O
" O
experiments O
. O
The O
results O
are O
shown O
in O
Figure O
3 O
. O
Although O
the O
detector O
performance O
at O
first O
improves O
as O
we O
add O
more O
silver O
training O
data O
, O
it O
reaches O
a O
plateau O
after O
the O
size O
is O
increased O
five O
- O
fold O
. O
This O
illustrates O
that O
while O
our O
approach O
is O
more O
effective O
compared O
to O
baseline O
generation O
methods O
, O
there O
is O
still O
a O
clear O
gap O
between O
our O
generated O
articles O
and O
human O
- O
crafted O
fake O
news O
, O
likely O
in O
aspects O
such O
as O
style O
( O
as O
discussed O
in O
§ O
5.2 O
) O
, O
intent O
( O
i.e. O
, O
limited O
modeling O
of O
propaganda O
techniques O
) O
, O
and O
falsehood O
( O
i.e. O
, O
the O
generated O
content O
is O
100 O
% O
false O
) O
. O
Despite O
the O
advantages O
of O
our O
generation O
approach O
, O
as O
compared O
to O
previous O
methods O
, O
it O
is O
uncapable O
of O
generating O
other O
propaganda O
techniques O
covered O
in O
( O
Da O
San O
Martino O
et O
al O
. O
, O
2019 O
) O
, O
such O
as O
straw O
man O
. O
Thus O
, O
our O
method O
is O
not O
generic O
enough O
to O
handle O
all O
types O
of O
propaganda O
techniques O
within O
a O
unified O
framework O
. O
Moreover O
, O
our O
approach O
is O
limited O
to O
generating O
English O
- O
only O
news O
articles O
, O
and O
can O
not O
be O
applied O
to O
other O
languages O
. O

Ethical O
Statement O
and O
Broader O
Impact O

Our O
objective O
for O
developing O
a O
generative O
approach O
that O
produces O
more O
realistic O
news O
articles O
is O
to O
advance O
the O
field O
of O
disinformation O
detection O
and O
to O
bring O
awareness O
that O
the O
current O
approaches O
for O
generating O
training O
data O
for O
fake B-TaskName
news I-TaskName
detection I-TaskName
are O
sub O
- O
optimal O
. O

We O
acknowledge O
that O
our O
generator O
may O
produce O
toxic O
text O
as O
it O
was O
fine O
- O
tuned O
on O
propagandistic O
datasets O
. O
We O
also O
understand O
the O
dual O
- O
use O
concerns O
for O
such O
a O
generation O
framework O
. O
One O
potential O
concern O
is O
the O
possibility O
of O
using O
the O
generator O
to O
produce O
fake O
news O
for O
political O
gain O
or O
to O
sow O
social O
discord O
. O
Another O
concern O
is O
the O
potential O
for O
the O
generator O
to O
be O
used O
to O
generate O
fake O
news O
that O
could O
cause O
harm O
, O
such O
as O
false O
medical O
information O
or O
misleading O
financial O
advice O
. O
Additionally O
, O
the O
generator O
might O
be O
used O
to O
create O
false O
evidence O
or O
to O
fabricate O
information O
to O
support O
false O
allegations O
in O
legal O
or O
regulatory O
proceedings O
. O

Therefore O
, O
to O
contribute O
to O
future O
studies O
on O
human O
- O
written O
disinformation O
detection O
, O
we O
decided O
to O
release O
the O
codebase O
for O
only O
the O
detectors O
used O
in O
the O
experiments O
as O
well O
as O
the O
generated O
data O
but O
not O
the O
generator O
. O

We O
highlight O
some O
scenarios O
that O
illustrate O
appropriate O
and O
inappropriate O
uses O
of O
our O
generator O
: O

• O
Appropriate O
: O
Researchers O
can O
use O
our O
framework O
to O
produce O
more O
challenging O
training O
data O
for O
learning O
stronger O
detectors O
. O

• O
Inappropriate O
: O
The O
method O
should O
not O
be O
used O
to O
intentionally O
create O
or O
propagate O
false O
information O
. O

• O
Inappropriate O
: O
The O
propaganda O
generation O
technique O
should O
not O
be O
used O
for O
political O
campaigns O
or O
any O
malicious O
purposes O
. O

Both O
inappropriate O
uses O
could O
lead O
to O
harmful O
consequences O
, O
such O
as O
undermining O
trust O
in O
the O
media O
and O
causing O
social O
unrest O
. O

A O
Distribution O
of O
Propaganda O

Figure O
4 O
shows O
the O
distribution O
of O
the O
propaganda O
techniques O
used O
in O
the O
human O
- O
written O
fake O
news O
we O
collected O
and O
analyzed O
in O
§ O
1 O
. O
Note O
that O
one O
article O
may O
contain O
multiple O
propaganda O
techniques O
. O
B O
Additional O
Research O
Questions O
Q1 O
: O
Is O
the O
detector O
learning O
to O
distinguish O
between O
fake O
/ O
real O
news O
articles O
or O
simply O
learning O
to O
detect O
the O
use O
of O
propaganda O
techniques O
? O

In O
Table O
3 O
, O
PROPANEWS B-DatasetName
w I-DatasetName
/ I-DatasetName
o I-DatasetName
AA I-DatasetName
& I-DatasetName
LL I-DatasetName
is O
the O
variant O
of O
our O
proposed O
dataset O
with O
both O
propaganda O
techniques O
removed O
. O
By O
training O
detectors O
on O
this O
version O
of O
the O
proposed O
dataset O
, O
the O
model O
is O
still O
effective O
in O
identifying O
human O
- O
written O
articles O
containing O
false O
information O
. O
Therefore O
, O
the O
detectors O
trained O
on O
our O
generated O
data O
have O
learned O
to O
distinguish O
between O
fake O
and O
real O
articles O
instead O
of O
exploiting O
propaganda O
information O
only O
. O
On O
the O
other O
hand O
, O
comparing O
the O
detectors O
trained O
on O
PROPANEWS B-DatasetName
and O
their O
counterparts O
trained O
on O
PROPANEWS B-DatasetName
w I-DatasetName
/ I-DatasetName
o I-DatasetName
AA I-DatasetName
& I-DatasetName
LL I-DatasetName
in O
Table O
3 O
, O
we O
see O
that O
propaganda O
can O
help O
improve O
the O
detection O
of O
real O
human O
- O
written O
fake O
news O
. O
We O
further O
want O
to O
emphasize O
that O
fake O
news O
detection O
is O
an O
extremely O
challenging O
task O
that O
requires O
both O
factual O
and O
stylistic O
analysis O
as O
demonstrated O
by O
our O
experiments O
and O
by O
the O
relatively O
low O
performance O
of O
prior O
SOTA O
models O
. O

Q2 O
: O
Do O
real O
articles O
make O
use O
of O
propaganda O
techniques O
, O
such O
as O
appeal O
to O
authority O
and O
loaded O
language O
? O
The O
similarity O
between O
our O
generated O
text O
and O
the O
real O
articles O
in O
PolitiFact O
is O
7.3 B-MetricValue
% I-MetricValue
as O
per O
the O
MAUVE B-MetricName
measure O
, O
which O
is O
much O
lower O
than O
the O
similarity O
between O
the O
generated O
text O
and O
the O
fake O
news O
articles O
, O
as O
discussed O
in O
§ O
5.2 O
. O
It O
is O
possible O
that O
some O
real O
news O
articles O
can O
contain O
propaganda O
. O
However O
, O
according O
to O
MAUVE B-MetricName
, O
the O
real O
articles O
in O
POLITIFACT B-DatasetName
do O
not O
contain O
much O
loaded O
language O
or O
appeal O
to O
authority O
. O

C O
Further O
Analysis O

C.1 O
Remaining O
Challenges O

To O
better O
understand O
the O
remaining O
disinformative O
articles O
that O
the O
detectors O
failed O
to O
identify O
, O
we O
conducted O
additional O
analysis O
by O
comparing O
the O
ROBERTA B-MethodName
predictions O
and O
the O
labels O
. O
As O
a O
result O
, O
we O
identified O
the O
following O
three O
major O
modeling O
capabilities O
required O
for O
successful O
detection O
: O

Static O
knowledge O
enrichment O
About O
30 B-MetricValue
% I-MetricValue
of O
misclassification B-MetricName
is O
due O
to O
the O
lack O
of O
static O
knowledge O
that O
can O
be O
found O
in O
public O
databases O
, O
such O
as O
law O
dictionaries O
. O
For O
example O
, O
in O
this O
article O
, O
10Alexandria O
Ocasio O
- O
Cortez O
falsely O
states O
that O
the O
U.S. O
Immigration O
Customs O
Enforcement O
( O
ICE O
) O
is O
required O
to O
fill O
34,000 O
beds O
every O
day O
. O
According O
to O
the O
Appropriations O
Act O
of O
2016 O
, O
11 O
, O
ICE O
is O
only O
required O
to O
detain O
34,000 O
available O
beds O
. O
Therefore O
, O
to O
detect O
such O
kind O
of O
misinformation O
, O
the O
detector O
needs O
to O
be O
enriched O
with O
static O
knowledge O
bases O
. O

Dynamic O
knowledge O
acquisition O
Around O
48 B-MetricValue
% I-MetricValue
of O
the O
misclassified B-MetricName
human I-MetricName
- I-MetricName
written I-MetricName
disinformation I-MetricName
is O
due O
to O
the O
inability O
to O
acquire O
dynamic O
knowledge O
from O
new O
news O
sources O
. O
For O
instance O
, O
COVID19 O
- O
related O
articles O
are O
usually O
published O
after O
2020 O
, O
while O
ROBERTA B-MethodName
was O
pre O
- O
trained O
on O
news O
articles O
released O
before O
2019 O
. O
It O
is O
very O
challenging O
for O
ROBERTA B-MethodName
to O
detect O
disinformation O
of O
such O
topics O
unless O
the O
detector O
is O
equipped O
with O
the O
capability O
to O
acquire O
dynamic O
knowledge O
from O
news O
articles O
. O
Particularly O
, O
ROBERTA B-MethodName
achieves O
an O
accuracy B-MetricName
of O
69.0 B-MetricValue
% I-MetricValue
on O
detecting B-TaskName
fake I-TaskName
articles I-TaskName
published O
before O
2019 O
, O
but O
its O
accuracy B-MetricName
drops O
to O
51.9 B-MetricValue
% I-MetricValue
when O
testing O
on O
articles O
published O
after O
2019 O
. O

Multi O
- O
document O
reasoning O

The O
rest O
of O
the O
incorrect O
detection O
is O
caused O
by O
the O
lack O
of O
multidocument O
reasoning O
ability O
. O
For O
instance O
, O
a O
news O
article O
12 O
wrongly O
associates O
Hillary O
Clinton O
with O
a O
flawed O
immigration O
policy O
of O
the O
former O
government O
, O
and O
strengthens O
such O
a O
statement O
by O
referring O
to O
a O
Senate O
report O
and O
relevant O
news O
articles O
. O
However O
, O
the O
cited O
report O
does O
not O
mention O
Clinton O
, O
and O
the O
other O
news O
articles O
contain O
disinformation O
. O
To O
correctly O
detect O
this O
piece O
of O
disinformation O
, O
detectors O
should O
reason O
across O
multiple O
documents O
. O

D O
Qualitative O
Examples O
of O
Generated O
Articles O

In O
Table O
8 O
, O
we O
show O
a O
comparison O
of O
generated O
articles O
given O
the O
same O
input O
data O
across O
different O
generative O
methods O
. O
Our O
approach O
produces O
articles O
with O
a O
small O
fraction O
of O
inaccurate O
information O
, O
which O
matches O
a O
property O
of O
human O
- O
written O
fake O
news O
discussed O
in O
§ O
1 O
. O

E O
Appeal O
to O
Authority O
Details O

To O
recap O
, O
we O
first O
gather O
a O
list O
of O
authorities O
Z O
for O
each O
article O
from O
Wikidata O
and O
the O
corresponding O
context O
. O
The O
best O
appeal O
to O
authority O
sequence O
s O
* O
is O
selected O
, O
i.e. O
, O
the O
one O
with O
the O
lowest O
perplexity O
s O
* O
= O
argmin O
s O
i O
PPL O
( O
s O
i O
) O
, O
where O
s O
i O
denotes O
the O
generated O
sequence O
using O
z O
i O
as O
the O
authority O
. O
However O
, O
this O
process O
results O
in O
every O
sequence O
s O
* O
containing O
the O
substring O
" O
confirms O
that O
" O
, O
which O
makes O
it O
trivial O
for O
detectors O
to O
classify O
these O
generated O
documents O
as O
fake O
by O
simply O
detecting O
such O
substrings O
. O
Therefore O
, O
we O
devise O
an O
algorithm O
to O
diversify O
the O
templates O
so O
that O
these O
generated O
articles O
are O
not O
easily O
detectable O
. O
First O
, O
we O
define O
a O
set O
of O
verbs O
V O
that O
can O
be O
swapped O
with O
" O
confirms O
" O
: O
V O
= O
{ O
said O
, O
concluded O
, O
confirmed O
, O
emphasized O
, O
stated O
, O
argued O
} O
. O
Then O
, O
we O
diversify O
the O
generated O
structure O
of O
the O
generated O
sentence O
s O
* O
by O
reordering O
the O
subject O
, O
the O
verb O
, O
and O
the O
object O
. O
Next O
, O
we O
swap O
the O
verb O
with O
another O
verb O
from O
V O
. O
Finally O
, O
in O
order O
to O
diversify O
the O
context O
, O
we O
append O
a O
preposition O
from O
the O
preposition O
set O
P O
P O
= O
{ O
on O
, O
at O
, O
in O
} O
to O
the O
output O
of O
the O
previous O
step O
, O
and O
then O
we O
feed O
the O
sequence O
to O
BART O
to O
generate O
the O
context O
. O
An O
example O
of O
this O
process O
is O
given O
in O
Table O
6 O
. O

F O
Intermediate O
Pre O
- O
training O
Details O

For O
domain O
adaptation O
, O
we O
perform O
intermediate O
pre O
- O
training O
( O
IPT O
) O
on O
the O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
dataset O
, O
a O
large O
summarization O
corpus O
containing O
more O
than O
280 O
K O
news O
articles O
from O
CNN O
and O
Daily O
Mail O
. O
The O
IPT O
objectives O
for O
disinformation O
generation O
and O
propaganda O
generation O
are O
mostly O
the O
same O
as O
described O
in O
the O
previous O
sections O
, O
but O
with O
some O
minor O
changes O
due O
to O
different O
goals O
in O
the O
IPT O
phase O
. O
When O
performing O
IPT O
for O
disinformation O
generation O
, O
we O
removed O
L O
s O
from O
the O
final O
loss O
function O
( O
Equation O
( O
4 O
) O
) O
as O
the O
goal O
for O
IPT O
is O
only O
to O
learn O
to O
generate O
coherent O
sentences O
, O
and O
thus O
IPT O
is O
not O
needed O
. O
Moreover O
, O
in O
order O
to O
create O
training O
samples O
for O
loaded O
language O
IPT O
, O
we O
gather O
all O
the O
appearances O
of O
adjectives O
pointing O
to O
a O
noun O
or O
adverbs O
pointing O
to O
a O
verb O
via O
dependency O
parsing O
graphs O
without O
considering O
whether O
the O
samples O
actually O
contain O
loaded O
terms O
since O
the O
goal O
here O
is O
to O
enable O
BART O
to O
identify O
where O
properly O
to O
insert O
which O
adjectives O
or O
adverbs O
. O

G O
Benchmarking O
Detectors O

The O
performance O
of O
various O
detectors O
on O
the O
PROPANEWS B-DatasetName
dataset O
is O
shown O
in O
Table O
5 O
. O
We O
find O
that O
ROBERTA B-MethodName
and O
GROVER B-MethodName
demonstrate O
advantages O
over O
BERT B-MethodName
. O
This O
could O
be O
explained O
by O
the O
fact O
that O
ROBERTA B-MethodName
and O
GROVER B-MethodName
are O
pre O
- O
trained O
on O
news O
domain O
corpora O
, O
whereas O
BERT B-MethodName
has O
no O
access O
to O
such O
domains O
during O
pre O
- O
training O
. O
In O
addition O
, O
we O
find O
that O
HDSF B-MethodName
performs O
much O
worse O
than O
the O
other O
three O
models O
. O
This O
reflects O
that O
largescale O
pre O
- O
training O
of O
language O
models O
brings O
more O
benefit O
to O
detection O
performance O
than O
explicit O
modeling O
of O
discourse O
- O
level O
features O
. O

H O
Human B-MetricName
Validation I-MetricName
Details O

Next O
, O
we O
describe O
the O
details O
of O
human B-MetricName
validation I-MetricName
, O
where O
AMT O
workers O
were O
tasked O
to O
validate O
whether O
the O
generated O
sentences O
contained O
inaccurate O
information O
. O
We O
recruited O
AMT O
workers O
from O
USA O
and O
Canada O
. O
To O
ensure O
the O
annotation O
quality O
, O
only O
workers O
who O
had O
an O
acceptance O
rate O
greater O
than O
95 O
% O
and O
more O
than O
100 O
accepted O
HITs O
in O
the O
past O
were O
allowed O
to O
work O
on O
our O
annotation O
task O
. O
This O
greatly O
reduced O
the O
chances O
of O
collecting O
annotations O
from O
scammers O
. O
Each O
HIT O
was O
designed O
such O
that O
the O
annotators O
were O
rewarded O
$ O
12- O
$ O
15 O
per O
hour O
, O
which O
complies O
with O
the O
ethical O
research O
standards O
outlined O
by O
AMT O
( O
Salehi O
et O
al O
. O
, O
2015 O
) O
. O
In O
each O
HIT O
, O
the O
annotators O
were O
presented O
an O
article O
with O
the O
generated O
part O
marked O
in O
boldface O
. O
The O
questions O
and O
the O
guidelines O
are O
given O
below O
. O
( O
Note O
that O
we O
only O
use O
the O
annotators O
' O
response O
for O
Q1 O
to O
validate O
our O
generated O
data O
. O
The O
annotations O
for O
the O
other O
questions O
will O
be O
used O
for O
future O
research O
. O
) O

Step O
Generated O
Sequence O

1 O

Panmure O
Gordon O
analyst O
Peter O
Hitchens O
confirmed O
that O
" O
the O
US O
government O
is O
likely O
to O
agree O
to O
reduce O
its O
estimate O
of O
the O
size O
of O
the O
spill O
, O
which O
would O
cut O
BP O
fines O
" O
. O

The O
US O
government O
is O
likely O
to O
agree O
to O
reduce O
its O
estimate O
of O
the O
size O
of O
the O
spill O
, O
which O
would O
cut O
BP O
fines O
, O
" O
Panmure O
Gordon O
analyst O
Peter O
Hitchens O
confirmed O
. O

3 O
" O
The O
US O
government O
is O
likely O
to O
agree O
to O
reduce O
its O
estimate O
of O
the O
size O
of O
the O
spill O
, O
which O
would O
cut O
BP O
fines O
, O
" O
Panmure O
Gordon O
analyst O
Peter O
Hitchens O
said O
. O

The O
US O
government O
is O
likely O
to O
agree O
to O
reduce O
its O
estimate O
of O
the O
size O
of O
the O
spill O
, O
which O
would O
cut O
BP O
fines O
, O
" O
Panmure O
Gordon O
analyst O
Peter O
Hitchens O
said O
in O
a O
conference O
. O

Table O
6 O
: O
An O
illustration O
of O
how O
appeal O
to O
authority O
is O
performed O
. O
In O
step O
1 O
, O
we O
generate O
a O
statement O
using O
BART O
with O
the O
prefix O
[ O
Panmure O
Gordon O
analyst O
Peter O
Hitchens O
confirmed O
that O
" O
] O
. O
In O
step O
2 O
, O
we O
move O
the O
subject O
and O
the O
verb O
to O
the O
back O
of O
the O
sentence O
to O
diversify O
the O
sentence O
structure O
. O
In O
step O
3 O
, O
we O
swap O
the O
verb O
with O
another O
verb O
from O
the O
verb O
set O
V O
. O
In O
step O
4 O
, O
we O
append O
a O
preposition O
in O
to O
the O
sequence O
in O
step O
3 O
and O
we O
use O
the O
resulting O
sequence O
as O
a O
prefix O
to O
BART O
's O
decoder O
to O
generate O
the O
rest O
of O
the O
context O
. O
For O
steps O
1 O
and O
4 O
, O
we O
mark O
the O
prefix O
sequence O
to O
the O
decoder O
in O
yellow O
, O
and O
the O
generated O
sequence O
in O
blue O
. O
To O
increase O
the O
diversity O
of O
the O
generated O
sequences O
, O
step O
2 O
to O
4 O
are O
each O
performed O
50 O
% O
of O
the O
time O
. O

Q1 O
: O
Is O
the O
generated O
text O
in O
boldface O
Accurate O
or O
Inaccurate O
? O
( O
If O
you O
can O
not O
find O
any O
supporting O
evidence O
, O
please O
select O
Inaccurate O
. O
) O
Note O
that O
a O
statement O
( O
in O
quotation O
marks O
) O
made O
by O
a O
person O
is O
only O
accurate O
if O
this O
person O
actually O
made O
the O
exact O
same O
statement O
. O
If O
the O
statement O
in O
quotation O
marks O
is O
just O
a O
paraphrase O
of O
what O
the O
person O
actually O
said O
, O
then O
the O
statement O
is O
inaccurate O
. O

-Inaccurate O
: O
Any O
false O
information O
presented O
in O
the O
generated O
text O
makes O
it O
inaccurate O
. O

-Accurate O
: O
All O
the O
information O
in O
the O
generated O
text O
must O
be O
accurate O
. O

Q2 O
: O
Enter O
the O
URL O
of O
the O
news O
article O
you O
found O
that O
supports O
your O
decision O
in O
the O
previous O
response O
in O
the O
below O
box O
. O
Put O
down O
" O
from O
context O
" O
if O
the O
evidence O
can O
be O
found O
in O
the O
context O
. O
Q3 O
: O
Does O
the O
generated O
text O
in O
boldface O
deliver O
the O
same O
sentiment O
as O
the O
rest O
of O
the O
article O
? O
-False O
: O
The O
sentiment O
of O
the O
generated O
text O
is O
NOT O
the O
same O
as O
the O
rest O
of O
the O
article O
. O

-True O
: O
The O
sentiment O
of O
the O
generated O
text O
is O
the O
same O
as O
the O
rest O
of O
the O
article O
. O

Q4 O
: O
Is O
the O
discourse O
of O
the O
generated O
text O
in O
boldface O
consistent O
with O
the O
rest O
of O
the O
article O
? O
-False O
: O
The O
discourse O
of O
the O
generated O
text O
is O
NOT O
consistent O
with O
the O
rest O
of O
the O
article O
. O

-True O
: O
The O
discourse O
of O
the O
generated O
text O
is O
consistent O
with O
the O
rest O
of O
the O
article O
. O

Q5 O
: O
If O
there O
is O
any O
grammatical O
error O
or O
inconsistent O
discourse O
, O
please O
rewrite O
and O
correct O
generated O
text O
and O
put O
it O
in O
the O
below O
box O
. O
Just O
put O
down O
the O
corrected O
generated O
text O
in O
bold O
is O
enough O
. O
For O
example O
, O
" O
Harry O
is O
a O
boy O
. O
He O
likes O
go O
to O
school O
. O
" O
Please O
put O
in O
" O
He O
likes O
to O
go O
to O
school O
. O
" O
in O
the O
box O
below O
. O

I O
Statistics O
about O
the O
Evaluation O
Datasets O

In O
Table O
7 O
, O
we O
give O
some O
statistics O
about O
the O
two O
evaluation O
datasets O
used O
in O
our O
experiments O
. O
The O
reported O
numbers O
are O
not O
the O
same O
as O
those O
in O
the O
original O
papers O
( O
Nguyen O
et O
al O
. O
, O
2020 O
; O
Shu O
et O
al O
. O
, O
2018 O
) O
since O
some O
of O
the O
articles O
were O
no O
longer O
accessible O
via O
the O
provided O
URLs O
. O

J O
Detector O
Implementation O
Details O

For O
our O
experiments O
with O
BERT B-MethodName
and O
ROBERTA B-MethodName
, O
we O
used O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
2 B-HyperparameterValue
and O
gradient B-HyperparameterName
accumulation I-HyperparameterName
steps I-HyperparameterName
of O
8 B-HyperparameterValue
. O
We O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
to O
5e-5 B-HyperparameterValue
and O
1e-5 B-HyperparameterValue
for O
the O
parameters O
that O
have O
been O
pre O
- O
trained O
, O
and O
1e-3 B-HyperparameterValue
and O
1e-3 B-HyperparameterValue
for O
the O
other O
parameters O
. O
For O
the O
GROVER B-MethodName
detector O
, O
we O
follow O
the O
original O
detection O
setting O
. O
GROVER B-MethodName
is O
trained O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
. O

Similarly O
, O
we O
follow O
the O
original O
recipe O
to O
train O
HDSF B-MethodName
, O
which O
is O
optimized O
with O
Adam O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-2 B-HyperparameterValue
. O
All O
detectors O
are O
fine O
- O
tuned O
for O
at O
most O
20 B-HyperparameterValue
epochs O
where O
the O
best O
model O
is O
determined O
by O
the O
accuracy B-MetricName
on O
the O
development O
set O
. O

All O
experiments O
are O
conducted O
on O
an O
Ubuntu O
18.04 O
machine O
with O
NVIDIA O
Tesla O
V100 O
. O
We O
use O
PyTorch O
1.10.0 O
and O
Transformers O
4.3.0 O
for O
constructing O
all O
models O
and O
loading O
pre O
- O
trained O
weights O
, O
except O
for O
GROVER B-MethodName
, O
which O
operates O
on O
Tensorflow O
1.13.1 O
. O
The O
training O
time O
for O
BERT B-MethodName
and O
ROBERTA B-MethodName
, O
each O
of O
which O
has O
340 O
M O
parameters O
, O
is O
around O
2 O
- O
3 O
hours O
, O
while O
for O
GROVER B-MethodName
, O
which O
contains O
355 O
M O
parameters O
, O
it O
is O
about O
1 O
hour O
. O

K O
Human O
Evaluation O
Details O

In O
this O
section O
, O
we O
describe O
the O
survey O
we O
did O
with O
AMT O
workers O
for O
evaluating O
the O
quality O
of O
the O
generated O
articles O
. O
The O
annotators O
were O
presented O
with O
a O
generated O
article O
and O
were O
asked O
to O
answer O
a O
few O
questions O
regarding O
its O
quality O
. O
Q2 O
is O
only O
applicable O
for O
evaluating O
generated O
articles O
from O
PROPANEWS B-DatasetName
, O
in O
which O
we O
show O
the O
sentence O
that O
contains O
propaganda O
. O
The O
low O
, O
the O
medium O
, O
and O
the O
high O
ratings O
in O
the O
response O
correspond O
to O
the O
1 O
, O
2 O
, O
and O
3 O
scores O
described O
in O
§ O
5.2 O
. O
The O
questions O
and O
the O
guidelines O
we O
gave O
were O
as O
follows O
: O

Q1 O
: O
How O
plausible O
do O
you O
think O
the O
article O
above O
is O
? O
-Low O
: O
It O
likely O
contains O
inaccurate O
information O
. O

-Medium O
: O
Not O
sure O
. O

-High O
: O
It O
is O
unlikely O
to O
contain O
inaccurate O
information O
. O

Q2 O
: O
How O
much O
does O
this O
sentence O
in O
the O
article O
affect O
your O
decision O
for O
the O
previous O
answer O
? O

-Low O
: O
This O
sentence O
does O
not O
affect O
my O
answer O
for O
the O
previous O
question O
. O

-Medium O
: O
This O
sentence O
somehow O
affects O
my O
answer O
for O
the O
previous O
question O
. O

-High O
: O
This O
sentence O
largely O
affects O
my O
answer O
for O
the O
previous O
question O
. O

The O
score O
distribution O
for O
Q1 O
is O
shown O
in O
Figure O
5 O
. O
We O
can O
see O
that O
our O
approach O
yields O
higherquality O
fake O
news O
compared O
to O
GROVER B-MethodName
- I-MethodName
GEN I-MethodName
. O

L O
Scientific O
Artifacts O

We O
re O
- O
purposed O
the O
Timeline17 B-DatasetName
dataset O
( O
Tran O
et O
al O
. O
, O
2013 O
) O
to O
produce O
our O
PROPANEWS B-DatasetName
dataset O
. O
Among O
the O
software O
we O
use O
, O
SpaCy O
( O
" O
en_core_web_md O
" O
) O
is O
licensed O
under O
the O
MIT O
License O
, O
Transformers O
and O
Stanza O
are O
licensed O
under O
the O
Apache O
License O
2.0 O
, O
and O
PyTorch O
is O
released O
under O
the O
Modified O
BSD O
License O
. O
In O
terms O
of O
models O
, O
both O
BART O
and O
the O
NLI O
model O
we O
used O
are O
licensed O
under O
the O
MIT O
License O
. O

In O
summary O
, O
all O
artifacts O
are O
produced O
with O
software O
and O
models O
released O
under O
open O
- O
source O
licenses O
. O
We O
will O
release O
the O
PROPANEWS B-DatasetName
dataset O
as O
well O
as O
the O
detectors O
with O
licenses O
compatible O
with O
the O
software O
discussed O
above O
such O
that O
they O
are O
consistent O
with O
their O
intended O
use O
. O

Method O
Generated O
Article O

GROVER B-MethodName
- I-MethodName
GEN I-MethodName

Libya O
to O
restart O
three O
oilfields O
on O
Sept O
1 O
: O
minister O
BENGHAZI O
, O
Libya O
, O
Aug O
28 O
( O
Reuters O
) O
-Libya O
will O
restart O
two O
eastern O
oil O
fields O
in O
mid O
- O
September O
and O
resume O
crude O
shipment O
from O
Tobruk O
by O
the O
end O
of O
the O
month O
, O
SNCM O
deputy O
oil O
minister O
Habib O
Hummed O
said O
on O
Sunday O
. O
He O
said O
the O
west O
of O
Sirte O
was O
being O
restored O
and O
a O
third O
field O
in O
the O
eastern O
province O
of O
Brega O
was O
about O
to O
restart O
operations O
. O
" O
Production O
at O
the O
Brega O
field O
has O
been O
increased O
by O
two O
to O
four O
percent O
and O
will O
resume O
its O
production O
on O
the O
end O
of O
the O
month O
, O
" O
he O
said O
at O
a O
political O
conference O
. O
" O
The O
second O
phase O
of O
production O
at O
the O
eastern O
field O
of O
Sirte O
came O
to O
full O
production O
by O
the O
end O
of O
the O
month O
and O
restarting O
the O
production O
from O
Brega O
field O
will O
begin O
in O
September O
, O
" O
he O
said O
. O
He O
said O
production O
at O
the O
west O
of O
Sirte O
and O
east O
of O
Tobruk O
had O
already O
reached O
95,000 O
barrels O
per O
day O
( O
bpd O
) O
on O
their O
main O
export O
terminals O
. O
Libya O
, O
awash O
with O
millions O
of O
dollars O
, O
most O
of O
it O
believed O
to O
have O
been O
siphoned O
off O
through O
crude O
theft O
, O
is O
pumping O
around O
290,000 O
bpd O
in O
total O
. O
The O
first O
fuel O
terminals O
in O
Libya O
have O
reopened O
and O
are O
loaded O
with O
diesel O
and O
petrol O
and O
tankers O
from O
Europe O
are O
ploughing O
in O
cargo O
to O
get O
through O
a O
key O
smuggling O
route O
from O
Derna O
to O
the O
Mediterranean O
. O
Libya O
was O
sharply O
divided O
last O
year O
when O
armed O
tribesmen O
struck O
crude O
export O
terminals O
along O
its O
border O
, O
denying O
it O
the O
lucrative O
gas O
exports O
it O
was O
using O
to O
service O
southern O
EU O
countries O
. O
Libya O
's O
southern O
oil O
terminal O
was O
hit O
in O
September O
by O
militant O
attacks O
. O
( O
Writing O
by O
Ulf O
Laessing O
; O
Editing O
by O
David O
Holmes O
) O
FACTGEN O
Shares O
Augmented O
and O
stored O
pipelines O
replaced O
by O
light O
pipes O
.Disputed O
concerns O
over O
biggest O
damage O
to O
Libya O
since O
2003 O
. O
Despite O
concerns O
oil O
could O
be O
leaking O
. O
' O
Russia O
says O
production O
is O
safe O
and O
it O
is O
ready O
to O
resume O
production O
. O
' O
Greece O
could O
use O
more O
sulfur O
oil O
by O
2020 O
. O
' O
Energy O
ministers O
say O
it O
could O
be O
used O
to O
power O
two O
- O
thirds O
of O
the O
world O
economy O
. O
' O
Saudi O
had O
been O
the O
biggest O
oil O
producer O
in O
the O
world O
before O
meltdown O
. O
" O
French O
to O
continue O
to O
contain O
crisis O
. O
" O
Russia O
on O
Libyan O
crisis O
: O
' O
we O
are O
facing O
a O
serious O
and O
continuing O
public O
health O
crisis O
that O
is O
needed O
' O
" O
But O
not O
in O
a O
vacuum O
of O
oil O
' O
" O
World O
affairs O
. O
" O
Greece O
: O
' O
we O
need O
to O
be O
doing O
more O
to O
combat O
damage O
to O
the O
energy O
system O
. O
' O
" O
leftasing O
all O
oil O
from O
Libya O
crisis O
. O
" O

Ours O
( O
PN B-MethodName
- I-MethodName
SILVER I-MethodName
) O
Tue O
Aug O
23 O
, O
2011 O
4:05 O
pm O
EDT O
* O
Limited O
Libyan O
oil O
production O
could O
start O
soon O
* O
Condition O
of O
oil O
export O
terminals O
still O
unclear O
* O
Clearing O
pipes O
clogged O
by O
waxy O
crude O
could O
delay O
output O
By O
David O
Sheppard O
and O
Sarah O
Young O
NEW O
YORK O
/ O
LONDON O
, O
Aug O
23 O
-LRB O
- O
Reuters O
-RRB O
-- O
Limited O
Libyan O
oil O
production O
ought O
to O
resume O
quite O
quickly O
as O
most O
fields O
appear O
to O
have O
survived O
the O
civil O
war O
unscathed O
, O
international O
oil O
services O
companies O
say O
, O
but O
many O
are O
still O
waiting O
for O
more O
stability O
in O
the O
country O
before O
sending O
employees O
back O
in O
. O
In O
interviews O
with O
some O
of O
the O
biggest O
oil O
services O
players O
in O
pre O
- O
civil O
war O
Libya O
, O
most O
echoed O
the O
view O
that O
major O
oil O
fields O
had O
largely O
avoided O
serious O
damage O
during O
the O
six O
months O
of O
fighting O
, O
though O
some O
voiced O
concerns O
about O
damage O
to O
export O
terminals O
and O
pipelines O
. O
OPS O
International O
Chairman O
Gavin O
De O
Salis O
told O
Reuters O
Insider O
television O
that O
Libyan O
crude O
oil O
, O
prized O
for O
its O
high O
yield O
of O
valuable O
light O
products O
such O
as O
gasoline O
and O
for O
its O
low O
sulfur O
content O
, O
was O
quite O
waxy O
, O
which O
could O
clog O
up O
pipelines O
if O
they O
had O
been O
left O
unused O
for O
some O
time O
. O
" O
There O
might O
be O
a O
little O
bit O
of O
effort O
unplugging O
pipelines O
, O
which O
is O
two O
to O
three O
months O
' O
worth O
of O
effort O
before O
they O
can O
resume O
full O
production O
, O
" O
De O
Salis O
said O
. O
" O
But O
that O
will O
not O
affect O
all O
of O
the O
pipelines O
or O
all O
of O
the O
fields O
, O
so O
they O
can O
certainly O
start O
limited O
production O
quite O
quickly O
. O
" O
Nilsson O
said O
contacts O
at O
Libya O
's O
rebel O
oil O
firm O
Arabian O
Gulf O
Oil O
Company O
-LRB O
- O
AGOCO O
-RRB O
- O
informed O
him O
there O
had O
been O
little O
damage O
to O
the O
oilfields O
in O
the O
east O
of O
the O
country O
during O
the O
six O
- O
month O
power O
struggle O
. O
" O
We O
have O
n't O
been O
able O
to O
work O
at O
the O
oilfields O
during O
the O
civil O
war O
as O
it O
has O
not O
been O
safe O
, O
but O
I O
think O
within O
a O
couple O
of O
weeks O
we O
could O
be O
back O
to O
almost O
normal O
, O
" O
Nilsson O
said O
by O
telephone O
from O
his O
office O
in O
Stockholm O
. O
" O
The O
oil O
income O
is O
essential O
to O
Libya O
and O
the O
new O
government O
so O
they O
will O
want O
to O
bring O
it O
back O
online O
as O
soon O
as O
possible O
. O
" O
Nilsson O
said O
they O
had O
several O
Swedish O
, O
Indian O
and O
Sudanese O
employees O
who O
had O
stayed O
in O
the O
country O
during O
the O
civil O
war O
, O
but O
total O
staff O
numbers O
in O
the O
country O
were O
down O
from O
around O
250 O
- O
300 O
. O
Nilsson O
said O
there O
was O
still O
a O
lot O
of O
work O
to O
be O
done O
in O
the O
country O
. O
De O
Salis O
said O
that O
" O
a O
lot O
of O
damage O
" O
had O
been O
done O
to O
Libya O
's O
oil O
infrastructure O
, O
including O
the O
destruction O
of O
some O
of O
the O
country O
's O
main O
oil O
export O
terminals O
, O
but O
he O
said O
it O
was O
too O
early O
to O
estimate O
the O
full O
extent O
of O
the O
damage O
. O
DAMAGE O
Oil O
firm O
's O
who O
supported O
the O
rebel O
government O
during O
the O
civil O
war O
are O
expected O
to O
win O
the O
lion O
's O
share O
of O
contracts O
to O
help O
relaunch O
the O
Libyan O
oil O
industry O
, O
which O
before O
the O
war O
produced O
some O
1.6 O
million O
barrels O
per O
day O
of O
crude O
... O
Table O
8 O
: O
A O
qualitative O
comparison O
between O
the O
generated O
articles O
from O
different O
approaches O
. O
The O
texts O
marked O
in O
orange O
indicate O
disinformation O
, O
and O
the O
texts O
in O
blue O
denote O
propaganda O
. O
We O
see O
that O
other O
approaches O
generate O
a O
large O
amount O
of O
inaccurate O
information O
, O
which O
contrasts O
with O
the O
property O
of O
human O
- O
written O
fake O
news O
mentioned O
in O
§ O
1 O
. O
We O
also O
note O
that O
the O
article O
generated O
using O
FACTGEN B-MethodName
appears O
to O
be O
low O
- O
quality O
. O
This O
is O
likely O
caused O
by O
the O
fact O
that O
the O
checkpoints O
reported O
in O
the O
paper O
were O
not O
released O
and O
we O
trained O
FACTGEN B-MethodName
from O
scratch O
by O
closely O
following O
the O
recipe O
described O
in O
. O
It O
is O
possible O
that O
some O
details O
about O
the O
training O
process O
of O
FACTGEN B-MethodName
were O
missing O
from O
their O
paper O
, O
which O
in O
turn O
affected O
our O
training O
, O
and O
resulted O
in O
low O
generation O
quality O
. O

Acknowledgement O

This O
research O
is O
based O
upon O
work O
supported O
by O
U.S. O
DARPA O
SemaFor O
Program O
No O
. O
HR001120C0123 O
and O
DARPA O
MIPs O
Program O
No O
. O
HR00112290105 O
. O
The O
views O
and O
the O
conclusions O
contained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
official O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
DARPA O
, O
or O
the O
U.S. O
Government O
. O
The O
U.S. O
Government O
is O
authorized O
to O
reproduce O
and O
to O
distribute O
reprints O
for O
governmental O
purposes O
notwithstanding O
any O
copyright O
annotation O
therein O
. O

A3 O
. O
Do O
the O
abstract O
and O
introduction O
summarize O
the O
paper O
's O
main O
claims O
? O

Abstract O
& O
Section O
1 O
. O

A4 O
. O
Have O
you O
used O
AI O
writing O
assistants O
when O
working O
on O
this O
paper O
? O

Grammarly O
is O
used O
to O
fix O
grammar O
errors O
throughout O
all O
sections O
of O
the O
paper O
. O
B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Appendix O
L O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
No O
personal O
/ O
sensitive O
information O
is O
collected O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Appendix O
L O
and O
K. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Section O
5.1 O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Appendix O
J O
. O

C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Appendix O
J O
. O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

Calibration B-MethodName
Meets I-MethodName
Explanation I-MethodName
: O
A O
Simple O
and O
Effective O
Approach O
for O
Model O
Confidence O
Estimates O

Calibration O
strengthens O
the O
trustworthiness O
of O
black O
- O
box O
models O
by O
producing O
better O
accurate O
confidence O
estimates O
on O
given O
examples O
. O
However O
, O
little O
is O
known O
about O
if O
model O
explanations O
can O
help O
confidence O
calibration O
. O
Intuitively O
, O
humans O
look O
at O
important O
features O
attributions O
and O
decide O
whether O
the O
model O
is O
trustworthy O
. O
Similarly O
, O
the O
explanations O
can O
tell O
us O
when O
the O
model O
may O
or O
may O
not O
know O
. O
Inspired O
by O
this O
, O
we O
propose O
a O
method O
named O
CME B-MethodName
that O
leverages O
model O
explanations O
to O
make O
the O
model O
less O
confident O
with O
non O
- O
inductive O
attributions O
. O
The O
idea O
is O
that O
when O
the O
model O
is O
not O
highly O
confident O
, O
it O
is O
difficult O
to O
identify O
strong O
indications O
of O
any O
class O
, O
and O
the O
tokens O
accordingly O
do O
not O
have O
high O
attribution O
scores O
for O
any O
class O
and O
vice O
versa O
. O
We O
conduct O
extensive O
experiments O
on O
six O
datasets O
with O
two O
popular O
pre O
- O
trained O
language O
models O
in O
the O
in O
- O
domain O
and O
out O
- O
of O
- O
domain O
settings O
. O
The O
results O
show O
that O
CME B-MethodName
improves O
calibration O
performance O
in O
all O
settings O
. O
The O
expected O
calibration O
errors O
are O
further O
reduced O
when O
combined O
with O
temperature B-MethodName
scaling I-MethodName
. O
Our O
findings O
highlight O
that O
model O
explanations O
can O
help O
calibrate O
posterior O
estimates O
. O

Introduction O

Accurate O
estimates O
of O
posterior O
probabilities O
are O
crucial O
for O
neural O
networks O
in O
various O
Natural O
Language O
Processing O
( O
NLP O
) O
tasks O
( O
Guo O
et O
al O
. O
, O
2017 O
; O
Lakshminarayanan O
et O
al O
. O
, O
2017 O
) O
. O
For O
example O
, O
it O
would O
be O
helpful O
for O
humans O
if O
the O
models O
deployed O
in O
practice O
abstain O
or O
interact O
when O
they O
can O
not O
make O
a O
decision O
with O
high O
confidence O
( O
Jiang O
et O
al O
. O
, O
2012 O
) O
. O
While O
Pre O
- O
trained O
Language O
Models O
( O
PLMs O
) O
have O
improved O
the O
performance O
of O
many O
NLP O
tasks O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
how O
to O
better O
avoid O
miscalibration O
is O
still O
an O
open O
research O
problem O
( O
Desai O
and O
Durrett O
, O
2020 O
; O
Dan O
and O
Roth O
, O
2021 O
) O
. O
In O
this O
paper O
, O
we O
investigate O
if O
Positive O
a O
fast O
funny O
highly O
enjoyable O
movie O
. O
Negative O
It O
's O
about O
following O
your O
dreams O
no O
matter O
what O
your O
parents O
think O
. O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
. O
The O
saturation O
of O
the O
colors O
signifies O
the O
magnitude O
. O
The O
confidence O
of O
the O
model O
should O
be O
easily O
recognized O
by O
looking O
at O
token O
attributions O
. O

and O
how O
model O
explanations O
can O
help O
calibrate O
the O
model O
. O
Explanation O
methods O
have O
attracted O
considerable O
research O
interest O
in O
recent O
years O
for O
revealing O
the O
internal O
reasoning O
processes O
behind O
models O
( O
Sundararajan O
et O
al O
. O
, O
2017 O
; O
Heo O
et O
al O
. O
, O
2018 O
; O
Shrikumar O
et O
al O
. O
, O
2017 O
) O
. O
Token B-MetricName
attribution I-MetricName
scores I-MetricName
generated O
by O
explanation O
methods O
represent O
the O
contribution O
to O
the O
prediction O
( O
Atanasova O
et O
al O
. O
, O
2020 O
) O
. O
Intuitively O
, O
one O
can O
draw O
some O
insight O
for O
analyzing O
and O
debugging O
neural O
models O
from O
these O
scores O
if O
they O
are O
correctly O
attributed O
, O
as O
shown O
in O
Table O
1 O
. O
For O
example O
, O
when O
the O
model O
identifies O
a O
highly O
indicative O
pattern O
, O
the O
tokens O
involved O
would O
have O
high O
attribution O
scores O
for O
the O
predicted O
label O
and O
low O
attribution O
scores O
for O
other O
labels O
. O
Similarly O
, O
if O
the O
model O
has O
difficulty O
recognizing O
the O
inductive O
information O
of O
any O
class O
( O
i.e. O
, O
the O
attribution O
scores O
are O
not O
high O
for O
any O
label O
) O
, O
the O
model O
should O
not O
be O
highly O
confident O
. O
As O
such O
, O
the O
computed O
explanation O
of O
an O
instance O
could O
indicate O
the O
confidence O
of O
the O
model O
in O
its O
prediction O
to O
some O
extent O
. O

Inspired O
by O
this O
, O
we O
propose O
a O
simple O
and O
effective O
method O
named O
CME B-MethodName
that O
can O
be O
applied O
at O
training O
time O
and O
improve O
the O
performance O
of O
the O
confidence O
estimates O
. O
The O
estimated O
confidence O
measures O
how O
confident O
the O
model O
is O
for O
a O
specific O
example O
. O
Ideally O
, O
reasonable O
confidence O
estimates O
should O
have O
higher O
confidence O
for O
correctly O
classified O
examples O
resulting O
in O
higher O
attributions O
than O
incorrect O
ones O
. O
Hence O
, O
given O
an O
example O
pair O
during O
training O
with O
an O
inverse O
classification O
relationship O
, O
we O
regularize O
the O
classifier O
by O
comparing O
the O
wrong O
example O
's O
attribution O
magnitude O
and O
the O
correct O
example O
's O
attribution O
magnitude O
. O

Our O
work O
is O
related O
to O
recent O
works O
on O
incorporating O
explanations O
into O
learning O
. O
Different O
from O
previous O
studies O
that O
leverage O
explanations O
to O
help O
users O
predict O
model O
decisions O
( O
Hase O
and O
Bansal O
, O
2021 O
) O
or O
improve O
the O
accuracy B-MetricName
( O
Rieger O
et O
al O
. O
, O
2020 O
) O
, O
we O
focus O
on O
answering O
the O
following O
question O
: O
are O
these O
explanations O
of O
black O
- O
box O
models O
useful O
for O
calibration O
? O
If O
so O
, O
how O
should O
we O
exploit O
the O
predictive O
power O
of O
these O
explanations O
? O
Considering O
the O
model O
may O
be O
uninterpretable O
due O
to O
the O
nature O
of O
neural O
networks O
and O
limitations O
of O
explanation O
method O
( O
Ghorbani O
et O
al O
. O
, O
2019 O
; O
Yeh O
et O
al O
. O
, O
2019 O
) O
, O
a O
calibrated O
model O
by O
CME B-MethodName
at O
least O
can O
output O
the O
unbiased O
confidence O
. O
Moreover O
, O
we O
exploit O
intrinsic O
explanation O
during O
training O
, O
which O
does O
not O
require O
designing O
heuristics O
( O
Ye O
and O
Durrett O
, O
2022 O
) O
and O
additional O
data O
augmentation O
( O
Park O
and O
Caragea O
, O
2022 O
) O
. O

We O
conduct O
extensive O
experiments O
using O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
to O
show O
the O
efficacy O
of O
our O
approach O
on O
three O
natural O
language O
understanding O
tasks O
( O
i.e. O
, O
natural O
language O
inference O
, O
paraphrase O
detection O
, O
and O
commonsense O
reasoning O
) O
under O
In O
- O
Domain O
( O
ID O
) O
and O
Out O
- O
of O
- O
Domain O
( O
OD O
) O
settings O
. O
CME B-MethodName
achieves O
the O
lowest O
expected O
calibration O
error O
without O
accuracy B-MetricName
drops O
compared O
with O
strong O
SOTA O
methods O
, O
e.g. O
, O
Park O
and O
Caragea O
( O
2022 O
) O
. O
When O
combined O
with O
Temperature B-MethodName
Scaling I-MethodName
( O
TS B-MethodName
) O
( O
Guo O
et O
al O
. O
, O
2017 O
) O
, O
the O
expected O
calibration O
errors O
are O
further O
reduced O
as O
better O
calibrated O
posterior O
estimates O
under O
these O
two O
settings O
. O

Method O

Problem O
Formulation O

A O
well O
- O
calibrated O
model O
is O
expected O
to O
output O
prediction B-MetricName
confidence I-MetricName
( O
e.g. O
, O
the O
highest O
probability O
after O
softmax O
activation O
) O
comparable O
to O
or O
aligned O
with O
its O
task O
accuracy B-MetricName
( O
i.e. O
, O
empirical O
likelihood O
) O
. O
For O
example O
, O
given O
100 O
examples O
with O
the O
prediction B-MetricName
confidence I-MetricName
of O
0.8 B-MetricValue
( O
or O
80 B-MetricValue
% I-MetricValue
) O
, O
we O
expect O
that O
80 B-MetricValue
examples O
will O
be O
correctly O
classified O
. O
Following O
Guo O
et O
al O
. O
( O
2017 O
) O
, O
we O
estimate O
the O
calibration O
error O
by O
empirical O
approximations O
. O
Specifically O
, O
we O
partition O
all O
examples O
into O
K B-HyperparameterName
bins B-HyperparameterName
of O
equal O
size O
ac O
- O
cording O
to O
their O
prediction O
confidences O
. O
Formally O
, O
for O
any O
p O
∈ O
[ O
ℓ O
k O
, O
u O
k O
) O
, O
we O
define O
the O
empirical B-MetricName
calibration I-MetricName
error I-MetricName
as O
: O

E O
k O
= O
1 O
|B O
k O
| O
i∈B O
k O
1 O
( O
ŷ O
i O
= O
y O
i O
) O
−p O
i O
, O
( O
1 O
) O

where O
y O
i O
, O
ŷ O
i O
andp O
i O
are O
the O
true O
label O
, O
predicted O
label O
and O
confidence O
for O
i O
- O
th O
example O
, O
and O
B O
k O
denotes O
the O
bin O
with O
prediction O
confidences O
bounded O
between O
ℓ O
k O
and O
u O
k O
. O
To O
evaluate O
the O
calibration O
error O
of O
classifiers O
, O
we O
further O
adopt O
a O
weighted B-MetricName
average I-MetricName
of I-MetricName
the I-MetricName
calibration I-MetricName
errors I-MetricName
of I-MetricName
all I-MetricName
bins I-MetricName
as O
the O
Expected B-MetricName
Calibration I-MetricName
Error I-MetricName
( O
ECE B-MetricName
) O
( O
Naeini O
et O
al O
. O
, O
2015 O
) O
: O

ECE B-MetricName
= O
K O
k=1 O
|B O
k O
| O
nÊ O
k O
, O
( O
2 O
) O

where O
n O
is O
the O
example O
number O
and O
lower O
is O
better O
. O
Note O
that O
the O
calibration O
goal O
is O
to O
minimize O
the O
calibration B-MetricName
error I-MetricName
without O
significantly O
sacrificing O
prediction O
accuracy B-MetricName
. O

Our O
Approach O

Generally O
, O
text O
classification O
models O
are O
optimized O
by O
Maximum O
Likelihood O
Estimation O
( O
MLE O
) O
, O
which O
minimizes O
the O
cross O
- O
entropy O
loss O
between O
the O
predicted O
and O
actual O
probability O
over O
k O
different O
classes O
. O
To O
minimize O
the O
calibration B-MetricName
error I-MetricName
, O
we O
add O
a O
regularization O
term O
to O
the O
original O
cross O
- O
entropy O
loss O
as O
a O
multi O
- O
task O
setup O
. O

Our O
intuition O
is O
that O
if O
the O
error O
of O
the O
model O
on O
example O
i O
is O
more O
significant O
than O
its O
error O
on O
example O
j O
( O
i.e. O
, O
example O
i O
is O
considered O
more O
difficult O
for O
the O
classifier O
) O
, O
then O
the O
magnitude O
of O
attributions O
on O
example O
i O
should O
not O
be O
greater O
than O
the O
magnitude O
of O
attributions O
on O
example O
j. O
Moreover O
, O
we O
penalize O
the O
magnitude O
of O
attributions O
with O
the O
model O
confidence O
( O
Xin O
et O
al O
. O
, O
2021 O
) O
, O
as O
the O
high O
error O
examples O
also O
should O
not O
have O
high O
confidence O
. O
Compared O
to O
the O
prior O
post O
- O
calibration O
methods O
( O
e.g. O
, O
temperature B-MethodName
scaling I-MethodName
learns O
a O
single O
parameter O
with O
a O
validation O
set O
to O
rescale O
all O
the O
logits O
) O
, O
our O
method O
is O
more O
flexible O
and O
sufficient O
to O
calibrate O
the O
model O
during O
training O
. O

Formally O
, O
given O
a O
training O
set O
D O
= O
{ O
( O
x O
1 O
, O
y O
1 O
) O
, O
• O
• O
• O
, O
( O
x O
n O
, O
y O
n O
) O
} O
where O
x O
i O
is O
the O
embeddings O
of O
input O
tokens O
and O
y O
i O
is O
the O
one O
- O
hot O
vector O
corresponding O
to O
its O
true O
label O
, O
an O
attribution O
of O
the O
golden O
label O
for O
input O
x O
i O
is O
a O
vector O
a O
i O
= O
( O
a O
i1 O
, O
• O
• O
• O
, O
a O
il O
) O
, O
and O
a O
ij O
is O
defined O
as O
the O
Backward O
model O
M O
for O
∇ O
θ O
L O
classif O
y O
( O
θ O
, O
Y O
) O
. O

6 O
: O

Calculate O
the O
attribution O
by O
scaled O
attention O
. O
7 O
: O

Computes O
absolute O
value O
of O
attributions O
. O
8 O
: O

Normalized O
it O
by O
applying O
Softmax O
function O
. O
9 O
: O

Calculate O
LCME O
by O
Eqn O
. O
3 O
, O
4 O
, O
5 O
, O
6 O
. O
10 O
: O

Optimize O
the O
model O
parameters O
θ O
by O
G O
: O
11 O
: O
θ O
← O
θ O
− O
η∇ O
θ O
LCME O
( O
θ O
, O
Y O
) O
. O
12 O
: O

end O
for O
13 O
: O
end O
for O
attribution O
of O
x O
ij O
( O
l O
is O
the O
length O
) O
. O
Here O
, O
attention O
scores O
are O
taken O
as O
the O
self O
- O
attention O
weights O
induced O
from O
the O
start O
index O
to O
all O
other O
indices O
in O
the O
penultimate O
layer O
of O
the O
model O
; O
this O
excludes O
weights O
associated O
with O
any O
special O
tokens O
added O
. O
Then O
, O
the O
token O
attribution O
a O
ij O
is O
the O
normalized O
attention O
score O
( O
Jain O
et O
al O
. O
, O
2020 O
) O
scaled O
by O
the O
corresponding O
gradients O
∇α O
ij O
= O
∂ŷ O
∂α O
ij O
( O
Serrano O
and O
Smith O
, O
2019 O
) O
. O
At O
last O
, O
our O
training O
minimizes O
the O
following O
loss O
: O

L O
CM O
E O
= O
L O
classif O
y O
+ O
λL B-HyperparameterName
calib O
, O
( O
3 O
) O

where O
λ B-HyperparameterName
is O
a O
weighted O
hyperparameter O
. O
The O
L O
calib O
is O
calculated O
as O
follows O
: O

L O
calib O
= O
1≤i O
, O
j≤n O
Ψ O
i O
, O
j O
1 O
[ O
e O
i O
> O
e O
j O
] O
, O
( O
4 O
) O

Ψ O
i O
, O
j O
= O
max O
[ O
0 O
, O
t O
( O
x O
i O
) O
− O
t O
( O
x O
j O
) O
] O
2 O
, O
( O
5 O
) O

t O
( O
x O
i O
) O
= O
∥a O
ij O
∥ O
2 O
* O
c O
i O
, O
( O
6 O
) O

where O
e O
i O
and O
e O
j O
are O
the O
error O
of O
example O
i O
and O
example O
j O
, O
the O
confidence B-MetricName
c B-MetricName
i I-MetricName
is O
estimated O
by O
the O
max O
probability O
of O
output O
( O
Hendrycks O
and O
Gimpel O
, O
2017 O
) O
, O
with O
the O
L2 O
aggregation O
. O
The O
products O
could O
be O
further O
scaled O
by O
√ O
l. O
In O
practice O
, O
strictly O
computing O
L O
calib O
for O
all O
example O
pairs O
is O
computationally O
prohibitive O
. O
Alternatively O
, O
we O
only O
consider O
examples O
from O
the O
mini O
- O
batch O
( O
similar O
lengths O
) O
of O
the O
current O
epoch O
. O
In O
other O
words O
, O
we O
consider O
all O
pairs O
where O
e O
i O
= O
1 O
and O
e O
j O
= O
0 O
where O
e O
is O
calculated O
by O
using O
zero O
- O
one O
error O
function O
. O
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
( O
commonsense O
reasoning O
) O
. O
We O
describe O
all O
datasets O
in O
details O
in O
Appendix O
A O
. O

Results O

Following O
Desai O
and O
Durrett O
( O
2020 O
) O
, O
we O
consider O
two O
settings O
: O
out O
- O
of O
- O
the O
- O
box O
( O
OOTB O
) O
calibration O
( O
i.e. O
, O
we O
directly O
evaluate O
off O
- O
the O
- O
shelf O
trained O
models O
) O
and O
post O
- O
hoc O
calibration O
-temperature B-MethodName
scaling I-MethodName
( O
TS B-MethodName
) O
( O
i.e. O
, O
we O
rescale O
logit O
vectors O
with O
a O
single O
temperature O
for O
all O
classes O
) O
. O
And O
we O
also O
experiment O
with O
Label B-MethodName
Smoothing I-MethodName
( O
LS B-MethodName
) O
( O
Pereyra O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2020 O
) O
compared O
to O
traditional O
MLE O
training O
. O
The O
models O
are O
trained O
on O
the O
ID O
training O
set O
for O
each O
task O
, O
and O
the O
performance O
is O
evaluated O
on O
the O
ID O
and O
OD O
test O
sets O
. O
Additionally O
, O
we O
present O
implementation O
details O
and O
case O
studies O
in O
the O
Appendix O
B O
and O
D O
. O

Table O
2 O
shows O
the O
comparison O
of O
experimental O
results O
( O
ECEs B-MetricName
) O
on O
BERT B-MethodName
and O
RoBERTa B-MethodName
. O
First O
, O
for O
OOTB O
calibration O
, O
we O
find O
that O
CME B-MethodName
achieves O
the O
lowest O
calibration B-MetricName
errors I-MetricName
in O
the O
ID O
datasets O
except O
for O
RoBERTa B-MethodName
in O
SWAG B-DatasetName
. O
At O
the O
same O
time O
, O
training O
with O
LS B-MethodName
( O
i.e. O
, O
CME+LS B-MethodName
) O
exhibits O
more O
improvements O
in O
the O
calibration O
compared O
with O
original O
models O
in O
the O
TPPDB B-DatasetName
and O
HellaSWAG B-DatasetName
datasets O
. O
However O
, O
in O
most O
cases O
, O
LS B-MethodName
models O
largely O
increase O
calibration B-MetricName
errors I-MetricName
for O
ID O
datasets O
. O
We O
conjecture O
that O
LS B-MethodName
may O
affect O
the O
smoothness O
of O
the O
gradient O
and O
thus O
produces O
poor O
calibrated O
results O
. O
Secondly O
, O
for O
post O
- O
hoc O
calibration O
, O
we O
observe O
that O
TS B-MethodName
always O
fails O
to O
correct O
miscalibrations O
of O
models O
with O
LS B-MethodName
( O
e.g. O
, O
CME B-MethodName
- I-MethodName
TS I-MethodName
0.64 B-MetricValue
vs. O
CME+LS B-MethodName
- I-MethodName
TS I-MethodName
2.16 B-MetricValue
in O
SNLI B-DatasetName
) O
under O
ID O
and O
OD O
settings O
. O
Nevertheless O
, O
TS B-MethodName
reduces O
the O
ECEs B-MetricName
in O
the O
OD O
setting O
by O
a O
large O
margin O
( O
e.g. O
, O
HellaSWAG B-DatasetName
BERT B-MethodName
11.64 B-MetricValue
→ O
2.11 B-MetricValue
) O
. O
Compared O
to O
baselines O
, O
CME B-MethodName
consistently O
improves O
over O
different O
tasks O
on O
calibration O
reduction O
of O
BERT B-MethodName
- O
based O
models O
. O
While O
we O
apply O
CME B-MethodName
to O
a O
relatively O
larger O
model O
, O
models O
with O
TS B-MethodName
may O
perform O
better O
. O
It O
indicates O
that O
our O
method O
can O
be O
complementary O
to O
these O
post O
- O
hoc O
calibration O
techniques O
. O
of O
our O
datasets O
. O
Our O
models O
have O
comparable O
accuracy B-MetricName
( O
even O
better O
) O
compared O
to O
fine O
- O
tuned O
counterparts O
. O
For O
example O
, O
RoBERTa B-MethodName
- I-MethodName
CME I-MethodName
has O
better O
accuracy B-MetricName
than O
RoBERTa B-MethodName
in O
the O
test O
set O
of O
the O
MNLI B-DatasetName
dataset O
( O
79.45 B-MetricValue
vs. O
78.79 B-MetricValue
) O
. O
Specifically O
, O
CME B-MethodName
performs O
poorly O
on O
the O
development O
set O
of O
Hel B-DatasetName
- I-DatasetName
laSWAG I-DatasetName
but O
performs O
comparably O
to O
baselines O
on O
the O
test O
set O
. O
As O
shown O
in O
Figure O
1 O
, O
we O
visualize O
the O
alignment O
between O
the O
posterior O
probability O
measured O
by O
the O
model O
confidence O
and O
the O
empirical O
output O
measured O
by O
the O
accuracy O
. O
Note O
that O
a O
perfectly O
calibrated O
model O
has O
confidence O
equals O
accuracy O
for O
each O
bucket O
. O
Our O
model O
performs O
well O
under O
both O
PLMs O
architectures O
. O
We O
observe O
that O
, O
in O
general O
, O
CME B-MethodName
helps O
calibrate O
the O
confidence O
of O
cases O
close O
to O
the O
decision O
boundary O
as O
it O
does O
not O
change O
most O
predictions O
. O
For O
example O
, O
compared O
to O
the O
baseline O
, O
CME B-MethodName
optimizes O
the O
samples O
whose O
predicted O
probabilities O
are O
higher O
than O
actual O
probabilities O
. O
Moreover O
, O
we O
find O
that O
training O
with O
label B-MethodName
smoothing I-MethodName
technique O
can O
make O
the O
model O
underestimates O
some O
examples O
with O
high O
predicted O
probabilities O
. O
In O
addition O
, O
we O
conducted O
preliminary O
experiments O
with O
different O
batch B-HyperparameterName
sizes I-HyperparameterName
, O
and O
found O
that O
more O
large O
sizes O
did O
not O
significantly O
impact O
calibration O
performance O
. O
On O
the O
other O
hand O
, O
we O
found O
that O
larger O
LMs O
usually O
achieve O
both O
higher O
accuracy B-MetricName
and O
better O
calibration O
performance O
( O
Table O
2 O
) O
, O
which O
is O
in O
line O
with O
the O
observation O
in O
question O
answering O
( O
Jiang O
et O
al O
. O
, O
2021 O
) O
. O

Analysis O

Related O
Work O

As O
accurate O
estimates O
are O
required O
for O
many O
difficult O
or O
sensitive O
prediction O
tasks O
( O
Platt O
, O
1999 O
) O
, O
probability O
calibration O
is O
an O
important O
uncertainty O
estimation O
task O
for O
NLP O
. O
Unlike O
other O
uncertainty O
estimation O
task O
( O
e.g. O
, O
out O
- O
of O
- O
domain O
detection O
, O
selective O
inference O
) O
, O
calibration O
focuses O
on O
aleatoric O
uncertainty O
measured O
by O
the O
probability O
of O
the O
prediction O
and O
adjusts O
the O
overall O
model O
confidence O
level O
( O
Hendrycks O
and O
Gimpel O
, O
2017 O
; O
Pereyra O
et O
al O
. O
, O
2017 O
; O
Guo O
et O
al O
. O
, O
2017 O
; O
Qin O
et O
al O
. O
, O
2021 O
) O
. O
For O
example O
, O
Gal O
and O
Ghahramani O
( O
2016 O
) O
propose O
to O
adopt O
multiple O
predictions O
with O
different O
dropout O
masks O
and O
then O
combine O
them O
to O
get O
the O
confidence O
estimate O
. O
Recently O
, O
several O
works O
focus O
on O
the O
calibration O
of O
PLMs O
models O
for O
NLP O
tasks O
( O
Hendrycks O
et O
al O
. O
, O
2019 O
; O
Desai O
and O
Durrett O
, O
2020 O
; O
Jung O
et O
al O
. O
, O
2020 O
; O
He O
et O
al O
. O
, O
2021 O
; O
Park O
and O
Caragea O
, O
2022 O
; O
Bose O
et O
al O
. O
, O
2022 O
) O
. O
Dan O
and O
Roth O
( O
2021 O
) O
investigate O
the O
calibration O
properties O
of O
different O
transformer O
architectures O
and O
sizes O
of O
BERT B-MethodName
. O
In O
line O
with O
recent O
work O
( O
Ye O
and O
Durrett O
, O
2022 O
) O
, O
our O
work O
focuses O
on O
how O
explanations O
can O
help O
calibration O
in O
three O
NLP O
tasks O
. O
However O
, O
we O
do O
not O
need O
to O
learn O
a O
calibrator O
by O
using O
model O
interpretations O
with O
heuristics O
, O
and O
also O
do O
not O
compare O
due O
to O
its O
intensive O
computation O
cost O
when O
generating O
attributions O
. O
In O
contrast O
, O
we O
explore O
whether O
model O
explanations O
are O
useful O
for O
calibrating O
black O
- O
box O
models O
during O
training O
. O

Conclusion O

We O
propose O
a O
method O
that O
leverages O
model O
attributions O
to O
address O
calibration O
estimates O
of O
PLMsbased O
models O
. O
Considering O
model O
attributions O
as O
facts O
about O
model O
behaviors O
, O
we O
show O
that O
CME B-MethodName
achieves O
the O
lowest O
ECEs B-MetricName
under O
most O
settings O
for O
two O
popular O
PLMs O
. O

Limitations O

Calibrated O
confidence O
is O
essential O
in O
many O
highstakes O
applications O
where O
incorrect O
predictions O
are O
highly O
problematic O
( O
e.g. O
, O
self O
- O
driving O
cars O
, O
medical O
diagnoses O
) O
. O
Though O
improving O
the O
performance O
on O
the O
calibration O
of O
pre O
- O
trained O
language O
models O
and O
achieving O
the O
comparable O
task O
performance O
, O
our O
explanation B-MethodName
- I-MethodName
based I-MethodName
calibration I-MethodName
method O
is O
still O
limited O
by O
the O
reliability O
and O
fidelity O
of O
interpretable O
methods O
. O
We O
adopt O
the O
scaled O
attention O
weight O
as O
the O
calculation O
method O
of O
attributions O
because O
( O
i O
) O
it O
has O
been O
shown O
to O
be O
more O
faithful O
in O
previous O
work O
( O
Chrysostomou O
and O
Aletras O
, O
2022 O
) O
, O
and O
( O
ii O
) O
the O
interpretation O
of O
the O
model O
is O
that O
the O
internal O
parameters O
of O
the O
model O
participate O
in O
the O
calculation O
and O
are O
derivable O
. O
Despite O
the O
above O
limitations O
, O
it O
does O
not O
undermine O
the O
main O
contribution O
of O
this O
paper O
, O
as O
involving O
explanations O
when O
training O
helps O
calibrate O
black O
- O
box O
models O
. O
Our O
approach O
can O
apply O
to O
most O
NLP O
models O
, O
incurs O
no O
additional O
overhead O
when O
testing O
, O
and O
is O
modularly O
pluggable O
. O
Another O
promising O
research O
direction O
is O
to O
explore O
using O
free O
- O
text O
explanations O
to O
help O
calibrate O
the O
model O
. O

Dataset O

Class O

Train B-HyperparameterName
/ O
Dev B-HyperparameterName
/ O
Test B-HyperparameterName
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
3 O
549367 B-HyperparameterValue
/ O
4921 B-HyperparameterValue
/ O
4921 B-HyperparameterValue
MNLI B-DatasetName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
3 O
391176 B-HyperparameterValue
/ O
4772 B-HyperparameterValue
/ O
4907 B-HyperparameterValue
QQP B-DatasetName
( O
Iyer O
et O
al O
. O
, O
2017 O
) O
2 O
363178 B-HyperparameterValue
/ O
20207 B-HyperparameterValue
/ O
20215 B-HyperparameterValue
TPPDB B-DatasetName
( O
Lan O
et O
al O
. O
, O
2017 O
) O
2 O
42200 B-HyperparameterValue
/ O
4685 B-HyperparameterValue
/ O
4649 B-HyperparameterValue
SWAG B-DatasetName
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
4 O
73546 B-HyperparameterValue
/ O
10003 B-HyperparameterValue
/ O
10003 B-HyperparameterValue
HellaSWAG B-DatasetName
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
4 O
39905 B-HyperparameterValue
/ O
5021 B-HyperparameterValue
/ O
5021 B-HyperparameterValue

A O
Dataset O
Statistics O

Table O
4 O
and O
Table O
5 O
present O
the O
characteristics O
of O
all O
datasets O
. O
The O
information O
across O
the O
three O
data O
splits O
includes O
the O
average O
sequence O
length O
and O
the O
number O
of O
examples O
under O
each O
label O
. O
Then O
we O
briefly O
introduce O
the O
datasets O
: O

Natural B-TaskName
Language I-TaskName
Inference I-TaskName
The O
in O
- O
domain O
dataset O
is O
the O
Stanford B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
( O
SNLI B-DatasetName
) O
dataset O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
. O
It O
is O
used O
to O
predict O
if O
the O
relationship O
between O
the O
hypothesis O
and O
the O
premise O
( O
i.e. O
, O
neutral O
, O
entailment O
and O
contradiction O
, O
) O
for O
natural B-MethodName
language I-MethodName
inference I-MethodName
task O
. O
The O
out O
- O
of O
- O
domain O
dataset O
is O
the O
Multi B-DatasetName
- I-DatasetName
Genre I-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
( O
MNLI B-DatasetName
) O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
which O
covers O
more O
diverse O
domains O
compared O
with O
SNLI B-DatasetName
. O

Paraphrase B-TaskName
Detection I-TaskName
The O
in O
- O
domain O
dataset O
is O
the O
Quora B-DatasetName
Question I-DatasetName
Pairs I-DatasetName
( O
QQP B-DatasetName
) O
dataset O
( O
Iyer O
et O
al O
. O
, O
2017 O
) O
. O
It O
is O
proposed O
to O
test O
if O
two O
questions O
are O
semantically O
equivalent O
as O
a O
paraphrase B-TaskName
detection I-TaskName
task O
. O
The O
out O
- O
of O
- O
domain O
dataset O
is O
the O
Twitter B-DatasetName
news I-DatasetName
URL I-DatasetName
Paraphrase I-DatasetName
Database I-DatasetName
( O
TPPDB B-DatasetName
) O
dataset O
( O
Lan O
et O
al O
. O
, O
2017 O
) O
. O
It O
is O
used O
to O
determine O
whether O
Twitter O
sentence O
pairs O
have O
similar O
semantics O
when O
they O
share O
URL O
and O
we O
set O
the O
label O
less O
than O
3 B-HyperparameterValue
as O
the O
first O
class O
, O
and O
the O
others O
as O
the O
second O
class O
following O
previous O
works O
. O

Commonsense B-TaskName
Reasoning I-TaskName

The O
in O
- O
domain O
dataset O
is O
the O
Situations B-DatasetName
With I-DatasetName
Adversarial I-DatasetName
Generations I-DatasetName
( O
SWAG B-DatasetName
) O
dataset O
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
. O

It O
is O
a O
popular O
benchmark O
for O
commonsense B-TaskName
reasoning I-TaskName
task O
where O
the O
objective O
is O
to O
pick O
the O
most O
logical O
continuation O
of O
a O
statement O
from O
a O
list O
of O
four O
options O
. O
The O
out O
- O
of O
- O
domain O
dataset O
is O
the O
HellaSWAG B-DatasetName
dataset O
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
. O
It O
is O
generated O
by O
adversarial O
filtering O
and O
is O
more O
challenging O
for O
out O
- O
of O
- O
domain O
generalization O
. O

B O
Experimental O
Settings O

For O
all O
experiments O
, O
we O
report O
the O
average O
performance O
results O
of O
five O
random O
seed O
initializations O
for O
a O
maximum O
of O
3 B-HyperparameterValue
epochs B-HyperparameterName
. O
For O
a O
fair O
comparison O
, O
we O
follow O
most O
of O
the O
hyperparameters O
of O
Desai O
and O
Durrett O
( O
2020 O
) O
unless O
reported O
below O
. O
For O
BERT B-MethodName
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
32 B-HyperparameterValue
( O
SNLI B-DatasetName
/ O
QQP B-DatasetName
) O
or O
8 B-HyperparameterValue
( O
SWAG B-DatasetName
) O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
the O
weight B-HyperparameterName
of I-HyperparameterName
gradient I-HyperparameterName
clip I-HyperparameterName
is O
1.0 B-HyperparameterValue
, O
and O
we O
exclude O
weight B-HyperparameterName
decay I-HyperparameterName
mechanism O
. O
For O
RoBERTa B-MethodName
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
( O
SNLI B-DatasetName
/ O
QQP B-DatasetName
) O
or O
8 B-HyperparameterValue
( O
SWAG B-DatasetName
) O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
the O
weight B-HyperparameterName
of I-HyperparameterName
gradient I-HyperparameterName
clip I-HyperparameterName
is O
1.0 B-HyperparameterValue
, O
and O
the O
weight B-HyperparameterName
decay I-HyperparameterName
is O
0.1 B-HyperparameterValue
. O
The O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
set O
to O
256 B-HyperparameterValue
. O
The O
optimal O
weights O
of O
λ B-HyperparameterName
in O
Eqn O
. O
3 O
are O
0.05 B-HyperparameterValue
and O
1.0 B-HyperparameterValue
for O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
respectively O
. O
We O
search O
the O
weight B-HyperparameterName
with O
respect O
to O
ECEs B-MetricName
on O
the O
development O
sets O
from O

C O
Ablation O
Study O

As O
shown O
in O
Figure O
2 O
, O
we O
find O
that O
using O
only O
confidence O
in O
Eqn O
. O
6 O
generally O
yields O
higher O
ECE B-MetricName
than O
other O
variants O
. O
Also O
using O
attention O
instead O
of O
scaled O
attention O
brings O
an O
increase O
in O
errors O
. O

D O
Case O
Study O

As O
shown O
in O
the O
Table O
6 O
, O
we O
list O
randomly O
- O
selected O
examples O
of O
BERT B-MethodName
- O
base O
models O
with O
MLE B-MethodName
and O
CME B-MethodName
. O
If O
models O
correctly O
predict O
the O
true O
label O
, O
the O
model O
confidence B-MetricName
should O
be O
greater O
than O
50 B-MetricValue
% I-MetricValue
. O
For O
example O
, O
in O
the O
second O
case O
of O
out O
- O
of O
- O
domain O
SNLI B-DatasetName
dataset O
, O
the O
model O
confidence O
of O
true O
label O
falls O
slightly O
below O
the O
borderline O
probability O
which O
results O
in O
an O
incorrect O
prediction O
( O
Probabilities O
: O
30.54 O
% O
, O
30.83 O
% O
, O
38.63 O
% O
vs. O
67.87 O
% O
, O
14.66 O
% O
, O
17.47 O
% O
) O
. O
In O
contrast O
, O
CME B-MethodName
leverages O
model O
explanation O
during O
training O
that O
helps O
calibrate O
the O
model O
confidence O
and O
predicts O
correctly O
. O

E O
Standard O
Deviations O

Table O
7 O
lists O
the O
standard O
deviations O
of O
each O
methods O
. O
We O
report O
the O
results O
across O
five O
runs O
with O
random B-HyperparameterName
seeds I-HyperparameterName
. O

Acknowledgements O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
and O
suggestions O
. O
This O
work O
is O
jointly O
supported O
by O
grants O
: O
National O
Key O
R O
& O
D O
Program O
of O
China O
( O
No O
. O
2021ZD0113301 O
) O
, O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62006061 O
) O
. O

Data O

Input O

True O
Label O
MLE B-MethodName
CME B-MethodName
SNLI B-DatasetName
Premise O
: O
The O
shadow O
silhouette O
of O
a O
woman O
standing O
near O
the O
water O
looking O
at O
a O
large O
attraction O
on O
the O
other O
side O
. O
Hypothesis O
: O
She O
is O
in O
the O
water O
. O
0.8 O
1.0 O
0.5 O
0.1 O
1.8 O
0.4 O
2.1 O
1.7 O
0.6 O
0.9 O
2.8 O
2.1 O
BERT+LS B-MethodName
0.3 O
0.5 O
0.4 O
0.7 O
1.0 O
1.1 O
1.4 O
0.9 O
0.8 O
0.7 O
0.6 O
0.9 O
Manifold B-MethodName
- I-MethodName
mixup I-MethodName
0.8 O
0.3 O
1.2 O
1.1 O
0.6 O
0.4 O
2.6 O
1.9 O
2.3 O
2.6 O
1.2 O
0.9 O
Manifold B-MethodName
- I-MethodName
mixup+LS I-MethodName
0.4 O
0.7 O
0.2 O
0.7 O
0.5 O
0.2 O
1.3 O
0.9 O
1.1 O
1.7 O
0.7 O
0.6 O
Park O
and O
Caragea O
( O
2022 O
) O
0.4 O
0.7 O
0.6 O
0.6 O
0.4 O
0.2 O
2.5 O
0.6 O
0.7 O
1.2 O
1.9 O
1.5 O
Park O
and O
Caragea O
( O
2022 O
) O
+LS O
0.3 O
1.0 O
0.9 O
0.1 O
0.7 O
0.3 O
1.0 O
0.5 O
1.0 O
1.1 O
0.8 O
0.7 O
CME B-MethodName
( O
Ours O
) O
0.3 O
0.2 O
0.5 O
0.1 O
0.2 O
0.2 O
0.3 O
0.6 O
0.8 O
0.2 O
1.8 O
0.4 O
CME+LS B-MethodName
( O
Ours O
) O
0.3 O
0.2 O
0.1 O
0.3 O
1.5 O
0.2 O
0.7 O
1.0 O
1.5 O
0.6 O
1.8 O
0.7 O

Methods O

In O
- O
Domain O
Out O
- O
of O
- O
Domain O
SNLI B-DatasetName
QQP B-DatasetName
SWAG B-DatasetName
MNLI B-DatasetName
TPPDB B-DatasetName
HellaSWAG B-DatasetName
OOTB B-MethodName
TS I-MethodName
OOTB B-MethodName
TS I-MethodName
OOTB B-MethodName
TS I-MethodName
OOTB B-MethodName
TS I-MethodName
OOTB B-MethodName
TS I-MethodName
OOTB B-MethodName
TS I-MethodName
RoBERTa B-MethodName
0.5 O
0.8 O
0.1 O
0.6 O
1.0 O
0.7 O
3.2 O
2.5 O
0.6 O
0.5 O
3.2 O
2.9 O
RoBERTa+LS B-MethodName
0.6 O
1.0 O
0.3 O
0.6 O
0.3 O
0.6 O
1.4 O
1.9 O
0.3 O
0.7 O
1.4 O
1.1 O
Manifold B-MethodName
- I-MethodName
mixup I-MethodName
0.8 O
0.4 O
0.5 O
0.6 O
1.2 O
0.3 O
3.1 O
1.3 O
1.8 O
2.1 O
2.8 O
1.5 O
Manifold B-MethodName
- I-MethodName
mixup+LS I-MethodName
1.0 O
0.9 O
0.7 O
0.6 O
1.5 O
0.4 O
1.6 O
1.0 O
0.9 O
1.1 O
0.6 O
1.6 O
Park O
and O
Caragea O
( O
2022 O
) O
0.7 O
0.5 O
0.6 O
0.2 O
0.1 O
0.2 O
1.9 O
1.4 O
0.9 O
1.2 O
1.8 O
1.5 O
Park O
and O
Caragea O
( O
2022 O
) O
+LS O
0.6 O
0.6 O
0.7 O
0.4 O
0.4 O
0.1 O
1.7 O
1.3 O
1.6 O
1.8 O
0.9 O
1.2 O
CME B-MethodName
( O
Ours O
) O
0.6 O
0.2 O
0.5 O
0.1 O
1.0 O
0.2 O
0.8 O
0.3 O
0.5 O
0.8 O
1.8 O
0.6 O
CME+LS B-MethodName
( O
Ours O
) O
0.4 O
0.3 O
0.4 O
0.2 O
0.9 O
0.2 O
0.6 O
1.0 O
0.6 O
0.4 O
1.3 O
1.6 O

Optimal O
Transport O
for O
Unsupervised B-TaskName
Hallucination I-TaskName
Detection I-TaskName
in O
Neural O
Machine O
Translation O

Neural O
machine O
translation O
( O
NMT O
) O
has O
become O
the O
de O
- O
facto O
standard O
in O
real O
- O
world O
machine O
translation O
applications O
. O
However O
, O
NMT O
models O
can O
unpredictably O
produce O
severely O
pathological O
translations O
, O
known O
as O
hallucinations O
, O
that O
seriously O
undermine O
user O
trust O
. O
It O
becomes O
thus O
crucial O
to O
implement O
effective O
preventive O
strategies O
to O
guarantee O
their O
proper O
functioning O
. O
In O
this O
paper O
, O
we O
address O
the O
problem O
of O
hallucination B-TaskName
detection I-TaskName
in O
NMT O
by O
following O
a O
simple O
intuition O
: O
as O
hallucinations O
are O
detached O
from O
the O
source O
content O
, O
they O
exhibit O
cross O
- O
attention O
patterns O
that O
are O
statistically O
different O
from O
those O
of O
good O
quality O
translations O
. O
We O
frame O
this O
problem O
with O
an O
optimal O
transport O
formulation O
and O
propose O
a O
fully O
unsupervised O
, O
plug O
- O
in O
detector O
that O
can O
be O
used O
with O
any O
attention O
- O
based O
NMT O
model O
. O
Experimental O
results O
show O
that O
our O
detector O
not O
only O
outperforms O
all O
previous O
model O
- O
based O
detectors O
, O
but O
is O
also O
competitive O
with O
detectors O
that O
employ O
external O
models O
trained O
on O
millions O
of O
samples O
for O
related O
tasks O
such O
as O
quality O
estimation O
and O
cross O
- O
lingual O
sentence O
similarity O
. O

Introduction O

Neural O
machine O
translation O
( O
NMT O
) O
has O
achieved O
tremendous O
success O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Kocmi O
et O
al O
. O
, O
2022 O
) O
, O
becoming O
the O
mainstream O
method O
in O
real O
- O
world O
applications O
and O
production O
systems O
for O
automatic O
translation O
. O
Although O
these O
models O
are O
becoming O
evermore O
accurate O
, O
especially O
in O
high O
- O
resource O
settings O
, O
they O
may O
unpredictably O
produce O
hallucinations O
. O
These O
are O
severely O
pathological O
translations O
that O
are O
detached O
from O
the O
source O
sequence O
content O
( O
Lee O
et O
al O
. O
, O
2018 O
; O
Müller O
et O
al O
. O
, O
2020 O
; O
Raunak O
et O
al O
. O
, O
2021 O
; O
Guerreiro O
et O
al O
. O
, O
2022 O
) O
. O
Crucially O
, O
these O
errors O
have O
the O
potential O
to O
seriously O
harm O
user O
trust O
in O
hard O
- O
to O
- O
predict O
ways O
( O
Perez O
et O
al O
. O
, O
2022 O
) O
, O
hence O
the O
evergrowing O
need O
to O
develop O
security O
mechanisms O
. O
One O
appealing O
strategy O
to O
address O
this O
issue O
is O
to O
develop O
effective O
on O
- O
the O
- O
fly O
detection O
systems O
. O

In O
this O
work O
, O
we O
focus O
on O
leveraging O
the O
crossattention O
mechanism O
to O
develop O
a O
novel O
hallucination O
detector O
. O
This O
mechanism O
is O
responsible O
for O
selecting O
and O
combining O
the O
information O
contained O
in O
the O
source O
sequence O
that O
is O
relevant O
to O
retain O
during O
translation O
. O
Therefore O
, O
as O
hallucinations O
are O
translations O
whose O
content O
is O
detached O
from O
the O
source O
sequence O
, O
it O
is O
no O
surprise O
that O
connections O
between O
anomalous O
attention O
patterns O
and O
hallucinations O
have O
been O
drawn O
before O
in O
the O
literature O
( O
Berard O
et O
al O
. O
, O
2019 O
; O
Raunak O
et O
al O
. O
, O
2021 O
; O
Ferrando O
et O
al O
. O
, O
2022 O
) O
. O
These O
patterns O
usually O
exhibit O
scattered O
source O
attention O
mass O
across O
the O
different O
tokens O
in O
the O
translation O
( O
e.g. O
most O
source O
attention O
mass O
is O
concentrated O
on O
a O
few O
irrelevant O
tokens O
such O
as O
punctuation O
and O
the O
end O
- O
of O
- O
sequence O
token O
) O
. O
Inspired O
by O
such O
observations O
, O
previous O
work O
has O
designed O
ad O
- O
hoc O
heuristics O
to O
detect O
hallucinations O
that O
specifically O
target O
the O
anomalous O
maps O
. O
While O
such O
heuristics O
can O
be O
used O
to O
detect O
hallucinations O
to O
a O
satisfactory O
extent O
( O
Guerreiro O
et O
al O
. O
, O
2022 O
) O
, O
we O
argue O
that O
a O
more O
theoretically O
- O
founded O
way O
of O
using O
anomalous O
attention O
information O
for O
hallucination B-TaskName
detection I-TaskName
is O
lacking O
in O
the O
literature O
. O

Rather O
than O
aiming O
to O
find O
particular O
patterns O
, O
we O
go O
back O
to O
the O
main O
definition O
of O
hallucinations O
and O
draw O
the O
following O
hypothesis O
: O
as O
hallucinationscontrary O
to O
good O
translations O
- O
are O
not O
supported O
by O
the O
source O
content O
, O
they O
may O
exhibit O
cross O
- O
attention O
patterns O
that O
are O
statistically O
different O
from O
those O
found O
in O
good O
quality O
translations O
. O
Based O
on O
this O
hypothesis O
, O
we O
approach O
the O
problem O
of O
hallucination B-TaskName
detection I-TaskName
as O
a O
problem O
of O
anomaly B-TaskName
detection I-TaskName
with O
an O
optimal O
transport O
( O
OT O
) O
formulation O
( O
Kantorovich O
, O
2006 O
; O
Peyré O
et O
al O
. O
, O
2019 O
) O
. O
Namely O
, O
we O
aim O
to O
find O
translations O
with O
source O
attention O
mass O
distributions O
that O
are O
highly O
distant O
from O
those O
of O
good O
translations O
. O
Intuitively O
, O
the O
more O
distant O
a O
translation O
's O
attention O
patterns O
are O
from O
those O
of O
good O
translations O
, O
the O
more O
anomalous O
it O
is O
in O
light O
of O
that O
distribution O
. O

Our O
key O
contributions O
are O
: O

• O
We O
propose O
an O
OT B-MethodName
- I-MethodName
inspired I-MethodName
fully I-MethodName
unsupervised I-MethodName
hallucination I-MethodName
detector I-MethodName
that O
can O
be O
plugged O
into O
any O
attention O
- O
based O
NMT O
model O
; O

• O
We O
find O
that O
the O
idea O
that O
attention O
maps O
for O
hallucinations O
are O
anomalous O
in O
light O
of O
a O
reference O
data O
distribution O
makes O
for O
an O
effective O
hallucination O
detector O
; O

• O
We O
show O
that O
our O
detector O
not O
only O
outperforms O
all O
previous O
model O
- O
based O
detectors O
, O
but O
is O
also O
competitive O
with O
external O
detectors O
that O
employ O
auxiliary O
models O
that O
have O
been O
trained O
on O
millions O
of O
samples O
. O
1 O

2 O
Background O

Cross O
- O
attention O
in O
NMT O
models O

A O
NMT O
model O
M O
defines O
a O
probability O
distribution O
p O
θ O
( O
y|x O
) O
over O
an O
output O
space O
of O
hypotheses O
Y O
conditioned O
on O
a O
source O
sequence O
x O
contained O
in O
an O
input O
space O
X O
. O
In O
this O
work O
, O
we O
focus O
on O
models O
parameterized O
by O
an O
encoder O
- O
decoder O
transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
a O
set O
of O
learned O
weights O
θ O
. O
In O
particular O
, O
we O
will O
look O
closely O
at O
the O
cross O
- O
attention O
mechanism O
, O
a O
core O
component O
of O
NMT O
models O
that O
has O
been O
extensively O
analysed O
in O
the O
literature O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
; O
Raganato O
and O
Tiedemann O
, O
2018 O
; O
Kobayashi O
et O
al O
. O
, O
2020 O
; O
Ferrando O
and O
Costa O
- O
jussà O
, O
2021 O
) O
. O
This O
mechanism O
is O
responsible O
for O
computing O
, O
at O
each O
generation O
step O
, O
a O
distribution O
over O
all O
source O
sentence O
words O
that O
informs O
the O
decoder O
on O
the O
relevance O
of O
each O
of O
those O
words O
to O
the O
current O
translation O
generation O
step O
. O
We O
follow O
previous O
work O
that O
has O
drawn O
connections O
between O
hallucinations O
and O
cross O
- O
attention O
( O
Berard O
et O
al O
. O
, O
2019 O
; O
Raunak O
et O
al O
. O
, O
2021 O
) O
, O
and O
focus O
specifically O
on O
the O
last O
layer O
of O
the O
decoder O
module O
. O
Concretely O
, O
for O
a O
source O
sequence O
of O
arbitrary O
length O
n O
and O
a O
target O
sequence O
of O
arbitrary O
length O
m O
, O
we O
will O
designate O
as O
Ω O
∈ O
[ O
0 O
, O
1 O
] O
m×n O
the O
matrix O
of O
attention O
weights O
that O
is O
obtained O
by O
averaging O
across O
all O
the O
cross O
- O
attention O
heads O
of O
the O
last O
layer O
of O
the O
decoder O
module O
. O
Further O
, O
given O
the O
model O
M O
we O
will O
designate O
π O
M O
( O
x O
) O
: O
= O
1 O
m O
[ O
Ω O
( O
x O
) O
] O
⊤ O
1 O
∈ O
△ O
n O
as O
the O
source O
( O
attention O
) O
mass O
distribution O
computed O
by O
M O
when O
x O
is O
presented O
as O
input O
, O
where O

△ O
n O
= O
{ O
p O
∈ O
R O
n O
| O
p O
≥ O
0 O
, O
1 O
⊤ O
p O
= O
1 O
} O
is O
the O
( O
n O
− O
1 O
) O
- O
dimensional O
probability O
simplex O
. O

1 O
Our O
code O
and O
data O
to O
replicate O
our O
experiments O
are O
available O
in O
https O
: O
/ O
/ O
github.com O
/ O
deep-spin O
/ O
ot O
- O
hallucination O
- O
detection O
. O

Optimal O
Transport O
Problem O
and O
Wasserstein O
Distance O

The O
first O
- O
order O
Wasserstein O
distance O
between O
two O
arbitrary O
probability O
distributions O
µ O
∈ O
△ O
n O
and O
ν O
∈ O
△ O
m O
is O
defined O
as O

W O
( O
µ O
, O
ν O
) O
= O
inf O
γ∈Π O
( O
µ O
, O
ν O
) O
E O
( O
u O
, O
v O
) O
∼γ O
[ O
c O
( O
u O
, O
v O
) O
] O
, O
( O
1 O
) O

where O
c O
: O
[ O
n O
] O
× O
[ O
m O
] O
→ O
R O
+ O
0 O
is O
a O
cost O
function O
, O
2 O
and O
Π O
( O
µ O
, O
ν O
) O
= O
{ O
γ O
∈ O
△ O
n×m O
: O
γ1 O
= O
µ O
; O
γ O
⊤ O
1 O
= O
ν O
} O
3 O
is O
the O
set O
of O
all O
joint O
probability O
distributions O
whose O
marginals O
are O
µ O
, O
ν O
. O
The O
Wasserstein O
distance O
arises O
from O
the O
method O
of O
optimal O
transport O
( O
OT O
) O
( O
Kantorovich O
, O
2006 O
; O
Peyré O
et O
al O
. O
, O
2019 O
) O
: O
OT O
measures O
distances O
between O
distributions O
in O
a O
way O
that O
depends O
on O
the O
geometry O
of O
the O
sample O
space O
. O
Intuitively O
, O
this O
distance O
indicates O
how O
much O
probability O
mass O
must O
be O
transferred O
from O
µ O
to O
ν O
in O
order O
to O
transform O
µ O
into O
ν O
while O
minimizing O
the O
transportation O
cost O
defined O
by O
c O
. O

A O
notable O
example O
is O
the O
Wasserstein-1 O
distance O
, O
W O
1 O
, O
also O
known O
as O
Earth O
Mover O
's O
Distance O
( O
EMD O
) O
, O
obtained O
for O
c O
( O
u O
, O
v O
) O
= O
∥u−v∥ O
1 O
. O
The O
name O
follows O
from O
the O
simple O
intuition O
: O
if O
the O
distributions O
are O
interpreted O
as O
" O
two O
piles O
of O
mass O
" O
that O
can O
be O
moved O
around O
, O
the O
EMD O
represents O
the O
minimum O
amount O
of O
" O
work O
" O
required O
to O
transform O
one O
pile O
into O
the O
other O
, O
where O
the O
work O
is O
defined O
as O
the O
amount O
of O
mass O
moved O
multiplied O
by O
the O
distance O
it O
is O
moved O
. O

Although O
OT O
has O
been O
explored O
for O
robustness O
( O
Paty O
and O
Cuturi O
, O
2019 O
; O
Staerman O
et O
al O
. O
, O
2021 O
) O
and O
out O
- O
of O
- O
distribution O
detection O
( O
Wang O
et O
al O
. O
, O
2021 O
; O
Yan O
et O
al O
. O
, O
2021 O
; O
Cheng O
et O
al O
. O
, O
2022 O
) O
in O
computer O
vision O
, O
the O
use O
of O
OT O
for O
anomaly B-TaskName
detection I-TaskName
in O
NLP O
applications O
remains O
largely O
overlooked O
. O

The O
problem O
of O
hallucinations O
in O
NMT O

Hallucinations O
are O
translations O
that O
lie O
at O
the O
extreme O
end O
of O
NMT O
pathologies O
( O
Raunak O
et O
al O
. O
, O
2021 O
) O
. O
Despite O
being O
a O
well O
- O
known O
issue O
, O
research O
on O
the O
phenomenon O
is O
hindered O
by O
the O
fact O
that O
these O
translations O
are O
rare O
, O
especially O
in O
highresource O
settings O
. O
As O
a O
result O
, O
data O
with O
hallucinations O
is O
scarce O
. O
To O
overcome O
this O
obstacle O
, O
many O
previous O
studies O
have O
focused O
on O
amplified O
settings O
where O
hallucinations O
are O
more O
likely O
to O
occur O
or O
are O
easier O
to O
detect O
. O
These O
include O
settings O
where O
( O
i O
) O
perturbations O
are O
induced O
either O
in O
the O
source O
sentence O
or O
in O
the O
target O
prefix O
( O
Lee O
et O
al O
. O
, O
2018 O
; O
Müller O
and O
Sennrich O
, O
2021 O
; O
Voita O
et O
al O
. O
, O
2021 O
; O
Ferrando O
et O
al O
. O
, O
2022 O
) O
; O
( O
ii O
) O
the O
training O
data O
is O
corrupted O
with O
noise O
( O
Raunak O
et O
al O
. O
, O
2021 O
) O
; O
( O
iii O
) O
the O
model O
is O
tested O
under O
domain O
shift O
( O
Wang O
and O
Sennrich O
, O
2020 O
; O
Müller O
et O
al O
. O
, O
2020 O
) O
; O
( O
iv O
) O
the O
detectors O
are O
validated O
on O
artificial O
hallucinations O
( O
Zhou O
et O
al O
. O
, O
2021 O
) O
. O
Nevertheless O
, O
these O
works O
have O
provided O
important O
insights O
towards O
better O
understanding O
of O
the O
phenomenon O
. O
For O
instance O
, O
it O
has O
been O
found O
that O
samples O
memorized O
by O
an O
NMT O
model O
are O
likely O
to O
generate O
hallucinations O
when O
perturbed O
( O
Raunak O
et O
al O
. O
, O
2021 O
) O
, O
and O
hallucinations O
are O
related O
to O
lower O
source O
contributions O
and O
over O
- O
reliance O
on O
the O
target O
prefix O
( O
Voita O
et O
al O
. O
, O
2021 O
; O
Ferrando O
et O
al O
. O
, O
2022 O
) O
. O

In O
this O
work O
, O
we O
depart O
from O
artificial O
settings O
, O
and O
focus O
on O
studying O
hallucinations O
that O
are O
naturally O
produced O
by O
the O
NMT O
model O
. O
To O
that O
end O
, O
we O
follow O
the O
taxonomy O
introduced O
in O
Raunak O
et O
al O
. O
( O
2021 O
) O
and O
later O
extended O
and O
studied O
in O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
. O
Under O
this O
taxonomy O
, O
hallucinations O
are O
translations O
that O
contain O
content O
that O
is O
detached O
from O
the O
source O
sentence O
. O
To O
disentangle O
the O
different O
types O
of O
hallucinations O
, O
they O
can O
be O
categorized O
as O
: O
largely O
fluent O
detached O
hallucinations O
or O
oscillatory O
hallucinations O
. O
The O
former O
are O
translations O
that O
bear O
little O
or O
no O
relation O
at O
all O
to O
the O
source O
content O
and O
may O
be O
further O
split O
according O
to O
the O
severity O
of O
the O
detachment O
( O
e.g. O
strong O
or O
full O
detachment O
) O
while O
the O
latter O
are O
inadequate O
translations O
that O
contain O
erroneous O
repetitions O
of O
words O
and O
phrases O
. O
We O
illustrate O
in O
Appendix O
A O
the O
categories O
described O
above O
through O
examples O
of O
hallucinated O
outputs O
. O

On B-TaskName
- I-TaskName
the I-TaskName
- I-TaskName
fly I-TaskName
detection I-TaskName
of I-TaskName
hallucinations I-TaskName

On O
- O
the O
- O
fly O
hallucination O
detectors O
are O
systems O
that O
can O
detect O
hallucinations O
without O
access O
to O
reference O
translations O
. O
These O
detectors O
are O
particularly O
relevant O
as O
they O
can O
be O
deployed O
in O
online O
applications O
where O
references O
are O
not O
readily O
available O
. O
4 O

Categorization O
of O
hallucination O
detectors O

Previous O
work O
on O
on O
- O
the O
- O
fly O
detection O
of O
hallucinations O
in O
NMT O
has O
primarily O
focused O
on O
two O
categories O
of O
detectors O
: O
external O
detectors O
and O
modelbased O
detectors O
. O
External O
detectors O
employ O
auxiliary O
models O
trained O
for O
related O
tasks O
such O
as O
quality O
estimation O
( O
QE O
) O
and O
cross O
- O
lingual O
embedding O
similarity O
. O
On O
the O
other O
hand O
, O
model O
- O
based O
detectors O
only O
require O
access O
to O
the O
NMT O
model O
that O
generates O
the O
translations O
, O
and O
work O
by O
leveraging O
relevant O
internal O
features O
such O
as O
model O
confidence O
and O
cross O
- O
attention O
. O
These O
detectors O
are O
attractive O
due O
to O
their O
flexibility O
and O
low O
memory O
footprint O
, O
as O
they O
can O
very O
easily O
be O
plugged O
in O
on O
a O
vast O
range O
of O
NMT O
models O
without O
the O
need O
for O
additional O
training O
data O
or O
computing O
infrastructure O
. O
Moreover O
, O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
show O
that O
model O
- O
based O
detectors O
can O
be O
predictive O
of O
hallucinations O
, O
outperforming O
QE O
models O
and O
even O
performing O
on O
par O
with O
state O
- O
of O
- O
the O
- O
art O
reference O
- O
based O
metrics O
. O

Problem O
Statement O

We O
will O
focus O
specifically O
on O
model O
- O
based O
detectors O
that O
require O
obtaining O
internal O
features O
from O
a O
model O
M. O
Building O
a O
hallucination O
detector O
generally O
consists O
of O
finding O
a O
scoring O
function O
s O
M O
: O
X O
→ O
R O
and O
a O
threshold O
τ O
∈ O
R O
to O
build O
a O
binary O
rule O
g O
M O
: O
X O
→ O
{ O
0 O
, O
1 O
} O
. O
For O
a O
given O
test O
sample O
x O
∈ O
X O
, O

g O
M O
( O
x O
) O
= O
1 O
{ O
s O
M O
( O
x O
) O
> O
τ O
} O
. O
( O
2 O
) O

If O
s O
M O
is O
an O
anomaly O
score O
, O
g O
M O
( O
x O
) O
= O
0 O
implies O
that O
the O
model O
M O
generates O
a O
' O
normal O
' O
translation O
for O
the O
source O
sequence O
x O
, O
and O
g O
M O
( O
x O
) O
= O
1 O
implies O
that O
M O
generates O
a O
' O
hallucination O
' O
instead O
. O
5 O

Unsupervised B-TaskName
Hallucination I-TaskName
Detection I-TaskName
with O
Optimal O
Transport O

Anomalous O
cross O
- O
attention O
maps O
have O
been O
connected O
to O
the O
hallucinatory O
mode O
in O
several O
works O
( O
Lee O
et O
al O
. O
, O
2018 O
; O
Berard O
et O
al O
. O
, O
2019 O
; O
Raunak O
et O
al O
. O
, O
2021 O
) O
. O
Our O
method O
builds O
on O
this O
idea O
and O
uses O
the O
Wasserstein O
distance O
to O
estimate O
the O
cost O
of O
transforming O
a O
translation O
source O
mass O
distribution O
into O
a O
reference O
distribution O
. O
Intuitively O
, O
the O
higher O
the O
cost O
of O
such O
transformation O
, O
the O
more O
distant O
- O
and O
hence O
the O
more O
anomalous O
- O
the O
attention O
of O
the O
translation O
is O
with O
respect O
to O
that O
of O
the O
reference O
translation O
. O

Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
: O
A O
data O
independent O
scenario O

In O
this O
scenario O
, O
we O
only O
rely O
on O
the O
generated O
translation O
and O
its O
source O
mass O
distribution O
to O
decide O
whether O
the O
translation O
is O
a O
hallucination O
or O
not O
. O
Concretely O
, O
for O
a O
given O
test O
sample O
x O
∈ O
X O
: O
1 O
. O
We O
first O
obtain O
the O
source O
mass O
attention O
distribution O
π O
M O
( O
x O
) O
∈ O
△ O
|x| O
; O

2 O
. O
We O
then O
compute O
an O
anomaly O
score O
, O
s O
wtu O
( O
x O
) O
, O
by O
measuring O
the O
Wasserstein O
distance O
between O
π O
M O
( O
x O
) O
and O
a O
reference O
distribution O
u O
: O

s O
wtu O
( O
x O
) O
= O
W O
( O
π O
M O
( O
x O
) O
, O
u O
) O
. O
( O
3 O
) O

Choice O
of O
reference O
translation O
. O
A O
natural O
choice O
for O
u O
is O
the O
uniform O
distribution O
, O
u O
= O
1 O
n O
• O
1 O
, O
where O
1 O
is O
a O
vector O
of O
ones O
of O
size O
n. O
In O
the O
context O
of O
our O
problem O
, O
a O
uniform O
source O
mass O
distribution O
means O
that O
all O
source O
tokens O
are O
equally O
attended O
. O

Choice O
of O
cost O
function O
. O
We O
consider O
the O
0 O
/ O
1 O
cost O
function O
, O
c O
( O
i O
, O
j O
) O
= O
1 O
[ O
i O
̸ O
= O
j O
] O
, O
as O
it O
guarantees O
that O
the O
cost O
of O
transporting O
a O
unit O
mass O
from O
any O
token O
i O
to O
any O
token O
j O
̸ O
= O
i O
is O
constant O
. O
For O
this O
distance O
function O
, O
the O
problem O
in O
Equation O
1 O
has O
the O
following O
closed O
- O
form O
solution O
( O
Villani O
, O
2009 O
) O
: O

W O
( O
π O
M O
( O
x O
) O
, O
u O
) O
= O
1 O
/ O
2 O
∥π O
M O
( O
x O
) O
− O
u∥ O
1 O
. O
( O
4 O
) O

This O
is O
a O
well O
- O
known O
result O
in O
optimal O
transport O
: O
the O
Wasserstein O
distance O
under O
the O
0 O
/ O
1 O
cost O
function O
is O
equivalent O
to O
the O
total O
variation O
distance O
between O
the O
two O
distributions O
. O
On O
this O
metric O
space O
, O
the O
Wasserstein O
distance O
depends O
solely O
on O
the O
probability O
mass O
that O
is O
transported O
to O
transform O
π O
M O
( O
x O
) O
to O
u. O
Importantly O
, O
this O
formulation O
ignores O
the O
starting O
locations O
and O
destinations O
of O
that O
probability O
mass O
as O
the O
cost O
of O
transporting O
a O
unit O
mass O
from O
any O
token O
i O
to O
any O
token O
j O
̸ O
= O
i O
is O
constant O
. O

Interpretation O
of O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
. O
Attention O
maps O
for O
which O
the O
source O
attention O
mass O
is O
highly O
concentrated O
on O
a O
very O
sparse O
set O
of O
tokens O
( O
regardless O
of O
their O
location O
in O
the O
source O
sentence O
) O
can O
be O
very O
predictive O
of O
hallucinations O
( O
Berard O
et O
al O
. O
, O
2019 O
; O
Guerreiro O
et O
al O
. O
, O
2022 O
) O
. O
Thus O
, O
the O
bigger O
the O
distance O
between O
the O
source O
mass O
distribution O
of O
a O
test O
sample O
and O
the O
uniform O
distribution O
, O
the O
more O
peaked O
the O
former O
is O
, O
and O
hence O
the O
closer O
it O
is O
to O
such O
predictive O
patterns O
. O

Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
: O
A O
data O
- O
driven O
scenario O

In O
this O
scenario O
, O
instead O
of O
using O
a O
single O
reference O
distribution O
, O
we O
use O
a O
set O
of O
reference O
source O
mass O
distributions O
, O
R O
x O
, O
obtained O
with O
the O
same O
model O
. O
By O
doing O
so O
, O
we O
can O
evaluate O
how O
anomalous O
a O
given O
translation O
is O
compared O
to O
a O
model O
data O
- O
driven O
distribution O
, O
rather O
than O
relying O
on O
an O
arbitrary O
choice O
of O
reference O
distribution O
. O

First O
, O
we O
use O
a O
held O
- O
out O
dataset O
D O
held O
that O
contains O
samples O
for O
which O
the O
model O
M O
generates O
good O
quality O
translations O
according O
to O
an O
automatic O
evaluation O
metric O
( O
in O
this O
work O
, O
we O
use O
COMET B-MetricName
( O
Rei O
et O
al O
. O
, O
2020 O
) O
) O
. O
We O
use O
this O
dataset O
to O
construct O
( O
offline O
) O
a O
set O
of O
held O
- O
out O
source O
attention O
distributions O
R O
held O
= O
{ O
π O
M O
( O
x O
) O
∈ O
△ O
|x| O
: O
x O
∈ O
D O
held O
} O
. O
Then O
, O
for O
a O
given O
test O
sample O
x O
∈ O
X O
, O
we O
apply O
the O
procedure O
illustrated O
in O
Figure O
1 O
: O
1 O
. O
We O
generate O
a O
translationŷ O
= O
( O
y O
1 O
, O
. O
. O
. O
, O
y O
m O
) O
and O
obtain O
the O
source O
mass O
attention O
distribution O
π O
M O
( O
x O
) O
∈ O
△ O
|x| O
; O
2 O
. O
We O
apply O
a O
length O
filter O
to O
construct O
the O
sample O
reference O
set O
R O
x O
, O
by O
restricting O
R O
x O
to O
contain O
source O
mass O
distributions O
of O
R O
held O
correspondent O
to O
translations O
of O
size O

[ O
( O
1 O
− O
δ B-HyperparameterName
) O
m O
, O
( O
1 O
+ O
δ B-HyperparameterName
) O
m O
] O
for O
a O
predefined O
δ B-HyperparameterName
∈ O
] O
0 O
, O
1 O
[ O
; O
6 O

3 O
. O
We O
compute O
pairwise O
Wasserstein-1 O
distances O
between O
π O
M O
( O
x O
) O
and O
each O
element O
r O
i O
of O
R O
x O
: O

W O
x O
= O
W O
1 O
( O
π O
M O
( O
x O
) O
, O
r O
1 O
) O
, O
. O
. O
. O
, O
( O
5 O
) O

W O
1 O
( O
π O
M O
( O
x O
) O
, O
r O
|Rx| O
) O
. O

4 O
. O
We O
obtain O
the O
anomaly O
score O
s O
wtd O
( O
x O
) O
by O
averaging O
the O
bottom O
- O
k O
distances O
in O
W O
x O
: O

s O
wtd O
( O
x O
) O
= O
1 O
k O
s O
i O
∈S O
s O
i O
, O
( O
6 O

where O
S O
is O
the O
set O
containing O
the O
k O
smallest O
elements O
of O
W O
x O
. O

Interpretation O
of O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
. O
Hallucinations O
, O
unlike O
good O
translations O
, O
are O
not O
fully O
supported O
by O
the O
source O
content O
. O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
evaluates O
how O
anomalous O
a O
translation O
is O
by O
comparing O
the O
source O
attention O
mass O
distribution O
of O
that O
translation O
to O
those O
of O
good O
translations O
. O
The O
higher O
the O
Wassto B-MethodName
- I-MethodName
Data I-MethodName
score O
, O
the O
more O
anomalous O
the O
source O
attention O
mass O
distribution O
of O
that O
translation O
is O
in O
comparison O
to O
those O
of O
good O
translations O
, O
and O
the O
more O
likely O
it O
is O
to O
be O
an O
hallucination O
. O

Relation O
to O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
. O
The O
Wasserstein-1 O
distance O
( O
see O
Section O
2.2 O
) O
between O
two O
distributions O
is O
equivalent O
to O
the O
ℓ O
1 O
-norm O
of O
the O
difference O
between O
their O
cumulative O
distribution O
functions O
( O
Peyré O
and O
Cuturi O
, O
2018 O
) O
. O
Note O
that O
this O
is O
different O
from O
the O
result O
in O
Equation O
4 O
, O
as O
the O
Wasserstein O
distance O
under O
c O
( O
i O
, O
j O
) O
= O
1 O
[ O
i O
̸ O
= O
j O
] O
as O
the O
cost O
function O
is O
proportional O
to O
the O
norm O
of O
the O
difference O
between O
their O
probability O
mass O
functions O
. O
Thus O
, O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
will O
be O
more O
sensitive O
to O
the O
overall O
structure O
of O
the O
distributions O
( O
e.g. O
sharp O
probability O
peaks O
around O
some O
points O
) O
, O
whereas O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
will O
be O
more O
sensitive O
to O
the O
specific O
values O
of O
the O
points O
in O
the O
two O
distributions O
. O

Wass B-MethodName
- I-MethodName
Combo I-MethodName
: O
The O
best O
of O
both O
worlds O

With O
this O
scoring O
function O
, O
we O
aim O
at O
combining O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
and O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
into O
a O
single O
detector O
. O
To O
do O
so O
, O
we O
propose O
using O
a O
two O
- O
stage O
process O
that O
exploits O
the O
computational O
benefits O
of O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
over O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
. O
7 O
Put O
simply O
, O
( O
i O
) O
we O
start O
by O
assessing O
whether O
a O
test O
sample O
is O
deemed O
a O
hallucination O
according O
to O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
, O
and O
if O
not O
( O
ii O
) O
we O
compute O
the O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
score O
. O
Formally O
, O

s O
wc O
( O
x O
) O
= O
1 O
[ O
s O
wtu O
( O
x O
) O
> O
τ B-HyperparameterName
wtu I-HyperparameterName
] O
×s O
wtu O
( O
x O
) O
( O
7 O
) O
+ O
1 O
[ O
s O
wtu O
( O
x O
) O
≤ O
τ B-HyperparameterName
wtu I-HyperparameterName
] O
× O
s O
wtd O
( O
x O
) O

for O
a O
predefined O
scalar O
threshold O
τ B-HyperparameterName
wtu I-HyperparameterName
. O
To O
set O
that O
threshold O
, O
we O
compute O
W O
wtu O
= O
{ O
s O
wtu O
( O
x O
) O
: O
x O
∈ O
D O
held O
} O
and O
set O
τ O
wtu O
= O
P O
K O
, O
i.e O
τ O
wtu O
is O
the O
K O
th O
percentile O
of O
W O
wtu O
with O
K O
∈ O
] O
98 O
, O
100 O
[ O
( O
in O
line O
with O
hallucinatory O
rates O
reported O
in O
( O
Müller O
et O
al O
. O
, O
2020 O
; O
Wang O
and O
Sennrich O
, O
2020 O
; O
Raunak O
et O
al O
. O
, O
2022 O
) O
) O
. O
8 O

5 O
Experimental O
Setup O

Model O
and O
Data O

We O
follow O
the O
setup O
in O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
. O
In O
that O
work O
, O
the O
authors O
released O
a O
dataset O
of O
3415 O
translations O
for O
WMT18 B-DatasetName
DE I-DatasetName
- I-DatasetName
EN I-DatasetName
news O
translation O
data O
( O
Bojar O
et O
al O
. O
, O
2018 O
) O
with O
annotations O
on O
critical O
errors O
and O
hallucinations O
. O
Our O
analysis O
in O
the O
main O
text O
focuses O
on O
this O
dataset O
as O
it O
is O
the O
only O
available O
dataset O
that O
contains O
human O
annotations O
on O
hallucinations O
produced O
naturally O
by O
an O
NMT O
model O
( O
we O
provide O
full O
details O
about O
the O
dataset O
and O
the O
model O
in O
Appendix O
A O
) O
. O
Nevertheless O
, O
in O
order O
to O
access O
the O
broader O
validity O
of O
our O
methods O
for O
other O
low O
and O
mid O
- O
resource O
language O
pairs O
and O
models O
, O
we O
follow O
a O
similar O
setup O
to O
that O
of O
Tang O
et O
al O
. O
( O
2022 O
) O
in O
which O
quality O
assessments O
are O
converted O
to O
hallucination O
annotations O
. O
For O
those O
experiments O
, O
we O
use O
the O
RO O
- O
EN O
( O
mid O
- O
resource O
) O
and O
NE O
- O
EN O
( O
low O
- O
resource O
) O
translations O
from O
the O
MLQE B-DatasetName
- I-DatasetName
PE I-DatasetName
dataset O
. O
In O
Appendix O
J O
, O
we O
present O
full O
details O
on O
the O
setup O
and O
report O
the O
results O
of O
these O
experiments O
. O
Importantly O
, O
our O
empirical O
observations O
are O
similar O
to O
those O
of O
the O
main O
text O
. O
For O
all O
our O
experiments O
, O
we O
obtain O
all O
modelbased O
information O
required O
to O
build O
the O
detectors O
using O
the O
same O
models O
that O
generated O
the O
translations O
in O
consideration O
. O
We O
display O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
and O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
scores O
on O
log O
- O
scale O
. O

Baseline O
detectors O

Model O
- O
based O
detectors O

We O
compare O
our O
methods O
to O
the O
two O
best O
performing O
model O
- O
based O
methods O
in O
Guerreiro O
et O
al O
. O

( O
2022 O
) O
. O
9 O

Attn B-MethodName
- I-MethodName
ign I-MethodName
- I-MethodName
SRC I-MethodName
. O
This O
method O
consists O
of O
computing O
the O
proportion O
of O
source O
words O
with O
a O
total O
incoming O
attention O
mass O
lower O
than O
a O
threshold O
λ B-HyperparameterName
: O

s O
ais O
( O
x O
) O
= O
1 O
n O
n O
j=1 O
1 O
( O
Ω O
⊤ O
( O
x O
) O
1 O
) O
j O
< O
λ B-HyperparameterName
. O
( O
8 O
) O

This O
method O
was O
initially O
proposed O
in O
Berard O
et O
al O
. O
( O
2019 O
) O
. O
We O
follow O
their O
work O
and O
use O
λ B-HyperparameterName
= O
0.2 B-HyperparameterValue
. O

Seq B-MethodName
- I-MethodName
Logprob I-MethodName
. O
We O
compute O
the O
length O
- O
normalised O
sequence O
log O
- O
probability O
of O
the O
translation O
: O

s O
slp O
( O
x O
) O
= O
1 O
m O
m O
k=1 O
log O
p O
θ O
( O
y O
k O
| O
y O
< O
k O
, O
x O
) O
. O
( O
9 O
) O

External O
detectors O

We O
provide O
a O
comparison O
to O
detectors O
that O
exploit O
state O
- O
of O
- O
the O
- O
art O
models O
in O
related O
tasks O
, O
as O
it O
helps O
monitor O
the O
development O
of O
model O
- O
based O
detectors O
. O

CometKiwi B-MethodName
. O
We O
compute O
sentence O
- O
level O
quality O
scores O
with O
CometKiwi O
( O
Rei O
et O
al O
. O
, O
2022 O
) O
, O
the O
winning O
reference O
- O
free O
model O
of O
the O
WMT22 O
QE O
shared O
task O
( O
Zerva O
et O
al O
. O
, O
2022 O
) O
. O
It O
has O
more O
than O
565 O
M O
parameters O
and O
it O
was O
trained O
on O
more O
than O
1 O
M O
human O
quality O
annotations O
. O
Importantly O
, O
this O
training O
data O
includes O
human O
annotations O
for O
several O
low O
- O
quality O
translations O
and O
hallucinations O
. O

LaBSE B-MethodName
. O
We O
leverage O
LaBSE B-MethodName
( O
Feng O
et O
al O
. O
, O
2020 O
) O
to O
compute O
cross O
- O
lingual O
sentence O
representations O
for O
the O
source O
sequence O
and O
translation O
. O
We O
use O
the O
cosine O
similarity O
of O
these O
representations O
as O
the O
detection O
score O
. O
The O
model O
is O
based O
on O
the O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
architecture O
and O
was O
trained O
on O
more O
than O
20 O
billion O
sentences O
. O
LaBSE B-MethodName
makes O
for O
a O
good O
baseline O
, O
as O
it O
was O
optimized O
in O
a O
self O
- O
supervised O
way O
with O
a O
translate O
matching O
objective O
that O
is O
very O
much O
aligned O
with O
the O
task O
of O
hallucination O
detection O
: O
during O
training O
, O
LaBSE B-MethodName
is O
given O
a O
source O
sequence O
and O
a O
set O
of O
translations O
including O
the O
true O
translation O
and O
multiple O
negative O
alternatives O
, O
and O
the O
model O
is O
optimized O
to O
specifically O
discriminate O
the O
true O
translation O
from O
the O
other O
negative O
alternatives O
by O
assigning O
a O
higher O
similarity O
score O
to O
the O
former O
. O

Evaluation O
metrics O

We O
report O
the O
Area B-MetricName
Under I-MetricName
the I-MetricName
Receiver I-MetricName
Operating I-MetricName
Characteristic I-MetricName
curve I-MetricName
( O
AUROC B-MetricName
) O
and O
the O
False B-MetricName
Positive I-MetricName
Rate I-MetricName
at I-MetricName
90 I-MetricName
% I-MetricName
True I-MetricName
Positive I-MetricName
Rate I-MetricName
( O
FPR B-MetricName
@ I-MetricName
90TPR I-MetricName
) O
to O
evaluate O
the O
performance O
of O
different O
detectors O
. O

Implementation O
Details O

We O
use O
WMT18 B-DatasetName
DE I-DatasetName
- I-DatasetName
EN I-DatasetName
data O
samples O
from O
the O
heldout O
set O
used O
in O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
, O
and O
construct O
D O
held O
to O
contain O
the O
250k O
samples O
with O
highest O
COMET B-MetricName
score O
. O
To O
obtain O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
scores O
, O
we O
set O
δ B-HyperparameterName
= O
0.1 B-HyperparameterValue
, O
|R| B-HyperparameterName
max I-HyperparameterName
= O
1000 B-HyperparameterValue
and O
k B-HyperparameterName
= O
4 B-HyperparameterValue
. O
To O
obtain O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Combo I-MethodName
scores O
, O
we O
set O
τ B-HyperparameterName
wtu I-HyperparameterName
= O
P B-HyperparameterValue
99.9 I-HyperparameterValue
. O
We O
perform O
extensive O
ablations O
on O
the O
construction O
of O
R O
held O
and O
on O
all O
other O
hyperparameters O
in O
Appendix O
G. O
We O
also O
report O
the O
computational O
runtime O
of O
our O
methods O
in O
Appendix O
D O
. O

6 O
Results O

Performance O
on O
on O
- O
the O
- O
fly O
detection O

We O
start O
by O
analyzing O
the O
performance O
of O
our O
proposed O
detectors O
on O
a O
real O
world O
on O
- O
the O
- O
fly O
detection O
scenario O
. O
In O
this O
scenario O
, O
the O
detector O
must O
be O
able O
to O
flag O
hallucinations O
regardless O
of O
their O
specific O
type O
as O
those O
are O
unknown O
at O
the O
time O
of O
detection O
. O

Wass B-MethodName
- I-MethodName
Combo I-MethodName
is O
the O
best O
model O
- O
based O
detector O
. O
other O
methods O
both O
in O
terms O
of O
AUROC B-MetricName
and O
FPR B-MetricName
. O

When O
compared O
to O
the O
previous O
best O
- O
performing O
model O
- O
based O
method O
( O
Seq B-MethodName
- I-MethodName
Logprob I-MethodName
) O
, O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
obtains O
boosts O
of O
approximately O
4 B-MetricValue
and O
10 B-MetricValue
points O
in O
AUROC B-MetricName
and O
FPR B-MetricName
, O
respectively O
. O
These O
performance O
boosts O
are O
further O
evidence O
that O
model O
- O
based O
features O
can O
be O
leveraged O
, O
in O
an O
unsupervised O
manner O
, O
to O
build O
effective O
detectors O
. O
Nevertheless O
, O
the O
high O
values O
of O
FPR B-MetricName
suggest O
that O
there O
is O
still O
a O
significant O
performance O
margin O
to O
reduce O
in O
future O
research O
. O

The O
notion O
of O
data O
proximity O
is O
helpful O
to O
detect O
hallucinations O
. O
Table O
1 O
shows O
that O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
outperforms O
the O
previous O
best O
- O
performing O
modelbased O
method O
( O
Seq B-MethodName
- I-MethodName
Logprob I-MethodName
) O
in O
both O
AUROC B-MetricName
and O
FPR B-MetricName
( O
by O
more B-MetricValue
than I-MetricValue
10 I-MetricValue
% I-MetricValue
) O
. O
This O
supports O
the O
idea O
that O
cross O
- O
attention O
patterns O
for O
hallucinations O
are O
anomalous O
with O
respect O
to O
those O
of O
good O
modelgenerated O
translations O
, O
and O
that O
our O
method O
can O
effectively O
measure O
this O
level O
of O
anomalousness O
. O
On O
the O
other O
hand O
, O
compared O
to O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Uni I-MethodName
, O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
shows O
a O
significant O
improvement O
of O
30 B-MetricValue
FPR B-MetricName
points O
. O
This O
highlights O
the O
effectiveness O
of O
leveraging O
the O
data O
- O
driven O
distribution O
of O
good O
translations O
instead O
of O
the O
ad O
- O
hoc O
uniform O
distribution O
. O
Nevertheless O
, O
Table O
1 O
and O
Figure O
2 O
show O
that O
combining O
both O
methods O
brings O
further O
performance O
improvements O
. O
This O
suggests O
that O
these O
methods O
may O
specialize O
in O
different O
types O
of O
hallucinations O
, O
and O
that O
combining O
them O
allows O
for O
detecting O
a O
broader O
range O
of O
anomalies O
. O
We O
will O
analyze O
this O
further O
in O
Section O
6.2 O
. O

Our O
model O
- O
based O
method O
achieves O
comparable O
performance O
to O
external O
models O
. O
Table O
1 O
shows O
that O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
outperforms O
CometKiwi B-MethodName
, O
with O
significant O
improvements O
on O
FPR B-MetricName
. O
However O
, O
there O
still O
exists O
a O
gap O
to O
LaBSE B-MethodName
, O
the O
best O
overall O
detector O
. O
This O
performance O
gap O
indicates O
that O
more O
powerful O
detectors O
can O
be O
built O
, O
paving O
the O
way O
for O
future O
work O
in O
model O
- O
based O
hallucination O
detection O
. O
Nevertheless O
, O
while O
relying O
on O
external O
models O
seems O
appealing O
, O
deploying O
and O
serving O
them O
in O
practice O
usually O
comes O
with O
additional O
infrastructure O
costs O
, O
while O
our O
detector O
relies O
on O
information O
that O
can O
be O
obtained O
when O
generating O
the O
translation O
. O

Translation O
quality O
assessments O
are O
less O
predictive O
than O
similarity O
of O
cross O
- O
lingual O
sentence O
representations O
. O
Table O
1 O
shows O
that O
LaBSE B-MethodName
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
quality O
estimation O
system O
CometKiwi B-MethodName
, O
with O
vast O
improvements O
in O
terms O
of O
FPR B-MetricName
. O
This O
shows O
that O
for O
hallucination B-TaskName
detection I-TaskName
, O
quality O
assessments O
obtained O
with O
a O
QE O
model O
are O
less O
predictive O
than O
the O
similarity O
between O
crosslingual O
sentence O
representations O
. O
This O
may O
be O
explained O
through O
their O
training O
objectives O
( O
see O
Section O
5.2.2 O
) O
: O
while O
CometKiwi B-MethodName
employs O
a O
more O
general O
regression O
objective O
in O
which O
the O
model O
is O
trained O
to O
match O
human O
quality O
assessments O
, O
LaBSE B-MethodName
is O
trained O
with O
a O
translate O
matching O
training O
objective O
that O
is O
very O
closely O
related O
to O
the O
task O
of O
hallucination B-TaskName
detection I-TaskName
. O

Do O
detectors O
specialize O
in O
different O
types O
of O
hallucinations O
? O

In O
this O
section O
, O
we O
present O
an O
analysis O
on O
the O
performance O
of O
different O
detectors O
for O
different O
types O
of O
hallucinations O
( O
see O
Section O
2.3 O
) O
. O
We O
report O
both O
a O
quantitative O
analysis O
to O
understand O
whether O
a O
detector O
can O
distinguish O
a O
specific O
hallucination O
type O
from O
other O
translations O
( O
Table O
2 O
) O
, O
and O
a O
qualitative O
analysis O
on O
a O
fixed O
- O
threshold O
scenario O
10 O
( O
Figure O
3 O
) O
. O
This O
analysis O
is O
particularly O
relevant O
to O
better O
understand O
how O
different O
detectors O
specialize O
in O
different O
types O
of O
hallucinations O
. O
In O
Appendix O
J O
, O
we O
show O
that O
the O
trends O
presented O
in O
this O
section O
hold O
for O
other O
mid O
- O
and O
low O
- O
resource O
language O
pairs O
. O
Fully O
detached O
hallucinations O
. O
Detecting O
fully O
detached O
hallucinations O
is O
remarkably O
easy O
for O
most O
detectors O
. O
Interestingly O
, O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
significantly O
outperforms O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
on O
this O
type O
of O
hallucination O
. O
This O
highlights O
how O
combining O
both O
methods O
can O
be O
helpful O
. O
In O
fact O
, O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
performs O
similarly O
to O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
, O
and O
can O
very O
easily O
separate O
most O
fully O
detached O
hallucinations O
from O
other O
translations O
on O
a O
fixed O
- O
threshold O
scenario O
( O
Figure O
3 O
) O
. O
Note O
that O
the O
performance O
of O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
for O
fully O
detached O
hallucinations O
closely O
mirrors O
that O
of O
Attn B-MethodName
- I-MethodName
ign I-MethodName
- I-MethodName
SRC I-MethodName
. O
This O
is O
not O
surprising O
, O
since O
both O
methods O
, O
at O
their O
core O
, O
try O
to O
capture O
similar O
patterns O
: O
translations O
for O
which O
the O
source O
attention O
mass O
distribution O
is O
highly O
concentrated O
on O
a O
small O
set O
of O
source O
tokens O
. O

Strongly O
detached O
hallucinations O
. O
These O
are O
the O
hardest O
hallucinations O
to O
detect O
with O
our O
methods O
. O
Nevertheless O
, O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
performs O
competitively O
with O
the O
previous O
best O
- O
performing O
modelbased O
method O
for O
this O
type O
of O
hallucinations O
( O
Seq B-MethodName
- I-MethodName
Logprob I-MethodName
) O
. O
We O
hypothesize O
that O
the O
difficulty O
in O
detecting O
these O
hallucinations O
may O
be O
due O
to O
the O
varying O
level O
of O
detachment O
from O
the O
source O
sequence O
. O
Indeed O
, O
Figure O
3 O
shows O
that O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
scores O
span O
from O
a O
cluster O
of O
strongly O
detached O
hallucinations O
with O
scores O
similar O
to O
other O
data O
samples O
to O
those O
similar O
to O
the O
scores O
of O
most O
fully O
detached O
hallucinations O
. O

Oscillatory O
hallucinations O
. O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
and O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
significantly O
outperform O
all O
previous O
model O
- O
based O
detectors O
on O
detecting O
oscillatory O
hallucinations O
. O
This O
is O
relevance O
in O
the O
context O
of O
model O
- O
based O
detectors O
, O
as O
previous O
detectors O
notably O
struggle O
with O
detecting O
these O
hallucinations O
. O
Moreover O
, O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
also O
manages O
to O
outperform O
LaBSE B-MethodName
with O
significant O
improvements O
in O
FPR B-MetricName
. O
This O
hints O
that O
the O
repetition O
of O
words O
or O
phrases O
may O
not O
be O
enough O
to O
create O
sentence O
- O
level O
representations O
that O
are O
highly O
dissimilar O
from O
the O
non O
- O
oscillatory O
source O
sequence O
. O
In O
contrast O
, O
we O
find O
that O
CometKiwi B-MethodName
appropriately O
penalizes O
oscillatory O
hallucinations O
, O
which O
aligns O
with O
observations O
made O
in O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
. O
Additionally O
, O
Figure O
3 O
shows O
that O
the O
scores O
for O
oscillatory O
hallucinations O
are O
scattered O
along O
a O
broad O
range O
. O
After O
close O
evaluation O
, O
we O
observed O
that O
this O
is O
highly O
related O
to O
the O
severity O
of O
the O
oscillation O
: O
almost O
all O
non O
- O
detected O
hallucinations O
are O
not O
severe O
oscillations O
( O
see O
Appendix O
I O
) O
. O

Conclusions O

We O
propose O
a O
novel O
plug O
- O
in O
model O
- O
based O
detector O
for O
hallucinations O
in O
NMT O
. O
Unlike O
previous O
attempts O
to O
build O
an O
attention O
- O
based O
detector O
, O
we O
do O
not O
rely O
on O
ad O
- O
hoc O
heuristics O
to O
detect O
hallucinations O
, O
and O
instead O
pose O
hallucination B-TaskName
detection I-TaskName
as O
an O
optimal O
transport O
problem O
: O
our O
detector O
aims O
to O
find O
translations O
whose O
source O
attention O
mass O
distribution O
is O
highly O
distant O
from O
those O
of O
good O
quality O
translations O
. O
Our O
empirical O
analysis O
shows O
that O
our O
detector O
outperforms O
all O
previous O
model O
- O
based O
detectors O
. O
Importantly O
, O
in O
contrast O
to O
these O
prior O
approaches O
, O
it O
is O
suitable O
for O
identifying O
oscillatory O
hallucinations O
, O
thus O
addressing O
an O
important O
gap O
in O
the O
field O
. O
We O
also O
show O
that O
our O
detector O
is O
competitive O
with O
external O
detectors O
that O
use O
state O
- O
of O
- O
the O
- O
art O
quality O
estimation O
or O
cross O
- O
lingual O
similarity O
models O
. O
Notably O
, O
this O
performance O
is O
achieved O
without O
the O
need O
for O
large O
models O
, O
or O
any O
data O
with O
quality O
annotations O
or O
parallel O
training O
data O
. O
Finally O
, O
thanks O
to O
its O
flexibility O
, O
our O
detector O
can O
be O
easily O
deployed O
in O
real O
- O
world O
scenarios O
, O
making O
it O
a O
valuable O
tool O
for O
practical O
applications O
. O

Limitations O

We O
highlight O
two O
main O
limitations O
of O
our O
work O
. O

Firstly O
, O
instead O
of O
focusing O
on O
more O
recent O
NMT O
models O
that O
use O
large O
pretrained O
language O
models O
as O
their O
backbone O
, O
our O
experiments O
were O
based O
on O
transformer O
base O
models O
. O
That O
is O
because O
we O
used O
the O
NMT O
models O
that O
produced O
the O
translations O
in O
the O
datasets O
we O
analyze O
, O
i.e O
, O
the O
models O
that O
actually O
hallucinate O
for O
the O
source O
sequences O
in O
the O
dataset O
. O
Nevertheless O
, O
research O
on O
hallucinations O
for O
larger O
NMT O
models O
makes O
for O
an O
exciting O
line O
of O
future O
work O
and O
would O
be O
valuable O
to O
assess O
the O
broad O
validity O
of O
our O
claims O
. O

Secondly O
, O
although O
our O
method O
does O
not O
require O
any O
training O
data O
or O
human O
annotations O
, O
it O
relies O
on O
access O
to O
a O
pre O
- O
existing O
database O
of O
source O
mass O
distributions O
. O
This O
can O
be O
easily O
obtained O
offline O
by O
running O
the O
model O
on O
monolingual O
data O
to O
obtain O
the O
distributions O
. O
Nevertheless O
, O
these O
datastores O
need O
not O
be O
costly O
in O
terms O
of O
memory O
. O
In O
fact O
, O
in O
Appendix O
J O
, O
we O
validate O
our O
detectors O
for O
datastores O
that O
contain O
less O
than O
100k O
distributions O
. O
dataset O
, O
which O
contains O
a O
high O
percentage O
of O
hallucinations O
for O
some O
language O
pairs O
( O
Specia O
et O
al O
. O
, O
2021 O
; O
Tang O
et O
al O
. O
, O
2022 O
) O
. O
We O
show O
the O
performance O
of O
both O
these O
versions O
in O
Table O
4 O
. O
CometKiwi B-MethodName
significantly O
outperforms O
the O
previous O
iteration O
of O
COMET B-MethodName
- I-MethodName
QE I-MethodName
. O
This O
hints O
that O
training O
quality O
estimation O
models O
with O
more O
negative O
examples O
can O
improve O
their O
ability O
to O
adequately O
penalize O
hallucinations O
. O

D O
Computational O
runtime O
of O
our O
detectors O

Our O
detectors O
do O
not O
require O
access O
to O
a O
GPU O
machine O
. O
All O
our O
experiments O
have O
been O
ran O
on O
a O
machine O
with O
2 O
physical O
Intel O
( O
R O
) O
Xeon O
( O
R O
) O
Gold O
6348 O
@ O
2.60GHz O
CPUs O
( O
total O
of O
112 O
threads O
) O
. O
Obtaining O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
scores O
for O
all O
the O
3415 O
translations O
from O
the O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
dataset O
takes O
less O
than O
half O
a O
second O
, O
while O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
scores O
are O
obtained O
in O
little O
over O
4 O
minutes O
. O

E O
Evaluation O
Metrics O

We O
use O
scikit O
- O
learn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
implementations O
of O
our O
evaluation O
metrics O
. O
15 O

F O
Tracing O
- O
back O
performance O
boosts O
to O
the O
construction O
of O
the O
reference O
set O
R O
x O

In O
Section O
6.1 O
in O
the O
main O
text O
, O
we O
showed O
that O
evaluating O
how O
distant O
a O
given O
translation O
is O
compared O
to O
a O
data O
- O
driven O
reference O
distribution O
- O
rather O
than O
to O
an O
ad O
- O
hoc O
reference O
distribution O
- O
led O
to O
increased O
performance O
. O
Therefore O
, O
we O
will O
now O
analyze O
the O
construction O
of O
the O
reference O
set O
R O
x O
to O
obtain O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
scores O
( O
step O
2 O
in O
Figure O
1 O
) O
. O
We O
conduct O
experiments O
to O
investigate O
the O
importance O
of O
the O
two O
main O
operations O
in O
this O
process O
: O
defining O
and O
length O
- O
filtering O
the O
distributions O
in O
R O
held O
. O

Construction O
of O
R O
held O
. O
To O
construct O
R O
held O
, O
we O
first O
need O
to O
obtain O
the O
source O
attention O
mass O
distributions O
for O
each O
sample O
in O
D O
held O
. O
If O
D O
held O
is O
a O
parallel O
corpus O
, O
we O
can O
force O
- O
decode O
the O
reference O
translations O
to O
construct O
R O
held O
. O
As O
shown O
in O
Table O
5 O
, O
this O
construction O
produces O
results O
similar O
to O
using O
good O
- O
quality O
model O
- O
generated O
translations O
. O
Moreover O
, O
we O
also O
evaluate O
the O
scenario O
where O
R O
held O
is O
constructed O
with O
translations O
of O
any O
quality O
. O
Table O
5 O
shows O
that O
although O
filtering O
for O
quality O

Category O
Source O
Sentence O
Reference O
Translation O
Hallucination O

Oscillatory O
Als O
Maß O
hierfür O
wird O
meist O
der O
sogenannte O
Pearl O
Index O
benutzt O
( O
so O
benannt O
nach O
einem O
Statistiker O
, O
der O
diese O
Berechnungsformel O
einführte O
) O
. O

As O
a O
measure O
of O
this O
, O
the O
so O
- O
called O
Pearl O
Index O
is O
usually O
used O
( O
so O
named O
after O
a O
statistician O
who O
introduced O
this O
calculation O
formula O
) O
. O

The O
term O
" O
Pearl O
Index O
" O
refers O
to O
the O
term O
" O
Pearl O
Index O
" O
( O
or O
" O
Pearl O
Index O
" O
) O
used O
to O
refer O
to O
the O
term O
" O
Pearl O
Index O
" O
( O
or O
" O
Pearl O
Index O
" O
) O
. O
improves O
performance O
, O
the O
gains O
are O
not O
substantial O
. O
This O
connects O
to O
findings O
by O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
: O
hallucinations O
exhibit O
different O
properties O
from O
other O
translations O
, O
including O
other O
incorrect O
translations O
. O
We O
offer O
further O
evidence O
that O
properties O
of O
hallucinations O
- O
in O
this O
case O
, O
the O
source O
attention O
mass O
distributions O
- O
are O
not O
only O
different O
to O
those O
of O
good O
- O
quality O
translations O
but O
also O
to O
most O
other O
model O
- O
generated O
translations O
. O

Length O
- O
filtering O
the O
distributions O
in O
R O
held O
. O
The O
results O
in O
Table O
6 O
show O
that O
length O
- O
filtering O
boosts O
performance O
significantly O
. O
This O
is O
expected O
: O
our O
translation O
- O
based O
length O
- O
filtering O
penalizes O
translations O
whose O
length O
is O
anomalous O
for O
their O
respective O
source O
sequences O
. O
This O
is O
particularly O
useful O
for O
detecting O
oscillatory O
hallucinations O
. O

G O
Ablations O

We O
perform O
ablations O
on O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
and O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
for O
all O
relevant O
hyperparameters O
: O
the O
length- B-HyperparameterName
filtering I-HyperparameterName
parameter I-HyperparameterName
δ I-HyperparameterName
, O
the O
maximum B-HyperparameterName
cardinality I-HyperparameterName
of I-HyperparameterName
R I-HyperparameterName
, O
|R| B-HyperparameterName
max I-HyperparameterName
, O
the O
value O
of O
k B-HyperparameterName
to O
compute O
the O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
scores O
( O
step O
4 O
in O
Figure O
1 O
) O
, O
and O
the O
threshold B-HyperparameterName
on O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
scores O
to O
compute O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
scores O
. O
The O
results O
are O
shown O
in O
Table O
7 O
to O
Table O
10 O
, O
respectively O
. O
We O
also O
report O
in O
Table O
11 O
the O
performance O
of O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
with O
a O
0 O
/ O
1 O
cost O
function O
instead O
of O
the O
ℓ O
1 O
distance O
function O
. O

On O
length B-HyperparameterName
- I-HyperparameterName
filtering I-HyperparameterName
. O
The O
results O
in O
Table O
7 O
show O
that O
, O
generally O
, O
the O
bigger O
the O
length O
window O
, O
the O
worse O
the O
performance O
. O
This O
is O
expected O
: O
if O
the O
test O
translation O
is O
very O
different O
in O
length O
to O
those O
obtained O
for O
the O
source O
sequences O
in O
R O
x O
, O
the O
more O
penalized O
it O
may O
be O
for O
the O
length O
mismatch O
instead O
of O
source O
attention O
distribution O
pattern O
anomalies O
. O

On O
the O
choice O
of O
|R| B-HyperparameterName
max I-HyperparameterName
. O
While O
this O
increase O
in O
performance O
may O
be O
desirable O
, O
it O
comes O
at O
the O
cost O
of O
higher O
runtime O
. O

On O
the O
choice O
of O
k. B-HyperparameterName
The O
results O
in O
Table O
9 O
show O
that O
the O
higher O
the O
value O
of O
k B-HyperparameterName
, O
the O
worse O
the O
performance O
. O
However O
, O
we O
do O
not O
recommend O
using O
the O
minimum O
distance O
( O
k B-HyperparameterName
= O
1 B-HyperparameterValue
) O
as O
it O
can O
be O
unstable O
. O

On O
the O
choice O
of O
threshold B-HyperparameterName
on O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
scores O
. O
Table O
10 O
show O
that O
, O
generally O
, O
a O
higher O
threshold B-HyperparameterName
τ I-HyperparameterName
leads O
to O
a O
better O
performance O
of O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
. O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
scores O
are O
generally O
very O
high O
for O
fully O
detached O
hallucinations O
, O
a O
type O
of O
hallucinations O
that O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
struggles O
more O
to O
detect O
. O
Thus O
, O
when O
combined O
in O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
, O
we O
obtain O
significant O
boosts O
in O
overall O
performance O
. O
However O
, O
if O
the O
threshold O
on O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
scores O
is O
set O
too O
low O
, O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Combo I-MethodName
will O
correspond O
to O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
more O
frequently O
which O
may O
not O
be O
desirable O
as O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
outperforms O
it O
for O
all O
other O
types O
of O
hallucinations O
. O
If O
set O
too O
high O
, O
fewer O
fully O
detached O
hallucinations O
may O
pass O
that O
threshold O
and O
may O
then O
be O
misidentified O
with O
Wassto B-MethodName
- I-MethodName
Data I-MethodName
scores O
. O

On O
the O
choice O
of O
Wass B-HyperparameterName
- I-HyperparameterName
to I-HyperparameterName
- I-HyperparameterName
Data I-HyperparameterName
cost O
function O
. O

Table O
11 O
shows O
that O
using O
the O
ℓ O
1 O
cost O
function O
instead O
of O
using O
the O
0 O
/ O
1 O
cost O
function O
to O
compute O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
scores O
leads O
to O
significant O
improve- O
ments O
. O
This O
suggests O
that O
when O
comparing O
the O
source O
mass O
attention O
distribution O
of O
a O
test O
translation O
to O
other O
such O
distributions O
obtained O
for O
other O
translations O
( O
instead O
of O
the O
ad O
- O
hoc O
uniform O
distribution O
used O
for O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
scores O
) O
, O
the O
information O
from O
the O
location O
of O
the O
source O
attention O
mass O
is O
helpful O
to O
obtain O
better O
scores O
. O

On O
the O
formulation O
of O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
. O
To O
combine O
the O
information O
from O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Unif I-MethodName
and O
Wassto B-MethodName
- I-MethodName
Data I-MethodName
, O
we O
could O
also O
perform O
a O
convex O
combination O
of O
the O
two O
scores O
: O

s O
wc O
( O
x O
) O
= O
λs O
wtd O
( O
x O
) O
+ O
( O
1 O
− O
λ O
) O
s O
wtu O
( O
x O
) O
( O
10 O
) O

for O
a O
predefined O
scalar O
parameter O
λ O
. O
In O
Table O
12 O
, O
we O
show O
that O
this O
method O
is O
consistently O
subpar O
to O
our O
two O
- O
pass O
approach O
. O
In O
fact O
, O
this O
linear O
interpolation O
is O
not O
able O
to O
bring O
additional O
gains O
in O
performance O
for O
any O
of O
the O
tested O
parameters O
λ O
when O
compared O
to O
Wass B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
Data I-MethodName
. O

H O
Analysis O
against O
ALTI+ B-MethodName

Concurrently O
to O
our O
work O
, O
Dale O
et O
al O
. O
( O
2022 O
) O
leveraged O
ALTI+ B-MethodName
( O
Ferrando O
et O
al O
. O
, O
2022 O
) O
, O
a O
method O
that O
evaluates O
the O
global O
relative O
contributions O
of O
both O
source O
and O
target O
prefixes O
to O
model O
predictions O
, O
for O
detection B-TaskName
of I-TaskName
hallucinations I-TaskName
. O
As O
hallucinations O
are O
translations O
detached O
from O
the O
source O
sequence O
, O
ALTI+ B-MethodName
is O
able O
to O
detect O
them O
by O
identifying O
sentences O
with O
minimal O
source O
contribution O
. O
In O
Table O
13 O
, O
we O
show O
that O
ALTI+ B-MethodName
slightly O
outperforms O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
for O
fully O
detached O
hallucinations O
, O
but O
lags O
considerably O
behind O
on O
what O
comes O
to O
de O
- O
Figure O
4 O
: O
Distribution O
of O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
scores O
( O
on O
log O
- O
scale O
) O
for O
each O
type O
of O
hallucination O
and O
performance O
on O
a O
fixed O
- O
threshold O
scenario O
. O
We O
highlight O
three O
hallucinations O
that O
are O
not O
detected O
by O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
. O
These O
represent O
hallucinations O
in O
the O
dataset O
that O
it O
struggles O
to O
identify O
: O
( O
1 O
) O
exact O
copies O
of O
the O
source O
sequence O
, O
( O
2 O
) O
small O
level O
of O
detachment O
in O
strongly O
detached O
hallucinations O
, O
and O
( O
3 O
) O
mild O
repetitions O
of O
1 O
- O
grams O
( O
" O
design O
" O
) O
. O
tecting O
strongly O
detached O
and O
oscillatory O
hallucinations O
. O

I O
Error O
Analysis O
of O
Wass B-MethodName
- I-MethodName
Combo I-MethodName

We O
show O
a O
qualitative O
analysis O
on O
the O
same O
fixedthreshold O
scenario O
described O
in O
Section O
6.2 O
in O
Fig- O
Our O
detector O
is O
not O
able O
to O
detect O
fully O
detached O
hallucinations O
that O
come O
in O
the O
form O
of O
exact O
copies O
of O
the O
source O
sentence O
. O
For O
these O
pathological O
translations O
, O
the O
attention O
map O
is O
mostly O
diagonal O
and O
is O
thus O
not O
anomalous O
. O
Although O
these O
are O
severe O
errors O
, O
we O
argue O
that O
, O
in O
a O
real O
- O
world O
application O
, O
such O
translations O
can O
be O
easily O
detected O
with O
string O
matching O
heuristics O
. O

We O
also O
find O
that O
our O
detector O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
struggles O
with O
oscillatory O
hallucinations O
that O
come O
in O
the O
form O
of O
mild O
repetitions O
of O
1 O
- O
grams O
or O
2- O
grams O
( O
see O
example O
in O
Figure O
4 O
) O
. O
To O
test O
this O
hypothesis O
, O
we O
implemented O
the O
binary O
heuristic O
top O
n O
- O
gram O
count O
( O
Raunak O
et O
al O
. O
, O
2021 O
; O
Guerreiro O
et O
al O
. O
, O
2022 O
) O
to O
verify O
whether O
a O
translation O
is O
a O
severe O
oscillation O
: O
given O
the O
entire O
D O
held O
, O
a O
translation O
is O
flagged O
as O
an O
oscillatory O
hallucination O
if O
( O
i O
) O
it O
is O
in O
the O
set O
of O
1 O
% O
lowest O
- O
quality O
translations O
according O
to O
CometKiwi B-MethodName
and O
( O
ii O
) O
the O
count O
of O
the O
top O
repeated O
4 O
- O
gram O
in O
the O
translation O
is O
greater O
than O
the O
count O
of O
the O
top O
repeated O
source O
4 O
- O
gram O
by O
at O
least O
2 O
. O
Indeed O
, O
more O
than O
90 O
% O
of O
the O
oscillatory O
hallucinations O
not O
detected O
by O
Wass B-MethodName
- I-MethodName
Combo I-MethodName
in O
Figure O
4 O
were O
not O
flagged O
by O
this O
heuristic O
. O
We O
provide O
8 O
examples O
randomly O
sampled O
from O
the O
set O
of O
oscillatory O
hallucinations O
not O
detected O
with O
Wass O
- O
Combo O
in O
Table O
14 O
. O
Close O
manual O
evaluation O
of O
these O
hallucinations O
further O
backs O
the O
hypothesis O
above O
. O

J O
Experiments O
on O
the O
MLQE B-DatasetName
- I-DatasetName
PE I-DatasetName
dataset O

In O
order O
to O
establish O
the O
broader O
validity O
of O
our O
model O
- O
based O
detectors O
, O
we O
present O
an O
analysis O
on O
their O
performance O
for O
other O
NMT O
models O
and O
on O
mid O
and O
low O
- O
resource O
language O
pairs O
. O
Overall O
, O
the O
detectors O
exhibit O
similar O
trends O
to O
those O
discussed O
in O
the O
main O
text O
( O
Section O
6 O
) O
. O

J.1 O
Model O
and O
Data O

The O
dataset O
from O
( O
Guerreiro O
et O
al O
. O
, O
2022 O
) O
analysed O
in O
the O
main O
text O
is O
the O
only O
available O
dataset O
that O
contains O
human O
annotations O
of O
hallucinated O
translations O
. O
Thus O
, O
in O
this O
analysis O
we O
will O
have O
to O
make O
use O
of O
other O
human O
annotations O
to O
infer O
annotations O
for O
hallucinations O
. O
For O
that O
end O
, O
we O
follow O
a O
similar O
setup O
to O
that O
of O
( O
Tang O
et O
al O
. O
, O
2022 O
) O
and O
use O
the O
MLQE B-DatasetName
- I-DatasetName
PE I-DatasetName
dataset O
-that O
has O
been O
reported O
to O
contain O
low O
- O
quality O
translations O
and O
hallucinations O
for O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
and O
RO B-DatasetName
- I-DatasetName
EN I-DatasetName
( O
Specia O
et O
al O
. O
, O
2021 O
) O
-to O
test O
the O
performance O
of O
our O
detectors O
on O
these O
language O
pairs O
. O
The O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
and O
RO B-DatasetName
- I-DatasetName
EN I-DatasetName
MLQE I-DatasetName
- I-DatasetName
PE I-DatasetName
datasets O
contain O
7000 O
translations O
and O
their O
respective O
human O
quality O
assessments O
( O
from O
1 O
to O
100 O
) O
. O
Each O
translation O
is O
scored O
by O
three O
different O
annotators O
. O
As O
hal O
- O
lucinations O
lie O
at O
the O
extreme O
end O
of O
NMT O
pathologies O
( O
Raunak O
et O
al O
. O
, O
2021 O
) O
, O
we O
consider O
a O
translation O
to O
be O
a O
hallucination O
if O
at O
least O
two O
annotators O
( O
majority O
) O
gave O
it O
a O
quality O
score O
of O
1 O
. O
16 O
This O
process O
leads O
to O
30 O
hallucinations O
for O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
and O
237 O
hallucinations O
for O
RO B-DatasetName
- I-DatasetName
EN I-DatasetName
. O
Although O
the O
number O
of O
hallucinations O
for O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
is O
relatively O
small O
, O
we O
decide O
to O
also O
report O
experiments O
on O
this O
language O
pair O
because O
the O
type O
of O
hallucinations O
found O
for O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
is O
very O
different O
to O
those O
found O
for O
RO B-DatasetName
- I-DatasetName
EN I-DatasetName
: O
almost O
all O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
hallucinations O
are O
oscillatory O
, O
whereas O
almost O
all O
RO B-DatasetName
- I-DatasetName
EN I-DatasetName
are O
fully O
detached O
. O

To O
obtain O
all O
model O
- O
based O
information O
required O
to O
build O
the O
detectors O
, O
we O
use O
the O
same O
Transformer O
models O
that O
generated O
the O
translations O
in O
the O
datasets O
in O
consideration O
. O
All O
details O
can O
be O
found O
in O
and O
the O
official O
project O
repository O
17 O
. O
Moreover O
, O
to O
build O
our O
heldout O
databases O
of O
source O
mass O
distributions O
, O
we O
used O
readily O
available O
Europarl O
data O
( O
Koehn O
, O
2005 O
) O
for O
RO B-DatasetName
- I-DatasetName
EN I-DatasetName
( O
∼100k O
samples O
) O
, O
and O
filtered O
Nepali O
Wikipedia O
monolingual O
data O
18 O
used O
in O
( O
Koehn O
et O
al O
. O
, O

Acknowledgments O

A O
Model O
and O
Data O
Details O
NMT O
Model O
. O
The O
NMT O
model O
used O
in O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
to O
create O
the O
hallucination O
dataset O
is O
a O
Transformer O
base O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
( O
hidden O
size O
of O
512 O
, O
feedforward O
size O
of O
2048 O
, O
6 O
encoder O
and O
6 O
decoder O
layers O
, O
8 O
attention O
heads O
) O
. O
The O
model O
has O
approximately O
77 O
M O
parameters O
. O
It O
was O
trained O
with O
the O
fairseq O
toolkit O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
on O
WMT18 B-DatasetName
DE I-DatasetName
- I-DatasetName
EN I-DatasetName
data O
( O
excluding O
Paracrawl O
) O
: O
the O
authors O
randomly O
choose O
2 O
/ O
3 O
of O
the O
dataset O
for O
training O
and O
use O
the O
remaining O
1 O
/ O
3 O
as O
a O
held O
- O
out O
set O
for O
analysis O
. O
We O
use O
that O
same O
held O
- O
out O
set O
in O
this O
work O
. O

Dataset O
Stats O
. O
The O
dataset O
used O
in O
this O
paper O
was O
introduced O
in O
Guerreiro O
et O
al O
. O
( O
2022 O
) O
. O
It O
consists O
of O
3415 O
translations O
from O
WMT18 B-DatasetName
DE I-DatasetName
- I-DatasetName
EN I-DatasetName
data O
with O
structured O
annotations O
on O
different O
types O
of O
hallucinations O
and O
pathologies O
. O
Overall O
, O
the O
dataset O
contains O
118 O
translations O
annotated O
as O
fully O
detached O
hallucinations O
, O
90 O
as O
strongly O
detached O
hallucinations O
, O
and O
86 O
as O
oscillatory O
hallucinations O
. O
11 O
The O
other O
translations O
are O
either O
incorrect O
( O
1073 O
) O
or O
correct O
( O
2048 O
) O
. O
Details O
on O
annotation O
, O
a O
high O
- O
level O
overview O
and O
other O
statistics O
can O
be O
found O
in O
the O
original O
paper O
. O
We O
show O
examples O
of O
hallucinations O
for O
each O
category O
in O
Table O
3 O
. O
12 O

B O
Details O
on O
External O
Detectors O

COMET B-MethodName
. O
We O
use O
models O
available O
in O
the O
official O
repository O
13 O
: O
wmt22 O
- O
cometkiwi O
- O
da O
for O
CometKiwi O
and O
wmt20 O
- O
comet O
- O
da O
for O
COMET B-MethodName
. O

LaBSE B-MethodName
. O
We O
use O
the O
version O
available O
in O
sentence O
- O
transformers O
( O
Reimers O
and O
Gurevych O
, O
2019 O
2019 O
) O
for O
NE B-DatasetName
- I-DatasetName
EN I-DatasetName
( O
∼80k O
samples O
) O
. O

J.2 O
Results O

The O
trends O
in O
Section O
6.1 O
hold O
for O
other O
language O
pairs O
. O
The O
results O
in O
Table O
15 O
establish O
the O
broader O
validity O
of O
our O
detectors O
for O
other O
NMT O
models O
and O
, O
importantly O
, O
for O
mid O
and O
low O
- O
resource O
language O
pairs O
. O
Similarly O
to O
the O
analysis O
in O
6.1 O
, O
we O
find O
that O
our O
detectors O
( O
i O
) O
exhibit O
better O
performance O
than O
other O
model O
- O
based O
detectors O
with O
significant O
gains O
on O
the O
low O
- O
resource O
NE O
- O
EN O
language O
pair O
; O
and O
( O
ii O
) O
can O
be O
competitive O
with O
external O
detectors O
that O
leverage O
large O
models O
. O
D2 O
. O
Did O
you O
report O
information O
about O
how O
you O
recruited O
( O
e.g. O
, O
crowdsourcing O
platform O
, O
students O
) O
and O
paid O
participants O
, O
and O
discuss O
if O
such O
payment O
is O
adequate O
given O
the O
participants O
' O
demographic O
( O
e.g. O
, O
country O
of O
residence O
) O
? O

No O
response O
. O

D3 O
. O
Did O
you O
discuss O
whether O
and O
how O
consent O
was O
obtained O
from O
people O
whose O
data O
you O
're O
using O
/ O
curating O
? O
For O
example O
, O
if O
you O
collected O
data O
via O
crowdsourcing O
, O
did O
your O
instructions O
to O
crowdworkers O
explain O
how O
the O
data O
would O
be O
used O
? O
No O
response O
. O

D4 O
. O
Was O
the O
data O
collection O
protocol O
approved O
( O
or O
determined O
exempt O
) O
by O
an O
ethics O
review O
board O
? O
No O
response O
. O

D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
No O
response O
. O

SPEECH B-MethodName
: O
Structured B-MethodName
Prediction I-MethodName
with I-MethodName
Energy I-MethodName
- I-MethodName
Based I-MethodName
Event I-MethodName
- I-MethodName
Centric I-MethodName
Hyperspheres I-MethodName

Event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
involves O
predicting O
structured O
outputs O
of O
events O
. O
In O
most O
NLP O
cases O
, O
event O
structures O
are O
complex O
with O
manifold O
dependency O
, O
and O
it O
is O
challenging O
to O
effectively O
represent O
these O
complicated O
structured O
events O
. O
To O
address O
these O
issues O
, O
we O
propose O
Structured B-MethodName
Prediction I-MethodName
with I-MethodName
Energybased I-MethodName
Event I-MethodName
- I-MethodName
Centric I-MethodName
Hyperspheres I-MethodName
( O
SPEECH B-MethodName
) O
. O
SPEECH B-MethodName
models O
complex O
dependency O
among O
event O
structured O
components O
with O
energybased O
modeling O
, O
and O
represents O
event O
classes O
with O
simple O
but O
effective O
hyperspheres O
. O
Experiments O
on O
two O
unified O
- O
annotated O
event O
datasets O
indicate O
that O
SPEECH B-MethodName
is O
predominant O
in O
event O
detection O
and O
event O
- O
relation O
extraction O
tasks O
. O

Introduction O

Structured B-TaskName
prediction I-TaskName
( O
Taskar O
et O
al O
. O
, O
2005 O
) O
is O
a O
task O
where O
the O
predicted O
outputs O
are O
complex O
structured O
components O
. O
This O
arises O
in O
many O
NLP O
tasks O
( O
Smith O
, O
2011 O
; O
Kreutzer O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2023 O
) O
and O
supports O
various O
applications O
( O
Jagannatha O
and O
Yu O
, O
2016 O
; O
Kreutzer O
et O
al O
. O
, O
2021 O
) O
. O
In O
event O
- O
centric O
NLP O
tasks O
, O
there O
exists O
strong O
complex O
dependency O
between O
the O
structured O
outputs O
, O
such O
as O
event B-TaskName
detection I-TaskName
( O
ED B-TaskName
) O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
, O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
( O
ERE B-TaskName
) O
, O
and O
event B-TaskName
schema I-TaskName
induction I-TaskName
. O
Thus O
, O
these O
tasks O
can O
also O
be O
revisited O
as O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
problems O
( O
Li O
et O
al O
. O
, O
2013 O
) O
. O

Event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
( O
ECSP B-TaskName
) O
tasks O
require O
to O
consider O
manifold O
structures O
and O
dependency O
of O
events O
, O
including O
intra- O
/ O
inter O
- O
sentence O
structures O
. O
For O
example O
, O
as O
seen O
in O
Figure O
1 O
, O
given O
a O
document O
containing O
some O
event O
mentions O
" O
David O
Warren O
shot O
and O
killed O
Henry O
Glover O
... O
David O
was O
convicted O
and O
sentenced O
to O
25 O
years O
and O
9 O
months O
... O
" O
, O
in O
ED B-TaskName
task O
mainly O
considering O
intra O
- O
sentence O
structures O
, O
we O
need O
to O
identify O
event O
triggers O
( O
killed O
, O
convicted O
) O
from O
these O
tokens O
and O
categorize O
them O
* O
Corresponding O
Author O
. O

… O
… O
[ O
S1 O
] O

Former O
NOPD O
police O
officer O
David O
Warren O
shot O
and O
killed O
Henry O
Glover O
. O
[ O
S2 O
] O
Five O
current O
and O
former O
officers O
of O
the O
NOPD O
were O
charged O
with O
Glover O
's O
death O
. O

[ O
S3 O
] O
David O
was O
convicted O
and O
sentenced O
to O
25 O
years O
and O
9 O
months O
in O
prison O
for O
shooting O
and O
killing O
Glover O
. O
… O
… O
. O
into O
event O
classes O
( O
killing O
, O
legal_rulings O
) O
; O
in O
ERE B-TaskName
task O
mainly O
considering O
inter O
- O
sentence O
structures O
, O
we O
need O
to O
find O
the O
relationship O
between O
each O
event O
mention O
pair O
, O
such O
as O
event O
coreference O
, O
temporal O
, O
causal O
and O
subevent O
relations O
. O

As O
seen O
from O
Figure O
1 O
, O
the O
outputs O
of O
ECSP B-TaskName
lie O
on O
a O
complex O
manifold O
and O
possess O
interdependent O
structures O
, O
e.g. O
, O
the O
long O
- O
range O
dependency O
of O
tokens O
, O
the O
association O
among O
triggers O
and O
event O
classes O
, O
and O
the O
dependency O
among O
event O
classes O
and O
event O
relations O
. O
Thus O
it O
is O
challenging O
to O
model O
such O
complex O
event O
structures O
while O
efficiently O
representing O
these O
events O
. O
Previous O
works O
increasingly O
apply O
deep O
representation O
learning O
to O
tackle O
these O
problems O
. O
; O
propose O
to O
predict O
event O
structures O
based O
on O
the O
event O
graph O
schema O
. O
Hsu O
et O
al O
. O
( O
2022 O
) O
generate O
event O
structures O
with O
manually O
designed O
prompts O
. O
However O
, O
these O
methods O
mainly O
focus O
on O
one O
of O
ECSP B-TaskName
tasks O
and O
their O
event O
structures O
are O
hard O
to O
represent O
effectively O
. O
Paolini O
et O
al O
. O
( O
2021 O
) O
; O
Lu O
et O
al O
. O
( O
2021Lu O
et O
al O
. O
( O
, O
2022 O
propose O
to O
extract O
multiple O
event O
structures O
from O
texts O
with O
a O
unified O
generation O
paradigm O
. O
However O
, O
the O
event O
structures O
of O
these O
approaches O
are O
usually O
quite O
simplistic O
and O
they O
often O
ignore O
the O
complex O
dependency O
among O
tasks O
. O
In O
this O
paper O
, O
we O
focus O
more O
on O
: O
( O
i O
) O
how O
to O
learn O
complex O
event O
structures O
for O
manifold O
ECSP B-TaskName
tasks O
; O
and O
( O
ii O
) O
how O
to O
simultane O
- O
ously O
represent O
events O
for O
these O
complex O
structured O
prediction O
models O
effectively O
. O

To O
resolve O
the O
first O
challenging O
problem O
of O
modeling O
manifold O
event O
structures O
, O
we O
utilize O
energy O
networks O
( O
Lecun O
et O
al O
. O
, O
2006 O
; O
Belanger O
and O
McCallum O
, O
2016 O
; O
Belanger O
et O
al O
. O
, O
2017 O
; O
Tu O
and O
Gimpel O
, O
2018 O
) O
, O
inspired O
by O
their O
potential O
benefits O
in O
capturing O
complex O
dependency O
of O
structured O
components O
. O
We O
define O
the O
energy O
function O
to O
evaluate O
compatibility O
of O
input O
/ O
output O
pairs O
, O
which O
places O
no O
limits O
on O
the O
size O
of O
the O
structured O
components O
, O
making O
it O
powerful O
to O
model O
complex O
and O
manifold O
event O
structures O
. O
We O
generally O
consider O
token- O
, O
sentence- O
, O
and O
document O
- O
level O
energy O
respectively O
for O
trigger O
classification O
, O
event O
classification O
and O
event O
- O
relation O
extraction O
tasks O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
work O
firstly O
address O
event O
- O
centric O
structured O
prediction O
with O
energy O
- O
based O
modeling O
. O

To O
resolve O
the O
second O
challenging O
problem O
of O
efficiently O
representing O
events O
, O
we O
take O
advantage O
of O
hyperspheres O
( O
Mettes O
et O
al O
. O
, O
2019 O
; O
Wang O
and O
Isola O
, O
2020 O
) O
, O
which O
is O
demonstrated O
to O
be O
a O
simple O
and O
effective O
approach O
to O
model O
class O
representation O
( O
Deng O
et O
al O
. O
, O
2022 O
) O
. O
We O
assume O
that O
the O
event O
mentions O
of O
each O
event O
class O
distribute O
on O
the O
corresponding O
energy O
- O
based O
hypersphere O
, O
so O
that O
we O
can O
represent O
each O
event O
class O
with O
a O
hyperspherical O
centroid O
and O
radius O
embedding O
. O
The O
geometrical O
modeling O
strategy O
( O
Ding O
et O
al O
. O
, O
2021 O
; O
Lai O
et O
al O
. O
, O
2021 O
) O
is O
demonstrated O
to O
be O
beneficial O
for O
modelling O
enriched O
class O
- O
level O
information O
and O
suitable O
for O
constructing O
measurements O
in O
Euclidean O
space O
, O
making O
it O
intuitively O
applicable O
to O
manifold O
eventcentric O
structured O
prediction O
tasks O
. O

Summarily O
, O
considering O
the O
two O
issues O
, O
we O
propose O
to O
address O
Structured B-MethodName
Prediction I-MethodName
with I-MethodName
Energybased I-MethodName
Event I-MethodName
- I-MethodName
Centric I-MethodName
Hyperspheres I-MethodName
( O
SPEECH B-MethodName
) O
, O
and O
our O
contributions O
can O
be O
summarized O
as O
follows O
: O

• O
We O
revisit O
the O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
tasks O
in O
consideration O
of O
both O
complex O
event O
structures O
with O
manifold O
dependency O
and O
efficient O
representation O
of O
events O
. O

• O
We O
propose O
a O
novel O
approach O
named O
SPEECH B-MethodName
to O
model O
complex O
event O
structures O
with O
energy O
- O
based O
networks O
and O
efficiently O
represent O
events O
with O
event O
- O
centric O
hyperspheres O
. O

• O
We O
evaluate O
SPEECH B-MethodName
on O
two O
newly O
proposed O
datasets O
for O
both O
event O
detection O
and O
eventrelation O
extraction O
, O
and O
experiments O
demonstrate O
that O
our O
model O
is O
advantageous O
. O

Related O
Work O

Event B-TaskName
- I-TaskName
Centric I-TaskName
Structured I-TaskName
Prediction I-TaskName
( O
ECSP B-TaskName
) O
. O
Since O
the O
boom O
in O
deep O
learning O
, O
traditional O
approaches O
to O
ECSP B-TaskName
mostly O
define O
a O
score O
function O
between O
inputs O
and O
outputs O
based O
on O
a O
neural O
network O
, O
such O
as O
CNN O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Deng O
et O
al O
. O
, O
2020 O
) O
, O
RNN O
( O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Meng O
and O
Rumshisky O
, O
2018 O
; O
Nguyen O
and O
Nguyen O
, O
2019 O
) O
, O
and O
GCN O
( O
Yan O
et O
al O
. O
, O
2019 O
; O
Lai O
et O
al O
. O
, O
2020 O
; O
Cui O
et O
al O
. O
, O
2020 O
) O
. O
With O
the O
development O
of O
pretrained O
large O
models O
, O
more O
recent O
research O
has O
entered O
a O
new O
era O
. O
Lu O
et O
al O
. O
( O
2022 O
) O
propose O
generative O
ECSP B-TaskName
models O
based O
on O
pre O
- O
trained O
T5 O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O
Wang O
et O
al O
. O
( O
2023 O
) O
tackle O
ECSP B-TaskName
with O
code O
generation O
based O
on O
code O
pretraining O
. O
However O
, O
these O
approaches O
are O
equipped O
with O
fairly O
simplistic O
event O
structures O
and O
have O
difficulty O
in O
tackling O
complex O
dependency O
in O
events O
. O
Besides O
, O
most O
of O
them O
fail O
to O
represent O
manifold O
events O
effectively O
. O
Energy O
Networks O
for O
Structured B-TaskName
Prediction I-TaskName
and O
Hyperspheres O
for O
Class O
Representation O
. O
Energy O
networks O
define O
an O
energy O
function O
over O
input O
/ O
output O
pairs O
with O
arbitrary O
neural O
networks O
, O
which O
places O
no O
limits O
on O
the O
size O
of O
the O
structured O
components O
, O
making O
it O
advantageous O
in O
modeling O
complex O
and O
manifold O
event O
structures O
. O
Lecun O
et O
al O
. O
( O
2006 O
) O
; O
Belanger O
and O
McCallum O
( O
2016 O
) O
associate O
a O
scalar O
measure O
to O
evaluate O
the O
compatibility O
to O
each O
configuration O
of O
inputs O
and O
outputs O
. O
( O
Belanger O
and O
McCallum O
, O
2016 O
) O
formulate O
deep O
energy O
- O
based O
models O
for O
structured O
prediction O
, O
called O
structured O
prediction O
energy O
networks O
( O
SPENs O
) O
. O
Belanger O
et O
al O
. O
( O
2017 O
) O
present O
end O
- O
to O
- O
end O
learning O
for O
SPENs O
, O
Tu O
and O
Gimpel O
( O
2018 O
) O
jointly O
train O
structured O
energy O
functions O
and O
inference O
networks O
with O
largemargin O
objectives O
. O
Some O
previous O
researches O
also O
regard O
event O
- O
centric O
NLP O
tasks O
as O
structured B-TaskName
prediction I-TaskName
( O
Li O
et O
al O
. O
, O
2013 O
; O
Paolini O
et O
al O
. O
, O
2021 O
) O
. O
Furthermore O
, O
to O
effectively O
obtain O
event O
representations O
, O
Deng O
et O
al O
. O
( O
2022 O
) O
demonstrate O
that O
hyperspherical O
prototypical O
networks O
( O
Mettes O
et O
al O
. O
, O
2019 O
) O
are O
powerful O
to O
encode O
enriched O
semantics O
and O
dependency O
in O
event O
structures O
, O
but O
they O
merely O
consider O
support O
for O
pairwise O
event O
structures O
. O

Methodology O

Preliminaries O

For O
structured B-TaskName
prediction I-TaskName
tasks O
, O
given O
input O
x O
∈ O
X O
, O
we O
denote O
the O
structured O
outputs O
by O
M O
Φ O
( O
x O
) O
∈Ỹ O
with O
a O
prediction O
model O
M O
Φ O
. O
Structured O
Prediction O
Energy O
Networks O
( O
SPENs O
) O
score O
structured O
outputs O
with O
an O
energy O
function O
E O
Θ O
: O
X O
×Ỹ O
→ O
R O
parameterized O
by O
Θ O
that O
iteratively O
optimize O
the O
energy O
between O
the O
input O
/ O
output O
pair O
( O
Belanger O
and O
Mc O
- O
Callum O
, O
2016 O
) O
, O
where O
lower O
energy O
means O
greater O
compatibility O
between O
the O
pair O
. O

We O
introduce O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
( O
ECSP B-TaskName
) O
following O
the O
similar O
setting O
as O
SPENs O
for O
multi O
- O
label O
classification O
and O
sequence O
labeling O
proposed O
by O
Tu O
and O
Gimpel O
( O
2018 O
) O
. O
Given O
a O
feature O
vector O
x O
belonging O
to O
one O
of O
T O
labels O
, O
the O
model O
output O
is O
M O
Φ O
( O
x O
) O
= O
{ O
0 O
, O
1 O
} O
T O
∈Ỹ O
for O
all O
x. O
The O
energy O
function O
contains O
two O
terms O
: O

E O
Θ O
( O
x O
, O
y O
) O
= O
E O
local O
Θ O
( O
x O
, O
y O
) O
+ O
E O
label O
Θ O
( O
y O
) O
= O
T O
i=1 O
y O
i O
V O
i O
f O
( O
x O
) O
+ O
w O
g O
( O
W O
y O
) O
( O
1 O
) O

where O

E O
local O
Θ O
( O
x O
, O
y O
) O
= O
T O
i=1 O
y O
i O
V O
i O
f O
( O
x O
) O

is O
the O
sum O
of O
linear O
models O
, O
and O
y O
i O
∈ O
y O
, O
V O
i O
is O
a O
parameter O
vector O
for O
label O
i O
and O
f O
( O
x O
) O
is O
a O
multi O
- O
layer O
perceptron O
computing O
a O
feature O
representation O
for O
the O
input O
x O
; O
E O
label O
Θ O
( O
y O
) O
= O
w O
g O
( O
W O
y O
) O
returns O
a O
scalar O
which O
quantifies O
the O
full O
set O
of O
labels O
, O
scoring O
y O
independent O
of O
x O
, O
thereinto O
, O
w O
is O
a O
parameter O
vector O
, O
g O
( O
• O
) O
is O
an O
elementwise O
non O
- O
linearity O
function O
, O
and O
W O
is O
a O
parameter O
matrix O
learned O
from O
data O
indicating O
the O
interaction O
between O
labels O
. O

After O
learning O
the O
energy O
function O
, O
prediction O
minimizes O
energy O
: O

y O
= O
arg O
min O
y∈Ỹ O
E O
Θ O
( O
x O
, O
y O
) O
( O
2 O
) O

The O
final O
theoretical O
optimum O
for O
SPEN O
is O
denoted O
by O
: O

min O
Θ O
max O
Φ O
( O
M O
Φ O
( O
x O
i O
) O
, O
y O
i O
) O
− O
E O
Θ O
( O
x O
i O
, O
M O
Φ O
( O
x O
i O
) O
) O
+ O
E O
Θ O
( O
x O
i O
, O
y O
i O
) O
+ O

( O
3 O
) O
where O
[ O
a O
] O
+ O
= O
max O
( O
0 O
, O
a O
) O
, O
and O
( O
ỹ O
, O
y O
) O
, O
often O
referred O
to O
" O
margin O
- O
rescaled O
" O
structured O
hinge O
loss O
, O
is O
a O
structured O
cost O
function O
that O
returns O
a O
nonnegative O
value O
indicating O
the O
difference O
between O
the O
predicted O
resultỹ O
and O
ground O
truth O
y O
. O

Problem O
Formulation O

In O
this O
paper O
, O
we O
focus O
on O
ECSP B-TaskName
tasks O
of O
event B-TaskName
detection I-TaskName
( O
ED B-TaskName
) O
and O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
( O
ERE B-TaskName
) O
. O
ED B-TaskName
can O
be O
divided O
into O
trigger O
classification O
for O
tokens O
and O
event O
classification O
for O
sentences O
. O
We O
denote O
the O
dataset O
by O
D O
= O
{ O
E O
, O
R O
, O
X O
} O
containing O
an O
event O
class O
set O
E O
, O
a O
multi O
- O
faceted O
eventrelation O
set O
R O
and O
the O
event O
corpus O
X O
, O
thereinto O
, O

E O
= O
{ O
e O
i O
| O
i O
∈ O
[ O
1 O
, O
|E| O
] O
} O
contains O
|E| O
event O
classes O
including O
a O
None O
; O
R O
= O
{ O
r O
i O
| O
i O
∈ O
[ O
1 O
, O
|R| O
] O
} O
contains O
|R| O
temporal O
, O

causal O
, O
subevent O
and O
coreference O
relationships O
among O
event O
mentions O
including O
a O
NA O
event O
- O
relation O
; O

X O
= O
{ O
X O
i O
| O
i O
∈ O
[ O
1 O
, O
K O
] O
} O
consists O
of O
K O
event O
mentions O

, O
where O
X O
i O
is O
denoted O
as O
a O
token O
sequence O
x O
= O
{ O
x O
j O
| O
j O
∈ O
[ O
1 O
, O
L B-HyperparameterName
] O
} O
with O
maximum O
L B-HyperparameterName
tokens O
. O
For O
trigger O
classification O
, O
the O
goal O
is O
to O
predict O
the O
index O
t O
( O
1 O
≤ O
t O
≤ O
L O
) O
of O
the O
trigger O
x O
t O
in O
each O
token O
sequence O
x O
and O
categorize O
x O
t O
into O
a O
specific O
event O
class O
e O
i O
∈ O
E. O
For O
event O
classification O
, O
we O
expect O
to O
predict O
the O
event O
label O
e O
i O
for O
each O
event O
mention O
X O
i O
. O
For O
event O
- O
relation O
extraction O
, O
we O
require O
to O
identify O
the O
relation O
r O
i O
∈ O
R O
for O
a O
pair O
of O
event O
mentionsẌ O
ij O
= O
( O
X O
i O
, O
X O
j O
) O
. O

In O
summary O
, O
our O
goal O
is O
to O
design O
an O
ECSP B-TaskName
model O
M O
Φ O
, O
aiming O
to O
tackle O
the O
tasks O
of O
: O
( O
1 O
) O
trigger B-TaskName
classification I-TaskName
: O
to O
predict O
the O
token O
labelỹ O
= O
M O
Φ O
( O
x O
) O
for O
the O
token O
list O
x O
; O
( O
2 O
) O
event B-TaskName
classification I-TaskName
: O
to O
predict O
the O
event O
class O
labelỸ O
= O
M O
Φ O
( O
X O
) O
for O
the O
event O
mention O
X O
; O
( O
3 O
) O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
: O
to O
predict O
the O
event O
- O
relation O
labelz O
= O
M O
Φ O
( O
Ẍ O
) O
for O
the O
event O
mention O
pairẌ O
. O

Model O
Overview O

As O
seen O
in O
Figure O
2 O
, O
SPEECH B-MethodName
combines O
three O
levels O
of O
energy O
: O
token O
, O
sentence O
, O
as O
well O
as O
document O
, O
and O
they O
respectively O
serve O
for O
three O
kinds O
of O
ECSP B-TaskName
tasks O
: O
( O
1 O
) O
token O
- O
level O
energy O
for O
trigger B-TaskName
classification I-TaskName
: O
considering O
energy O
- O
based O
modeling O
is O
able O
to O
capture O
long O
- O
range O
dependency O
among O
tokens O
without O
limits O
to O
token O
size O
; O
( O
2 O
) O
sentencelevel O
energy O
for O
event B-TaskName
classification I-TaskName
: O
considering O
energy O
- O
based O
hyperspheres O
can O
model O
the O
complex O
event O
structures O
and O
represent O
events O
efficiently O
; O
and O
( O
3 O
) O
document O
- O
level O
energy O
for O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
: O
considering O
energy O
- O
based O
modeling O
enables O
us O
to O
address O
the O
association O
among O
event O
mention O
pairs O
and O
event O
- O
relations O
. O
We O
leverage O
the O
trigger O
embeddings O
as O
event O
mention O
embeddings O
; O
the O
energy O
- O
based O
hyperspheres O
with O
a O
centroid O
and O
a O
radius O
as O
event O
class O
embeddings O
, O
and O
these O
three O
tasks O
are O
associative O
to O
each O
other O
. O

ℰ O
+2 O
ℰ O
+1 O
ℰ O
+1 O
2 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+2 O
ℰ O
+2 O

[ O
CLS O
] O
Former O
NOPD O
police O
officer O
David O
Warren O
shot O
and O
killed O
Henry O
Glover O

[ O
SEP O
] O
[ O
PAD O
] O
[ O
PAD O
] O
[ O
PAD O
] O
ℰ O
+2 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+1 O
3 O
ℰ O
+1 O
ℰ O
+1 O
ℰ O
+2 O
ℰ O
+2 O
ℰ O
+2 O
ℰ O
+2 O
… O
… O
… O
… O
… O
… O
… O
… O
… O
… O

Energy O
- O
based O
hypersphere O
embedding O
with O
a O
centroid O
and O
a O
radius O
as O
event O
type O
embedding O

Token O
- O
Level O
Energy O

Token O
- O
level O
energy O
serves O
for O
trigger B-TaskName
classification I-TaskName
. O
Given O
a O
token O
sequence O
x O
= O
{ O
x O
j O
|j O
∈ O
[ O
1 O
, O
L O
] O
} O
with O
trigger O
x O
t O
, O
we O
leverage O
a O
pluggable O
backbone O
encoder O
to O
obtain O
the O
contextual O
representation O
f O
1 O
( O
x O
) O
for O
each O
token O
, O
such O
as O
pre O
- O
trained O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa O
, O
Distil O
- O
BERT O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
and O
so O
on O
. O
We O
then O
predict O
the O
labelỹ O
= O
M O
Φ O
( O
x O
) O
of O
each O
token O
with O
an O
additional O
linear O
classifier O
. O
Inspired O
by O
SPENs O
for O
sequence O
labeling O
( O
Tu O
and O
Gimpel O
, O
2018 O
) O
, O
we O
also O
adopt O
an O
energy O
function O
for O
token O
classification O
. O
Energy O
Function O
. O
The O
token O
- O
level O
energy O
function O
is O
inherited O
from O
Eq O
( O
1 O
) O
, O
defined O
as O
: O

E O
Θ O
( O
x O
, O
y O
) O
= O
− O
 O
 O
 O
L O
n=1 O
|E|+2 O
i=1 O
y O
i O
n O
V O
1 O
, O
i O
f O
1 O
( O
x O
n O
) O
local O
+ O
L O
n=1 O
y O
n−1 O
W O
1 O
y O
n O
label O
 O
 O
 O
( O
4 O
) O

where O
y O
i O
n O
is O
the O
i O
th O
entry O
of O
the O
vector O
y O
n O
∈ O
y O
, O
indicating O
the O
probability O
of O
the O
n O
th O
token O
x O
n O
being O
labeled O
with O
i O
( O
i O
for O
e O
i O
, O
|E|+1 O
for O
non O
- O
trigger O
and O
|E|+2 O
for O
padding O
token O
) O
. O
f O
1 O
( O
• O
) O
denotes O
the O
feature O
encoder O
of O
tokens O
. O
Here O
our O
learnable O
parameters O
are O
Θ O
= O
( O
V O
1 O
, O
W O
1 O
) O
, O
thereinto O
, O
V O
1 O
, O
i O
∈ O
R O
d O
is O
a O
parameter O
vector O
for O
token O
label O
i O
, O
and O
W O
1 O
∈ O
R O
( O
|E|+2 O
) O
× O
( O
|E|+2 O
) O
contains O
the O
bilinear O
product O
between O
y O
n−1 O
and O
y O
n O
for O
token O
label O
pair O
terms O
. O

Loss O
Function O
. O
The O
training O
objective O
for O
trigger B-TaskName
classification I-TaskName
is O
denoted O
by O
: O

L O
tok O
= O
L O
i=1 O
( O
ỹ O
i O
, O
y O
i O
) O
− O
E O
Θ O
( O
x O
i O
, O
ỹ O
i O
) O
+ O
E O
Θ O
( O
x O
i O
, O
y O
i O
) O
+ O
+ O
µ B-HyperparameterName
1 I-HyperparameterName
L O
CE O
( O
ỹ O
i O
, O
y O
i O
) O
( O
5 O
) O

whereỹ O
i O
and O
y O
i O
respectively O
denote O
predicted O
results O
and O
ground O
truth O
. O
The O
first O
half O
of O
Eq O
( O
5 O
) O
is O
inherited O
from O
Eq O
( O
3 O
) O
for O
the O
energy O
function O
, O
and O
in O
the O
latter O
half O
, O
L O
CE O
( O
ỹ O
i O
, O
y O
i O
) O
is O
the O
trigger B-TaskName
classification I-TaskName
cross O
entropy O
loss O
, O
and O
µ B-HyperparameterName
1 I-HyperparameterName
is O
its O
ratio O
. O

Sentence O
- O
Level O
Energy O

Sentence O
- O
level O
energy O
serves O
for O
event B-TaskName
classification I-TaskName
. O
Given O
the O
event O
mention O
X O
i O
with O
the O
trigger O
x O
t O
, O
we O
utilize O
the O
trigger O
embedding O
f O
1 O
( O
x O
t O
) O
as O
the O
event O
mention O
embedding O
f O
2 O
( O
X O
) O
, O
where O
f O
2 O
( O
• O
) O
denotes O
the O
feature O
encoder O
of O
event O
mentions O
. O
We O
then O
predict O
the O
class O
of O
each O
event O
mention O
with O
energy O
- O
based O
hyperspheres O
, O
denoted O
bỹ O
Y O
= O
M O
Φ O
( O
X O
) O
. O

Specifically O
, O
we O
use O
an O
energy O
- O
based O
hypersphere O
to O
represent O
each O
event O
class O
, O
and O
assume O
that O
the O
event O
mentions O
of O
each O
event O
class O
should O
distribute O
on O
the O
corresponding O
hypersphere O
with O
the O
lowest O
energy O
. O
We O
then O
calculate O
the O
probability O
of O
the O
event O
mention O
X O
categorizing O
into O
the O
class O
e O
i O
with O
a O
hyperspherical O
measurement O
function O
: O

S O
( O
X O
, O
P O
i O
) O
= O
exp O
− O
[ O
P O
i O
−f O
2 O
( O
X O
) O
2 O
−γ O
] O
+ O
|E| O
j=1 O
exp O
− O
[ O
P O
j O
−f O
2 O
( O
X O
) O
2 O
−γ O
] O
+ O
( O
6 O
) O

where O
[ O
a O
] O
+ O
= O
max O
( O
0 O
, O
a O
) O
, O
P O
i O
denotes O
the O
hypersphere O
centroid O
embedding O
of O
e O
i O
. O
• O
denotes O
the O
Euclidean O
distance O
. O
γ O
is O
the O
radius O
of O
the O
hypersphere O
, O
which O
can O
be O
scalable O
or O
constant O
. O
We O
simply O
set O
γ O
= O
1 O
in O
this O
paper O
, O
meaning O
that O
each O
event O
class O
is O
represented O
by O
a O
unit O
hypersphere O
. O
Larger O
S O
( O
X O
, O
P O
i O
) O
signifies O
that O
the O
event O
mention O
X O
are O
more O
likely O
be O
categorized O
into O
P O
i O
corresponding O
to O
e O
i O
. O
To O
measure O
the O
energy O
score O
between O
event O
classes O
and O
event O
mentions O
, O
we O
also O
adopt O
an O
energy O
function O
for O
event B-TaskName
classification I-TaskName
. O
Energy O
Function O
. O
The O
sentence O
- O
level O
energy O
function O
is O
inherited O
from O
Eq O
( O
1 O
) O
, O
defined O
as O
: O

E O
Θ O
( O
X O
, O
Y O
) O
= O
− O
 O
 O
 O
|E| O
i=1 O
Y O
i O
V O
2 O
, O
i O
f O
2 O
( O
X O
) O
local O
+ O
w O
2 O
g O
( O
W O
2 O
Y O
) O
label O
 O
 O
 O
( O
7 O
) O

where O
Y O
i O
∈ O
Y O
indicates O
the O
probability O
of O
the O
event O
mention O
X O
being O
categorized O
to O
e O
i O
. O
Here O
our O
learnable O
parameters O
are O
Θ O
= O
( O
V O
2 O
, O
w O
2 O
, O
W O
2 O
) O
, O
thereinto O
, O
V O
2 O
, O
i O
∈ O
R O
d O
is O
a O
parameter O
vector O
for O
e O
i O
, O
w O
2 O
∈ O
R O
|E| O
and O
W O
2 O
∈ O
R O
|E|×|E| O
. O
Loss O
Function O
. O
The O
training O
objective O
for O
event B-TaskName
classification I-TaskName
is O
denoted O
by O
: O

L O
sen O
= O
K O
i=1 O
Ỹ O
i O
, O
Y O
i O
− O
E O
Θ O
X O
i O
, O
Ỹ O
i O
+ O
E O
Θ O
( O
X O
i O
, O
Y O
i O
) O
+ O
+ O
µ B-HyperparameterName
2 I-HyperparameterName
L O
CE O
Ỹ O
i O
, O
Y O
i O
( O
8 O
) O

where O
the O
first O
half O
is O
inherited O
from O
Eq O
( O
3 O
) O
, O
and O
in O
the O
latter O
half O
, O
L O
CE O
is O
a O
cross O
entropy O
loss O
for O
predicted O
resultsỸ O
i O
and O
ground O
truth O
Y O
i O
. O
µ B-HyperparameterName
2 I-HyperparameterName
is O
a O
ratio O
for O
event B-TaskName
classification I-TaskName
cross O
entropy O
loss O
. O

Document O
- O
Level O
Energy O

Document O
- O
level O
energy O
serves O
for O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
. O
Given O
event O
mentions O
X O
in O
each O
document O
, O
we O
model O
the O
embedding O
interactions O
of O
each O
event O
mention O
pair O
with O
a O
comprehensive O
feature O
vector O
f O
3 O
( O
Ẍ O
ij O
) O
= O
f O
2 O
( O
X O
i O
) O
, O
f O
2 O
( O
X O
j O
) O
, O
f O
2 O
( O
X O
i O
) O
f O
2 O
( O
X O
j O
) O
. O
We O
then O
predict O
the O
relation O
between O
each O
event O
mention O
pair O
with O
a O
linear O
classifier O
, O
denoted O
byz O
= O
M O
Φ O
( O
Ẍ O
) O
. O
Inspired O
by O
SPENs O
for O
multi O
- O
label O
classification O
( O
Tu O
and O
Gimpel O
, O
2018 O
) O
, O
we O
also O
adopt O
an O
energy O
function O
for O
ERE B-TaskName
. O

Energy O
Function O
. O
The O
document O
- O
level O
energy O
function O
is O
inherited O
from O
Eq O
( O
1 O
) O
, O
defined O
as O
: O

E O
Θ O
( O
Ẍ O
, O
z O
) O
= O
− O
 O
 O
 O
|R| O
i=1 O
z O
i O
V O
3 O
, O
i O
f O
3 O
( O
Ẍ O
) O
local O
+ O
w O
3 O
g O
( O
W O
3 O
z O
) O
label O
 O
 O
 O
( O
9 O
) O

where O
z O
i O
∈ O
z O
indicates O
the O
probability O
of O
the O
event O
mention O
pairẌ O
having O
the O
relation O
of O
r O
i O
. O
Here O
our O
learnable O
parameters O
are O
Θ O
= O
( O
V O
3 O
, O
w O
3 O
, O
W O
3 O
) O
, O
thereinto O
, O
V O
3 O
, O
i O
∈ O
R O
3d O
is O
a O
parameter O
vector O
for O
r O
i O
, O
w O
3 O
∈ O
R O
|R| O
and O
W O
3 O
∈ O
R O
|R|×|R| O
. O

Loss O
Function O
. O
The O
training O
objective O
for O
eventrelation B-TaskName
extraction I-TaskName
is O
denoted O
by O
: O

L O
doc O
= O
N O
k=1 O
( O
z O
k O
, O
z O
k O
) O
− O
E O
Θ O
Ẍ O
k O
, O
z O
k O
+ O
E O
Θ O
Ẍ O
k O
, O
z O
k O
+ O
+ O
µ B-HyperparameterName
3 I-HyperparameterName
L O
CE O
( O
z O
k O
, O
z O
k O
) O
( O
10 O
) O

where O
the O
first O
half O
is O
inherited O
from O
Eq O
( O
3 O
) O
, O
and O
in O
the O
latter O
half O
, O
L O
CE O
( O
z O
k O
, O
z O
k O
) O
is O
the O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
cross O
entropy O
loss O
, O
µ B-HyperparameterName
3 I-HyperparameterName
is O
its O
ratio O
, O
and O
N O
denotes O
the O
quantity O
of O
event O
mention O
pairs O
. O

The O
final O
training O
loss O
for O
SPEECH B-MethodName
M O
Φ O
parameterized O
by O
Φ O
is O
defined O
as O
: O

L O
= O
λ B-HyperparameterName
1 I-HyperparameterName
L O
tok O
+ O
λ B-HyperparameterName
2 I-HyperparameterName
L O
sen O
+ O
λ B-HyperparameterName
3 I-HyperparameterName
L O
doc O
+ O
Φ O
2 O
2 O
( O
11 O
) O

where O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
λ B-HyperparameterName
3 I-HyperparameterName
are O
the O
loss O
ratios O
respectively O
for O
trigger B-TaskName
classification I-TaskName
, O
event B-TaskName
classification I-TaskName
and O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
tasks O
. O
We O
add O
the O
penalty O
term O
Φ O
2 O
2 O
with O
L O
2 O
regularization O
. O

Experiments O

The O
experiments O
refer O
to O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
( O
ECSP B-TaskName
) O
and O
comprise O
three O
tasks O
: O
( O
1 O
) O
Trigger B-TaskName
Classification I-TaskName
; O

( O
2 O
) O
Event B-TaskName
Classification I-TaskName
; O
and O

( O
3 O
) O
Event B-TaskName
- I-TaskName
Relation I-TaskName
Extraction I-TaskName
. O
Datasets O
. O
Considering O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
tasks O
in O
this O
paper O
require O
fine O
- O
grained O
annotations O
for O
events O
, O
such O
as O
labels O
of O
tokens O
, O
event O
mentions O
, O
and O
event O
- O
relations O
, O
we O
select O
two O
newly O
- O
proposed O
datasets O
meeting O
the O
requirements O
: O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
and O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
. O
Note O
that O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
is O
derived O
from O
ONTOEVENT B-DatasetName
which O
is O
formatted O
in O
a O
sentence O
level O
. O
We O
reorganize O
it O
and O
make O
it O
format O
in O
a O
document O
level O
, O
similar O
to O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
. O
Thus O
the O
train O
, O
validation O
, O
and O
test O
sets O
of O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
are O
also O
different O
from O
the O
original O
ONTO B-DatasetName
- I-DatasetName
EVENT I-DatasetName
. O
We O
release O
the O
reconstructed O
dataset O
and O
Baselines O
. O
For O
trigger B-TaskName
classification I-TaskName
and O
event B-TaskName
classification I-TaskName
, O
we O
adopt O
models O
aggregated O
dynamic O
multi O
- O
pooling O
mechanism O
, O
i.e. O
, O
DMCNN B-MethodName
( O
Chen O
et O
al O
. O
, O
2015 O
) O
and O
DMBERT B-MethodName
( O
Wang O
et O
al O
. O
, O
2019 O
) O
; O
sequence O
labeling O
models O
with O
conditional O
random O
field O
( O
CRF O
) O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
, O
i.e. O
, O
BiLSTM B-MethodName
- I-MethodName
CRF I-MethodName
and O
BERT B-MethodName
- I-MethodName
CRF I-MethodName
; O
generative O
ED O
models O
, O
i.e. O
, O
TANL B-MethodName
( O
Paolini O
et O
al O
. O
, O
2021 O
) O
and O
TEXT2EVENT B-MethodName
( O
Lu O
et O
al O
. O
, O
2021 O
) O
. O
We O
also O
adopt O
some O
ED B-TaskName
models O
considering O
document O
- O
level O
associations O
, O
i.e. O
, O
MLBiNet B-MethodName
( O
Lou O
et O
al O
. O
, O
2021 O
) O
and O
CorED B-MethodName
- I-MethodName
BERT I-MethodName
( O
Sheng O
et O
al O
. O
, O
2022 O
) O
. O
Besides O
, O
we O
compare O
our O
energy O
- O
based O
hyperspheres O
with O
the O
vanilla O
hyperspherical B-MethodName
prototype I-MethodName
network I-MethodName
( O
HPN B-MethodName
) O
( O
Mettes O
et O
al O
. O
, O
2019 O
) O
and O
prototype O
- O
based O
model O
OntoED B-MethodName
. O
Note O
that O
unlike O
vanilla O
HPN B-MethodName
( O
Mettes O
et O
al O
. O
, O
2019 O
) O
which O
represents O
all O
classes O
on O
one O
hypersphere O
, O
the O
HPN B-MethodName
adopted O
in O
this O
paper O
represents O
each O
class O
with O
a O
distinct O
hypersphere O
. O
For O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
, O
we O
select O
RoBERTa B-MethodName
, O
which O
is O
the O
same O
baseline O
used O
in O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
, O
and O
also O
serves O
as O
the O
backbone O
for O
most O
of O
recent O
ERE B-TaskName
models O
( O
Hwang O
et O
al O
. O
, O
2022 O
; O
Man O
et O
al O
. O
, O
2022 O
) O
. O

Datasets O
and O
Baselines O

1 O
https O
: O
/ O
/ O
github.com O
/ O
zjunlp O
/ O
SPEECH B-MethodName
. O

Implementation O
Details O

With O
regard O
to O
settings O
of O
the O
training O
process O
, O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer B-HyperparameterName
is O
used O
, O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
. O
The O
maximum O
length O
L B-HyperparameterName
of O
a O
token O
sequence O
is O
128 B-HyperparameterValue
, O
and O
the O
maximum B-HyperparameterName
quantity I-HyperparameterName
of I-HyperparameterName
event I-HyperparameterName
mentions I-HyperparameterName
in I-HyperparameterName
one I-HyperparameterName
document I-HyperparameterName
is O
set O
to O
40 B-HyperparameterValue
for O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
and O
50 B-HyperparameterValue
for O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
. O
The O
loss O
ratios O
, O
µ B-HyperparameterName
1 I-HyperparameterName
, O
µ B-HyperparameterName
2 I-HyperparameterName
, O
µ B-HyperparameterName
3 I-HyperparameterName
, O
for O
token O
, O
sentence O
and O
document O
- O
level O
energy O
function O
are O
all O
set O
to O
1 B-HyperparameterValue
. O
The O
value O
of O
loss O
ratio O
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
λ B-HyperparameterName
3 I-HyperparameterName
, O
for O
trigger B-TaskName
classification I-TaskName
, O
event B-TaskName
classification I-TaskName
and O
eventrelation B-TaskName
extraction I-TaskName
depends O
on O
different O
tasks O
, O
and O
we O
introduce O
them O
in O
Appendix O
B. O
We O
evaluate O
the O
performance O
of O
ED B-TaskName
and O
ERE B-TaskName
with O
micro B-MetricName
precision I-MetricName
( O
P B-MetricName
) O
, O
Recall B-MetricName
( O
R B-MetricName
) O
and O
F1 B-MetricName
Score O
( O
F1 B-MetricName
) O
. O

Event B-TaskName
Trigger I-TaskName
Classification I-TaskName

We O
present O
details O
of O
event B-TaskName
trigger I-TaskName
classification I-TaskName
experiment O
settings O
in O
Appendix O
B.1 O
. O
As O
seen O
from O
the O
results O
in O
Table O
2 O
, O
SPEECH B-MethodName
demonstrates O
superior O
performance O
over O
all O
baselines O
, O
notably O
MLBi B-MethodName
- I-MethodName
Net I-MethodName
( O
Lou O
et O
al O
. O
, O
2021 O
) O
and O
CorED B-MethodName
- I-MethodName
BERT I-MethodName
( O
Sheng O
et O
al O
. O
, O
2022 O
) O
, O
even O
if O
these O
two O
models O
consider O
cross O
- O
sentence O
semantic O
information O
or O
incorporate O
type O
- O
level O
and O
instance O
- O
level O
correlations O
. O
The O
main O
reason O
may O
be O
due O
to O
the O
energy O
- O
based O
nature O
of O
SPEECH B-MethodName
. O
As O
seen O
from O
the O
last O
row O
of O
Table O
2 O
, O
the O
removal O
of O
energy O
functions O
from O
SPEECH B-MethodName
can O
result O
in O
a O
performance O
decrease O
. O
Specifically O
for O
trigger B-TaskName
classification I-TaskName
, O
energy O
- O
based O
modeling O
enables O
capture O
long O
- O
range O
dependency O
of O
tokens O
and O
places O
no O
limits O
on O
the O
size O
of O
event O
structures O
. O
In O
addition O
, O
SPEECH B-MethodName
also O
excels O
generative O
models O
, O
i.e. O
, O
TANL B-MethodName
( O
Paolini O
et O
al O
. O
, O
2021 O
) O
and O
TEXT2EVENT B-MethodName
( O
Lu O
et O
al O
. O
, O
2021 O
) O
, O
thereby O
demonstrating O
the O
efficacy O
of O
energy O
- O
based O
modeling O
. O

Event B-TaskName
Classification I-TaskName

The O
specifics O
of O
event B-TaskName
classification I-TaskName
experiment O
settings O
are O
elaborated O
in O
Appendix O
B.2 O
, O
with O
results O
illustrated O
in O
Table O
3 O
. O
We O
can O
observe O
that O
SPEECH B-MethodName
provides O
considerable O
advantages O
on O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
, O
while O
the O
performance O
on O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
is O
not O
superior O
enough O
. O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
contains O
overlapping O
where O
multiple O
event O
classes O
may O
exist O
in O
the O
same O
event O
mention O
, O
which O
could O
be O
the O
primary O
reason O
for O
SPEECH B-MethodName
not O
performing O
well O
enough O
in O
this O
case O
. O
This O
impact O
could O
be O
exacerbated O
when O
joint O
training O
with O
other O
ECSP B-TaskName
tasks O
. O

Upon O
comparison O
with O
prototype O
- O
based O
methods O
without O
energy O
- O
based O
modeling O
, O
i.e. O
, O
HPN B-MethodName
( O
Mettes O
et O
al O
. O
, O
2019 O
) O
and O
OntoED B-MethodName
, O
SPEECH B-MethodName
is O
still O
dominant O
on O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
, O
despite O
HPN B-MethodName
represents O
classes O
with O
hyperspheres O
and O
On B-MethodName
- I-MethodName
toED I-MethodName
leverages O
hyperspheres O
integrated O
with O
eventrelation O
semantics O
. O
If O
we O
exclude O
energy O
functions O
from O
SPEECH B-MethodName
, O
performance O
will O
degrade O
, O
as O
seen O
from O
the O
last O
row O
in O
Table O
3 O
. O
This O
insight O
suggests O
that O
energy O
functions O
contribute O
positively O
to O
event B-TaskName
classification I-TaskName
, O
which O
enable O
the O
model O
to O
directly O
capture O
complicated O
dependency O
between O
event O
mentions O
and O
event O
types O
, O
instead O
of O
implicitly O
inferring O
from O
data O
. O
Besides O
, O
SPEECH B-MethodName
also O
outperforms O
generative O
models O
like O
TANL B-MethodName
and O
TEXT2EVENT B-MethodName
on O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
, O
indicating O
the O
superiority O
of O
energy O
- O
based O
hyperspherical O
modeling O
. O

Event B-TaskName
- I-TaskName
Relation I-TaskName
Extraction I-TaskName

We O
present O
the O
specifics O
of O
event B-TaskName
- I-TaskName
relation I-TaskName
extraction I-TaskName
experiment O
settings O
in O
Appendix O
B.3 O
. O
As O
seen O
from O
the O
results O
in O
. O
" O
All O
Joint O
" O
in O
the O
last O
two O
rows O
denotes O
treating O
all O
ERE B-TaskName
tasks O
as O
one O
task O
. O
mention O
pairs O
and O
event O
- O
relation O
labels O
. O
While O
on O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
, O
SPEECH B-MethodName
significantly O
outperforms O
RoBERTa B-MethodName
on O
ERE B-TaskName
subtasks O
referring O
to O
subevent O
relations O
or O
trained O
on O
all O
event O
- O
relations O
, O
but O
fails O
to O
exceed O
RoBERTa B-MethodName
on O
ERE B-TaskName
subtasks O
referring O
to O
temporal O
and O
causal O
relations O
. O
The O
possible O
reason O
is O
that O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
contains O
less O
positive O
eventrelations O
than O
negative O
NA O
relations O
. O
Given O
that O
SPEECH B-MethodName
models O
all O
these O
relations O
equivalently O
with O
the O
energy O
function O
, O
it O
becomes O
challenging O
to O
classify O
NA O
effectively O
. O
But O
this O
issue O
will O
be O
markedly O
improved O
if O
the O
quantity O
of O
positive O
eventrelations O
decreases O
, O
since O
SPEECH B-MethodName
performs O
better O
on O
subevent O
relations O
despite O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
having O
much O
less O
subevent O
relations O
than O
temporal O
and O
causal O
ones O
as O
shown O
in O
Table O
1 O
. O
Furthermore O
, O
even O
though O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
containing O
fewer O
positive O
event O
- O
relations O
than O
NA O
overall O
, O
SPEECH B-MethodName
still O
performs O
well O
. O
These O
results O
suggest O
that O
SPEECH B-MethodName
excels O
in O
modeling O
classes O
with O
fewer O
samples O
. O
Note O
that O
SPEECH B-MethodName
also O
performs O
well O
when O
training O
on O
all O
event O
- O
relations O
( O
" O
All O
Joint O
" O
) O
of O
the O
two O
datasets O
, O
indicating O
that O
SPEECH B-MethodName
is O
still O
advantageous O
in O
the O
scenario O
with O
more O
classes O
. O

Further O
Analysis O

Analysis O
On O
Energy O
- O
Based O
Modeling O

We O
list O
some O
values O
of O
energy O
loss O
defined O
in O
Eq O
( O
5 O
) O
, O
( O
8 O
) O
and O
( O
10 O
) O
when O
training O
respectively O
for O
token O
, O
sentence O
and O
document O
, O
as O
presented O
in O
Figure O
3 O
. O
The O
values O
of O
token O
- O
level O
energy O
loss O
are O
observably O
larger O
than O
those O
at O
the O
sentence O
and O
document O
levels O
. O
This O
can O
be O
attributed O
to O
the O
fact O
that O
the O
energy O
loss O
is O
related O
to O
the O
quantity O
of O
samples O
, O
and O
a O
single O
document O
typically O
contains O
much O
more O
tokens O
than O
sentences O
or O
sentence O
pairs O
. O
All O
three O
levels O
of O
energy O
loss O
exhibit O
a O
gradual O
decrease O
over O
the O
course O
of O
training O
, O
indicating O
that O
SPEECH B-MethodName
, O
through O
energy O
- O
based O
modeling O
, O
effectively O
minimizes O
the O
discrepancy O
between O
predicted O
results O
and O
ground O
truth O
. O
The O
energy O
functions O
for O
token O
, O
sentence O
and O
document O
defined O
in O
Eq O
( O
4 O
) O
, O
( O
7 O
) O
and O
( O
9 O
) O
, O
reflect O
that O
the O
implementation O
of O
energy O
- O
based O
modeling O
in O
SPEECH B-MethodName
is O
geared O
towards O
enhancing O
compatibility O
between O
input O
/ O
output O
pairs O
. O
The O
gradually O
- O
decreasing O
energy O
loss O
demonstrates O
that O
SPEECH B-MethodName
can O
model O
intricate O
event O
structures O
at O
the O
token O
, O
sentence O
, O
and O
document O
levels O
through O
energy O
- O
based O
optimization O
, O
thereby O
improving O
the O
outcomes O
of O
structured B-TaskName
prediction I-TaskName
. O

Case O
Study O
: O
Energy O
- O
Based O
Hyperspheres O

As O
seen O
in O
Figure O
4 O
, O
we O
visualize O
the O
event O
class O
embedding O
of O
" O
Attack O
" O
and O
20 O
event O
mention O
embeddings O
as O
generated O
by O
both O
SPEECH B-MethodName
and O
SPEECH B-MethodName
without I-MethodName
energy I-MethodName
functions I-MethodName
. O
We O
observe O
that O
for O
SPEECH B-MethodName
with O
energy O
- O
based O
modelling O
, O
the O
instances O
lie O
near O
the O
surface O
of O
the O
corresponding O
hypersphere O
, O
while O
they O
are O
more O
scattered O
when O
not O
equipped O
with O
energy O
- O
based O
modeling O
, O
which O
subsequently O
diminishes O
the O
performance O
of O
event O
classification O
. O
This O
observation O
suggests O
that O
SPEECH B-MethodName
derives O
significant O
benefits O
from O
modeling O
with O
energy O
- O
based O
hyperspheres O
. O
The O
visualiza O
- O
tion O
results O
further O
demonstrate O
the O
effectiveness O
of O
SPEECH B-MethodName
equipped O
with O
energy O
- O
based O
modeling O
. O

Error O
Analysis O

We O
further O
conduct O
error O
analysis O
by O
a O
retrospection O
of O
experimental O
results O
and O
datasets O
. O
( O
1 O
) O
One O
typical O
error O
relates O
to O
the O
unbalanced O
data O
distribution O
. O

Considering O
every O
event O
type O
and O
event O
- O
relation O
contain O
different O
amount O
of O
instances O
, O
unified O
modeling O
with O
energy O
- O
based O
hyperspheres O
may O
not O
always O
be O
impactful O
. O

( O
2 O
) O
The O
second O
error O
relates O
to O
the O
overlapping O
event O
mentions O
among O
event O
types O
, O
meaning O
that O
the O
same O
sentence O
may O
mention O
multiple O
event O
types O
. O
As O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
contains O
many O
overlappings O
, O
it O
might O
be O
the O
reason O
for O
its O
mediocre O
performance O
on O
ED O
. O

( O
3 O
) O
The O
third O
error O
relates O
to O
associations O
with O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
tasks O
. O
As O
trigger B-TaskName
classification I-TaskName
is O
closely O
related O
to O
event B-TaskName
classification I-TaskName
, O
wrong O
prediction O
of O
tokens O
will O
also O
influence O
classifying O
events O
. O

Conclusion O
and O
Future O
Work O

In O
this O
paper O
, O
we O
propose O
a O
novel O
approach O
entitled O
SPEECH B-MethodName
to O
tackle O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
with O
energy O
- O
based O
hyperspheres O
. O
We O
represent O
event O
classes O
as O
hyperspheres O
with O
token O
, O
sentence O
and O
document O
- O
level O
energy O
, O
respectively O
for O
trigger B-TaskName
classification I-TaskName
, O
event B-TaskName
classification I-TaskName
and O
event B-TaskName
relation I-TaskName
extraction I-TaskName
tasks O
. O
We O
evaluate O
SPEECH B-MethodName
on O
two O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
datasets O
, O
and O
experimental O
results O
demonstrate O
that O
SPEECH B-MethodName
is O
able O
to O
model O
manifold O
event O
structures O
with O
dependency O
and O
obtain O
effective O
event O
representations O
. O
In O
the O
future O
, O
we O
intend O
to O
enhance O
our O
work O
by O
modeling O
more O
complicated O
structures O
and O
extend O
it O
to O
other O
structured O
prediction O
tasks O
. O

Acknowledgements O

We O
would O
like O
to O
express O
gratitude O
to O
the O
anonymous O
reviewers O
for O
their O
kind O
comments O
. O
This O
work O
was O
supported O
by O
the O
Zhejiang O
Provincial O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O

LGG22F030011 O
) O
, O
Yongjiang O
Talent O
Introduction O
Programme O
( O
2021A-156 O
- O
G O
) O
, O
CAAI O
- O
Huawei O
Mind O
- O
Spore O
Open O
Fund O
, O
and O
NUS O
- O
NCS O
Joint O
Laboratory O
( O
A-0008542 O
- O
00 O
- O
00 O
) O
. O

Limitations O

Although O
SPEECH B-MethodName
performs O
well O
on O
event B-TaskName
- I-TaskName
centric I-TaskName
structured I-TaskName
prediction I-TaskName
tasks O
in O
this O
paper O
, O
it O
still O
has O
some O
limitations O
. O
The O
first O
limitation O
relates O
to O
efficiency O
. O
As O
SPEECH B-MethodName
involves O
many O
tasks O
and O
requires O
complex O
calculation O
, O
the O
training O
process O
is O
not O
very O
prompt O
. O
The O
second O
limitation O
relates O
to O
robustness O
. O
As O
seen O
in O
the O
experimental O
analysis O
in O
§ O
4.5 O
, O
SPEECH B-MethodName
seems O
not O
always O
robust O
to O
unevenly O
- O
distributed O
data O
. O
The O
third O
limitation O
relates O
to O
universality O
. O
Not O
all O
eventcentric B-TaskName
structured I-TaskName
prediction I-TaskName
tasks O
can O
simultaneously O
achieve O
the O
best O
performance O
at O
the O
same O
settings O
of O
SPEECH B-MethodName
. O

Appendices O
A O
Multi O
- O
Faceted O
Event O
- O
Relations O

Note O
that O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
and O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
both O
includes O
multi O
- O
faceted O
event O
- O
relations O
. O

MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
in O
this O
paper O
contains O
6 O
temporal O
relations O
: O
BEFORE O
, O
OVERLAP O
, O
CONTAINS O
, O
SIMULTANEOUS O
, O
BEGINS O
- O
ON O
, O
ENDS O
- O
ON O
; O
2 O
causal O
relations O
: O
CAUSE O
, O
PRECONDITION O
; O
and O
1 O
subevent O
relation O
: O
subevent_relations O
. O

ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
in O
this O
paper O
contains O
3 O
temporal O
relations O
: O
BEFORE O
, O
AFTER O
, O
EQUAL O
; O
and O
2 O
causal O
relations O
: O
CAUSE O
, O
CAUSEDBY O
. O

We O
also O
add O
a O
NA O
relation O
to O
signify O
no O
relation O
between O
the O
event O
mention O
pair O
for O
the O
two O
datasets O
. O

B O
Implementation O
Details O
for O
Different O
Tasks O
B.1 O
Event B-TaskName
Trigger I-TaskName
Classification I-TaskName

Settings O
. O
We O
follow O
the O
similar O
evaluation O
protocol O
of O
standard O
ED O
models O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Sheng O
et O
al O
. O
, O
2022 O
) O
on O
trigger B-TaskName
classification I-TaskName
tasks O
. O
We O
present O
the O
results O
in O
Table O
2 O
when O
jointly O
training O
with O
event B-TaskName
classification I-TaskName
and O
the O
whole O
ERE B-TaskName
task O
( O
" O
All O
Joint O
" O
in O
Table O
4 O
) O
. O
The O
backbone O
encoder O
is O
pretrained O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
The O
loss O
ratio O
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
λ B-HyperparameterName
3 I-HyperparameterName
in O
Eq O
( O
11 O
) O
are O
respectively O
set O
to O
1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
for O
both O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
and O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
. O

B.2 O
Event B-TaskName
Classification I-TaskName

Settings O
. O
We O
follow O
the O
similar O
evaluation O
protocol O
of O
standard O
ED B-TaskName
models O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
on O
event B-TaskName
classification I-TaskName
tasks O
. O
We O
present O
the O
results O
in O
Table O
3 O
when O
jointly O
training O
with O
trigger B-TaskName
classification I-TaskName
and O
all O
ERE B-TaskName
subtasks O
( O
" O
+ O
joint O
" O
in O
Table O
4 O
) O
. O
The O
backbone O
encoder O
is O
pretrained O
DistilBERT O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
. O
The O
loss O
ratio O
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
λ B-HyperparameterName
3 I-HyperparameterName
in O
Eq O
( O
11 O
) O
are O
respectively O
set O
to O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
for O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
and O
1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
for O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
. O

B.3 O
Event B-TaskName
- I-TaskName
Relation I-TaskName
Extraction I-TaskName

Settings O
. O
We O
follow O
the O
similar O
ERE B-TaskName
experiment O
settings O
with O
on O
several O
subtasks O
, O
by O
separately O
and O
jointly O
training O
on O
temporal O
, O
causal O
, O
and O
subevent O
event O
- O
relations O
. O
We O
present O
the O
results O
in O
Table O
4 O
when O
jointly O
training O
with O
trigger B-TaskName
classification I-TaskName
and O
event B-TaskName
classification I-TaskName
tasks O
. O
The O
backbone O
encoder O
is O
pretrained O
Distil O
- O
BERT O
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
. O
On O
ONTOEVENT B-DatasetName
- I-DatasetName
DOC I-DatasetName
dataset O
, O
the O
loss O
ratio O
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
λ B-HyperparameterName
3 I-HyperparameterName
in O
Eq O
( O
11 O
) O
are O
respectively O
set O
to O
1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
for O
all O
ERE B-TaskName
subtasks O
. O
On O
MAVEN B-DatasetName
- I-DatasetName
ERE I-DatasetName
dataset O
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
λ B-HyperparameterName
3 I-HyperparameterName
are O
respectively O
set O
to O
0.1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
1 B-HyperparameterValue
for O
" O
All O
Joint O
" O
ERE B-TaskName
subtasks O
in O
Table O
4 O
; O
1 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
4 B-HyperparameterValue
for O
" O
+ O
joint O
" O
; O
1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
for O
" O
Temporal O
" O
and O
" O
Causal O
" O
; O
and O
1 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
0.08 B-HyperparameterValue
for O
" O
Subevent O
" O
. O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
D1 O
. O
Did O
you O
report O
the O
full O
text O
of O
instructions O
given O
to O
participants O
, O
including O
e.g. O
, O
screenshots O
, O
disclaimers O
of O
any O
risks O
to O
participants O
or O
annotators O
, O
etc O
. O
? O
No O
response O
. O

D2 O
. O
Did O
you O
report O
information O
about O
how O
you O
recruited O
( O
e.g. O
, O
crowdsourcing O
platform O
, O
students O
) O
and O
paid O
participants O
, O
and O
discuss O
if O
such O
payment O
is O
adequate O
given O
the O
participants O
' O
demographic O
( O
e.g. O
, O
country O
of O
residence O
) O
? O
No O
response O
. O

D3 O
. O
Did O
you O
discuss O
whether O
and O
how O
consent O
was O
obtained O
from O
people O
whose O
data O
you O
're O
using O
/ O
curating O
? O
For O
example O
, O
if O
you O
collected O
data O
via O
crowdsourcing O
, O
did O
your O
instructions O
to O
crowdworkers O
explain O
how O
the O
data O
would O
be O
used O
? O
No O
response O
. O

D4 O
. O
Was O
the O
data O
collection O
protocol O
approved O
( O
or O
determined O
exempt O
) O
by O
an O
ethics O
review O
board O
? O
No O
response O
. O
D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
No O
response O
. O

Combating O
the O
Curse O
of O
Multilinguality O
in O
Cross O
- O
Lingual O
WSD B-TaskName
by O
Aligning O
Sparse O
Contextualized O
Word O
Representations O

In O
this O
paper O
, O
we O
advocate O
for O
using O
large B-MethodName
pretrained I-MethodName
monolingual I-MethodName
language I-MethodName
models I-MethodName
in O
cross B-TaskName
lingual I-TaskName
zero I-TaskName
- I-TaskName
shot I-TaskName
word I-TaskName
sense I-TaskName
disambiguation I-TaskName
( O
WSD O
) O
coupled O
with O
a O
contextualized O
mapping O
mechanism O
. O
We O
also O
report O
rigorous O
experiments O
that O
illustrate O
the O
effectiveness O
of O
employing O
sparse O
contextualized O
word O
representations O
obtained O
via O
a O
dictionary O
learning O
procedure O
. O
Our O
experimental O
results O
demonstrate O
that O
the O
above O
modifications O
yield O
a O
significant O
improvement O
of O
nearly O
6.5 B-MetricValue
points O
of O
increase O
in O
the O
average B-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
( O
from O
62.0 B-MetricValue
to O
68.5 B-MetricValue
) O
over O
a O
collection O
of O
17 O
typologically O
diverse O
set O
of O
target O
languages O
. O
We O
release O
our O
source O
code O
for O
replicating O
our O
experiments O
at O
https O
: O
/ O
/ O
github.com O
/ O
begab O
/ O
sparsity_makes_sense O
. O

Introduction O

Word O
sense O
disambiguation O
( O
WSD O
) O
is O
a O
longstanding O
and O
fundamental O
problem O
of O
Natural O
Language O
Processing O
, O
known O
to O
be O
affected O
by O
the O
knowledge O
acquisition O
bottleneck O
( O
Gale O
et O
al O
. O
, O
1992 O
) O
. O
Large O
pre O
- O
trained O
neural O
language O
models O
are O
known O
to O
effectively O
mitigate O
the O
problems O
related O
to O
the O
paucity O
of O
high O
quality O
, O
large O
- O
coverage O
sense O
annotated O
training O
data O
for O
WSD O
( O
Loureiro O
and O
Jorge O
, O
2019 O
; O
Loureiro O
et O
al O
. O
, O
2021b O
; O
inter O
alia O
) O
. O

Most O
recently O
, O
the O
knowledge O
acquisition O
bottleneck O
has O
been O
identified O
as O
an O
immense O
problem O
in O
the O
cross O
- O
lingual O
setting O
as O
well O
( O
Pasini O
, O
2020 O
) O
. O
A O
straightforward O
solution O
for O
handling O
this O
problem O
is O
to O
apply O
large O
multilingual O
pre O
- O
trained O
language O
models O
in O
a O
zero O
- O
shot O
setting O
, O
however O
, O
this O
approach O
has O
a O
potential O
limitation O
owing O
to O
the O
curse O
of O
multilinguality O
( O
Conneau O
et O
al O
. O
, O
2020a O
) O
, O
i.e. O
, O
the O
inability O
of O
such O
models O
to O
handle O
the O
large O
number O
of O
languages O
involved O
during O
training O
such O
models O
to O
an O
equally O
good O
quality O
. O

The O
research O
community O
replied O
to O
the O
limitations O
of O
large O
massively O
multilingual O
models O
by O
developing O
language O
- O
specific O
monolingual O
language O
ISO O
Huggingface O
model O
identifier O
bg O
DeepPavlov O
/ O
bert O
- O
base O
- O
bg O
- O
cs O
- O
pl O
- O
ru O
- O
cased O
( O
Arkhipov O
et O
al O
. O
, O
2019 O
) O
ca O
PlanTL O
- O
GOB O
- O
ES O
/ O
roberta O
- O
base O
- O
ca O
( O
Armengol O
- O
Estapé O
et O
al O
. O
, O
2021 O
) O
da O
Maltehb O
/ O
danish O
- O
bert O
- O
botxo O
de O
bert O
- O
base O
- O
german O
- O
cased O
es O
dccuchile O
/ O
bert O
- O
base O
- O
spanish O
- O
wwm O
- O
cased O
( O
Cañete O
et O
al O
. O
, O
2020 O
) O
et O
EMBEDDIA O
/ O
finest O
- O
BERT O
( O
Ulčar O
and O
Robnik O
- O
Šikonja O
, O
2020 O
) O
eu O
ixa O
- O
ehu O
/ O
berteus O
- O
base O
- O
cased O
( O
Agerri O
et O
al O
. O
, O
2020 O
) O
fr O
camembert O
- O
base O
( O
Martin O
et O
al O
. O
, O
2020 O
) O
gl O
dvilares O
/ O
bertinho O
- O
gl O
- O
base O
- O
cased O
( O
Vilares O
et O
al O
. O
, O
2021 O
) O
hr O
EMBEDDIA O
/ O
crosloengual O
- O
bert O
( O
Ulčar O
and O
Robnik O
- O
Šikonja O
, O
2020 O
) O
hu O
SZTAKI O
- O
HLT O
/ O
hubert O
- O
base O
- O
cc O
( O
Nemeskey O
, O
2021 O
) O
it O
Musixmatch O
/ O
umberto O
- O
commoncrawl O
- O
cased O
- O
v1 O
ja O
cl O
- O
tohoku O
/ O
bert O
- O
base O
- O
japanese O
- O
whole O
- O
word O
- O
masking O
ko O
snunlp O
/ O
KR O
- O
BERT O
- O
char16424 O
nl O
GroNLP O
/ O
bert O
- O
base O
- O
dutch O
- O
cased O
( O
de O
Vries O
et O
al O
. O
, O
2019 O
) O
sl O
EMBEDDIA O
/ O
sloberta O
zh O
bert O
- O
base O
- O
chinese O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
covering O
all O
the O
( O
non O
- O
English O
) O
languages O
of O
the O
XL O
- O
WSD O
dataset O
. O
models O
. O
1 O
Table O
1 O
provides O
a O
shortlist O
of O
recently O
published O
monolingual O
large O
pre O
- O
trained O
language O
models O
, O
related O
to O
the O
languages O
involved O
in O
the O
cross O
- O
lingual O
WSD O
test O
suit O
, O
XL O
- O
WSD O
. O

With O
the O
prevalence O
of O
large B-MethodName
monolingual I-MethodName
pretrained I-MethodName
models I-MethodName
, O
the O
important O
research O
question O
arises O
if O
their O
language O
- O
specific O
nature O
can O
be O
successfully O
exploited O
during O
zero O
- O
shot O
learning O
. O
Our O
research O
provides O
a O
thorough O
comparison O
of O
the O
application O
of O
large O
multilingual O
and O
monolingual O
pre O
- O
trained O
language O
models O
for O
zero O
- O
shot O
WSD O
. O

Another O
crucial O
aspect O
that O
we O
carefully O
investigate O
in O
this O
paper O
is O
the O
integration O
of O
sparse O
contextualized O
word O
representations O
into O
cross B-TaskName
- I-TaskName
lingual I-TaskName
zero I-TaskName
- I-TaskName
shot I-TaskName
WSD I-TaskName
. O
Sparse O
word O
representations O
have O
a O
demonstrated O
ability O
to O
align O
with O
word O
senses O
( O
Balogh O
et O
al O
. O
, O
2020 O
; O
Yun O
et O
al O
. O
, O
2021 O
) O
. O
While O
the O
benefits O
of O
employing O
sparsity O
has O
been O
shown O
for O
WSD O
in O
English O
( O
Berend O
, O
2020a O
) O
, O
its O
viability O
in O
the O
cross O
- O
lingual O
setting O
has O
not O
yet O
been O
verified O
. O

In O
order O
to O
conduct O
such O
an O
analysis O
, O
we O
propose O
an O
algorithm O
for O
obtaining O
cross O
- O
lingual O
sparse O
contextualized O
word O
representations O
from O
independently O
trained O
monolingual O
language O
models O
. O

Related O
work O

The O
analysis O
and O
the O
investigation O
of O
the O
transfer O
capabilities O
of O
large O
pre O
- O
trained O
language O
models O
( O
such O
as O
mBERT B-MethodName
or O
XLM B-MethodName
) O
across O
languages O
has O
spurred O
significant O
research O
interest O
( O
Pires O
et O
al O
. O
, O
2019 O
; O
Dredze O
, O
2019 O
, O
2020 O
; O
K O
et O
al O
. O
, O
2020 O
) O
. O
In O
contrast O
to O
the O
availability O
of O
multilingual O
neural O
language O
models O
, O
a O
series O
of O
recent O
papers O
have O
argued O
for O
the O
creation O
of O
dedicated O
neural O
language O
models O
for O
different O
languages O
( O
see O
e.g. O
Table O
1 O
) O
. O
While O
monolingual O
neural O
language O
models O
can O
more O
accurately O
model O
the O
distinct O
languages O
, O
models O
that O
are O
trained O
in O
isolation O
of O
other O
languages O
can O
not O
directly O
benefit O
from O
downstream O
application O
- O
specific O
annotated O
training O
data O
available O
in O
different O
languages O
. O
Artetxe O
et O
al O
. O
( O
2020 O
) O
proposed O
an O
approach O
for O
making O
monolingual O
models O
compatible O
with O
each O
other O
by O
first O
pre O
- O
training O
a O
masked O
language O
model O
on O
a O
source O
language O
, O
then O
freezing O
its O
parameters O
apart O
from O
its O
embedding O
layer O
that O
get O
replaced O
and O
trained O
for O
additional O
target O
languages O
using O
a O
standard O
masked O
language O
modeling O
objective O
. O
Note O
that O
this O
approach O
is O
complementary O
and O
strictly O
more O
resource O
intensive O
to O
ours O
, O
as O
it O
involves O
the O
pre O
- O
training O
of O
a O
( O
freezed O
) O
transformer O
model O
with O
respect O
its O
embedding O
layer O
for O
a O
target O
language O
. O
In O
contrast O
, O
our O
approach O
can O
operate O
on O
monolingual O
language O
models O
fully O
pre O
- O
trained O
in O
total O
isolation O
from O
the O
source O
language O
encoder O
. O
Also O
, O
our O
approach O
learns O
substantially O
fewer O
parameters O
in O
the O
form O
of O
an O
alignment O
matrix O
between O
the O
hidden O
representations O
of O
the O
contextualized O
target O
and O
source O
language O
spaces O
. O
Conneau O
et O
al O
. O
( O
2020b O
) O
analyzed O
the O
multilingual O
patterns O
emerging O
in O
large O
pre O
- O
trained O
language O
models O
. O
The O
authors O
found O
that O
" O
language O
universal O
representations O
emerge O
in O
pre O
- O
trained O
models O
without O
the O
requirement O
of O
any O
shared O
vocabulary O
or O
domain O
similarity O
" O
. O
That O
work O
have O
demonstrated O
that O
monolingual O
BERT B-MethodName
models O
can O
be O
effectively O
mapped O
for O
performing O
zero O
- O
shot O
crosslingual O
named O
entity O
recognition O
and O
syntactic O
parsing O
. O
Similarly O
, O
Wang O
et O
al O
. O
( O
2019 O
) O
; O
Schuster O
et O
al O
. O
( O
2019 O
) O
also O
illustrated O
the O
efficacy O
of O
linear O
transformations O
for O
using O
BERT B-MethodName
- O
derived O
representations O
in O
cross O
- O
lingual O
dependency O
parsing O
. O

WSD O
has O
been O
a O
fundamental O
and O
challenging O
problem O
in O
NLP O
for O
many O
decades O
, O
dating O
back O
to O
( O
Weaver O
, O
1949 O
( O
Weaver O
, O
/ O
1955 O
. O
The O
utilization O
of O
contextualized O
word O
representations O
was O
first O
advocated O
by O
Peters O
et O
al O
. O
( O
2018 O
) O
, O
later O
popularized O
by O
( O
Loureiro O
and O
Jorge O
, O
2019 O
; O
Loureiro O
et O
al O
. O
, O
2021a O
) O
. O
Bevilacqua O
et O
al O
. O
( O
2021 O
) O
offers O
a O
survey O
of O
the O
recent O
approaches O
. O

Most O
recently O
, O
Rezaee O
et O
al O
. O
( O
2021 O
) O
have O
explored O
the O
usage O
of O
multilingual O
language O
models O
( O
XLM B-MethodName
) O
in O
zero O
- O
shot O
WSD O
. O
While O
the O
experiments O
in O
( O
Rezaee O
et O
al O
. O
, O
2021 O
) O
cover O
four O
related O
target O
languages O
( O
German O
, O
Spanish O
, O
French O
and O
Italian O
) O
, O
our O
investigation O
involves O
a O
typologically O
diverse O
set O
of O
17 O
target O
languages O
( O
beyond O
English O
) O
from O
. O
Our O
work O
also O
extends O
that O
line O
of O
research O
in O
important O
aspects O
, O
as O
we O
show O
that O
the O
application O
of O
monolingual O
neural O
language O
models O
can O
vastly O
improve O
the O
performance O
of O
crosslingual B-TaskName
zero I-TaskName
- I-TaskName
shot I-TaskName
WSD I-TaskName
. O
Additionally O
, O
we O
also O
provide O
a O
careful O
evaluation O
of O
sparse O
contextualized O
word O
representations O
in O
zero O
- O
shot O
WSD O
. O

Berend O
( O
2020a O
) O
introduced O
sparse O
contextualized O
word O
representations O
via O
the O
application O
of O
dictionary O
learning O
, O
and O
showed O
that O
sense O
representations O
that O
are O
obtained O
from O
the O
co O
- O
occurrence O
statistics O
of O
the O
sparsity O
structure O
of O
the O
contextualized O
word O
representations O
and O
their O
sense O
annotations O
can O
provide O
significant O
improvement O
in O
monolingual O
WSD O
. O
Our O
work O
relates O
to O
that O
line O
of O
research O
by O
providing O
a O
mapping O
- O
based O
procedure O
, O
which O
enables O
the O
usage O
of O
such O
sense O
representations O
created O
in O
some O
source O
language O
to O
be O
applied O
in O
other O
target O
languages O
as O
well O
. O
The O
kind O
of O
mapping O
we O
employ O
can O
be O
viewed O
as O
a O
generalization O
of O
the O
approach O
introduced O
in O
( O
Berend O
, O
2020b O
) O
with O
the O
notable O
exception O
that O
in O
this O
work O
, O
we O
obtain O
sparse O
word O
representations O
for O
contextualized O
models O
as O
opposed O
to O
static O
word O
embeddings O
. O

Methodology O

In O
order O
to O
allow O
for O
zero O
- O
shot O
transfer O
between O
monolingual O
language O
models O
pre O
- O
trained O
in O
isolation O
from O
each O
other O
, O
we O
need O
to O
determine O
a O
mapping O
between O
their O
hidden O
representations O
. O
We O
first O
introduce O
our O
methodology O
for O
doing O
so O
, O
then O
we O
integrate O
this O
to O
the O
creation O
of O
sparse O
contextualized O
word O
representations O
. O

Mapping O
hidden O
representations O

The O
alignment O
of O
word O
representations O
between O
independently O
constructed O
semantic O
spaces O
can O
be O
conveniently O
and O
efficiently O
performed O
via O
linear O
transformations O
. O
This O
has O
been O
a O
standard O
approach O
for O
non O
- O
contextualized O
word O
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Xing O
et O
al O
. O
, O
2015 O
; O
Smith O
et O
al O
. O
, O
2017 O
) O
, O
but O
it O
has O
been O
shown O
to O
be O
useful O
in O
the O
contextualized O
case O
as O
well O
( O
Conneau O
et O
al O
. O
, O
2020b O
) O
. O

The O
standard O
approach O
is O
to O
obtain O
a O
collection O
of O
pairs O
of O
anchor O
points O
{ O
x O
i O
, O
y O
i O
} O
n O
i=1 O
with O
x O
i O
and O
y O
i O
denoting O
the O
representation O
of O
semantically O
equivalent O
words O
in O
the O
target O
and O
source O
languages O
, O
respectively O
. O
The O
mapping O
W O
is O
then O
obtained O
as O

min O
W O
n O
i=1 O
∥W O
x O
i O
− O
y O
i O
∥ O
2 O
2 O
. O

( O
1 O
) O

As O
we O
deal O
with O
contextualized O
models O
, O
we O
can O
obtain O
various O
representations O
for O
a O
word O
even O
in O
the O
same O
context O
, O
by O
considering O
the O
hidden O
representations O
from O
different O
layers O
of O
the O
neural O
language O
models O
employed O
. O
Additionally O
, O
as O
constraining O
the O
mapping O
matrix O
to O
be O
an O
isometric O
one O
have O
proven O
to O
be O
a O
useful O
requirement O
, O
we O
define O
our O
learning O
task O
to O
be O
of O
the O
form O

min O
W O
s.t O
. O
W O
⊺ O
W O
= O
I O
n O
i=1 O
∥W O
x O
( O
lt O
) O
i O
− O
y O
( O
ls O
) O
i O
∥ O
2 O
2 O
, O
( O
2 O

with O
I O
denoting O
the O
identity O
matrix O
, O
x O
( O
lt O
) O
i O
and O
y O

( O
ls O
) O
i O
denoting O
the O
hidden O
representations O
obtained O
from O
the O
l O
t O
th O
and O
l O
s O
th O
layers O
of O
the O
target O
and O
source O
language O
neural O
language O
models O
, O
respectively O
. O

Finding O
the O
optimal O
isometric O
W O
can O
be O
viewed O
as O
an O
instance O
of O
the O
orthogonal O
Procrustes O
problem O
( O
Schönemann O
, O
1966 O
) O
which O
can O
be O
solved O
by O
W O
⊥ O
= O
U O
V O
, O
with O
U O
and O
V O
originating O
from O
the O
singular O
value O
decomposition O
of O
the O
matrix O
product O
Y O
⊺ O
X O
, O
where O
X O
and O
Y O
include O
the O
stacked O
target O
and O
source O
language O
contextual O
representations O
of O
pairs O
of O
semantically O
equivalent O
words O
. O

As O
words O
of O
the O
input O
sequences O
to O
the O
neural O
language O
models O
can O
be O
split O
into O
multiple O
subtokens O
, O
we O
followed O
the O
common O
practice O
of O
obtaining O
word O
- O
level O
neural O
representations O
by O
performing O
mean O
pooling O
of O
the O
subword O
representations O
. O
Throughout O
our O
experiments O
, O
we O
also O
relied O
on O
the O
RCSLS O
criterion O
( O
Joulin O
et O
al O
. O
, O
2018 O
) O
, O
which O
offers O
a O
retrieval O
- O
based O
alternative O
of O
obtaining O
a O
mapping O
from O
the O
target O
to O
the O
source O
language O
representations O
. O

Cross O
- O
lingual O
sparse O
contextualized O
word O
representations O

Our O
approach O
extends O
the O
information O
theoretic O
algorithm O
introduced O
in O
( O
Berend O
, O
2020a O
) O
for O
its O
application O
in O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
zero I-TaskName
- I-TaskName
shot I-TaskName
WSD I-TaskName
setting O
. O

In O
order O
to O
obtain O
sparse O
contextualized O
representations O
for O
the O
source O
language O
, O
we O
first O
populate O
Y O
∈ O
R O
d×N O
with O
d O
- O
dimensional O
contextualized O
representations O
of O
words O
determined O
for O
texts O
in O
the O
source O
language O
, O
and O
minimize O
the O
objective O

min O
D∈C O
, O
α O
i O
∈R O
k O
≥0 O
N O
i=1 O
1 O
2 O
∥y O
i O
− O
Dα O
i O
∥ O
2 O
2 O
+ O
λ∥α O
i O
∥ O
1 O
, O
( O
3 O
) O

where O
C O
denotes O
the O
convex O
set O
of O
d O
× O
k O
matrices O
with O
column O
norm O
at O
most O
1 O
, O
λ O
is O
a O
regularization O
coefficient O
and O
the O
sparse O
coefficients O
in O
α O
are O
required O
to O
be O
non O
- O
negative O
. O
We O
used O
the O
SPAMS O
library O
( O
Mairal O
et O
al O
. O
, O
2009 O
) O
for O
calculating O
D O
and O
α O
. O

Having O
obtained O
D O
for O
the O
source O
language O
, O
we O
determine O
a O
sparse O
contextualized O
word O
representation O
for O
a O
target O
language O
word O
with O
dense O
contextualized O
representation O
x O
i O
as O

min O
α O
i O
∈R O
k O
≥0 O
1 O
2 O
∥W O
x O
i O
− O
Dα O
i O
∥ O
2 O
2 O
+ O
λ∥α O
i O
∥ O
1 O
, O
( O
4 O

where O
W O
is O
the O
alignment O
transformation O
as O
described O
earlier O
in O
Section O
3.1 O
. O
Eq O
. O
( O
4 O
) O
reveals O
that O
the O
cross O
- O
lingual O
applicability O
of O
the O
sparse O
codes O
are O
assured O
by O
the O
mapping O
transformation O
W O
and O
the O
fact O
that O
the O
sparse O
target O
language O
representations O
are O
also O
using O
the O
same O
D O
that O
was O
determined O
for O
the O
source O
language O
, O
which O
also O
ensures O
the O
efficient O
calculation O
of O
sparse O
representations O
during O
inference O
time O
. O

Apart O
from O
these O
crucial O
extensions O
we O
made O
for O
providing O
the O
use O
of O
contextualized O
sparse O
representations O
in O
the O
cross O
- O
lingual O
setting O
, O
the O
way O
we O
utilized O
them O
for O
the O
determination O
of O
sense O
representation O
and O
inference O
is O
identical O
to O
( O
Berend O
, O
2020a O
) O
. O
That O
is O
, O
for O
all O
sense O
- O
annotated O
words O
in O
the O
training O
corpus O
, O
we O
calculated O
a O
weighted O
cooccurrence O
statistics O
between O
a O
word O
pertaining O
to O
a O
specific O
semantic O
category O
and O
having O
non O
- O
zero O
coordinates O
along O
a O
specific O
dimension O
in O
their O
sparse O
contextualied O
word O
representations O
. O
These O
statistics O
are O
then O
transformed O
into O
pointwise O
mutual O
information O
( O
PMI O
) O
scores O
, O
resulting O
in O
a O
sense O
representation O
for O
all O
the O
senses O
in O
the O
training O
sense O
inventory O
. O

Sense O
representations O
obtained O
that O
way O
measure O
the O
strength O
of O
the O
relation O
of O
the O
senses O
to O
the O
different O
( O
sparse O
) O
coordinates O
. O
Inference O
for O
a O
word O
with O
sparse O
representation O
α O
is O
simply O
taken O
as O
arg O
max O
s O
Φα O
⊺ O
, O
where O
Φ O
is O
the O
previously O
defined O
matrix O
of O
PMI O
values O
and O
s O
corresponds O
to O
the O
sense O
at O
which O
position O
the O
above O
matrix O
- O
vector O
products O
takes O
its O
largest O
value O
. O

Experimental O
results O

All O
the O
neural O
language O
models O
that O
we O
relied O
on O
during O
our O
experiments O
were O
obtained O
from O
the O
transformers O
library O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O
We O
used O
four O
NVIDIA O
Titan O
2080 O
GPUs O
for O
our O
experiments O
. O

As O
the O
multilingual O
language O
model O
, O
we O
used O
the O
24 O
- O
layer O
transformer O
architecture O
, O
XLM B-MethodName
- I-MethodName
RoBERTa I-MethodName
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
for O
short O
) O
( O
Conneau O
et O
al O
. O
, O
2020a O
) O
. O
We O
chose O
the O
cased O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
large O
model O
as O
the O
monolingual O
model O
for O
encoding O
English O
text O
. O
As O
for O
the O
rest O
of O
the O
monolingual O
language O
models O
involved O
in O
our O
experiments O
, O
we O
relied O
on O
the O
models O
listed O
in O
Table O
1 O
. O
These O
monolingual O
models O
have O
the O
same O
size O
as O
the O
BERT B-MethodName
- I-MethodName
base I-MethodName
model O
, O
i.e. O
, O
they O
consist O
of O
12 B-HyperparameterValue
transformer B-HyperparameterName
blocks I-HyperparameterName
and O
employ O
hidden B-HyperparameterName
representations I-HyperparameterName
of O
768 B-HyperparameterValue
dimensions O
. O

For O
evaluation O
purposes O
, O
we O
used O
the O
extra O
- O
large O
cross O
- O
lingual O
evaluation O
benchmark O
XL B-DatasetName
- I-DatasetName
WSD I-DatasetName
, O
recently O
proposed O
in O
. O
The O
database O
contains O
a O
high O
- O
quality O
sense O
annotated O
corpus O
for O
English O
as O
the O
concatenation O
of O
the O
Sem O
- O
Cor O
dataset O
( O
Miller O
et O
al O
. O
, O
1994 O
) O
and O
the O
sense O
definitions O
and O
example O
sentences O
from O
WordNet O
( O
Fellbaum O
, O
1998 O
) O
. O
XL B-DatasetName
- I-DatasetName
WSD I-DatasetName
uses O
the O
unified O
crosslingual O
sense O
inventory O
of O
BabelNet O
( O
Navigli O
and O
Ponzetto O
, O
2012 O
) O
. O

The O
dataset O
contains O
17 O
additional O
typologically O
diverse O
languages O
besides O
English O
( O
that O
we O
listed O
in O
Table O
1 O
) O
. O
The O
authors O
also O
released O
machine O
translated O
silver O
standard O
sense O
annotated O
training O
corpora O
for O
all O
the O
languages O
, O
which O
makes O
the O
language O
- O
specific O
fine O
- O
tuning O
of O
monolingual O
models O
possible O
, O
however O
, O
as O
shown O
in O
, O
that O
approach O
resulted O
in O
inferior O
results O
compared O
to O
the O
application O
of O
multilingual O
models O
in O
the O
zero O
- O
shot O
setting O
. O

Throughout O
the O
application O
of O
sparse O
contextualized O
representations O
, O
we O
employ O
the O
same O
set O
of O
hyperparameters O
that O
were O
used O
in O
( O
Berend O
, O
2020a O
) O
, O
i.e. O
, O
we O
set O
the O
number O
of O
the O
regularization B-HyperparameterName
coefficient I-HyperparameterName
to O
λ B-HyperparameterName
= O
0.05 B-HyperparameterValue
and O
the O
number B-HyperparameterName
of I-HyperparameterName
( I-HyperparameterName
sparse I-HyperparameterName
) I-HyperparameterName
coordinates I-HyperparameterName
to O
k B-HyperparameterName
= O
3000 B-HyperparameterValue
. O
There O
made O
one O
optional O
change O
, O
i.e. O
, O
we O
decided O
whether O
to O
use O
the O
normalization O
of O
PMI O
values O
( O
Bouma O
, O
2009 O
) O
during O
the O
calculation O
of O
the O
sense O
representation O
matrix O
Φ O
on O
a O
per O
language O
basis O
based O
on O
development O
set O
performances O
. O
An O
ablation O
study O
related O
to O
the O
( O
optional O
) O
normalization O
of O
PMI O
scores O
is O
reported O
in O
Table O
5 O
, O
Appendix O
B O
. O

When O
we O
do O
not O
employ O
the O
sparsification O
of O
the O
contextualized O
word O
representations O
for O
determining O
the O
sense O
representations O
, O
we O
follow O
the O
approach O
introduced O
in O
( O
Loureiro O
and O
Jorge O
, O
2019 O
) O
. O
That O
is O
, O
we O
take O
the O
centroid O
of O
word O
vectors O
belonging O
to O
a O
particular O
sense O
as O
the O
representation O
of O
that O
sense O
, O
and O
perform O
a O
nearest O
neighbor O
search O
during O
inference O
. O

Alignment O
of O
contextualized O
representations O

As O
the O
different O
layers O
of O
neural O
language O
models O
have O
been O
shown O
to O
provide O
different O
levels O
of O
utility O
towards O
different O
tasks O
, O
we O
experimented O
with O
mappings O
between O
different O
combinations O
of O
layers O
from O
the O
target O
and O
source O
language O
neural O
language O
models O
. O
Since O
the O
last O
few O
layers O
of O
the O
neural O
models O
are O
generally O
agreed O
to O
be O
the O
most O
useful O
for O
semantics O
- O
related O
tasks O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Tenney O
et O
al O
. O
, O
2019 O
; O
Reif O
et O
al O
. O
, O
2019 O
) O
, O
we O
decided O
to O
learn O
mappings O
between O
the O
hidden O
representations O
of O
any O
of O
the O
last O
four O
layers O
of O
the O
target O
and O
source O
language O
encoders O
. O
We O
used O
BERT B-MethodName
as O
the O
language O
specific O
encoder O
for O
the O
source O
language O
texts O
in O
English O
, O
but O
we O
also O
investigated O
the O
application O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
so O
that O
we O
can O
see O
the O
effects O
of O
replacing O
it O
by O
an O
encoder O
especially O
tailored O
for O
English O
. O
As O
for O
the O
target O
languages O
, O
we O
used O
the O
respective O
models O
for O
each O
language O
as O
listed O
in O
Table O
1 O
. O
Similar O
to O
the O
source O
language O
, O
we O
also O
investigated O
the O
case O
when O
target O
languages O
were O
encoded O
by O
the O
multilingual O
model O
. O

In O
what O
follows O
, O
we O
label O
the O
different O
experimental O
settings O
according O
to O
the O
followings O
: O

• O
multi→multi O
means O
that O
we O
map O
the O
target O
language O
representations O
obtained O
by O
the O
multilingual O
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
model O
to O
the O
representation O
space O
of O
the O
source O
language O
also O
obtained O
by O
the O
multilingual O
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
encoder O
, O

• O
multi→mono O
, O
means O
that O
we O
map O
the O
target O
language O
representations O
obtained O
by O
the O
multilingual O
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
model O
to O
the O
representation O
space O
of O
the O
source O
language O
obtained O
by O
the O
monolingual O
( O
English B-MethodName
BERT I-MethodName
) O
encoder O
, O

• O
mono→multi O
, O
means O
that O
we O
map O
the O
target O
language O
representations O
obtained O
by O
their O
respective O
monolingual O
language O
model O
to O
the O
representation O
space O
of O
the O
source O
language O
obtained O
by O
the O
multilingual O
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
encoder O
, O

• O
mono→mono O
, O
means O
that O
we O
map O
the O
target O
language O
representations O
obtained O
by O
their O
respective O
monolingual O
language O
model O
to O
the O
representation O
space O
of O
the O
source O
language O
obtained O
by O
the O
monolingual O
( O
English B-MethodName
BERT I-MethodName
) O
encoder O
. O

In O
order O
to O
obtain O
the O
cross O
- O
representational O
mappings O
, O
we O
accessed O
the O
Tatoeba O
corpus O
( O
Tiedemann O
, O
2012 O
) O
through O
the O
datasets O
library O
( O
Lhoest O
et O
al O
. O
, O
2021 O
) O
. O
The O
Tatoeba O
corpus O
contains O
translated O
sentence O
pairs O
for O
several O
hundreds O
of O
languages O
which O
we O
used O
for O
obtaining O
the O
pivot O
word O
mention O
pairs O
together O
with O
their O
contexts O
. O

In O
addition O
to O
the O
Tatoeba O
corpus O
, O
we O
used O
the O
word2word O
library O
( O
Choe O
et O
al O
. O
, O
2020 O
) O
containing O
word O
translation O
pairs O
between O
more O
than O
3,500 O
language O
pairs O
. O
By O
denoting O
( O
S O
s O
i O
, O
S O
t O
i O
) O
the O
i O
th O
translated O
sentence O
pair O
from O
the O
Tatoeba O
corpus O
, O
we O
treated O
those O
( O
w O
s O
∈ O
S O
s O
i O
, O
w O
t O
∈ O
S O
t O
i O
) O
word O
occurrences O
as O
being O
semantically O
equivalent O
, O
for O
which O
the O
w O
t O
∈ O
T O
ranslationOf O
( O
w O
s O
) O
and O
the O
w O
s O
∈ O
T O
ranslationOf O
( O
w O
t O
) O
relations O
simultaneously O
held O
according O
to O
the O
translation O
list O
provided O
by O
word2word O
. O

As O
an O
example O
, O
given O
the O
German O
- O
English O
translation O
pair O
from O
Tatoeba O
, O
{ O
' O
de O
: O
' O
' O
Es O
steht O
ein O
Glas O
auf O
dem O
Tisch O
. O
' O
, O
' O
en O
' O
: O
' O
There O
is O
a O
glass O
on O
the O
table O
. O
} O
, O
underlined O
pairs O
of O
words O
with O
the O
same O
color O
would O
be O
treated O
as O
contextualized O
translation O
pairs O
of O
each O
other O
. O

One O
benefit O
of O
our O
approach O
for O
determining O
contextual O
alignment O
of O
word O
pairs O
is O
that O
it O
does O
not O
require O
word O
level O
alignment O
of O
the O
parallel O
sentences O
, O
hence O
it O
suits O
such O
lower O
resource O
scenarios O
better O
, O
when O
only O
parallel O
sentences O
( O
without O
word O
level O
alignments O
) O
and O
a O
list O
of O
word O
translation O
pairs O
are O
provided O
. O
Naturally O
, O
different O
contextual O
alignment O
approaches O
could O
be O
integrated O
into O
our O
approach O
at O
this O
point O
, O
and O
this O
is O
something O
that O
we O
regard O
as O
potential O
future O
extension O
of O
our O
work O
. O

We O
evaluated O
the O
quality O
of O
the O
mapping O
learned O
between O
the O
target O
and O
the O
source O
language O
representations O
by O
defining O
a O
contextualized O
translation O
retrieval O
task O
and O
evaluating O
it O
on O
its O
accuracy B-MetricName
@ I-MetricName
1 I-MetricName
metric O
, O
i.e. O
, O
for O
what O
fraction O
of O
the O
contextualized O
translation O
pairs O
-not O
seen O
during O
the O
determination O
of O
the O
mapping O
between O
the O
two O
representation O
spaces O
-are O
we O
able O
to O
rank O
the O
original O
translated O
context O
as O
the O
highest O
. O

In O
the O
multi→multi O
case O
, O
i.e. O
, O
when O
both O
the O
target O
and O
source O
languages O
are O
encoded O
by O
the O
same O
multilingual O
model O
( O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
, O
it O
also O
makes O
sense O
to O
use O
the O
identity O
matrix O
as O
the O
mapping O
operator O
for O
mapping O
the O
target O
language O
contextual O
text O
representations O
to O
the O
semantic O
space O
of O
the O
source O
language O
( O
as O
long O
as O
the O
target O
and O
source O
language O
texts O
are O
obtained O
from O
the O
same O
layer O
of O
the O
multilingual O
encoder O
) O
. O
We O
also O
evaluated O
the O
quality O
of O
this O
approach O
in O
our O
experiments O
that O
we O
refer O
to O
as O
the O
identity O
approach O
. O

We O
list O
the O
statistics O
of O
the O
Tatoeba O
corpus O
and O
the O
size O
of O
the O
training O
and O
test O
contextualized O
translation O
pairs O
in O
Table O
2 O
. O
Our O
results O
on O
the O
top-1 O
contextualized O
translation O
retrieval O
accuracies O
along O
the O
different O
languages O
and O
combination O
of O
target O
and O
source O
encoder O
usage O
are O
reported O
in O
Figure O
1 O
. O
The O
quality O
of O
the O
combination O
which O
uses O
monolingual O
encoders O
for O
both O
the O
target O
and O
source O
languages O
( O
mono→mono O
) O
performed O
the O
best O
. O

Monolingual O
evaluation O

We O
first O
conducted O
evaluations O
in O
the O
monolingual O
setting O
, O
i.e. O
, O
we O
used O
the O
sense O
annotated O
training O
data O
to O
train O
and O
evaluate O
WSD O
models O
in O
English O
. O
The O
results O
of O
these O
experiments O
-depending O
on O
the O
encoder O
architecture O
used O
( O
BERT B-MethodName
/ O
XLM B-MethodName
- I-MethodName
R I-MethodName
) O
, O
the O
layer O
of O
the O
encoder O
utilized O
( O
{ O
21 O
, O
. O
. O
. O
, O
24 O
} O
) O
, O
and O
whether O
the O
sparsification O
of O
the O
contextualized O
representations O
took O
place O
( O
Dense O
/ O
Sparse O
) O
-are O
included O
in O
Table O
3 O
. O
Unsurprisingly O
, O
the O
application O
of O
the O
languagespecific O
BERT B-MethodName
model O
achieved O
better O
scores O
compared O
to O
that O
of O
XLM B-MethodName
- I-MethodName
R. I-MethodName
An O
interesting O
observation O
though O
, O
is O
that O
the O
drop O
in O
performance O
is O
much O
more O
subtle O
for O
those O
cases O
when O
the O
contextualized O
representations O
are O
enhanced O
via O
sparsification O
, O
i.e. O
, O
the O
typical O
loss O
in O
performance O
across O
the O
layers O
is O
only O
3 O
points O
( O
apart O
from O
the O
final O
layer O
) O
, O
opposed O
to O
the O
typical O
loss O
of O
4 O
- O
7 O
points O
in O
the O
dense O
case O
. O

Cross O
- O
lingual O
zero O
- O
shot O
evaluation O

Table O
4 O
includes O
the O
zero B-TaskName
- I-TaskName
shot I-TaskName
cross I-TaskName
- I-TaskName
lingual I-TaskName
WSD I-TaskName
results O
for O
a O
collection O
of O
baseline O
approaches O
( O
Table O
4a O
) O
from O
, O
followed O
by O
our O
models O
not O
utilizing O
the O
sparsification O
of O
the O
contextualized O
embeddings O
( O
Table O
4b O
) O
and O
the O
ones O
that O
additionally O
benefit O
from O
sparsification O
as O
well O
( O
Table O
4c O
) O
. O
It O
is O
useful O
to O
note O
that O
the O
mono→ O
* O
approaches O
are O
strictly O
more O
resource O
efficient O
during O
inference O
as O
they O
are O
based O
on O
12 B-HyperparameterValue
- O
layer B-HyperparameterName
encoders I-HyperparameterName
instead O
of O
the O
24 O
layers O
of O
the O
multilingual O
XLM B-MethodName
- I-MethodName
R I-MethodName
model O
. O

At O
this O
point O
, O
we O
separate O
the O
multi→multi O
results O
into O
two O
, O
i.e. O
, O
1 O
) O
those O
obtained O
when O
relying O
on O
the O
hidden O
representations O
from O
the O
same O
layer O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
without O
mapping O
( O
or O
equivalently O
, O
with O
the O
identity O
mapping O
from O
the O
target O
to O
source O
representations O
) O
; O
and O
2 O
) O
those O
obtained O
when O
the O
target O
and O
source O
language O
contextual O
representations O
could O
originate O
from O
different O
layers O
of O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
encoder O
, O
and O
a O
non O
- O
identity O
( O
either O
isometric O
or O
RC O
- O
SLS O
) O
mapping O
was O
employed O
. O
We O
keep O
referring O
to O
the O
latter O
as O
multi→multi O
, O
and O
denote O
the O
former O
type O
of O
experiments O
as O
multi O
( O
without O
the O
→multi O
suffix O
as O
there O
were O
no O
real O
mappings O
performed O
in O
these O
cases O
) O
. O
Inspecting O
the O
first O
two O
rows O
of O
Table O
4b O
and O
Table O
4c O
reveals O
that O
enhancing O
the O
multilingual O
encoder O
towards O
the O
treatment O
of O
a O
particular O
pair O
of O
languages O
by O
providing O
it O
a O
language O
pair O
specific O
mapping O
has O
a O
larger O
positive O
effect O
when O
using O
dense O
vectors O
. O
In O
fact O
, O
it O
increased O
the O
micro B-MetricName
- I-MetricName
averaged I-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
over O
the O
17 O
languages O
by O
1.72 B-MetricValue
and O
0.11 B-MetricValue
points O
for O
the O
dense O
and O
the O
sparse O
cases O
, O
respectively O
. O

Overall O
, O
the O
micro B-MetricName
- I-MetricName
averaged I-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
of O
our O
final O
approach O
managed O
to O
improve O
nearly O
6.5 B-MetricValue
points O
( O
cf O
. O
the O
first O
row O
of O
Table O
4b O
and O
the O
last O
row O
in O
4 O
: O
Test O
set O
results O
on O
the O
XL B-DatasetName
- I-DatasetName
WSD I-DatasetName
benchmark O
. O
The O
hyperparameters O
of O
the O
individual O
approaches O
( O
e.g. O
which O
layer O
of O
the O
target O
language O
encoder O
to O
align O
with O
which O
layer O
of O
the O
source O
language O
encode O
) O
were O
determined O
based O
on O
the O
development O
set O
of O
each O
language O
. O

to O
the O
replacement O
of O
the O
XLM B-MethodName
- I-MethodName
R I-MethodName
encoder O
for O
both O
the O
source O
language O
during O
training O
and O
target O
languages O
for O
inference O
( O
cf O
. O
the O
first O
and O
last O
row O
of O
Table O
4b O
) O
and O
an O
additional O
1.5 B-MetricValue
points O
of O
improvement O
was O
an O
effect O
of O
our O
sparsification O
in O
the O
crosslingual O
setting O
. O
The O
inspection O
of O
the O
third O
and O
fourth O
rows O
in O
both O
Table O
4b O
and O
Table O
4c O
reveals O
that O
using O
a O
monolingual O
encoder O
during O
inference O
helps O
more O
compared O
to O
the O
application O
of O
a O
monolingual O
encoder O
for O
encoding O
the O
source O
language O
during O
training O
. O

We O
conducted O
the O
McNemar O
test O
between O
our O
system O
outputs O
when O
a O
non O
- O
identity O
mapping O
was O
used O
between O
a O
pair O
of O
languages O
. O
Our O
investigation O
revealed O
that O
all O
such O
8 O
2 O
pairs O
of O
system O
outputs O
from O
Table O
4b O
and O
Table O
4c O
differ O
significantly O
from O
each O
other O
with O
p O
< O
0.0007 O
, O
with O
only O
four O
exceptions O
, O
i.e O
, O
1 O
) O
multi→multi O
and O
multi→mono O
from O
Table O
4b O
; O
2 O
) O
multi→multi O
and O
multi→mono O
from O
Table O
4c O
; O
3 O
) O
mono→multi O
from O
Table O
4c O
and O
mono→mono O
from O
Table O
4b O
; O
4 O
) O
multi→mono O
from O
Table O
4c O
and O
mono→multi O
from O
Table O
4b O
. O

Figure O
2 O
summarizes O
the O
results O
of O
all O
the O
possible O
runs O
conducted O
. O
When O
using O
the O
multilingual O
encoder O
for O
both O
the O
target O
and O
source O
languages O
without O
a O
mapping O
step O
between O
the O
two O
( O
multi O
) O
, O
we O
ran O
4 O
different O
experiments O
per O
each O
language O
based O
on O
the O
hidden O
representations O
obtained O
from O
one O
of O
the O
last O
4 O
layers O
of O
the O
multilingual O
encoder O
. O

For O
the O
remaining O
experiments O
relying O
on O
the O
dense O
and O
sparse O
representations O
, O
there O
were O
32 O
and O
64 O
experiments O
for O
each O
language O
, O
respectively O
. O
The O
32 O
experiments O
were O
a O
result O
of O
choosing O
any O
of O
the O
16 O
possible O
combination O
of O
the O
final O
four O
layers O
on O
the O
target O
and O
source O
language O
encoder O
, O
coupled O
with O
the O
type O
of O
mapping O
utilized O
( O
isometric O
/ O
RCSLS O
) O
. O

For O
the O
experiments O
involving O
the O
sparse O
representations O
, O
there O
was O
an O
extra O
parameter O
, O
whether O
the O
normalization O
of O
the O
PMI O
scores O
for O
obtaining O
the O
sense O
representations O
to O
be O
performed O
, O
resulting O

PRQR O
PRQR O
PRQR O
PRQRZVSDUVLW\ O
0DSSLQJ O
LVRPHWULF O
5 O
& O
6 O
/ O
6 O

Figure O
3 O
: O
Comparison O
of O
the O
two O
best O
performing O
systems O
when O
the O
same O
hyperparameters O
were O
employed O
. O

in O
2 O
× O
32 O
experiments O
all O
together O
. O
Our O
ablation O
study O
in O
Table O
5 O
illustrates O
that O
this O
extra O
factor O
of O
2 O
for O
the O
sparse O
experiments O
did O
not O
provided O
us O
an O
unfair O
advantage O
, O
i.e. O
, O
when O
fixing O
the O
value O
of O
normalization O
in O
any O
way O
, O
the O
overall O
results O
did O
not O
differ O
substantially O
. O

The O
difference O
in O
the O
average O
performance O
of O
our O
approach O
transforming O
sparse O
contextualized O
representations O
obtained O
by O
monolingual O
models O
is O
significant O
( O
using O
unpaired O
t O
- O
test O
2 O
, O
p O
< O
0.005 O
) O
compared O
to O
any O
other O
configuration O
. O
This O
suggests O
that O
the O
mono→mono O
approach O
has O
a O
robust O
advantage O
over O
alternative O
variants O
, O
and O
the O
improvements O
seen O
in O
Table O
4 O
are O
not O
an O
effect O
of O
careful O
hyperparameter O
selection O
, O
but O
they O
generalize O
over O
a O
wide O
range O
of O
choices O
. O

This O
effect O
is O
further O
corroborated O
in O
Figure O
3 O
, O
which O
offers O
a O
comparison O
between O
the O
two O
systems O
with O
the O
best O
average O
performance O
, O
i.e. O
, O
mono→mono O
that O
operates O
with O
the O
dense O
vectors O
( O
results O
are O
along O
the O
x O
- O
axis O
) O
and O
the O
same O
model O
but O
with O
the O
enhancement O
of O
sparsification O
( O
results O
are O
along O
the O
y O
- O
axis O
) O
. O
Each O
data O
point O
corresponds O
to O
a O
setting O
with O
the O
same O
hyperparameter O
choices O
, O
and O
points O
above O
the O
diagonal O
line O
with O
slope O
one O
demonstrate O
the O
benefits O
of O
sparsification O
. O

We O
have O
demonstrated O
the O
improved O
utility O
of O
mapping O
language O
- O
specific O
sparse O
contextualized O
representations O
for O
conducting O
zero O
- O
shot O
WSD O
, O
requiring O
large O
pre O
- O
trained O
language O
- O
specific O
text O
encoders O
for O
the O
target O
languages O
. O
While O
such O
models O
are O
available O
for O
all O
languages O
in O
XL O
- O
WSD O
, O
a O
vari O
- O
ety O
of O
the O
existing O
languages O
lack O
their O
dedicated O
language O
- O
specific O
pre O
- O
trained O
language O
model O
. O
As O
such O
, O
an O
important O
question O
emerges O
whether O
it O
is O
possible O
to O
enjoy O
the O
benefits O
of O
mapping O
sparse O
contextualized O
representations O
for O
zero O
- O
shot O
WSD O
in O
the O
absence O
of O
a O
large O
pre O
- O
trained O
language O
model O
dedicated O
to O
the O
target O
language O
. O
To O
this O
end O
, O
we O
shall O
inspect O
the O
results O
of O
our O
multi→mono O
approach O
in O
Table O
4 O
, O
a O
series O
of O
mapping O
- O
based O
experiments O
in O
which O
we O
acted O
as O
if O
the O
monolingual O
language O
models O
( O
other O
than O
the O
one O
for O
English O
) O
did O
not O
exist O
. O
In O
these O
experiments O
, O
the O
sense O
embeddings O
were O
obtained O
with O
bert B-MethodName
- I-MethodName
large I-MethodName
- I-MethodName
cased I-MethodName
( O
being O
specialized O
to O
English O
) O
, O
and O
the O
mapping O
to O
the O
non O
- O
English O
target O
languages O
were O
performed O
towards O
their O
XLM B-MethodName
- I-MethodName
R I-MethodName
representations O
during O
the O
evaluation O
. O
This O
way O
, O
we O
could O
simulate O
the O
effects O
of O
the O
absence O
of O
language O
- O
specific O
models O
. O

The O
multi→mono O
approach O
provided O
a O
substantially O
better O
average O
performance O
compared O
to O
the O
mere O
utilization O
of O
a O
multilingual O
encoder O
in O
the O
case O
of O
dense O
contextualized O
representations O
as O
it O
can O
be O
seen O
in O
Table O
4b O
. O
The O
average O
results O
of O
multi→mono O
are O
slightly O
inferior O
( O
albeit O
statistically O
insignificantly O
) O
to O
that O
of O
the O
multi O
approach O
for O
the O
application O
of O
sparse O
contextualized O
representations O
. O
However O
, O
when O
comparing O
the O
multi→multi O
results O
with O
that O
of O
multi→mono O
, O
we O
can O
see O
that O
by O
relying O
on O
a O
multilingual O
encoder O
alone O
, O
and O
allowing O
a O
mapping O
to O
be O
employed O
between O
its O
hidden O
representations O
pertaining O
to O
different O
languages O
, O
one O
can O
obtain O
the O
same O
( O
or O
even O
slightly O
better O
) O
performance O
as O
with O
the O
multi→mono O
approach O
. O
This O
highlights O
the O
importance O
of O
monolingual O
encoders O
for O
the O
target O
language O
, O
which O
seems O
to O
be O
more O
important O
than O
having O
access O
to O
a O
monolingual O
encoder O
for O
the O
source O
language O
. O

Conclusions O

In O
this O
paper O
we O
provided O
a O
systematic O
investigation O
of O
the O
benefits O
of O
using O
large O
monolingual O
pretrained O
language O
models O
in O
place O
of O
multilingual O
language O
models O
, O
such O
as O
XLM B-MethodName
- I-MethodName
R. I-MethodName
We O
have O
shown O
that O
since O
monolingual O
neural O
language O
models O
are O
specifically O
tailored O
for O
a O
single O
( O
or O
at O
most O
a O
few O
related O
) O
languages O
, O
they O
can O
effectively O
mitigate O
the O
curse O
of O
multilinguality O
typical O
of O
multilingual O
models O
, O
and O
their O
application O
can O
significantly O
improve O
the O
F B-MetricName
- I-MetricName
scores I-MetricName
in O
zero O
- O
shot O
WSD O
. O
We O
additionally O
showed O
that O
the O
benefits O
of O
sparse O
con O
- O
textualized O
word O
representations O
, O
obtained O
via O
a O
dictionary O
learning O
procedure O
, O
also O
convey O
to O
the O
cross O
- O
lingual O
setting O
, O
and O
that O
it O
provides O
complementary O
improvements O
to O
the O
usage O
of O
monolingual O
neural O
language O
models O
. O

A O
Analysis O
of O
the O
language O
models O

We O
compare O
some O
of O
the O
basic O
properties O
of O
the O
pretrained O
language O
models O
that O
we O
employed O
in O
Figure O
4 O
and O
Figure O
5 O
. O
This O
can O
be O
useful O
as O
the O
monolingual O
quality O
of O
the O
language O
models O
we O
used O
could O
influence O
and O
account O
for O
their O
utility O
when O
used O
in O
conjunction O
with O
our O
mapping O
- O
based O
algorithm O
. O

Figure O
4 O
includes O
quantitative O
scores O
over O
the O
different O
languages O
related O
to O
the O
subword O
tokenizers O
employed O
by O
the O
various O
language O
models O
. O
Fertility O
in O
Figure O
4a O
refers O
to O
the O
average O
number O
of O
subtokens O
a O
single O
token O
gets O
separated O
into O
by O
the O
tokenizer O
of O
the O
given O
language O
model O
. O
Multi O
- O
token O
ratio O
( O
MTR O
) O
in O
Figure O
4b O
indicates O
the O
fraction O
of O
tokens O
that O
gets O
split O
into O
more O
than O
one O
piece O
upon O
tokenization O
( O
Ács O
, O
2019 O
; O
Rust O
et O
al O
. O
, O
2021 O
) O
. O
Smaller O
values O
of O
MTR O
mean O
a O
better O
adaptation O
of O
the O
tokenizer O
to O
the O
peculiarities O
of O
the O
given O
language O
. O
It O
can O
be O
seen O
that O
the O
monolingual O
models O
do O
a O
much O
better O
job O
compared O
to O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
which O
can O
be O
part O
of O
the O
reason O
why O
mapping O
independently O
trained O
monolingual O
. O

In O
Figure O
5a O
, O
we O
refer O
to O
the O
last O
four O
layers O
of O
the O
investigated O
models O
as O
{ O
-4 O
, O
-3 O
, O
-2 O
, O
-1 O
} O
as O
the O
English B-MethodName
BERT I-MethodName
is O
a O
24 O
- O
layer O
model O
, O
whereas O
the O
rest O
of O
the O
monolingual O
models O
consist O
of O
12 O
layers O
. O
This O
means O
that O
layer O
-1 O
refers O
to O
layer O
24 O
for O
English O
and O
layer O
12 O
for O
some O
non O
- O
English O
model O
. O
Even O
though O
Figure O
5a O
shows O
pathological O
masked O
language O
modeling O
( O
MLM O
) O
losses O
for O
certain O
monolingual O
models O
( O
e.g. O
Bulgarian O
or O
Basque O
) O
when O
measured O
on O
the O
XL B-DatasetName
- I-DatasetName
WSD I-DatasetName
database O
, O
their O
mappingbased O
utilization O
in O
zero O
- O
shot O
WSD O
was O
still O
possible O
as O
indicated O
by O
our O
main O
results O
( O
see O
Table O
4 O
) O
. O
A O
further O
interesting O
phenomenon O
is O
that O
the O
performance O
of O
XLM B-MethodName
- I-MethodName
R I-MethodName
exceeds O
that O
of O
the O
bert B-MethodName
- I-MethodName
largecased I-MethodName
model O
in O
terms O
of O
MLM O
for O
English O
. O
These O
results O
suggest O
that O
the O
masked O
language O
modeling O
performance O
of O
pretrained O
language O
models O
and O
their O
utility O
in O
WSD O
are O
not O
strongly O
related O
with O
each O
other O
. O

B O
Analysis O
on O
using O
the O
normalization O
of O
PMI O
scores O

Upon O
the O
calculation O
of O
the O
sense O
representation O
matrix O
Φ O
, O
involving O
the O
calculation O
of O
PMI O
scores O
between O
the O
various O
senses O
from O
the O
sense O
inventory O
and O
the O
coordinates O
of O
a O
sparse O
contextual O
representation O
being O
non O
- O
zero O
, O
Berend O
( O
2020a O
) O
suggested O
the O
use O
of O
normalized O
PMI O
scores O
( O
Bouma O
, O
2009 O
) O
. O
Our O
preliminary O
results O
suggested O
that O
the O
normalization O
of O
PMI O
scores O
can O
have O
a O
mixed O
effect O
over O
the O
different O
languages O
. O
( O
a O
) O
Our O
results O
based O
on O
sparse O
sense O
vectors O
when O
always O
using O
the O
normalization O
of O
PMI O
scores O
as O
done O
in O
( O
Berend O
, O
2020a O
( O
b O
) O
Our O
results O
based O
on O
sparse O
sense O
vectors O
when O
not O
using O
the O
normalization O
of O
PMI O
scores O
as O
done O
in O
( O
Berend O
, O
2020a O
( O
Bouma O
, O
2009 O
) O
( O
a O
) O
mandatory O
, O
( O
b O
) O
prohibited O
, O
( O
c O
) O
optional O
to O
use O
( O
based O
on O
development O
set O
results O
) O
during O
the O
creation O
of O
the O
sparse O
sense O
representations O
. O

Acknowledgments O

The O
research O
was O
supported O
by O
the O
Ministry O
of O
Innovation O
and O
Technology O
NRDI O
Office O
within O
the O
framework O
of O
the O
Artificial O
Intelligence O
National O
Laboratory O
Program O
. O
Additionally O
, O
we O
are O
thankful O
for O
the O
usage O
of O
ELKH O
Cloud O
( O
https O
: O
/ O
/ O
sciencecloud.hu O
/ O
) O
that O
helped O
us O
achieving O
the O
results O
published O
in O
this O
paper O
. O

CLAPSpeech B-MethodName
: O
Learning B-TaskName
Prosody I-TaskName
from O
Text O
Context O
with O
Contrastive O
Language O
- O
Audio O
Pre O
- O
training O

Improving O
text O
representation O
has O
attracted O
much O
attention O
to O
achieve O
expressive O
text B-TaskName
- I-TaskName
tospeech I-TaskName
( O
TTS B-TaskName
) O
. O
However O
, O
existing O
works O
only O
implicitly O
learn O
the O
prosody O
with O
masked O
token O
reconstruction O
tasks O
, O
which O
leads O
to O
low O
training O
efficiency O
and O
difficulty O
in O
prosody B-TaskName
modeling I-TaskName
. O
We O
propose O
CLAPSpeech B-MethodName
, O
a O
cross O
- O
modal O
contrastive O
pre O
- O
training O
framework O
that O
explicitly O
learns O
the O
prosody O
variance O
of O
the O
same O
text O
token O
under O
different O
contexts O
. O
Specifically O
, O
1 O
) O
We O
encourage O
the O
model O
to O
connect O
the O
text O
context O
with O
its O
corresponding O
prosody O
pattern O
in O
the O
joint O
multi O
- O
modal O
space O
with O
the O
elaborate O
design O
of O
the O
encoder O
inputs O
and O
contrastive O
loss O
; O
2 O
) O
We O
introduce O
a O
multi O
- O
scale O
pretraining O
pipeline O
to O
capture O
prosody O
patterns O
in O
multiple O
levels O
. O
We O
show O
how O
to O
incorporate O
CLAPSpeech B-MethodName
into O
existing O
TTS B-MethodName
models I-MethodName
for O
better O
prosody O
. O
Experiments O
on O
three O
datasets O
not O
only O
show O
that O
CLAPSpeech B-MethodName
could O
improve O
the O
prosody B-TaskName
prediction I-TaskName
for O
existing O
TTS B-MethodName
methods I-MethodName
, O
but O
also O
demonstrate O
its O
generalization O
ability O
to O
adapt O
to O
multiple B-MethodName
languages I-MethodName
and I-MethodName
multi I-MethodName
- I-MethodName
speaker I-MethodName
TTS I-MethodName
. O
We O
also O
deeply O
analyze O
the O
principle O
behind O
the O
performance O
of O
CLAPSpeech B-MethodName
. O
Ablation O
studies O
demonstrate O
the O
necessity O
of O
each O
component O
in O
our O
method O
. O
Source O
code O
and O
audio O
samples O
are O
available O
at O
https O
: O
/ O
/ O
clapspeech.github.io O
. O

Introduction O

With O
the O
development O
of O
deep O
learning O
, O
the O
audio O
quality O
of O
modern O
TTS B-MethodName
systems I-MethodName
has O
been O
improved O
, O
yet O
prosody O
modeling O
is O
still O
a O
challenging O
problem O
. O
Previous O
works O
on O
expressive O
TTS B-TaskName
have O
utilized O
external O
variation O
predictors O
( O
prediction B-MethodName
- I-MethodName
based I-MethodName
, O
PB B-MethodName
) O
( O
Ren O
et O
al O
. O
, O
2021a O
) O
and O
variational O
generative O
models O
( O
variation B-MethodName
- I-MethodName
based I-MethodName
, O
VB B-MethodName
) O
to O
inject O
prosody O
variance O
into O
the O
TTS B-MethodName
model I-MethodName
. O
Another O
popular O
direction O
is O
to O
learn O
better O
text O
representation O
for O
prosody B-TaskName
prediction I-TaskName
( O
Tan O
et O
al O
. O
, O
2021 O
) O
. O
However O
, O
the O
existing O
text O
representation O
learning O
methods O
for O
TTS B-TaskName
are O
either O
based O
on O
the O
masked O
language O
model O
task O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2021 O
) O
( O
i.e. O
, O
learn O
a O
BERT B-MethodName
- O
like O
large O
language O
model O
on O
a O
text O
corpus O
) O
or O
masked O
acoustic O
model O
task O
( O
Chen O
et O
al O
. O
, O
2020 O
; O
Bai O
et O
al O
. O
, O
2022 O
) O
( O
i.e. O
, O
reconstruct O
the O
masked O
mel O
- O
spectrogram O
based O
on O
the O
input O
text O
) O
, O
which O
result O
in O
two O
disadvantages O
. O
Firstly O
, O
they O
only O
implicitly O
learn O
prosody O
with O
reconstruction O
losses O
, O
which O
distracts O
the O
model O
from O
improving O
the O
prosody O
modeling O
. O
Secondly O
, O
they O
do O
not O
decouple O
the O
pronunciation O
space O
and O
prosody O
space O
, O
which O
leads O
to O
low O
training O
efficiency O
and O
a O
waste O
of O
model O
capacity O
. O
We O
perform O
a O
case O
study O
in O
Section O
4.3.1 O
, O
in O
which O
we O
can O
see O
that O
previous O
text O
representation O
used O
in O
TTS B-TaskName
can O
not O
capture O
the O
prosody O
variance O
under O
different O
text O
contexts O
. O

Technically O
, O
prosody O
can O
be O
regarded O
as O
the O
pitch O
and O
duration O
variance O
of O
the O
same O
token O
under O
different O
conditions O
( O
such O
as O
text O
contexts O
and O
speakers O
) O
( O
Tan O
et O
al O
. O
, O
2021 O
) O
. O
This O
paper O
mainly O
studies O
the O
prosody O
correlated O
to O
the O
text O
context O
. O
For O
instance O
, O
for O
the O
same O
word O
" O
higher O
" O
, O
saying O
" O
higher O
up O
" O
or O
" O
slightly O
higher O
" O
can O
lead O
to O
different O
prosodies O
. O
Inspired O
by O
recent O
cross O
- O
modal O
contrastive O
learning O
works O
in O
the O
text O
- O
to O
- O
image O
task O
( O
Radford O
et O
al O
. O
, O
2021 O
; O
Elizalde O
et O
al O
. O
, O
2022 O
) O
, O
we O
propose O
a O
contrastive O
learning O
method O
that O
connects O
the O
text O
context O
and O
the O
high O
- O
level O
prosody O
pattern O
in O
the O
text O
- O
speech O
joint O
multi O
- O
modal O
space O
, O
namely O
Contrastive O
Language O
- O
Audio O
Pre O
- O
Training O

Prosody O
Encoder O
Text O
Encoder O
S O
! O
S O
" O
S O
# O
... O
S O
$ O
T O
! O
T O
" O
T O
# O
… O
T O
$ O

... O
higher O
up O
could O
see O
... O
rising O
higher O
and O
higher O
... O
of O
a O
slightly O
higher O
age O
... O
token O
encoding O
with O
context O
information O
texts O
that O
contain O
the O
selected O
token O
" O
higher O
" O
speech O
segments O
of O
the O
selected O
token O
" O
higher O
" O
for O
Text B-TaskName
- I-TaskName
to I-TaskName
- I-TaskName
Speech I-TaskName
( O
CLAPSpeech B-MethodName
) O
. O
Specifically O
, O
we O
learn O
a O
text O
encoder O
to O
predict O
the O
prosody O
from O
the O
text O
context O
and O
a O
prosody O
encoder O
to O
extract O
the O
ground B-MethodName
- I-MethodName
truth I-MethodName
( O
GT B-MethodName
) O
prosody O
from O
the O
speech O
segment O
of O
the O
selected O
token O
. O
During O
training O
, O
we O
select O
N O
text O
- O
speech O
pairs O
that O
contain O
the O
same O
pronounceable O
token O
( O
e.g. O
, O
the O
word O
" O
higher O
" O
or O
phoneme O
" O
AE0 O
" O
) O
. O
By O
aligning O
the O
text O
token O
with O
its O
corresponding O
prosody O
( O
extracted O
from O
GT B-MethodName
speech O
) O
and O
pushing O
away O
the O
prosody O
representation O
from O
other O
text O
contexts O
, O
the O
text O
encoder O
is O
encouraged O
to O
extract O
prosody O
from O
the O
text O
context O
. O
An O
intuitive O
example O
of O
pre O
- O
training O
CLAPSpeech B-MethodName
can O
be O
found O
in O
Figure O
1 O
. O
We O
also O
observe O
that O
the O
prosody O
pattern O
can O
be O
expressed O
at O
multiple O
levels O
. O
Therefore O
, O
we O
propose O
a O
multi O
- O
scale O
pre O
- O
training O
framework O
that O
learns O
two O
CLAPSpeech B-MethodName
models O
to O
capture O
the O
prosody O
information O
at O
the O
phoneme O
and O
word O
levels O
, O
respectively O
. O
After O
the O
pre O
- O
training O
stage O
, O
our O
CLAPSpeech B-MethodName
can O
be O
regarded O
as O
a O
plugin O
text O
encoder O
applicable O
to O
all O
TTS B-MethodName
models I-MethodName
to O
provide O
fine O
- O
grained O
prosody O
representation O
. O

… O
S O
! O
$ O
T O
! O
S O
! O
$ O
T O
" O
S O
! O
$ O
T O
# O
S O
! O
$ O
T O
$ O
S O
" O
$ O
T O
! O
S O
" O
$ O
T O
" O
S O
" O
$ O
T O
# O
S O
" O
$ O
T O
$ O
S O
# O
$ O
T O
! O
S O
# O
$ O
T O
" O
S O
# O
$ O
T O
# O
S O
# O
$ O
T O
$ O
S O
$ O
$ O
T O
! O
S O
$ O
$ O
T O
" O
S O
$ O
$ O
T O
# O
S O
$ O
$ O

To O
prove O
the O
effectiveness O
and O
generalizability O
of O
our O
approach O
, O
we O
use O
two O
large O
- O
scale O
automatic O
speech O
recognition O
( O
ASR O
) O
datasets O
( O
LibriSpeech B-DatasetName
( O
Panayotov O
et O
al O
. O
, O
2015 O
) O
for O
English O
and O
Wenet O
- O
Speech O
for O
Chinese O
) O
to O
pretrain O
the O
CLAPSpeech B-MethodName
model O
. O
The O
pre O
- O
trained O
text O
encoder O
of O
CLAPSpeech B-MethodName
is O
then O
plugged O
into O
prediction B-MethodName
/ I-MethodName
variation I-MethodName
- I-MethodName
based I-MethodName
TTS I-MethodName
baselines O
to O
demonstrate O
the O
improvement O
of O
CLAPSpeech B-MethodName
to O
the O
exist O
- O
ing O
expressive O
TTS B-MethodName
systems I-MethodName
. O
We O
then O
evaluate O
the O
performance O
on O
three O
TTS B-TaskName
datasets O
, O
including O
one O
single O
- O
speaker O
English O
dataset O
, O
one O
single O
- O
speaker O
Chinese O
corpus O
, O
and O
one O
multi O
- O
speaker O
English O
dataset O
. O
Experiments O
on O
all O
datasets O
show O
that O
CLAPSpeech B-MethodName
improves O
the O
prosody O
of O
the O
TTS B-MethodName
models I-MethodName
and O
outperforms O
previous O
representation O
learning O
methods O
. O

To O
summarize O
, O
CLAPSpeech B-MethodName
has O
three O
prominent O
advantages O
: O
1 O
) O
It O
can O
provide O
better O
prosody O
representation O
than O
previous O
representation O
learning O
methods O
with O
a O
much O
smaller O
model O
scale O
, O
thanks O
to O
its O
contrastive O
objective O
that O
explicitly O
learns O
the O
prosody O
. O
2 O
) O
The O
text O
representation O
of O
CLAPSpeech B-MethodName
can O
be O
conveniently O
used O
in O
existing O
TTS B-MethodName
systems I-MethodName
, O
only O
with O
a O
minor O
modification O
of O
the O
front O
- O
end O
network O
architecture O
. O
3 O
) O
We O
also O
show O
its O
potential O
applications O
such O
as O
fine O
- O
grained O
prosody O
transfer O
in O
Section O
4.3.2 O
. O

Related O
Work O

Expressive O
TTS B-TaskName

In O
the O
past O
few O
years O
, O
modern O
neural O
TTS B-TaskName
has O
made O
significant O
progress O
in O
high O
practicality O
and O
audio O
quality O
( O
Ren O
et O
al O
. O
, O
2019 O
; O
Elias O
et O
al O
. O
, O
2021 O
; O
He O
et O
al O
. O
, O
2022 O
; O
Miao O
et O
al O
. O
, O
2021 O
; O
Kim O
et O
al O
. O
, O
2021 O
; O
Donahue O
et O
al O
. O
, O
2021 O
; O
Jiang O
et O
al O
. O
, O
2022 O
; O
Huang O
et O
al O
. O
, O
2022c O
; O
He O
et O
al O
. O
, O
2023 O
; O
Jiang O
et O
al O
. O
, O
2021 O
; O
Huang O
et O
al O
. O
, O
2022b O
, O
a O
) O
. O
However O
, O
modeling O
expressive O
prosody O
given O
the O
plain O
input O
text O
is O
still O
challenging O
. O
To O
achieve O
expressive O
TTS B-TaskName
, O
one O
common O
practice O
is O
to O
use O
a O
reference O
encoder O
and O
style O
tokens O
. O
But O
it O
is O
difficult O
to O
select O
appropriate O
reference O
audios O
during O
inference O
( O
Tan O
et O
al O
. O
, O
2021 O
) O
. O
Other O
works O
seek O
to O
improve O
prosody B-TaskName
modeling I-TaskName
with O
advanced O
network O
designs O
, O
which O
can O
be O
categorized O
into O
two O
classes O
: O
( O
1 O
) O
the O
prediction B-MethodName
- I-MethodName
based I-MethodName
( O
PB B-MethodName
) O
TTS B-MethodName
systems I-MethodName
( O
Ren O
et O
al O
. O
, O
2021a O
) O
learn O
several O
external O
predictors O
to O
predict B-TaskName
the I-TaskName
prosody I-TaskName
attributes O
such O
as O
pitch O
contour O
, O
duration O
, O
and O
energy O
; O
( O
2 O
) O
the O
variation B-MethodName
- I-MethodName
based I-MethodName
( O
VB B-MethodName
) O
TTS B-MethodName
systems I-MethodName
leverage O
variational O
auto O
- O
encoder O
( O
VAE O
) O
( O
Ren O
et O
al O
. O
, O
2021b O
) O
or O
normalizing O
flow O
to O
model O
the O
prosody O
in O
the O
latent O
space O
. O

There O
are O
also O
some O
works O
that O
explore O
providing O
better O
text O
presentation O
with O
rich O
prior O
knowledge O
to O
help O
the O
prosody B-TaskName
prediction I-TaskName
. O
For O
instance O
, O
and O
incorporate O
syntax O
information O
through O
dedicated O
modeling O
methods O
such O
as O
graph O
networks O
. O
Representation O
learning O
methods O
for O
text O
pre O
- O
training O
and O
speech O
pre O
- O
training O
also O
show O
improvements O
in O
the O
prosody O
of O
TTS B-TaskName
. O
We O
will O
discuss O
the O
representation O
learning O
works O
for O
TTS B-TaskName
in O
the O
next O
section O
. O

Representation O
Learning O
for O
TTS B-TaskName

Self O
- O
supervised O
pre O
- O
training O
methods O
have O
been O
leveraged O
in O
TTS B-TaskName
to O
enhance O
text O
processing O
or O
speech O
generation O
capabilities O
( O
Chung O
et O
al O
. O
, O
2019 O
; O
. O
Some O
early O
works O
( O
Wang O
et O
al O
. O
, O
2015 O
) O
use O
pre O
- O
trained O
word O
embeddings O
to O
improve O
the O
robustness O
of O
TTS B-MethodName
systems I-MethodName
. O
Recently O
, O
some O
works O
explore O
incorporating O
pre O
- O
trained O
large O
masked O
language O
models O
( O
MLMs O
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2021 O
; O
to O
enjoy O
the O
rich O
semantic O
information O
learned O
from O
the O
webscale O
text O
corpus O
. O
However O
, O
the O
above O
- O
mentioned O
works O
only O
focus O
on O
the O
text O
space O
, O
it O
is O
challenging O
for O
them O
to O
model O
expressive O
prosody O
considering O
the O
models O
are O
unaware O
of O
the O
high O
variable O
prosody O
patterns O
in O
the O
speech O
space O
. O
There O
are O
several O
inspiring O
speech O
representation O
learning O
methods O
in O
ASR O
. O
Baevski O
et O
al O
. O
( O
2020 O
) O
and O
Hsu O
et O
al O
. O
( O
2021 O
) O
utilize O
masked O
continuous O
speech O
features O
to O
predict O
predetermined O
cluster O
assignments O
. O
As O
for O
TTS B-TaskName
, O
ProsoSpeech O
designs O
a O
word O
- O
level O
vector O
quantization O
bottleneck O
to O
extract O
discrete O
prosody O
representation O
from O
speech O
. O
Masked O
acoustic O
model O
( O
MAM O
) O
( O
Chen O
et O
al O
. O
, O
2020 O
) O
proposes O
to O
learn O
a O
speech O
encoder O
that O
generates O
continuous O
speech O
( O
prosody O
) O
representations O
. O
Specifically O
, O
during O
training O
they O
replace O
a O
span O
of O
speech O
spectrogram O
with O
mask O
tokens O
and O
learn O
to O
recover O
the O
masked O
spectrogram O
without O
text O
conditions O
. O
A O
3 O
T O
( O
Bai O
et O
al O
. O
, O
2022 O
) O
additionally O
learns O
a O
text O
encoder O
as O
auxiliary O
information O
for O
MAM O
to O
reconstruct O
the O
masked O
mel O
- O
spectrogram O
. O

The O
difference O
between O
CLAPSpeech B-MethodName
and O
previous O
representation O
works O
in O
TTS B-TaskName
is O
obvious O
: O
While O
previous O
works O
implicitly O
learn O
the O
prosody O
information O
with O
the O
masked O
token O
reconstruction O
task O
, O
CLAPSpeech B-MethodName
is O
the O
first O
work O
that O
utilizes O
the O
cross O
- O
modal O
contrastive O
learning O
to O
explicitly O
learn O
the O
context O
- O
correlated O
prosody O
, O
which O
leads O
to O
better O
prosody B-TaskName
prediction I-TaskName
and O
more O
efficient O
usage O
of O
model O
capacity O
. O

CLAPSpeech B-MethodName

We O
propose O
CLAPSpeech B-MethodName
, O
a O
cross O
- O
modal O
contrastive O
learning O
approach O
to O
provide O
better O
text O
representation O
for O
prosody B-TaskName
prediction I-TaskName
in O
TTS B-TaskName
. O
As O
shown O
in O
Figure O
1 O
, O
CLAPSpeech B-MethodName
comprises O
a O
text O
encoder O
and O
a O
prosody O
encoder O
, O
whose O
training O
objective O
is O
to O
connect O
the O
text O
token O
and O
the O
speech O
segment O
in O
the O
joint O
prosody O
space O
. O
In O
this O
section O
, O
we O
first O
design O
the O
network O
structure O
and O
input O
features O
of O
these O
two O
encoders O
. O
These O
elaborate O
designs O
enable O
the O
text O
encoder O
to O
effectively O
process O
the O
text O
context O
and O
ensure O
that O
the O
prosody O
encoder O
focuses O
on O
extracting O
the O
high O
- O
level O
prosody O
pattern O
from O
the O
speech O
segment O
while O
eliminating O
other O
variables O
, O
such O
as O
timbre O
. O
Then O
we O
introduce O
the O
multi O
- O
scale O
contrastive O
pre O
- O
training O
framework O
, O
which O
enables O
CLAPSpeech B-MethodName
to O
capture O
prosody O
in O
both O
phoneme O
and O
word O
levels O
. O
Finally O
, O
we O
show O
how O
the O
pre O
- O
trained O
text O
encoder O
of O
CLAPSpeech B-MethodName
can O
be O
conveniently O
plugged O
into O
modern O
TTS B-TaskName
systems O
to O
improve O
prosody B-TaskName
prediction I-TaskName
. O
We O
describe O
these O
designs O
in O
detail O
in O
the O
following O
subsections O
and O
provide O
more O
technical O
details O
in O
Appendix O
A O
. O

Text O
Encoder O
and O
Prosody O
Encoder O

The O
prosody O
of O
the O
same O
pronounceable O
token O
1 O
varies O
in O
different O
text O
contexts O
. O
CLAPSpeech B-MethodName
aims O
to O
model O
the O
correlation O
between O
the O
text O
context O
and O
the O
high O
- O
level O
prosody O
pattern O
. O
To O
this O
end O
, O
we O
design O
a O
text O
encoder O
and O
a O
prosody O
encoder O
to O
construct O
a O
text O
- O
speech O
multi O
- O
modal O
prosody O
embedding O
space O
. O

As O
shown O
in O
Figure O
2 O
In O
subfigure O
( O
a O
) O
, O
" O
WP O
" O
and O
" O
Word2Ph O
" O
denotes O
word O
pooling O
and O
Word2Ph O
expanding O
operation O
, O
which O
are O
illustrated O
in O
Figure O
3 O
. O
et O
al O
. O
, O
1999 O
) O
of O
the O
input O
text O
as O
the O
input O
. O
The O
phoneme O
and O
BPE O
sequence O
help O
the O
model O
extract O
the O
prosody O
pattern O
related O
to O
phonological O
habits O
( O
such O
as O
the O
linking O
phenomenon O
in O
English O
) O
and O
semantic O
information O
( O
which O
may O
imply O
different O
emotional O
overtones O
) O
, O
respectively O
. O
The O
network O
structure O
of O
the O
text O
encoder O
is O
composed O
of O
several O
Feed B-MethodName
Forward I-MethodName
Transformers I-MethodName
( O
FFT B-MethodName
) O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
have O
proven O
the O
robustness O
in O
processing O
long O
text O
sequences O
in O
TTS B-MethodName
models I-MethodName
. O
Specifically O
, O
we O
learn O
two O
independent O
FFT B-MethodName
blocks O
to O
process O
the O
phoneme O
and O
BPE O
sequences O
, O
respectively O
. O
This O
way O
, O
the O
phoneme B-MethodName
FFT I-MethodName
block I-MethodName
could O
model O
the O
phonological O
habits O
in O
phonetic O
space O
, O
and O
the O
BPE B-MethodName
FFT I-MethodName
block I-MethodName
could O
extract O
the O
semantic O
information O
. O
One O
difficulty O
is O
fusing O
the O
phoneme O
and O
BPE O
sequence O
of O
mismatched O
length O
. O
Instead O
of O
concatenating O
these O
two O
sequences O
in O
the O
time O
axis O
, O
we O
use O
word O
- O
level O
pooling O
( O
WP O
) O
from O
Ren O
et O
al O
. O
( O
2021b O
) O
to O
process O
the O
BPE O
encoding O
to O
the O
word O
level O
, O
then O
expand O
it O
to O
the O
phoneme O
level O
( O
namely O
the O
word2ph O
operation O
) O
. O
To O
be O
specific O
, O
as O
shown O
in O
Figure O
3 O
( O
a O
) O
, O
the O
WP O
operation O
averages O
the O
phoneme O
hidden O
states O
inside O
each O
word O
according O
to O
the O
word O
boundary O
, O
and O
the O
word2ph O
operation O
repeats O
the O
word O
hidden O
states O
for O
each O
phoneme O
insides O
the O
word O
boundary O
as O
illustrated O
in O
Figure O
3 O
( O
b O
) O
. O

Once O
the O
phoneme O
sequence O
and O
BPE O
seqneuce O
is O
fused O
, O
we O
then O
use O
an O
additional O
FFT B-MethodName
block I-MethodName
to O
fuse O
the O
aligned O
phoneme O
and O
BPE O
encoding O
to O
get O
the O
final O
phoneme O
- O
level O
text O
encoding O
. O
During O
the O
pre O
- O
training O
phase O
, O
since O
only O
one O
selected O
token O
is O
analyzed O
, O
we O
index O
from O
the O
phoneme O
- O
level O
text O
encoding O
to O
obtain O
the O
encoding O
of O
the O
selected O
token O
( O
namely O
the O
token O
encoding O
in O
Figure O
2 O
( O
a O
) O
) O
and O
then O
linearly O
project O
it O
into O
the O
multi O
- O
modal O
embedding O
space O
. O
During O
the O
TTS B-TaskName
phase O
, O
the O
phoneme O
- O
level O
output O
of O
the O
text O
encoder O
can O
be O
conveniently O
utilized O
as O
auxiliary O
features O
for O
TTS B-MethodName
systems I-MethodName
, O
which O
we O
will O
discuss O
in O
Section O
3.3 O
. O
The O
prosody O
encoder O
aims O
to O
extract O
prosody O
patterns O
from O
the O
GT O
speech O
segment O
of O
the O
selected O
token O
. O
Therefore O
, O
we O
clip O
the O
mel O
- O
spectrogram O
with O
the O
word O
boundary O
2 O
as O
the O
input O
speech O
feature O
. O
Then O
the O
prosody O
encoder O
processes O
the O
input O
mel O
- O
spectrogram O
into O
a O
global O
encoding O
to O
be O
connected O
with O
the O
token O
encoding O
. O
Note O
that O
the O
clipped O
speech O
segment O
only O
contains O
the O
local O
prosody O
information O
for O
the O
selected O
token O
without O
leaking O
any O
contextual O
information O
. O
Thanks O
to O
the O
contrastive O
learning O
setting O
, O
the O
extracted O
global O
prosody O
encoding O
is O
disentangled O
from O
phonetic O
and O
speaker O
space O
: O
1 O
) O
since O
the O
positive O
sample O
and O
negative O
samples O
belong O
to O
the O
same O
pronounceable O
token O
, O
the O
phonetic O
information O
is O
eliminated O
; O
2 O
) O
as O
the O
speaker O
information O
is O
not O
provided O
to O
the O
text O
encoder O
3 O
, O
the O
prosody O
encoder O
will O
filter O
out O
speaker O
information O
to O
maximize O
the O
prosody O
information O
in O
the O
output O
features O
during O
training O
. O
This O
way O
, O
by O
connecting O
the O
context O
- O
aware O
text O
encoding O
with O
the O
context O
- O
unaware O
mel O
encoding O
, O
on O
the O
one O
hand O
, O
the O
prosody O
encoder O
learns O
to O
extract O
the O
high O
- O
level O
prosody O
information O
from O
the O
speech O
segment O
; O
on O
the O
other O
hand O
, O
the O
text O
encoder O
is O
encouraged O
to O
utilize O
the O
text O
context O
to O
predict O
the O
prosody O
extracted O
by O
the O
prosody O
encoder O
. O
As O
shown O
in O
Figure O
2 O
( O
b O
) O
, O
we O
use O
ResNet-50 O
( O
He O
et O
al O
. O
, O
2016 O
) O
as O
the O
backbone O
of O
the O
prosody O
encoder O
due O
to O
its O
robustness O
. O
We O
make O
several O
modifications O
to O
the O
original O
version O
: O
1 O
) O
to O
better O
process O
the O
melspectrogram O
, O
we O
use O
1D O
convolution O
with O
layer O
normalization O
to O
build O
the O
fundamental O
residual O
block O
; O
2 O
) O
to O
handle O
the O
speech O
segment O
of O
dynamic O
lengths O
, O
we O
use O
an O
attentive O
pooling O
layer O
from O
Radford O
et O
al O
. O
( O
2021 O
) O
to O
aggregate O
the O
output O
feature O
map O
of O
the O
ResNet O
. O

HH O
AE1 O
Z O
| O
N O
EH1 O
V O
ER0 O
Phoneme O
/ O
BPE O
Encoder O
WP O
HH O
AE1 O
Z O
| O
N O
EH1 O
V O
ER0 O

Multi O
- O
scale O
Contrastive O
Pre O
- O
training O

The O
key O
idea O
of O
CLAPSpeech B-MethodName
is O
to O
model O
the O
prosody O
variance O
of O
the O
same O
text O
token O
under O
different O
contexts O
. O
Therefore O
, O
to O
construct O
a O
minibatch O
for O
contrastive O
pre O
- O
training O
, O
we O
randomly O
select O
a O
text O
token O
, O
then O
sample O
a O
batch O
of O
N O
text O
- O
speech O
pairs O
that O
contain O
the O
selected O
token O
( O
one O
intuitive O
sample O
is O
shown O
in O
Figure O
1 O
, O
where O
we O
sample O
the O
text O
- O
speech O
pairs O
that O
contain O
the O
word O
" O
higher O
" O
) O
. O
To O
better O
extract O
prosody O
variance O
at O
the O
phoneme O
and O
word O
level O
, O
we O
introduce O
a O
multi O
- O
scale O
contrastive O
training O
framework O
. O
To O
be O
specific O
, O
we O
learn O
two O
CLAPSpeech B-MethodName
models O
for O
phoneme B-MethodName
- I-MethodName
level I-MethodName
and O
word B-MethodName
- I-MethodName
level I-MethodName
text O
tokens O
, O
respectively O
. O

For O
clarity O
, O
we O
first O
illustrate O
the O
training O
process O
of O
phoneme B-MethodName
- I-MethodName
level I-MethodName
CLAPSpeech I-MethodName
. O
Let O
the O
text O
context O
that O
contains O
the O
selected O
phoneme O
token O
( O
e.g. O
, O
" O
AE0 O
" O
) O
be O
represented O
by O
X O
text O
. O
Let O
the O
processed O
speech O
segment O
of O
the O
phoneme O
token O
be O
X O
speech O
s.t O
. O
X O
speech O
∈ O
R O
F O
×T O
, O
where O
F O
is O
the O
number O
of O
Mel O
bins O
and O
T O
is O
the O
number O
of O
time O
bins O
. O
For O
simplicity O
, O
we O
use O
X O
text O
and O
X O
speech O
to O
represent O
a O
batch O
of O
N O
text O
- O
speech O
pairs O
. O
The O
text O
and O
speech O
are O
passed O
through O
the O
text O
encoder O
f O
text O
( O
• O
) O
and O
prosody O
encoder O
f O
speech O
( O
• O
) O
, O
respectively O
. O
As O
can O
be O
seen O
in O
Figure O
2 O
( O
a O
) O
, O
the O
output O
of O
the O
text O
encoder O
f O
text O
( O
X O
text O
) O
is O
the O
phonemelevel O
encoding O
of O
the O
input O
text O
, O
hence O
we O
index O
from O
it O
to O
obtain O
the O
encoding O
of O
the O
phoneme O
token O
f O
text O
( O
X O
text O
) O
i O
ph O
, O
where O
i O
ph O
denotes O
the O
index O
of O
the O
phoneme O
token O
in O
the O
phoneme O
- O
level O
text O
sequence O
. O
As O
can O
be O
seen O
in O
Figure O
2 O
( O
b O
) O
, O
the O
output O
speech O
encoding O
f O
speech O
( O
X O
speech O
) O
is O
a O
global O
representation O
of O
the O
input O
speech O
segment O
. O
The O
output O
representations O
are O
normalized O
and O
then O
linearly O
projected O
into O
the O
multi O
- O
modal O
embedding O
space O
: O

T O
ph O
= O
L O
text O
( O
LN O
( O
f O
text O
( O
X O
text O
) O
i O
ph O
) O
) O
S O
= O
L O
speech O
( O
LN O
( O
f O
speech O
( O
X O
speech O
) O
) O
) O
, O
( O
1 O
) O

where O
T O
ph O
∈ O
R O
N O
×C O
is O
the O
phoneme O
token O
representation O
and O
S O
∈ O
R O
N O
×C O
is O
the O
speech O
representation O
of O
channel O
size O
C. O
LN O
means O
layer O
normalization O
, O
L O
text O
and O
L O
speech O
are O
linear O
projections O
. O
Now O
that O
the O
text O
and O
speech O
embeddings O
are O
comparable O
, O
CLAPSpeech B-MethodName
is O
trained O
to O
predict O
which O
of O
the O
N O
× O
N O
possible O
text O
- O
speech O
pairings O
across O
a O
batch O
actually O
occurred O
. O
Specifically O
, O
the O
text O
encoder O
and O
prosody O
encoder O
are O
encouraged O
to O
maximize O
the O
cosine O
similarity O
of O
the O
text O
and O
speech O
encoding O
of O
the O
N O
real O
pairs O
in O
the O
batch O
while O
minimizing O
the O
cosine O
similarity O
of O
the O
embeddings O
of O
the O
N O
2 O
− O
N O
incorrect O
pairings O
. O
Following O
Radford O
et O
al O
. O
( O
2021 O
) O
, O
we O
optimize O
a O
symmetric O
cross O
- O
entropy O
loss O
over O
these O
similarity O
scores O
: O

L O
ph O
= O
0.5× O
( O
l O
text O
( O
τ O
•C O
ph O
) O
+ O
l O
speech O
( O
τ O
•C O
ph O
) O
) O
( O
2 O
) O

where O
C O
ph O
∈ O
R O
N O
×N O
is O
the O
cosine O
similarity O
matrix O
between O
the O
phoneme O
token O
encoding O
T O
ph O
and O
the O
speech O
encoding O
S O
, O
measured O
by O
C O
ph O
= O
T O
ph O
• O
S O
T O
; O
τ O
is O
a O
learnable O
temperature O
parameter O
to O
scale O
the O
range O
of O
logits O
; O
and O
l O
k O
= O
1 O
N O
Σ O
N O
i=0 O
log O
diag O
( O
softmax O
( O
C O
) O
) O
is O
the O
cross O
entropy O
function O
along O
the O
text O
and O
speech O
axis O
in O
C O
. O

The O
word B-MethodName
- I-MethodName
level I-MethodName
CLAPSpeech I-MethodName
can O
be O
trained O
similarly O
. O
As O
shown O
in O
Figure O
2 O
( O
a O
) O
, O
for O
the O
word B-MethodName
- I-MethodName
level I-MethodName
CLAPSpeech I-MethodName
, O
we O
use O
word O
pooling O
to O
process O
the O
phoneme O
- O
level O
text O
encoding O
into O
word O
level O
, O
then O
index O
from O
it O
to O
obtain O
the O
word O
token O
encoding O
T O
word O
. O
Similar O
to O
Equation O
2 O
, O
the O
training O
loss O
for O
word B-MethodName
- I-MethodName
level I-MethodName
CLAPSpeech I-MethodName
is O
formulated O
as O
: O

L O
word O
= O
0.5× O
( O
l O
text O
( O
τ O
•C O
word O
) O
+ O
l O
speech O
( O
τ O
•C O
word O
) O
) O

( O
3 O
) O
where O
C O
word O
is O
the O
cosine O
similarity O
matrix O
between O
the O
word O
token O
encoding O
T O
word O
and O
the O
speech O
encoding O
S O
. O

CLAPSpeech B-MethodName
Plugged O
in O
TTS B-MethodName
Systems I-MethodName

The O
text O
encoder O
of O
CLAPSpeech B-MethodName
could O
provide O
text O
representation O
with O
rich O
prosody O
information O
for O
the O
TTS B-TaskName
task O
. O
Since O
the O
generated O
text O
representation O
is O
at O
the O
phoneme O
level O
, O
which O
is O
in O
line O
with O
the O
majority O
of O
current O
TTS B-MethodName
models I-MethodName
that O
also O
utilize O
phoneme O
sequence O
as O
the O
text O
input O
, O
CLAPSpeech B-MethodName
can O
be O
a O
convenient O
plugin O
unit O
for O
TTS B-MethodName
systems I-MethodName
to O
improve O
prosody O
prediction O
. O
Specifically O
, O
we O

Experiments O

Experimental O
Setup O

Datasets O
and O
Baselines O
We O
pre O
- O
train O
CLAP B-MethodName
- I-MethodName
Speech I-MethodName
on O
two O
ASR O
datasets O
: O
1 O
) O
LibriSpeech B-DatasetName
( O
Panayotov O
et O
al O
. O
, O
2015 O
) O
, O
an O
English O
database O
that O
contains O
982 O
hours O
of O
speech O
from O
2484 O
speakers O
; O
2 O
) O
WenetSpeech B-DatasetName
, O
a O
Chinese O
speech O
corpus O
consisting O
of O
10,000 O
hours O
of O
speech O
4 O
. O
Then O
we O
evaluate O
the O
pre B-MethodName
- I-MethodName
trained I-MethodName
CLAP I-MethodName
- I-MethodName
Speech I-MethodName
on O
three O
TTS O
datasets O
: O
1 O
) O
LJSpeech B-DatasetName
( O
Ito O
and O
Johnson O
, O
2017 O
) O
, O
a O
single O
- O
speaker O
database O
that O
contains O
13,100 O
English O
audio O
clips O
with O
a O
total O
of O
nearly O
24 O
hours O
of O
speech O
; O
2 O
) O
Biaobei B-DatasetName
5 O
, O
a O
Chinese O
speech O
corpus O
consisting O
of O
10,000 O
sentences O
( O
about O
12 O
hours O
) O
from O
a O
Chinese O
speaker O
; O
3 O
) O
LibriTTS B-DatasetName
( O
Zen O
et O
al O
. O
, O
2019 O
) O
Model O
Configuration O
CLAPSpeech B-MethodName
consists O
of O
a O
text O
encoder O
and O
a O
prosody O
encoder O
, O
whose O
structures O
are O
shown O
in O
Figure O
2 O
and O
discussed O
in O
Section O
3.2 O
. O
As O
for O
the O
PB B-MethodName
and O
VB B-MethodName
TTS I-MethodName
models O
, O
we O
use O
the O
same O
structure O
in O
the O
original O
papers O
with O
an O
additional O
multi O
- O
length O
discriminator O
to O
improve O
audio O
quality O
. O
The O
multi O
- O
length O
discriminator O
consists O
of O
multiple O
stacked O
convolutional O
layers O
with O
batch O
normalization O
and O
treats O
the O
input O
spectrogram O
as O
images O
. O
We O
put O
more O
detailed O
model O
configurations O
in O
Appendix O
B.1 O
. O

Training O
and O
Evaluation O
Our O
approach O
is O
implemented O
with O
Pytorch O
. O
We O
pre O
- O
train O
CLAPSpeech B-MethodName
on O
4 O
Nvidia O
3090Ti O
GPUs O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
1,024 B-HyperparameterValue
text O
- O
speech O
pairs O
( O
256 O
pairs O
per O
GPU O
) O
. O
We O
use O
the O
Adam O
optimizer B-HyperparameterName
with O
an O
initial O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.0005 B-HyperparameterValue
. O
We O
train O
the O
CLAPSpeech O
model O
for O
640,000 B-HyperparameterValue
iterations B-HyperparameterName
( O
which O
takes O
about O
1 O
week O
) O
and O
follow O
the O
cosine O
learning O
rate O
schedule O
in O
CLIP O
. O

Then O
we O
train O
the O
TTS B-MethodName
models I-MethodName
on O
1 O
Nvidia O
2080Ti O
GPU O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
sentences O
, O
following O
the O
learning B-HyperparameterName
rate I-HyperparameterName
schedule I-HyperparameterName
in O
Vaswani O
et O
al O
. O
( O
2017 O
) O
. O
We O
use O
HiFi O
- O
GAN O
as O
the O
vocoder O
. O
We O
conduct O
the O
mean B-MetricName
opinion I-MetricName
score I-MetricName
( O
MOS B-MetricName
) O
and O
comparative B-MetricName
mean I-MetricName
opinion I-MetricName
score I-MetricName
( O
CMOS B-MetricName
) O
evaluation O
to O
measure O
the O
prosody O
and O
audio O
quality O
. O
Details O
about O
the O
subjective O
evaluation O
can O
be O
found O
in O
Appendix O
B.2 O
. O
As O
for O
the O
objective O
evaluation O
, O
following O
Ren O
et O
al O
. O
( O
2021b O
) O
, O
we O
evaluate O
the O
prosody O
from O
the O
aspects O
of O
pitch O
and O
duration O
: O
1 O
) O
we O
compute O
the O
average B-MetricName
dynamic I-MetricName
time I-MetricName
warping I-MetricName
( O
DTW B-MetricName
) O
( O
Muller O
, O
2007 O
) O
distances O
between O
the O
pitch O
contours O
of O
GT O
speech O
and O
synthesized O
speech O
to O
measure O
the O
pitch O
accuracy O
; O
2 O
) O
we O
calculate O
the O
average B-MetricName
absolute I-MetricName
duration I-MetricName
error I-MetricName
( O
DE B-MetricName
) O
in O
micro O
- O
seconds O
6 O
to O
measure O
the O
duration O
accuracy O
. O

Performance O

We O
compare O
the O
performance O
of O
our O
CLAPSpeech B-MethodName
against O
BERT B-MethodName
and O
A B-MethodName
3 I-MethodName
T I-MethodName
in O
PB B-MethodName
/ I-MethodName
VB I-MethodName
TTS I-MethodName
models O
. O
GT B-MethodName
( O
the O
ground B-MethodName
- I-MethodName
truth I-MethodName
audio I-MethodName
) O
and O
GT B-MethodName
( I-MethodName
voc I-MethodName
. I-MethodName
) I-MethodName
( O
the O
audio O
waveform O
generated O
by O
the O
vocoder O
using O
the O
GT O
mel O
- O
spectrogram O
) O
are O
also O
included O
in O
the O
experiment O
. O
We O
perform O
the O
TTS O
experiments O
on O
three O
datasets O
as O
mentioned O
in O
Section O
4.1 O
. O
The O
results O
are O
shown O
in O
Table O
1 O
. O
We O
can O
see O
that O
CLAPSpeech B-MethodName
outperforms O
other O
representation O
learning O
methods O
in O
both O
PB B-MethodName
and O
VB B-MethodName
TTS I-MethodName
baselines O
in O
terms O
of O
MOS B-MetricName
, O
pitch B-MetricName
accuracy I-MetricName
, O
and O
duration B-MetricName
accuracy I-MetricName
, O
which O
proves O
that O
CLAPSpeech B-MethodName
could O
effectively O
improve O
the O
prosody B-TaskName
prediction I-TaskName
in O
current O
expressive O
TTS B-MethodName
models I-MethodName
( O
no O
matter O
prediction B-MethodName
- I-MethodName
based I-MethodName
or O
variationbased B-MethodName
) O
. O
Besides O
, O
we O
observe O
that O
CLAPSpeech B-MethodName
achieves O
better O
performance O
than O
BERT B-MethodName
and O
A B-MethodName
3 I-MethodName
T I-MethodName
with O
much O
fewer O
model O
parameters O
. O
We O
suspect O
it O
is O
due O
to O
the O
fact O
that O
the O
MLM O
- O
based O
method O
( O
i.e. O
, O
BERT B-MethodName
) O
require O
a O
large O
model O
capacity O
to O
store O
the O
semantic O
information O
and O
MAM O
- O
based O
method O
( O
i.e. O
, O
A B-MethodName
3 I-MethodName
T I-MethodName
) O
have O
to O
jointly O
learn O
the O
phonetic O
information O
to O
reconstruct O
the O
masked O
mel O
- O
spectrogram O
. O
By O
contrast O
, O
our O
CLAPSpeech B-MethodName
eliminates O
the O
phonetic O
space O
and O
only O
focus O
on O
the O
prosody O
space O
during O
pre O
- O
training O
, O
which O
is O
parameter O
- O
efficient O
. O

We O
then O
visualize O
the O
mel O
- O
spectrograms O
generated O
by O
different O
methods O
in O
Figure O
5 O
. O
We O
can O
see O
that O
CLAPSpeech B-MethodName
can O
generate O
results O
with O
more O
realistic O
pitch O
contours O
, O
which O
result O
in O
expressive O
prosody O
. O
In O
conclusion O
, O
our O
experiments O
demonstrate O
that O
CLAPSpeech B-MethodName
could O
help O
TTS B-MethodName
systems I-MethodName
synthesize O
more O
expressive O
and O
prosodic O
audio O
. O

Deeper O
Analysis O

Token O
Representation O
Self O
- O
similarity O

To O
better O
understand O
the O
performance O
superiority O
of O
CLAPSPeech B-MethodName
over O
existing O
representation O
learning O
methods O
for O
TTS B-TaskName
, O
we O
analyze O
the O
token O
represen O
- O
tation O
learned O
by O
CLAPSpeech B-MethodName
and O
other O
methods O
. O
Following O
Su O
et O
al O
. O
( O
2021 O
) O
, O
we O
define O
the O
averaged O
similarity O
on O
the O
selected O
token O
under O
different O
contexts O
T O
= O
[ O
T O
1 O
, O
... O
, O
T O
N O
] O
as O
, O

s B-MetricName
( I-MetricName
T I-MetricName
) I-MetricName
= O
1 O
N O
( O
N O
− O
1 O
) O
N O
i=1 O
N O
j=1 O
, O
j̸ O
= O
i O
cosine O
( O
T O
i O
, O
T O
j O
) O
( O
4 O
) O

where O
T O
i O
and O
T O
j O
are O
the O
selected O
token O
's O
encoding O
extracted O
by O
the O
model O
from O
different O
text O
contexts O
. O
Intuitively O
, O
a O
lower O
s B-MetricName
( I-MetricName
T I-MetricName
) I-MetricName
indicates O
that O
the O
selected O
token O
itself O
plays O
a O
smaller O
role O
in O
generating O
its O
representation O
, O
which O
means O
that O
the O
model O
captures O
more O
context O
- O
related O
information O
from O
the O
input O
text O
sequence O
, O
and O
thus O
predicts O
better O
prosody O
. O

Quantitative O
Evaluation O
We O
sample O
10,000 O
batches O
( O
each O
batch O
consists O
of O
256 O
sentences O
that O
contain O
the O
same O
selected O
token O
) O
from O
the O
ASR O
validation O
datasets O
and O
compute O
the O
averaged B-MetricName
selfsimilarity I-MetricName
. O
The O
result O
is O
shown O
in O
Table O
2 O
. O
We O
observe O
that O
our O
CLAPSpeech B-MethodName
learned O
with O
the O
contrastive O
objective O
( O
in O
Equation O
2 O
) O
achieves O
the O
lowest O
similarity B-MetricName
in O
the O
off O
- O
diagonal O
entries O
of O
the O
similarity O
matrix O
, O
which O
denotes O
that O
the O
model O
has O
made O
use O
of O
the O
text O
context O
to O
capture O
the O
prosody O
variance O
of O
the O
same O
token O
, O
thus O
achieve O
the O
best O
prosody O
performance O
in O
Table O
1 O
. O
Besides O
, O
we O
can O
see O
that O
BERT B-MethodName
also O
achieves O
a O
relatively O
low O
offdiagonal B-MetricName
similarity I-MetricName
, O
which O
is O
due O
to O
its O
MLM O
task O
during O
pre O
- O
training O
, O
in O
which O
the O
model O
needs O
to O
extract O
semantic O
information O
from O
context O
to O
predict O
the O
masked O
token O
. O
By O
contrast O
, O
the O
vanilla B-MethodName
TTS I-MethodName
text I-MethodName
encoder I-MethodName
and O
A B-MethodName
3 I-MethodName
T I-MethodName
fail O
to O
achieve O
a O
low O
offdiagonal B-MetricName
similarity I-MetricName
, O
which O
means O
that O
both O
models O
can O
not O
extract O
discriminative O
information O
from O
different O
contexts O
. O
We O
suspect O
the O
failure O
of O
A B-MethodName
3 I-MethodName
T I-MethodName
is O
due O
to O
the O
fact O
that O
its O
MAM O
objective O
encourages O
the O
model O
to O
predict O
the O
masked O
mel O
- O
spectrogram O
patch O
based O
on O
the O
input O
unmasked O
text O
sequence O
, O
which O
increases O
the O
model O
's O
demand O
for O
phonetic O
information O
of O
the O
selected O
token O
. O

Qualitative O
Evaluation O
We O
sample O
8 O
sentences O
7 O
that O
contain O
the O
word O
" O
higher O
" O
from O
LibriSpeech B-DatasetName
and O
visualize O
the O
self B-MetricName
- I-MetricName
similarity I-MetricName
matrix I-MetricName
M O
( O
where O
M O
i O
, O
j O
= O
cosine O
( O
T O
i O
, O
T O
j O
) O
) O
produced O
by O
CLAPSpeech B-MethodName
and O
vanilla B-MethodName
TTS I-MethodName
text I-MethodName
encoder I-MethodName
. O
The O
results O
are O
shown O
in O
Figure O
6 O
, O
where O
a O
darker O
color O
denotes O
a O
higher O
self O
- O
similarity O
score O
. O
We O
also O
provide O
the O
selfsimilarity B-MetricName
matrix I-MetricName
of O
BERT B-MethodName
and O
A B-MethodName
3 I-MethodName
T I-MethodName
in O
Figure O
9 O
of O
Appendix O
C. O
We O
can O
see O
that O
the O
self B-MetricName
- I-MetricName
similarities I-MetricName
of O
CLAPSpeech B-MethodName
are O
much O
lower O
in O
the O
off O
- O
diagonal O
entries O
. O

Fine O
- O
grained O
Prosody O
Transfer O

We O
perform O
an O
intuitive O
case O
study O
about O
prosody O
transfer O
to O
further O
validate O
that O
our O
CLAPSpeech B-MethodName
's O
text O
- O
speech O
joint O
multi O
- O
modal O
space O
represents O
high O
- O
level O
prosody O
patterns O
( O
i.e. O
, O
the O
pitch O
contours O
and O
duration O
information O
) O
. O
We O
take O
s7 O
/ O
8 O
in O
Table O
5 O
as O
the O
reference O
/ O
source O
audio O
and O
expect O
to O
transfer O
the O
word O
" O
higher O
" O
's O
prosody O
pattern O
from O
s7 O
to O
s8 O
. O
Specifically O
, O
we O
use O
the O
text O
encoder O
of O
CLAP B-MethodName
- I-MethodName
Speech I-MethodName
to O
extract O
the O
text O
prosody O
encoding O
of O
s7 O
and O
s8 O
, O
then O
replace O
the O
text O
token O
encoding O
of O
" O
higher O
" O
in O
s8 O
with O
that O
in O
s7 O
. O
As O
shown O
in O
Figure O
7 O
, O
the O
prosody O
pattern O
of O
" O
higher O
" O
in O
s8 O
8 O
in O
Figure O
7 O
( O
a O
) O
has O
been O
successfully O
transferred O
into O
s7 O
in O
8 O
the O
pitch O
contours O
in O
reference O
remain O
flat O
in O
the O
early O
stage O
and O
then O
rise O
in O
the O
late O
stage O
1.00 O
1.00 O
1.00 O
1.00 O
1.00 O
0.99 O
0.96 O
1.00 O
1.00 O
1.00 O
1.00 O
1.00 O
1.00 O
1.00 O
0.96 O
1.00 O
1.00 O
1.00 O
1.00 O
0.98 O
1.00 O
1.00 O
0.96 O
1.00 O
1.00 O
1.00 O
0.98 O
1.00 O
0.99 O
1.00 O
0.96 O
1.00 O
1.00 O
1.00 O
1.00 O
0.99 O
1.00 O
1.00 O
0.98 O
0.99 O
0.99 O
1.00 O
1.00 O
1.00 O
1.00 O
1.00 O
0.96 O
1.00 O
0.96 O
0.96 O
0.96 O
0.96 O
0.98 O
0.96 O
1.00 O
0.96 O
1.00 O
1.00 O
1.00 O
1.00 O
0.99 O
1.00 O
0.96 O
1.00 O
Figure O
7 O
( O
c O
) O
. O
We O
also O
provide O
audio O
samples O
of O
this O
case O
study O
on O
our O
demo O
page O
. O
The O
manipulation O
of O
the O
local O
prosody O
proves O
that O
our O
CLAPSpeech B-MethodName
extract O
prosody O
representation O
effectively O
influences O
the O
prosody B-TaskName
prediction I-TaskName
of O
the O
TTS B-MethodName
system I-MethodName
. O

Ablation O
Studies O

Use O
BPE O
as O
Auxiliary O
Features O
We O
first O
analyze O
the O
effectiveness O
of O
the O
BPE O
as O
an O
auxiliary O
feature O
to O
help O
extract O
prosody O
information O
from O
the O
text O
context O
. O
During O
the O
pre O
- O
training O
phase O
of O
CLAPSpeech B-MethodName
, O
we O
found O
removing O
BPE O
from O
the O
text O
encoder O
significantly O
degrades O
the O
validation O
CLIP O
loss O
from O
0.3692 O
to O
0.6764 O
. O
Then O
in O
the O
TTS O
phase O
, O
as O
can O
be O
seen O
in O
line O
3 O
in O
Table O
3 O
, O
the O
ablated O
model O
using O
the O
pre O
- O
trained O
text O
encoder O
without O
BPE O
leads O
to O
a O
performance O
drop O
in O
terms O
of O
CMOS O
, O
DTW O
, O
and O
DE O
. O
This O
is O
possibly O
due O
to O
the O
fact O
that O
BPE O
could O
better O
represent O
the O
semantic O
information O
than O
the O
low O
- O
level O
phoneme O
sequence O
. O

Multi O
- O
scale O
Pre O
- O
training O

To O
demonstrate O
the O
effectiveness O
of O
multi O
- O
scale O
pre O
- O
training O
, O
as O
can O
be O
seen O
in O
line O
4 O
/ O
5 O
in O
Table O
3 O
, O
we O
tried O
to O
remove O
phoneme B-MethodName
- I-MethodName
level I-MethodName
or O
word B-MethodName
- I-MethodName
level I-MethodName
CLAPSpeech I-MethodName
from O
the O
model O
, O
which O
leads O
to O
a O
worse O
prosody O
performance O
. O
We O
also O
tried O
to O
use O
the O
untrained O
CLAP O
- O
Speech O
to O
prove O
the O
necessity O
of O
the O
pre O
- O
training O
process O
, O
and O
we O
found O
this O
ablated O
model O
( O
line O
6 O
) O
achieves O
a O
slightly O
worse O
performance O
than O
the O
TTS B-MethodName
baseline I-MethodName
( O
line O
3 O
) O
. O

Conclusion O

In O
this O
paper O
, O
we O
propose O
CLAPSpeech B-MethodName
, O
a O
crossmodal O
contrastive O
pre O
- O
training O
framework O
that O
provides O
better O
text O
representation O
with O
rich O
prosody O
information O
for O
TTS B-TaskName
. O
With O
the O
design O
of O
a O
text O
encoder O
and O
a O
prosody O
encoder O
, O
CLAPSpeech B-MethodName
learns O
to O
connect O
the O
text O
context O
with O
its O
corresponding O
prosody O
pattern O
in O
the O
speech O
. O
We O
also O
introduced O
multi O
- O
scale O
pre O
- O
training O
to O
extract O
prosody O
patterns O
at O
multiple O
levels O
. O
We O
have O
demonstrated O
the O
performance O
and O
generalization O
ability O
of O
CLAPSpeech B-MethodName
on O
three O
TTS B-TaskName
datasets O
( O
English O
, O
Chinese O
, O
and O
multispeaker O
, O
respectively O
) O
. O
We O
have O
also O
deeply O
analyzed O
the O
principle O
behind O
the O
improvement O
of O
CLAPSpeech B-MethodName
and O
performed O
ablation O
studies O
to O
prove O
the O
necessity O
of O
each O
component O
. O

Limitations O

There O
are O
majorly O
two O
limitations O
: O
Firstly O
, O
in O
this O
work O
, O
we O
only O
consider O
the O
current O
- O
sentence O
text O
context O
- O
related O
prosody O
. O
In O
future O
work O
, O
we O
will O
focus O
on O
improving O
the O
inter O
- O
sentence O
prosody O
to O
achieve O
coherent O
, O
expressive O
TTS B-TaskName
for O
long O
- O
form O
text O
. O
Secondly O
, O
other O
variables O
are O
not O
considered O
during O
the O
contrastive O
pre O
- O
training O
. O
One O
can O
explore O
similar O
approaches O
that O
connect O
prosody O
to O
other O
conditions O
such O
as O
speaker O
, O
emotion O
, O
etc O
. O

Ethics O
Statement O

CLAPSpeech B-MethodName
improves O
the O
prosody O
of O
the O
synthesized O
speech O
, O
which O
may O
cause O
unemployment O
for O
people O
with O
related O
occupations O
. O
Besides O
, O
the O
production O
of O
fake O
speeches O
may O
cause O
voice O
security O
issues O
. O
Further O
efforts O
in O
automatic O
speaker O
verification O
should O
be O
made O
to O
improve O
voice O
security O
. O
We O
show O
how O
to O
integrate O
CLAPSpeech B-MethodName
into O
a O
popular O
prediction B-MethodName
- I-MethodName
based I-MethodName
TTS I-MethodName
system I-MethodName
, O
FastSpeech B-MethodName
2 I-MethodName
. O

As O
shown O
in O
Figure O
8 O
, O
the O
pre O
- O
trained O
text O
encoders O
of O
CLAPSpeech B-MethodName
( O
marked O
with O
a O
red O
dashed O
rectangle O
) O
perform O
as O
an O
auxiliary O
encoder O
to O
the O
original O
phonetic O
encoder O
of O
FastSpeech B-MethodName
2 I-MethodName
. O
The O
phonemelevel O
outputs O
of O
the O
phonetic O
encoder O
and O
CLAP B-MethodName
- I-MethodName
Speech I-MethodName
text O
encoder O
are O
fused O
and O
processed O
by O
the O
following O
encoder O
. O
Note O
that O
we O
fix O
the O
parameters O
of O
CLAPSpeech B-MethodName
text O
encoders O
during O
the O
training O
of O
the O
TTS B-MethodName
system I-MethodName
to O
avoid O
overfitting O
. O

A.2 O
Multi O
- O
length O
Adversarial O
Training O

For O
the O
tested O
TTS B-MethodName
baselines I-MethodName
, O
we O
adopt O
an O
additional O
multi O
- O
length O
discriminator O
to O
provide O
a O
least O
squared O
GAN O
loss O
to O
improve O
the O
audio O
quality O
. O
The O
multi O
- O
length O
discriminator O
is O
an O
ensemble O
of O
multiple O
CNN O
- O
based O
discriminators O
which O
evaluates O
the O
mel O
- O
spectrogram O
based O
on O
random O
windows O
of O
different O
lengths O
. O
One O
could O
refer O
to O
for O
more O
details O
. O

B O
Detailed O
Experimental O
Settings O

B.1 O
Model O
Configurations O

We O
list O
the O
hyper O
- O
parameters O
of O
CLAPSpeech B-MethodName
and O
the O
tested O
TTS B-MethodName
baselines I-MethodName
in O
Table O
4 O
. O

B.2 O
Subjective O
Evaluation O

For O
each O
tested O
dataset O
, O
we O
randomly O
select O
10 O
texts O
from O
the O
test O
set O
and O
use O
the O
TTS B-MethodName
systems I-MethodName
to O
generate O
the O
audio O
samples O
. O
Each O
audio O
has O
been O
listened O
to O
by O
at O
least O
20 O
native O
listeners O
, O
who O
are O

C.2 O
Self B-MetricName
- I-MetricName
similarity I-MetricName
of O
Other O
Baselines O

The O
self B-MetricName
- I-MetricName
similarity I-MetricName
visualization O
of O
A B-MethodName
3 I-MethodName
T I-MethodName
and O
BERT B-MethodName
can O
be O
found O
in O
Figure O
9 O
. O
We O
discuss O
the O
results O
in O
Section O
4.3.1 O
. O
s1 O
... O
for O
the O
reputation O
of O
the O
stern O
judge O
stands O
not O
higher O
than O
that O
of O
the O
compassionate O
... O
s2 O
As O
I O
went O
on O
, O
the O
precipices O
rose O
higher O
and O
seemed O
to O
overhang O
. O
The O
channel O
grew O
narrower O
... O
s3 O
Better O
, O
and O
better O
, O
and O
better O
! O
Her O
voice O
went O
higher O
with O
each O
better O
, O
till O
it O
got O
quite O
to O
a O
squeak O
at O
last O
. O
s4 O
... O
and O
the O
native O
graduates O
of O
our O
higher O
institutions O
have O
begun O
to O
show O
their O
strength O
... O
s5 O
Innocence O
is O
higher O
than O
virtue O
. O
s6 O
Nothing O
seems O
more O
unfit O
to O
give O
a O
deeper O
meaning O
to O
life O
and O
a O
higher O
value O
. O
s7 O
Higher O
up O
could O
be O
seen O
some O
chinamen O
, O
but O
whether O
they O
were O
fishing O
or O
washing O
we O
could O
not O
tell O
. O
s8 O
May O
they O
become O
convalescents O
and O
overcomers O
, O
and O
create O
higher O
bodies O
for O
themselves O
! O
B2 O
. O
Did O
you O
discuss O
the O
license O
or O
terms O
for O
use O
and O
/ O
or O
distribution O
of O
any O
artifacts O
? O
No O
response O
. O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O

No O
response O
. O

B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
No O
response O
. O

B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
No O
response O
. O

B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O

No O
response O
. O

This O
work O
was O
supported O
in O
part O
by O
the O
National O
Key O
R O
& O
D O
Program O
of O
China O
under O
Grant O
No.2022ZD0162000 O
, O
National O
Natural O
Science O
Foundation O
of O
China O
under O
Grant O
No O
. O
62222211 O
and O
Grant O
No.61836002 O
and O
Grant O
No.62072397 O
, O
and O
Yiwise O
. O

Robust O
Self O
- O
Augmentation O
for O
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
with O
Meta B-MethodName
Reweighting I-MethodName

Self O
- O
augmentation O
has O
received O
increasing O
research O
interest O
recently O
to O
improve O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
performance O
in O
lowresource O
scenarios O
. O
Token O
substitution O
and O
mixup O
are O
two O
feasible O
heterogeneous O
selfaugmentation O
techniques O
for O
NER B-TaskName
that O
can O
achieve O
effective O
performance O
with O
certain O
specialized O
efforts O
. O
Noticeably O
, O
self O
- O
augmentation O
may O
introduce O
potentially O
noisy O
augmented O
data O
. O
Prior O
research O
has O
mainly O
resorted O
to O
heuristic O
rule O
- O
based O
constraints O
to O
reduce O
the O
noise O
for O
specific O
self O
- O
augmentation O
methods O
individually O
. O
In O
this O
paper O
, O
we O
revisit O
these O
two O
typical O
self O
- O
augmentation O
methods O
for O
NER B-TaskName
, O
and O
propose O
a O
unified O
meta B-MethodName
- I-MethodName
reweighting I-MethodName
strategy O
for O
them O
to O
achieve O
a O
natural O
integration O
. O
Our O
method O
is O
easily O
extensible O
, O
imposing O
little O
effort O
on O
a O
specific O
self O
- O
augmentation O
method O
. O
Experiments O
on O
different O
Chinese O
and O
English O
NER B-TaskName
benchmarks O
show O
that O
our O
token O
substitution O
and O
mixup O
method O
, O
as O
well O
as O
their O
integration O
, O
can O
achieve O
effective O
performance O
improvement O
. O
Based O
on O
the O
meta B-MethodName
- I-MethodName
reweighting I-MethodName
mechanism O
, O
we O
can O
enhance O
the O
advantages O
of O
the O
self O
- O
augmentation O
techniques O
without O
much O
extra O
effort O
. O

Introduction O

Named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
, O
which O
aims O
to O
extract O
predefined O
named O
entities O
from O
a O
piece O
of O
unstructured O
text O
, O
is O
a O
fundamental O
task O
in O
the O
natural O
language O
processing O
( O
NLP O
) O
community O
, O
and O
has O
been O
studied O
extensively O
for O
several O
decades O
( O
Hammerton O
, O
2003 O
; O
Huang O
et O
al O
. O
, O
2015 O
; O
Chiu O
and O
Nichols O
, O
2016 O
; O
Ma O
and O
Hovy O
, O
2016 O
) O
. O
Recently O
, O
supervised O
sequence O
labeling O
neural O
models O
have O
been O
exploited O
most O
popularly O
for O
NER B-TaskName
, O
leading O
to O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
performance O
( O
Zhang O
and O
Yang O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2020a O
; O
Ma O
et O
al O
. O
, O
2020 O
) O
. O

Although O
great O
progress O
has O
been O
made O
, O
developing O
an O
effective O
NER B-TaskName
model O
usually O
requires O
a O
Figure O
1 O
: O
The O
main O
idea O
of O
our O
work O
, O
where O
the O
two O
heterogeneous O
self O
- O
augmentation O
methods O
( O
i.e. O
, O
token O
substitution O
and O
mixup O
) O
are O
integrated O
by O
a O
unified O
meta O
reweighting O
framework O
. O

large O
- O
scale O
and O
high O
- O
quality O
labeled O
training O
corpus O
, O
which O
is O
often O
difficult O
to O
be O
obtained O
in O
real O
- O
world O
scenarios O
due O
to O
the O
expensive O
and O
time O
- O
consuming O
annotations O
by O
human O
experts O
. O
Moreover O
, O
it O
would O
be O
extremely O
serious O
because O
the O
target O
language O
, O
target O
domain O
, O
and O
the O
desired O
entity O
type O
could O
all O
be O
infinitely O
varied O
. O
As O
a O
result O
, O
the O
low O
- O
resource O
setting O
with O
only O
a O
small O
amount O
of O
annotated O
corpus O
available O
is O
far O
more O
common O
in O
practice O
, O
even O
though O
it O
may O
result O
in O
significant O
performance O
degradation O
due O
to O
the O
overfitting O
problem O
. O

Self O
- O
augmentation O
is O
a O
prospective O
solution O
to O
this O
problem O
, O
which O
has O
received O
widespread O
attention O
Wei O
and O
Zou O
, O
2019 O
; O
Dai O
and O
Adel O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2020 O
; O
Karimi O
et O
al O
. O
, O
2021 O
) O
. O
The O
major O
motivation O
is O
to O
generate O
a O
pseudo O
training O
example O
set O
deduced O
from O
the O
original O
gold O
- O
labeled O
training O
data O
automatically O
. O
For O
NER B-TaskName
, O
a O
token O
- O
level O
task O
, O
the O
feasible O
self O
- O
augmentation O
techniques O
include O
token O
substitution O
( O
Dai O
and O
Adel O
, O
2020 O
; O
Zeng O
et O
al O
. O
, O
2020 O
) O
and O
mixup O
( O
Zhang O
et O
al O
. O
, O
2020a O
; O
Chen O
et O
al O
. O
, O
2020 O
) O
, O
which O
are O
deformed O
at O
the O
ground O
- O
level O
inputs O
and O
the O
high O
- O
level O
hidden O
representations O
, O
respectively O
. O

Nonetheless O
, O
there O
are O
still O
some O
limitations O
currently O
for O
the O
above O
token O
substitution O
and O
mixup O
methods O
. O
For O
one O
thing O
, O
both O
of O
them O
require O
some O
specialized O
efforts O
to O
improve O
their O
effectiveness O
due O
to O
the O
potential O
noise O
introduced O
by O
the O
selfaugmentation O
, O
which O
may O
restrict O
the O
valid O
seman O
- O
tic O
representation O
of O
the O
augmented O
data O
. O
For O
instance O
, O
token O
substitution O
is O
typically O
limited O
to O
the O
named O
entities O
in O
the O
training O
corpus O
, O
and O
the O
mixup O
tends O
to O
be O
imposed O
on O
the O
example O
pairs O
with O
small O
semantic O
distance O
gaps O
( O
Chen O
et O
al O
. O
, O
2020 O
) O
. O
For O
another O
thing O
, O
though O
the O
two O
techniques O
seem O
to O
be O
orthogonal O
and O
probably O
complementary O
to O
each O
other O
, O
it O
remains O
a O
potential O
challenge O
to O
effectively O
and O
naturally O
integrate O
them O
. O

In O
this O
work O
, O
we O
revisit O
the O
token O
substitution O
and O
mixup O
methods O
for O
NER B-TaskName
, O
and O
investigate O
the O
two O
heterogeneous O
techniques O
under O
a O
unified O
meta O
reweighting O
framework O
( O
as O
illustrated O
in O
Figure O
1 O
) O
. O
First O
, O
we O
try O
to O
relax O
the O
previous O
constraints O
to O
a O
broader O
scope O
for O
these O
methods O
, O
allowing O
for O
more O
diverse O
and O
larger O
- O
scale O
pseudo O
training O
examples O
. O
However O
, O
this O
would O
inevitably O
produce O
some O
lowquality O
augmented O
examples O
( O
i.e. O
, O
noisy O
pseudo O
data O
) O
in O
terms O
of O
linguistic O
correctness O
, O
which O
may O
negatively O
affect O
the O
model O
performance O
. O
To O
this O
end O
, O
we O
present O
a O
meta O
reweighting O
strategy O
for O
controlling O
the O
quality O
of O
the O
augmented O
examples O
and O
leading O
to O
noise O
- O
robust O
training O
. O
Also O
, O
we O
can O
naturally O
integrate O
the O
two O
methods O
by O
using O
the O
example O
reweighting O
mechanism O
, O
without O
any O
specialization O
in O
a O
specific O
self O
- O
augmentation O
method O
. O

Finally O
, O
we O
carry O
out O
experiments O
on O
several O
Chinese O
and O
English O
NER B-TaskName
benchmark O
datasets O
to O
evaluate O
our O
proposed O
methods O
. O
We O
mainly O
focus O
on O
the O
low O
- O
resource O
settings O
, O
which O
can O
be O
simulated O
by O
using O
only O
part O
of O
the O
standard O
training O
set O
when O
the O
scale O
is O
large O
. O
Experimental O
results O
show O
that O
both O
our O
token O
substitution O
and O
mixup O
method O
coupled O
with O
the O
meta O
- O
reweighting O
can O
effectively O
improve O
the O
performance O
of O
our O
baseline O
model O
, O
and O
the O
combination O
can O
bring O
consistent O
improvement O
. O
Positive O
gains O
become O
more O
significant O
as O
the O
scale O
of O
the O
training O
data O
decreases O
, O
indicating O
that O
our O
self O
- O
augmentation O
methods O
can O
handle O
the O
low O
- O
resource O
NER B-TaskName
well O
. O
In O
addition O
, O
our O
methods O
can O
still O
work O
even O
with O
a O
large O
amount O
of O
training O
data O
. O
The O
code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
LindgeW O
/ O
MetaAug4NER O
. O

Our O
Approach O

In O
this O
section O
, O
we O
firstly O
describe O
our O
baseline O
model O
. O
Then O
, O
we O
present O
our O
self O
- O
augmentation O
methods O
to O
enhance O
the O
baseline O
model O
in O
the O
lowresource O
settings O
. O
Finally O
, O
we O
elaborate O
on O
our O
meta B-MethodName
reweighting I-MethodName
strategy O
, O
which O
aims O
to O
alleviate O
the O
negative O
impact O
of O
the O
noisy O
augmented O
examples O
caused O
by O
the O
self O
- O
augmentation O
while O
also O
elegantly O
combining O
these O
augmentation O
methods O
. O

Baseline O
Model O

NER B-TaskName
task O
is O
typically O
formulated O
as O
a O
sequence O
labeling O
problem O
, O
which O
transforms O
entities O
/ O
nonentities O
into O
token O
- O
level O
boundary O
label O
sequence O
by O
using O
the O
BIO O
or O
BIOES O
schema O
( O
Huang O
et O
al O
. O
, O
2015 O
; O
Lample O
et O
al O
. O
, O
2016 O
) O
. O
In O
this O
work O
, O
we O
adopt O
BERT B-MethodName
- I-MethodName
BiLSTM I-MethodName
- I-MethodName
CRF I-MethodName
as O
our O
basic O
model O
architecture O
which O
consists O
of O
four O
components O
: O
( O
1 O
) O
input O
representation O
, O
( O
2 O
) O
BiLSTM O
encoding O
, O
( O
3 O
) O
CRF O
decoding O
, O
and O
( O
4 O
) O
training O
objective O
. O

Input O
Representation O
Given O
an O
input O
sequence O
X O
= O
( O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
) O
of O
length O
n O
, O
we O
first O
convert O
it O
into O
sequential O
hidden O
vectors O
using O
the O
pre O
- O
trained O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
: O

e O
1 O
, O
• O
• O
• O
, O
e O
n O
= O
BERT O
( O
X O
) O
, O
( O
1 O
) O

where O
each O
token O
is O
mapped O
to O
a O
contextualized O
representation O
correspondingly O
. O

Encoding O
We O
use O
a O
bidirectional O
LSTM O
layer O
to O
further O
extract O
the O
contextual O
representations O
, O
where O
the O
process O
can O
be O
formalized O
as O
: O

h O
1 O
, O
• O
• O
• O
, O
h O
n O
= O
BiLSTM O
( O
e O
1 O
, O
• O
• O
• O
, O
e O
n O
) O
, O
( O
2 O
) O

where O
h O
i O
is O
the O
hidden O
state O
output O
of O
the O
i O
- O
th O
token O
in O
the O
sequence O
( O
i O
∈ O
[ O
1 O
, O
n O
] O
) O
. O

Decoding O
First O
, O
a O
linear O
transformation O
layer O
is O
used O
to O
calculate O
the O
initial O
label O
scores O
. O
Then O
, O
a O
label O
transition O
matrix O
T O
is O
used O
to O
model O
the O
label O
dependency O
. O
Let O
Y O
= O
( O
y O
1 O
, O
• O
• O
• O
, O
y O
n O
) O
be O
a O
label O
sequence O
, O
the O
score O
s O
( O
Y O
|X O
) O
can O
be O
computed O
by O
: O

o O
i O
= O
W O
h O
i O
+ O
b O
, O
s O
( O
Y O
|X O
) O
= O
n O
i=1 O
( O
T O
y O
i−1 O
, O
y O
i O
+ O
o O
i O
[ O
y O
i O
] O
) O
, O
( O
3 O
) O

where O
W O
, O
b O
and O
T O
are O
the O
model O
parameters O
. O
Finally O
, O
we O
employ O
the O
Viterbi O
algorithm O
( O
Viterbi O
, O
1967 O
) O
to O
find O
the O
best O
label O
sequence O
Y O
. O
scoring O
function O
defined O
in O
Equation O
3 O
, O
and O
then O
apply O
a O
cross O
- O
entropy O
function O
to O
calculate O
the O
single O
example O
loss O
: O

p O
( O
Y O
|X O
) O
= O
exp O
s O
( O
Y O
|X O
) O
Y O
exp O
s O
( O
Y O
|X O
) O
, O
L O
( O
X O
, O
Y O
) O
= O
− O
log O
p O
( O
Y O
|X O
) O
. O
( O
4 O
) O

where O
Y O
denotes O
the O
candidate O
label O
sequences O
. O

Self O
- O
Augmentation O

Self O
- O
augmentation O
methods O
can O
reduce O
the O
demand O
for O
abundant O
manually O
- O
annotated O
examples O
, O
which O
can O
be O
implemented O
at O
the O
input O
level O
and O
representation O
level O
. O
Token O
substitution O
and O
mixup O
are O
two O
popular O
methods O
for O
NER B-TaskName
that O
correspond O
to O
these O
two O
distinct O
levels O
. O
Here O
, O
we O
try O
to O
extend O
these O
two O
self O
- O
augmentation O
methods O
. O

Token O
Substitution O
Token O
substitution O
aims O
to O
generate O
pseudo O
examples O
based O
on O
the O
original O
gold O
- O
labeled O
training O
data O
by O
replacing O
the O
tokens O
of O
input O
sentence O
with O
their O
synonym O
alternatives O
Dai O
and O
Adel O
, O
2020 O
) O
. O
For O
NER B-TaskName
, O
adopted O
this O
method O
to O
obtain O
performance O
gains O
on O
Chinese O
datasets O
where O
the O
substituted O
objects O
are O
limited O
to O
named O
entities O
. O
Dai O
and O
Adel O
( O
2020 O
) O
empirically O
demonstrated O
the O
superiority O
of O
synonym O
replacement O
among O
various O
augmentation O
schemes O
where O
the O
synonyms O
are O
retrieved O
from O
the O
off O
- O
the O
- O
shelf O
WordNet O
thesaurus O
. O

Our O
token O
substitution O
is O
performed O
by O
building O
a O
synonym O
dictionary O
, O
which O
covers O
the O
named O
entity O
synonyms O
as O
well O
as O
numerous O
normal O
word O
synonyms O
. O
Following O
, O
we O
treat O
all O
entities O
of O
the O
same O
type O
from O
the O
training O
set O
as O
synonyms O
, O
which O
are O
added O
to O
the O
entity O
dictionary O
. O
We O
name O
it O
as O
entity O
mention O
substitution O
( O
EMS O
) O
. O
Meanwhile O
, O
we O
extend O
the O
substitution O
to O
non O
- O
entity O
tokens O
( O
i.e. O
, O
the O
corresponding O
label O
is O
' O
O O
' O
) O
, O
which O
is O
named O
as O
normal O
word O
substitution O
( O
NWS O
) O
. O
Since O
unlabeled O
data O
in O
a O
specific O
domain O
is O
easily O
accessible O
, O
we O
adopt O
the O
word2vec O
- O
based O
algorithm O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
to O
mine O
tokens O
with O
similar O
semantics O
on O
Wikidata O
via O
distributed O
word O
representation O
( O
Yamada O
et O
al O
. O
, O
2020 O
) O
, O
and O
build O
a O
normal O
word O
synonym O
dictionary O
from O
the O
k O
- O
nearest O
token O
set O
based O
on O
cosine O
similarity O
distance O
. O
Note O
that O
this O
scheme O
does O
not O
require O
access O
to O
thesaurus O
for O
a O
specific O
domain O
in O
order O
to O
obtain O
synonyms O
. O
Figure O
2 O
presents O
an O
example O
of O
token O
substitution O
, O
where O
EMS O
and O
NWS O
are O
both O
involved O
. O
Specifically O
, O
for O
a O
given O
gold O
- O
labeled O
training O
example O
( O
X O
, O
Y O
) O
, O
we O
replace O
the O
entity O
token O
of O
X O
with O
a O
sampled O
entity O
from O
the O
entity O
dictionary O
which O
has O
the O
same O
entity O
type O
, O
and O
meanwhile O
replace O
the O
non O
- O
entity O
token O
of O
X O
with O
a O
sampled O
synonym O
. O
Then O
, O
we O
can O
obtain O
a O
pseudo O
example O
( O
X O
, O
Ȳ O
) O
. O
Especially O
, O
we O
balance O
the O
EMS O
and O
NWS O
strategies O
based O
on O
a O
ratio O
γ O
by O
adjusting O
the O
percentage O
of O
EMS O
operations O
, O
aiming O
for O
a O
good O
trade O
- O
off O
between O
entity O
diversity O
and O
context O
diversity O
. O
And O
, O
we O
refer O
to O
this O
method O
as O
TS O
in O
the O
rest O
of O
this O
paper O
for O
short O
. O

Mixup O
for O
CRF O
Unlike O
token O
substitution O
performed O
at O
the O
ground O
input O
, O
the O
mixup O
technique O
generates O
virtual O
examples O
at O
the O
feature O
representation O
level O
in O
the O
NLP O
field O
( O
Guo O
et O
al O
. O
, O
2019 O
) O
. O
The O
main O
idea O
is O
to O
perform O
linear O
interpolations O
on O
both O
the O
input O
and O
groundtruth O
output O
between O
randomly O
sampled O
example O
pairs O
from O
the O
given O
training O
set O
. O
Chen O
et O
al O
. O
( O
2020 O
) O
presented O
the O
first O
work O
based O
on O
the O
token O
classification O
framework O
for O
the O
NER O
task O
, O
and O
their O
mixup O
strategy O
is O
constrained O
to O
the O
examples O
pairs O
where O
the O
input O
sentences O
are O
semantically O
similar O
by O
using O
specific O
heuristic O
rules O
. O
Different O
from O
their O
method O
, O
we O
extend O
the O
mixup O
technique O
to O
the O
CRF O
decoding O
. O
Formally O
, O
give O
an O
example O
pair O
( O
X O
1 O
, O
Y O
1 O
) O
and O
( O
X O
2 O
, O
Y O
2 O
) O
randomly O
sampled O
from O
the O
gold O
- O
labeled O
training O
set O
, O
we O
firstly O
obtain O
their O
vector O
representations O
through O
Equation O
1 O
, O
resulting O
in O
e O
1,1 O
• O
• O
• O
e O
1 O
, O
n O
1 O
, O
and O
e O
2,1 O
• O
• O
• O
e O
2 O
, O
n O
2 O
, O
respectively O
. O
Then O
we O
apply O
the O
linear O
interpolation O
to O
obtain O
a O
new O
virtual O
training O
example O
( O
X O
, O
Ȳ O
) O
. O
Here O
we O
assume O
a O
regularization O
of O
pair O
- O
wise O
linear O
interpolation O
over O
the O
input O
representations O
and O
the O
output O
scores O
, O
where O
the O
following O
attributes O
should O
be O
satisfied O
: O

X O
: O
BERT O
( O
X O
) O
= O
ē O
1 O
• O
• O
•ē O
n O
e O
i O
= O
λe O
1 O
, O
i O
+ O
( O
1 O
− O
λ O
) O
e O
2 O
, O
i O
, O
i O
∈ O
[ O
1 O
, O
n O
] O
Y O
: O
s O
( O
Ȳ O
|X O
) O
= O
λs O
( O
Y O
1 O
|X O
) O
+ O
( O
1 O
− O
λ O
) O
s O
( O
Y O
2 O
|X O
) O
, O
( O
5 O
) O

where O
n O
= O
max O
( O
n O
1 O
, O
n O
2 O
) O
1 O
and O
λ O
is O
sampled O
from O
a O
Beta O
( O
α O
, O
α O
) O
distribution O
( O
λ O
∈ O
[ O
0 O
, O
1 O
] O
and O
α O
> O
0 O
) O
. O
According O
to O
this O
formulation O
, O
the O
loss O
function O
can O
be O
reformulated O
as O
: O

L O
( O
X O
, O
Ȳ O
) O
= O
− O
log O
exp O
s O
( O
Ȳ O
|X O
) O
Y O
exp O
s O
( O
Y O
|X O
) O
= O
λL O
( O
X O
, O
Y O
1 O
) O
+ O
( O
1 O
− O
λ O
) O
L O
( O
X O
, O
Y O
2 O
) O
. O
( O
6 O
) O

which O
aligns O
with O
the O
training O
objective O
of O
Equation O
4 O
. O
In O
this O
way O
, O
our O
mixup O
method O
can O
fit O
well O
with O
the O
structural O
decoding O
. O

Meta B-MethodName
Reweighting I-MethodName

Although O
the O
self O
- O
augmentation O
techniques O
can O
efficiently O
generate O
numerous O
pseudo O
training O
examples O
, O
how O
to O
control O
the O
quality O
of O
augmented O
examples O
is O
a O
potential O
challenge O
that O
can O
not O
be O
overlooked O
. O
In O
particular O
, O
unlike O
sentence O
- O
level O
classification O
tasks O
, O
entity O
recognition O
is O
highly O
sensitive O
to O
the O
semantics O
of O
the O
context O
. O
While O
positive O
augmented O
examples O
can O
help O
our O
model O
advance O
, O
some O
low O
- O
quality O
augmented O
examples O
that O
are O
inevitably O
introduced O
during O
self O
- O
augmentation O
may O
hurt O
the O
final O
model O
performance O
. O

In O
this O
paper O
, O
we O
leverage O
a O
meta O
reweighting O
mechanism O
to O
dynamically O
and O
adaptively O
assign O
Initialize O
the O
trainable O
parameter O
ϵ O
. O

3 O
: O
{ O
x O
c O
, O
y O
c O
} O
← O
SampleMiniBatch O
( O
D O
, O
m O
) O
. O
4 O
: O
{ O
x O
a O
, O
y O
a O
} O
← O
SampleMiniBatch O
( O
D O
, O
n O
) O
. O
5 O
: O
L O
a O
← O
n O
i=1 O
ϵ O
i O
L O
( O
f O
( O
x O
a O
, O
i O
; O
Θ O
( O
t O
) O

) O
, O
y O
a O
, O
i O
) O
. O

6 O
: O

∇Θ O
( O
t O
) O
← O
Grad O
( O
L O
a O
, O
Θ O
( O
t O
) O
) O
. O
7 O
: O
Θ O
( O
t O
) O
← O
Θ O
( O
t O
) O
− O
β∇Θ O
( O
t O
) O
. O
8 O
: O
L O
c O
← O
1 O
m O
m O
i=1 O
L O
( O
f O
( O
x O
c O
, O
i O
; O
Θ O
( O
t O
) O
) O
, O
y O
c O
, O
i O
) O
. O
9 O
: O

∇ϵ O
← O
Grad O
( O
L O
c O
, O
ϵ O
) O
. O
10 O
: O
ŵ O
← O
Sigmoid O
( O
−∇ϵ O
) O
. O

11 O
: O

w O
←ŵ O
jŵ O
j O
+ O
δ O
. O
12 O
: O
L O
a O
← O
n O
i=1 O
w O
i O
L O
( O
f O
( O
x O
a O
, O
i O
; O
Θ O
( O
t O
) O
) O
, O
y O
a O
, O
i O
) O
. O

13 O
: O
t O
) O
) O
. O

∇Θ O
( O
t O
) O
← O
Grad O
( O
L O
a O
, O
Θ O
( O

14 O
: O
t O
) O
, O
∇Θ O
( O
t O
) O
) O
. O
15 O
: O
end O
for O
the O
example O
- O
wise O
weights O
to O
each O
mini O
- O
batch O
of O
training O
data O
, O
motivated O
by O
Ren O
et O
al O
. O
( O
2018 O
) O
. O
The O
key O
idea O
is O
that O
a O
small O
and O
clean O
meta O
- O
data O
set O
is O
applied O
to O
guide O
the O
training O
of O
model O
parameters O
, O
and O
the O
loss O
produced O
by O
the O
mini O
- O
batch O
of O
meta O
- O
data O
is O
exploited O
to O
reweight O
the O
augmented O
examples O
in O
each O
batch O
online O
. O
Intuitively O
, O
if O
the O
data O
distribution O
and O
gradient O
- O
descent O
direction O
of O
the O
augmented O
example O
are O
similar O
to O
those O
of O
the O
sample O
in O
the O
meta O
- O
data O
set O
, O
our O
model O
could O
better O
fit O
this O
positive O
augmented O
sample O
and O
increase O
its O
weight O
, O
and O
vice O
versa O
. O
In O
other O
words O
, O
the O
clean O
and O
valid O
augmented O
examples O
are O
more O
likely O
to O
be O
fully O
trained O
. O

Θ O
( O
t+1 O
) O
← O
OptimizerStep O
( O
Θ O
( O

More O
specifically O
, O
suppose O
that O
we O
have O
a O
set O
of O

N O
augmented O
training O
examplesD O
= O
{ O
( O
X O
i O
, O
Y O
i O
) O
} O
N O
i=1 O

, O
our O
final O
optimizing O
objective O
can O
be O
formulated O
as O
a O
weighted O
loss O
as O
follows O
: O

Θ O
* O
( O
w O
) O
= O
arg O
min O
Θ O
N O
i=1 O
w O
i O
L O
( O
f O
( O
X O
i O
; O
Θ O
) O
, O
Y O
i O
) O
, O
( O
7 O
) O

where O
w O
i O
≥ O
0 O
is O
the O
learnable O
weight O
for O
the O
loss O
of O
i O
- O
th O
training O
example O
. O
f O
( O
• O
; O
Θ O
) O
represents O
the O
forward O
process O
of O
our O
model O
( O
with O
parameter O
Θ O
) O
. O
The O
optimal O
parameter O
w O
is O
further O
determined O
by O
minimizing O
the O
following O
loss O
computed O
on O
the O
meta O
example O
set O

D O
= O
{ O
( O
X O
m O
i O
, O
Y O
m O
i O
) O
} O
M O
i=1 O
( O
M O
≪ O
N O
) O
: O
w O
* O
= O
arg O
min O
w O
1 O
M O
M O
i=1 O
L O
( O
f O
( O
X O
m O
i O
; O
Θ O
* O
( O
w O
) O
) O
, O
Y O
m O
i O
) O
, O
( O
8 O
) O

Accordingly O
, O
we O
need O
to O
calculate O
the O
optimal O
Θ O
* O
and O
w O
* O
in O
Equation O
7 O
and O
8 O
based O
on O
two O
nested O
loops O
of O
optimization O
iteratively O
. O
For O
simplicity O
and O
efficiency O
, O
we O
take O
a O
single O
gradient O
- O
descent O
step O
for O
each O
training O
iteration O
to O
update O
them O
via O
an O
online O
- O
approximation O
manner O
. O
At O
every O
training O
step O
t O
, O
we O
sample O
a O
mini O
- O
batch O
augmented O
examples O
{ O
( O
X O
i O
, O
Y O
i O
) O
} O
n O
i=1 O
initialized O
with O
the O
learnable O
weights O
ϵ. O
After O
a O
single O
optimization O
step O
, O
we O
have O
: O

Θ O
( O
t+1 O
) O
( O
ϵ O
) O
= O
Θ O
( O
t O
) O
− O
β∇ O
Θ O
n O
i=1 O
ϵ O
i O
L O
( O
f O
( O
X O
i O
; O
Θ O
) O
, O
Y O
i O
) O
, O
( O
9 O

where O
β O
is O
the O
inner O
- O
loop O
step O
size O
. O
Based O
on O
the O
updated O
parameters O
, O
we O
then O
calculate O
the O
loss O
of O
the O
sampled O
mini O
- O
batch O
meta O
examples O

{ O
( O
X O
meta O
j O
, O
Y O
meta O
j O
) O
} O
m O
j=1 O
: O
L O
meta O
( O
Θ O
) O
= O
1 O
m O
m O
j=1 O
L O
( O
f O
( O
X O
meta O
j O
; O
Θ O
( O
t+1 O
) O
) O
, O
Y O
meta O
j O
) O
, O
( O
10 O
) O

To O
generalize O
the O
parametersΘ O
well O
to O
the O
metadata O
set O
, O
we O
take O
the O
gradients O
of O
ϵ O
w.r.t O
the O
meta O
loss O
to O
produce O
example O
weights O
and O
normalize O
it O
along O
mini O
- O
batch O
: O

w O
i O
= O
σ O
( O
−∇ O
ϵ O
i O
L O
meta O
( O
Θ O
) O
ϵ O
i O
= O
0 O

w O
i O
= O
ŵ O
i O
jŵ O
j O
+ O
δ O
. O
( O
11 O

where O
σ O
( O
• O
) O
is O
the O
sigmoid O
function O
and O
δ O
is O
a O
small O
value O
to O
avoid O
division O
by O
zero O
. O
Finally O
, O
we O
optimize O
the O
model O
parameters O
over O
augmented O
examples O
with O
the O
calculated O
weights O
. O

Experiments O

Settings O

Datasets O
To O
validate O
our O
methods O
, O
we O
conduct O
experiments O
on O
Chinese O
benchmarks O
: O
OntoNotes B-DatasetName
4.0 O
( O
Weischedel O
et O
al O
. O
, O
2011 O
) O
and O
Weibo B-DatasetName
NER I-DatasetName
( O
Peng O
and O
Dredze O
, O
2015 O
) O
, O
as O
well O
as O
English O
benchmarks O
: O
CoNLL B-DatasetName
2003 I-DatasetName
( O
Sang O
andMeulder O
, O
2003 O
) O
and O
OntoNotes B-DatasetName
5.0 I-DatasetName
2 I-DatasetName
( O
Pradhan O
et O
al O
. O
, O
2013 O
) O
. O
The O
Chinese O
datasets O
are O
split O
into O
training O
, O
development O
and O
test O
sections O
following O
Zhang O
and O
Yang O
( O
2018 O
) O
while O
we O
take O
the O
same O
data O
split O
as O
Benikova O
et O
al O
. O
( O
2014 O
) O
and O
Pradhan O
et O
al O
. O
( O
2012 O
) O
on O
the O
English O
datasets O
. O
We O
follow O
Lample O
et O
al O
. O
( O
2016 O
) O
to O
use O
the O
BIOES O
tagging O
scheme O
for O
all O
datasets O
. O
The O
detailed O
statistics O
can O
be O
found O
in O
Table O
4 O
. O

Dataset O

Type O
Train O
Dev O
Test O
Implementation O
Details O
We O
use O
one O
- O
layer O
BiL O
- O
STM O
and O
the O
hidden B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
768 B-HyperparameterValue
. O
The O
dropout B-HyperparameterName
ratio I-HyperparameterName
is O
set O
to O
0.5 B-HyperparameterValue
for O
the O
input O
and O
output O
of O
BiLSTM O
. O
Regarding O
BERT O
, O
we O
adopt O
BERTbase B-MethodName
model O
3 O
( O
BERT B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
cased I-MethodName
for O
the O
English O
NER B-TaskName
) O
and O
fine O
- O
tune O
the O
inside O
parameters O
together O
with O
all O
other O
module O
parameters O
. O
We O
use O
the O
AdamW B-HyperparameterValue
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
optimizer B-HyperparameterName
to O
update O
the O
trainable O
parameters O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.99 B-HyperparameterValue
. O
For O
the O
BERT O
parameters O
, O
the O
learning B-MethodName
rate I-MethodName
is O
set O
to O
2e−5 B-HyperparameterValue
. O
For O
other O
module O
parameters O
excluding O
BERT O
, O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e−3 B-HyperparameterValue
and O
weight B-HyperparameterName
decay I-HyperparameterName
of O
1e−4 B-HyperparameterValue
are O
used O
. O
Gradient B-HyperparameterName
clipping I-HyperparameterName
is O
used O
to O
avoid O
gradient O
explosion O
by O
a O
maximum O
value O
of O
5.0 B-HyperparameterValue
. O
All O
the O
models O
are O
trained O
on O
NIVIDIA O
Tesla O
V100 O
( O
32 O
G O
) O
GPUs O
. O
The O
higher O
library O
4 O
is O
utilized O
for O
the O
implementation O
of O
second O
- O
order O
optimization O
involved O
in O
Algorithm O
1 O
. O

For O
the O
NWS O
, O
we O
use O
the O
word O
vectors O
trained O
on O
Wikipedia O
data O
5 O
based O
on O
the O
GloVe O
model O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
build O
the O
synonym O
set O
for O
any O
given O
non O
- O
entity O
word O
based O
on O
the O
top-5 O
cosine O
similarity O
, O
where O
stop O
- O
words O
are O
excluded O
. O
As O
mentioned O
in O
Section O
2.2 O
, O
we O
defined O
two O
core O
hyper O
- O
parameters O
for O
our O
self O
- O
augmentation O
methods O
, O
one O
for O
TS O
( O
i.e. O
, O
γ B-HyperparameterName
) O
and O
the O
other O
for O
mixup O
( O
i.e. O
, O
λ B-HyperparameterName
) O
. O
Specifically O
, O
we O
set O
γ B-HyperparameterName
= O
20 B-HyperparameterValue
% I-HyperparameterValue
and O
λ B-HyperparameterName
by O
sampled O
from O
the O
Beta B-HyperparameterValue
( I-HyperparameterValue
α I-HyperparameterValue
, I-HyperparameterValue
α I-HyperparameterValue
) I-HyperparameterValue
distribution I-HyperparameterValue
with I-HyperparameterValue
α I-HyperparameterValue
= I-HyperparameterValue
7 I-HyperparameterValue
, O
where O
the O
details O
will O
be O
shown O
in O
the O
analysis O
section O
. O
Meanwhile O
, O
we O
conduct O
the O
augmentation O
up O
to O
5 O
times O
corresponding O
to O
the O
original O
training O
data O
. O

Evaluation O
We O
conduct O
each O
experiment O
by O
5 O
times O
and O
report O
the O
average B-MetricName
F1 I-MetricName
score I-MetricName
. O
The O
bestperforming O
model O
on O
the O
development O
set O
is O
then O
used O
to O
evaluate O
on O
the O
test O
set O
. O

Main O
Results O

The O
main O
results O
are O
presented O
in O
In O
addition O
, O
as O
the O
scale O
of O
the O
training O
data O
decreases O
, O
the O
effectiveness O
of O
the O
augmentation O
methods O
can O
be O
more O
significant O
, O
indicating O
that O
our O
selfaugmentation O
methods O
are O
highly O
beneficial O
for O
the O
low O
- O
resource O
settings O
, O
and O
the O
two O
- O
stage O
combination O
of O
the O
two O
heterogeneous O
methods O
can O
yield O
better O
performance O
consistently O
. O

Full O
- O
Scale O
Setting O
Table O
2 O
and O
3 O
show O
the O
results O
using O
full O
- O
scale O
training O
data O
. O
The O
results O
demonstrate O
that O
our O
baseline O
model O
is O
already O
strong O
. O
The O
model O
after O
vanilla O
augmentation O
could O
perform O
slightly O
worse O
since O
each O
training O
example O
is O
treated O
equally O
even O
if O
it O
is O
noisy O
. O
This O
also O
implies O
our O
meta O
reweighting O
makes O
great O
sense O
. O
Furthermore O
, O
our O
final O
model O
( O
+ O
Both O
) O
can O
further O
achieve O
performance O
gains O
by O
integrating O
these O
selfaugmentation O
methods O
with O
the O
meta O
reweighting O
mechanism O
. O
The O
overall O
trend O
is O
similar O
to O
the O
lowresource O
setting O
, O
but O
the O
gains O
are O
relatively O
smaller O
when O
the O
training O
data O
is O
sufficient O
. O
That O
may O
be O
attributed O
that O
the O
size O
of O
training O
data O
is O
large O
enough O
to O
narrow O
the O
performance O
gap O
between O
the O
baseline O
and O
augmented O
models O
. O
It O
also O
suggests O
that O
our O
method O
does O
not O
hurt O
the O
model O
performance O
even O
when O
using O
enough O
training O
data O
. O

Comparison O
with O
Previous O
Work O

We O
also O
compare O
our O
method O
with O
previous O
representative O
SOTA O
work O
, O
where O
all O
referred O
systems O
exploit O
the O
pre O
- O
trained O
BERT O
model O
. O
As O
shown O
, O
compared O
to O
Dai O
and O
Adel O
( O
2020 O
) O
and O
Chen O
et O
al O
. O
( O
2020 O
) O
, O
our O
method O
either O
outperforms O
or O
performs O
on O
par O
with O
theirs O
when O
using O
limited O
training O
data O
. O
For O
Chen O
et O
al O
. O
( O
2020 O
) O
, O
the O
pure O
mixup O
performs O
slightly O
better O
due O
to O
the O
well O
- O
designed O
example O
sampling O
strategy O
, O
but O
our O
overall O
framework O
outperforms O
theirs O
. O
Moreover O
, O
our O
method O
can O
match O
the O
performance O
of O
the O
semi O
- O
supervised O
setting O
that O
uses O
additional O
10 O
K O
unlabeled O
training O
data O
. O
Besides O
, O
our O
final O
model O
, O
without O
utilizing O
much O
external O
knowledge O
, O
can O
achieve O
very O
competitive O
results O
on O
the O
full O
training O
set O
in O
comparison O
to O
most O
previous O
systems O
. O

Analysis O

In O
this O
subsection O
, O
we O
further O
conduct O
detailed O
experimental O
analyses O
on O
the O
CoNLL03 B-DatasetName
dataset O
for O
a O
better O
understanding O
of O
our O
method O
. O
Our O
main O
concern O
is O
on O
the O
low O
- O
resource O
setting O
, O
therefore O
the O
models O
based O
on O
5 O
% O
, O
10 O
% O
and O
30 O
% O
of O
the O
original O
training O
data O
are O
our O
main O
focus O
. O

Augmentation O
Times O

The O
size O
of O
augmented O
examples O
is O
an O
essential O
factor O
in O
final O
model O
performance O
. O
Typically O
, O
we O
examine O
the O
5 O
% O
CoNLL03 B-DatasetName
training O
data O
. O
As O
illustrated O
in O
Figure O
3 O
, O
the O
larger O
pseudo O
examples O
can O
obtain O
better O
performance O
in O
a O
certain O
range O
. O
However O
, O
as O
the O
times O
of O
augmentation O
increases O
, O
the O
uptrend O
of O
performance O
slows O
down O
. O
The O
improvement O
tends O
to O
be O
stable O
when O
the O
pseudo O
samples O
are O
increased O
to O
about O
5 O
times O
the O
original O
training O
data O
. O
Excessively O
increasing O
the O
augmentation O
times O
does O
not O
necessarily O
bring O
consistent O
performance O
improvement O
. O
And O
we O
select O
an O
appropriate O
value O
for O
training O
data O
of O
different O
sizes O
from O
a O
range O
[ O
1,8 O
] O
. O

Influence O
of O
γ O
for O
Token O
Substitution O
Regarding O
our O
TS O
strategy O
, O
we O
take O
both O
NWS O
and O
EMS O
into O
account O
simultaneously O
. O
The O
two O
parts O
are O
blended O
by O
a O
percentage O
parameter O
γ O
, O
namely O
γ O
for O
EMS O
and O
1 O
− O
γ O
for O
NWS O
. O
Here O
we O
examine O
the O
influence O
of O
γ O
in O
the O
sole O
self O
- O
augmentation O
model O
by O
TS O
. O
Figure O
4 O
shows O
the O
results O
, O
where O
γ B-HyperparameterName
= O
0 B-HyperparameterValue
and O
γ B-HyperparameterName
= O
100 B-HyperparameterValue
% I-HyperparameterValue
denote O
the O
model O
with O
only O
NWS O
and O
EMS O
, O
respectively O
. O
As O
shown O
, O
our O
model O
can O
achieve O
the O
overall O
better O
performance O
when O
γ B-HyperparameterName
= O
20 B-HyperparameterValue
% I-HyperparameterValue
, O
indicating O
that O
both O
of O
them O
are O
helpful O
for O
the O
TS O
strategy O
, O
and O
NWS O
can O
be O
slightly O
better O
. O
One O
possible O
reason O
is O
that O
the O
entity O
words O
in O
original O
training O
examples O
are O
relatively O
sparse O
( O
i.e. O
, O
the O
' O
O O
' O
label O
is O
dominant O
) O
, O
allowing O
the O
NWS O
to O
produce O
more O
diverse O
pseudo O
examples O
. O

Mixup O
Parameter O

We O
further O
inspect O
the O
model O
with O
the O
mixup O
strategy O
alone O
so O
as O
to O
understand O
the O
important O
factors O
of O
the O
mixup O
model O
. O
First O
, O
we O
analyze O
the O
influence O
of O
the O
mixing O
parameter O
α O
. O
As O
depicted O
in O
Figure O
5 O
, O
we O
can O
see O
that O
α O
indeed O
affects O
the O
effectiveness O
of O
the O
mixup O
method O
greatly O
. O
Considering O
the O
feature O
of O
Beta O
distribution O
, O
the O
sampled O
λ B-HyperparameterName
will O
be O
more O
concentrated O
around O
0.5 B-HyperparameterValue
as O
the O
α O
value O
becomes O
large O
, O
resulting O
in O
a O
relatively O
balanced O
weight O
between O
the O
mixed O
example O
pairs O
. O
The O
model O
performance O
remains O
stable O
when O
α B-HyperparameterName
is O
around O
7 B-HyperparameterValue
. O
Second O
, O
we O
study O
where O
to O
conduct O
the O
mixup O
operation O
since O
there O
are O
two O
main O
options O
in O
our O
framework O
, O
i.e. O
, O
the O
hidden O
representations O
of O
either O
the O
BERT O
or O
BiLSTM O
for O
linear O
interpolation O
. O
Table O
5 O
reports O
the O
comparison O
results O
, O
demonstrating O
the O
former O
is O
a O
better O
choice O
. O

Case O
Study O
To O
further O
understand O
the O
effectiveness O
of O
the O
meta O
- O
reweighting O
mechanism O
, O
we O
present O
several O
high O
- O
quality O
and O
low O
- O
quality O
examples O
in O
Table O
6 O
. O
As O
shown O
, O
the O
difference O
between O
the O
positive O
and O
negative O
examples O
for O
TS O
could O
be O

Related O
Work O

In O
recent O
years O
, O
research O
on O
NER B-TaskName
has O
concentrated O
on O
either O
enriching O
input O
text O
representations O
( O
Zhang O
and O
Yang O
, O
2018 O
; O
Nie O
et O
al O
. O
, O
2020b O
; O
Ma O
et O
al O
. O
, O
2020 O
) O
or O
refining O
model O
architectures O
with O
various O
external O
knowledge O
( O
Zhang O
and O
Yang O
, O
2018 O
; O
Ye O
and O
Ling O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2020a O
; O
Xuan O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020b O
; O
Shen O
et O
al O
. O
, O
2021 O
) O
. O
Particularly O
, O
NER B-TaskName
model O
, O
with O
the O
aid O
of O
large O
pretrained O
language O
models O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
, O
has O
achieved O
impressive O
performance O
gains O
. O
However O
, O
these O
models O
mostly O
depend O
on O
rich O
manual O
annotations O
, O
making O
it O
hard O
to O
cope O
with O
the O
low O
- O
resource O
challenges O
in O
real O
- O
world O
applications O
. O
Instead O
of O
pursuing O
a O
sophisticated O
model O
architecture O
, O
in O
this O
work O
, O
we O
exploit O
the O
BiLSTM B-MethodName
- I-MethodName
CRF I-MethodName
model O
coupled O
with O
the O
pre O
- O
trained O
BERT O
as O
our O
basic O
model O
structure O
. O
Self O
- O
augmentation O
methods O
have O
been O
widely O
investigated O
in O
various O
NLP O
tasks O
Wei O
and O
Zou O
, O
2019 O
; O
Dai O
and O
Adel O
, O
2020 O
; O
Zeng O
et O
al O
. O
, O
2020 O
; O
Ding O
et O
al O
. O
, O
2020 O
) O
. O
The O
mainstream O
methods O
can O
be O
broadly O
categorized O
into O
three O
types O
: O
( O
1 O
) O
token O
substitution O
( O
Kobayashi O
, O
2018 O
; O
Wei O
and O
Zou O
, O
2019 O
; O
Dai O
and O
Adel O
, O
2020 O
; O
Zeng O
et O
al O
. O
, O
2020 O
) O
, O
which O
performs O
local O
substitution O
for O
a O
given O
sentence O
, O
( O
2 O
Xie O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020b O
) O
, O
which O
involves O
sentence O
- O
level O
rewriting O
without O
significantly O
changing O
the O
semantics O
, O
and O
( O
3 O
) O
mixup O
Chen O
et O
al O
. O
, O
2020 O
; O
Sun O
et O
al O
. O
, O
2020 O
) O
, O
which O
carries O
out O
the O
feature O
- O
level O
augmentation O
. O
As O
a O
data O
- O
agnostic O
augmentation O
technique O
, O
mixup O
can O
help O
improve O
the O
generalization O
and O
robustness O
of O
our O
neural O
model O
acting O
as O
an O
useful O
regularizer O
( O
Verma O
et O
al O
. O
, O
2019 O
) O
. O
For O
NER B-TaskName
, O
token O
substitution O
and O
mixup O
are O
very O
suitable O
and O
have O
been O
exploited O
successfully O
with O
specialized O
efforts O
( O
Dai O
and O
Adel O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2020 O
; O
Zeng O
et O
al O
. O
, O
2020 O
) O
, O
while O
the O
paraphrasing O
strategy O
may O
result O
in O
structure O
incompleteness O
and O
token O
- O
label O
inconsistency O
, O
thus O
there O
has O
not O
been O
widely O
concerned O
yet O
. O
In O
this O
work O
, O
we O
mainly O
investigate O
the O
token O
substitution O
and O
mixup O
techniques O
for O
NER B-TaskName
, O
as O
well O
as O
their O
integration O
. O
Despite O
the O
success O
of O
various O
self O
- O
augmentation O
methods O
, O
quality O
control O
may O
be O
an O
issue O
easily O
overlooked O
by O
most O
methods O
. O

Many O
previous O
studies O
have O
explored O
the O
example O
weighting O
mechanism O
in O
domain O
adaption O
( O
Jiang O
and O
Zhai O
, O
2007 O
; O
Wang O
et O
al O
. O
, O
2017 O
; O
Osumi O
et O
al O
. O
, O
2019 O
) O
. O
Xia O
et O
al O
. O
( O
2018 O
) O
and O
looked O
into O
the O
example O
weighting O
methods O
for O
cross O
- O
domain O
tasks O
. O
Ren O
et O
al O
. O
( O
2018 O
) O
adapted O
the O
MAML O
algorithm O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
and O
proposed O
a O
meta O
- O
learning O
algorithm O
to O
automatically O
weight O
training O
examples O
of O
the O
noisy O
label O
using O
a O
small O
unbiased O
validation O
set O
. O
Inspired O
by O
their O
work O
, O
we O
extend O
the O
meta O
example O
reweighting O
mechanism O
to O
the O
NER B-TaskName
task O
, O
which O
is O
exploited O
to O
adaptively O
reweight O
mini O
- O
batch O
augmented O
examples O
during O
training O
. O
The O
main O
purpose O
is O
to O
mitigate O
the O
potential O
noise O
effects O
brought O
by O
the O
self O
- O
augmentation O
techniques O
, O
advancing O
a O
noiserobust O
model O
, O
especially O
in O
low O
- O
resource O
scenarios O
. O

Conclusion O

In O
this O
paper O
, O
we O
re O
- O
examine O
two O
heterogeneous O
self O
- O
augmentation O
methods O
( O
i.e. O
, O
TS O
and O
mixup O
) O
for O
NER B-TaskName
, O
extending O
them O
into O
more O
unrestricted O
augmentations O
without O
heuristic O
constraints O
. O
We O
further O
exploit O
a O
meta O
reweighting O
strategy O
to O
alleviate O
the O
potential O
negative O
impact O
of O
noisy O
augmented O
examples O
introduced O
by O
the O
aforementioned O
relaxation O
. O
Experiments O
conducted O
on O
several O
benchmarks O
show O
that O
our O
self O
- O
augmentation O
methods O
along O
with O
the O
meta O
reweighting O
mechanism O
are O
very O
effective O
in O
low O
- O
resource O
settings O
, O
and O
still O
work O
when O
enough O
training O
data O
is O
used O
. O
The O
combination O
of O
the O
two O
methods O
can O
lead O
to O
consistent O
performance O
improvement O
across O
all O
datasets O
. O
Since O
our O
framework O
is O
general O
and O
does O
not O
rely O
on O
a O
specific O
model O
backbone O
, O
we O
will O
further O
investigate O
other O
feasible O
model O
structures O
. O

Acknowledgements O

We O
thank O
the O
valuable O
comments O
of O
all O
anonymous O
reviewers O
. O
This O
work O
is O
supported O
by O
grants O
from O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
62176180 O
) O
. O

Models O
In O
a O
Spelling O
Bee O
: O
Language O
Models O
Implicitly O
Learn O
the O
Character O
Composition O
of O
Tokens O

Standard O
pretrained O
language O
models O
operate O
on O
sequences O
of O
subword O
tokens O
without O
direct O
access O
to O
the O
characters O
that O
compose O
each O
token O
's O
string O
representation O
. O
We O
probe B-TaskName
the I-TaskName
embedding I-TaskName
layer I-TaskName
of I-TaskName
pretrained I-TaskName
language I-TaskName
models I-TaskName
and O
show O
that O
models O
learn O
the O
internal O
character O
composition O
of O
whole O
word O
and O
subword O
tokens O
to O
a O
surprising O
extent O
, O
without O
ever O
seeing O
the O
characters O
coupled O
with O
the O
tokens O
. O
Our O
results O
show O
that O
the O
embedding O
layers O
of O
RoBERTa B-MethodName
and O
GPT2 B-MethodName
each O
hold O
enough O
information O
to O
accurately O
spell O
up O
to O
a O
third O
of O
the O
vocabulary O
and O
reach O
high O
character O
ngram O
overlap O
across O
all O
token O
types O
. O
We O
further O
test O
whether O
enriching O
subword O
models O
with O
character O
information O
can O
improve O
language O
modeling O
, O
and O
observe O
that O
this O
method O
has O
a O
near O
- O
identical O
learning O
curve O
as O
training O
without O
spelling O
- O
based O
enrichment O
. O
Overall O
, O
our O
results O
suggest O
that O
language O
modeling O
objectives O
incentivize O
the O
model O
to O
implicitly O
learn O
some O
notion O
of O
spelling O
, O
and O
that O
explicitly O
teaching O
the O
model O
how O
to O
spell O
does O
not O
appear O
to O
enhance O
its O
performance O
on O
such O
tasks O
. O
1 O

Introduction O

Contemporary O
subword O
tokenization O
algorithms O
such O
as O
BPE O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
partition O
a O
string O
into O
contiguous O
spans O
of O
characters O
. O
Each O
span O
represents O
a O
frequent O
character O
ngram O
, O
from O
individual O
characters O
( O
a O
) O
, O
through O
prefixes O
( O
uni O
) O
and O
suffixes O
( O
tion O
) O
, O
and O
even O
complete O
words O
( O
cats O
) O
. O
The O
tokenizer O
then O
converts O
each O
such O
span O
into O
a O
discrete O
symbol O
( O
a O
token O
) O
with O
no O
internal O
structure O
, O
effectively O
discarding O
the O
token O
's O
orthographic O
information O
. O
Therefore O
, O
a O
model O
operating O
over O
sequences O
of O
subword O
tokens O
should O
be O
oblivious O
to O
the O
spelling O
of O
each O
token O
. O
In O
this O
work O
, O
we O
show O
that O
despite O
having O
no O
direct O
access O
to O
the O
subwords O
' O
internal O
character O
composition O
, O
pretrained O
language O
models O
do O
learn O
some O
notion O
of O
spelling O
. O

To O
examine O
what O
pretrained O
language O
models O
learn O
about O
spelling O
, O
we O
present O
the O
SpellingBee B-MethodName
probe O
. O
SpellingBee B-MethodName
is O
a O
generative O
language O
model O
that O
predicts O
the O
character O
composition O
of O
a O
token O
given O
only O
its O
( O
uncontextualized O
) O
vector O
representation O
from O
the O
pretrained O
model O
's O
embeddings O
matrix O
. O
SpellingBee B-MethodName
is O
trained O
on O
part O
of O
the O
model O
's O
vocabulary O
, O
and O
then O
tested O
by O
spelling O
unseen O
token O
types O
. O
If O
the O
probe O
can O
successfully O
reconstruct O
the O
correct O
character O
sequence O
from O
an O
unseen O
token O
's O
embedding O
, O
then O
there O
must O
be O
significant O
orthographic O
information O
encoded O
in O
the O
vector O
. O

We O
find O
that O
the O
embedding O
layers O
of O
several O
pretrained O
language O
models O
contain O
surprising O
amounts O
of O
character O
information O
. O
SpellingBee B-MethodName
accurately B-MetricName
spells O
31.8 B-MetricValue
% I-MetricValue
of O
the O
held O
- O
out O
vocabulary O
for O
RoBERTa B-MethodName
- I-MethodName
Large I-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
32.9 B-MetricValue
% I-MetricValue
for O
GPT2 B-MethodName
- I-MethodName
Medium I-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
and O
40.9 B-MetricValue
% I-MetricValue
for O
the O
Arabic O
language O
model O
AraBERT B-MethodName
- I-MethodName
Large I-MethodName
( O
Antoun O
et O
al O
. O
, O
2020 O
) O
. O
A O
softer O
metric O
that O
is O
sensitive O
to O
partially B-MetricName
- I-MetricName
correct I-MetricName
spellings I-MetricName
( O
chrF B-MetricName
) O
( O
Popović O
, O
2015 O
) O
shows O
a O
similar O
trend O
, O
with O
48.7 B-MetricValue
for O
RoBERTa B-MethodName
- I-MethodName
Large I-MethodName
and O
62.3 B-MetricValue
for O
AraBERT B-MethodName
- I-MethodName
Large I-MethodName
. O
These O
results O
are O
much O
higher O
than O
the O
baseline O
of O
applying O
SpellingBee B-MethodName
to O
randomly O
- O
initialized O
vectors O
, O
which O
fails O
to O
spell O
a O
single O
token O
. O

Given O
that O
subword O
models O
learn O
some O
notion O
of O
character O
composition O
to O
fulfill O
language O
modeling O
objectives O
, O
could O
they O
perhaps O
benefit O
from O
knowing O
the O
exact O
spelling O
of O
each O
token O
a O
priori O
? O
To O
that O
end O
, O
we O
reverse O
SpellingBee B-MethodName
's O
role O
and O
use O
it O
to O
pretrain O
the O
embedding O
layer O
of O
a O
randomlyinitialized O
model O
, O
thus O
imbuing O
each O
token O
representation O
with O
its O
orthographic O
information O
before O
training O
the O
whole O
model O
on O
the O
masked O
language O
modeling O
objective O
. O
We O
compare O
the O
pretraining O
process O
of O
the O
character O
- O
infused O
model O
to O
that O
of O
an O
identical O
model O
whose O
embedding O
layer O
is O
randomly O
initialized O
( O
and O
not O
pretrained O
) O
, O
and O
find O
that O
both O
learning O
curves O
converge O
to O
virtually O
identical O
values O
within O
the O
first O
1,000 O
gradient O
updates O
, O
a O
fraction O
of O
the O
total O
optimization O
process O
. O
This O
experiment O
suggests O
that O
while O
language O
models O
may O
need O
to O
learn O
some O
notion O
of O
spelling O
to O
optimize O
their O
objectives O
, O
they O
might O
also O
be O
able O
to O
quickly O
acquire O
most O
of O
the O
character O
- O
level O
information O
they O
need O
from O
plain O
token O
sequences O
without O
directly O
observing O
the O
composition O
of O
each O
token O
. O

Spelling B-MethodName
Bee I-MethodName

To O
measure O
how O
much O
a O
model O
knows O
the O
character O
composition O
of O
its O
tokens O
, O
we O
introduce O
Spelling B-MethodName
- I-MethodName
Bee I-MethodName
, O
a O
generative O
probe O
that O
tries O
to O
spell O
out O
a O
token O
character O
- O
by O
- O
character O
. O
Specifically O
, O
Spelling B-MethodName
- I-MethodName
Bee I-MethodName
probes O
the O
original O
model O
's O
embedding O
matrix O
, O
since O
spelling O
is O
a O
property O
of O
token O
types O
, O
invariant O
to O
context O
. O
For O
example O
, O
given O
the O
embedding O
of O
the O
token O
cats O
, O
SpellingBee B-MethodName
will O
try O
to O
generate O
the O
sequence O
[ O
c O
, O
a O
, O
t O
, O
s O
] O
. O
We O
do O
so O
by O
modeling O
SpellingBee B-MethodName
as O
a O
character O
- O
based O
language O
model O
, O
2 O
where O
the O
first O
token O
is O
a O
vector O
representation O
of O
the O
vocabulary O
item O
. O
3 O
Training O
We O
split O
the O
vocabulary O
to O
train O
and O
test O
sets O
, O
4 O
and O
use O
teacher O
forcing O
to O
train O
SpellingBee B-MethodName
. O
In O
the O
example O
of O
cats O
, O
SpellingBee B-MethodName
will O
compute O
the O
following O
probabilities O
: O

P O
( O
x O
1 O
= O
c O
| O
x O
0 O
= O
cats O
) O
P O
( O
x O
2 O
= O
a O
| O
x O
0 O
= O
cats O
, O
x O
1 O
= O
c O
) O
P O
( O
x O
3 O
= O
t O
| O
x O
0 O
= O
cats O
, O
x O
1 O
= O
c O
, O
x O
2 O
= O
a O
) O

. O
. O
. O
All O
of O
SpellingBee B-MethodName
's O
parameters O
are O
randomly O
initialized O
. O
The O
only O
parameters O
that O
are O
pretrained O
are O
the O
token O
embeddings O
( O
e.g. O
the O
representation O
of O
cats O
or O
a O
) O
, O
which O
are O
taken O
from O
the O
original O
pretrained O
language O
model O
we O
intend O
to O
probe O
, O
and O
treated O
as O
constants O
; O
i.e. O
kept O
frozen O
during O
SpellingBee B-MethodName
's O
training O
. O

Inference O
& O
Evaluation O
Once O
SpellingBee B-MethodName
is O
trained O
, O
we O
apply O
it O
to O
the O
test O
set O
using O
greedy O
decoding O
. O
For O
each O
vocabulary O
item O
w O
in O
the O
test O
set O
, O
SpellingBee B-MethodName
is O
given O
only O
the O
corresponding O
embedding O
vector O
e O
w O
, O
and O
is O
expected O
to O
generate O
the O
character O
sequence O
w O
1 O
, O
. O
. O
. O
, O
w O
n O
that O
defines O
w. O
We O
measure O
success O
on O
the O
test O
set O
using O
two O
metrics O
: O
exact B-MetricName
match I-MetricName
( O
EM B-MetricName
) O
, O
and O
character B-MetricName
ngram I-MetricName
overlap I-MetricName
score I-MetricName
using O
chrF B-MetricName
( O
Popović O
, O
2015 O
) O
. O
While O
EM B-MetricName
is O
strict O
, O
chrF B-MetricName
allows O
us O
to O
measure O
partial O
success O
. O
We O
also O
report O
edit O
distance O
using O
Levenshtein B-MetricName
distance I-MetricName
ratio O
in O
Appendix O
A O
. O

SpellingBee B-MethodName
for O
Pretraining O
Embeddings O

While O
we O
mainly O
use O
SpellingBee B-MethodName
as O
a O
probe O
, O
a O
variation O
of O
our O
method O
could O
potentially O
imbue O
the O
embedding O
layer O
with O
character O
information O
before O
training O
a O
language O
model O
. O
We O
could O
train O
a O
probe O
with O
randomly O
- O
initialized O
embeddings O
( O
instead O
of O
pretrained O
embeddings O
from O
another O
model O
) O
to O
predict O
the O
spelling O
of O
all O
vocabulary O
items O
, O
and O
use O
these O
trained O
probe O
embeddings O
to O
initialize O
any O
target O
model O
's O
embedding O
layer O
( O
instead O
of O
random O
initialization O
) O
. O
We O
experiment O
with O
this O
method O
in O
Section O
5 O
, O
but O
find O
that O
it O
does O
not O
have O
any O
significant O
impact O
on O
the O
convergence O
of O
language O
models O
. O

Experiment O
Setup O

We O
begin O
with O
a O
series O
of O
probing O
experiments O
, O
where O
we O
apply O
SpellingBee B-MethodName
to O
the O
embedding O
layer O
of O
various O
pretrained O
models O
. O
5 O

Pretrained O
Models O
We O
probe O
four O
pretrained O
models O
: O
RoBERTa B-MethodName
- I-MethodName
Base I-MethodName
and O
Large O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
GPT2 B-MethodName
- I-MethodName
Medium I-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
and O
AraBERT B-MethodName
- I-MethodName
Large I-MethodName
( O
Antoun O
et O
al O
. O
, O
2020 O
) O
. O
This O
set O
introduces O
some O
diversity O
in O
vocabulary O
, O
objective O
, O
and O
scale O
: O
the O
first O
three O
models O
are O
trained O
on O
English O
corpora O
, O
while O
AraBERT B-MethodName
is O
trained O
on O
text O
in O
Arabic O
; O
GPT2 B-MethodName
is O
an O
autoregressive O
language O
model O
, O
while O
the O
rest O
are O
masked O
language O
models O
; O
RoBERTa B-MethodName
- I-MethodName
Base I-MethodName
consists O
of O
125 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
( O
with O
768 B-HyperparameterValue
dimensions B-HyperparameterName
per I-HyperparameterName
embedding I-HyperparameterName
) O
, O
while O
the O
other O
models O
have O
approximately O
350 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
( O
with O
1024 B-HyperparameterValue
dimensions B-HyperparameterName
per I-HyperparameterName
embedding I-HyperparameterName
) O
. O

Control O
Since O
SpellingBee B-MethodName
is O
a O
trained O
probe O
, O
we O
wish O
to O
establish O
the O
probe O
's O
baseline O
performance O
when O
provided O
with O
inputs O
with O
no O
orthographic O
information O
. O
As O
an O
empirical O
control O
, O
we O
train O
and O
test O
SpellingBee B-MethodName
on O
randomly O
- O
initialized O
vectors O
, O
in O
addition O
to O
the O
main O
experiments O
where O
we O
utilize O
the O
pretrained O
embedding O
layers O
. O
Training O
& O
Testing O
Data O
We O
split O
the O
vocabulary O
into O
training O
and O
testing O
data O
using O
the O
following O
protocol O
. O
First O
, O
we O
randomly O
sample O
1000 O
token O
types O
as O
test O
. O
We O
then O
filter O
the O
remaining O
vocabulary O
to O
eliminate O
tokens O
that O
may O
be O
too O
similar O
to O
the O
test O
tokens O
, O
and O
randomly O
sample O
32000 O
training O
examples O
. O
We O
experiment O
with O
three O
filters O
: O
none O
, O
which O
do O
not O
remove O
tokens O
beyond O
the O
test O
- O
set O
tokens O
; O
similarity O
, O
which O
removes O
the O
top O
20 O
most O
similar O
tokens O
for O
every O
token O
in O
test O
, O
according O
to O
the O
cosine O
similarity O
induced O
by O
the O
embedding O
vectors O
; O
lemma O
, O
which O
removes O
any O
token O
type O
that O
shares O
a O
lemma O
with O
a O
test O
- O
set O
token O
( O
e.g. O
if O
diving O
is O
in O
the O
test O
set O
, O
then O
diver O
can O
not O
be O
in O
the O
training O
set O
) O
. O
6 O
The O
lemma O
filter O
always O
applies O
the O
similarity O
filter O
first O
, O
providing O
an O
even O
more O
adversarial O
approach O
for O
splitting O
the O
data O
. O

To O
control O
for O
variance O
, O
we O
create O
10 O
such O
splits O
for O
each O
model O
and O
filter O
, O
and O
report O
the O
averaged O
evaluation O
metrics O
over O
all O
10 O
test O
sets O
. O

Results O

Main O
Result O
Table O
1 O
shows O
how O
well O
Spelling B-MethodName
- I-MethodName
Bee I-MethodName
can O
spell O
a O
vocabulary O
token O
using O
only O
its O
frozen O
pretrained O
embedding O
. O
We O
observe O
that O
SpellingBee B-MethodName
is O
able O
to O
accurately O
recover O
the O
spelling O
of O
up O
to O
40.9 B-MetricValue
% I-MetricValue
of O
the O
test O
set O
, O
while O
the O
control O
is O
unable O
to O
spell O
even O
a O
single O
word O
correctly O
. O
A O
similar O
trend O
can O
be O
seen O
when O
considering O
the O
finer B-MetricName
character I-MetricName
ngram I-MetricName
metric I-MetricName
( O
chrF B-MetricName
) O
. O
Manu O
- O
ally O
analyzing O
the O
predictions O
of O
the O
control O
baselines O
( O
see O
Appendix O
D O
) O
indicate O
that O
it O
primarily O
generates O
combinations O
of O
frequent O
character O
sequences O
, O
which O
mildly O
contributes O
to O
the O
chrF B-MetricName
score O
, O
but O
does O
not O
affect O
EM B-MetricName
. O
These O
results O
are O
persistent O
across O
different O
models O
and O
filters O
, O
strongly O
indicating O
that O
the O
embedding O
layer O
of O
pretrained O
models O
contains O
significant O
amounts O
of O
information O
about O
each O
token O
's O
character O
composition O
. O

One O
may O
suggest O
that O
training O
SpellingBee B-MethodName
over O
32,000 O
examples O
may O
leak O
information O
from O
the O
test O
set O
; O
for O
example O
, O
if O
dog O
was O
seen O
during O
training O
, O
then O
spelling O
out O
dogs O
might O
be O
easy O
. O
We O
thus O
consider O
the O
similarity O
and O
lemma O
filters O
, O
which O
remove O
such O
near O
- O
neighbors O
from O
the O
training O
set O
. O
While O
results O
are O
indeed O
lower O
( O
and O
probably O
do O
account O
for O
some O
level O
of O
information O
leakage O
) O
, O
they O
are O
still O
considerably O
higher O
than O
the O
control O
, O
both O
in O
terms O
of O
EM B-MetricName
and O
chrF. B-MetricName
Results O
using O
the O
similarity O
and O
lemma O
filters O
are O
rather O
similar O
, O
suggesting O
that O
embedding O
- O
space O
similarity O
captures O
some O
information O
about O
each O
token O
's O
lemma O
. O

Finally O
, O
we O
find O
that O
the O
properties O
of O
pretrained O
models O
also O
seem O
to O
have O
a O
significant O
effect O
on O
the O
amount O
of O
spelling O
information O
SpellingBee B-MethodName
can O
extract O
. O
Larger O
models O
tend O
to O
score O
higher O
in O
the O
probe O
, O
and O
the O
model O
trained O
on O
text O
in O
Arabic O
appears O
to O
have O
substantially O
higher O
EM B-MetricName
and O
chrF B-MetricName
scores O
than O
those O
trained O
on O
English O
corpora O
. O
One O
possibility O
is O
that O
Arabic O
's O
rich O
morphology O
incentivizes O
the O
model O
to O
store O
more O
information O
about O
each O
token O
's O
character O
composition O
; O
however O
, O
it O
is O
also O
possible O
that O
AraBERT B-MethodName
's O
different O
vocabulary O
, O
which O
allocates O
shorter O
character O
sequences O
to O
each O
token O
type O
, O
might O
explain O
this O
difference O
( O
we O
discuss O
the O
link O
between O
sequence O
length O
and O
accuracy O
in O
Appendix O
C O
) O
. O

Overall O
, O
our O
probing O
experiments O
show O
that O
even O
though O
subword O
- O
based O
language O
models O
do O
not O
have O
direct O
access O
to O
spelling O
, O
they O
can O
and O
do O
learn O
a O
surprising O
amount O
of O
information O
about O
the O
character O
composition O
of O
each O
vocabulary O
token O
. O

Character O
- O
Aware O
Models O
Some O
models O
are O
provided O
with O
the O
raw O
character O
sequence O
of O
each O
token O
. O
To O
test O
whether O
the O
embedding O
layers O
of O
such O
models O
are O
indeed O
more O
informed O
about O
each O
token O
's O
spelling O
, O
we O
apply O
SpellingBee O
to O
Character B-MethodName
- I-MethodName
BERT I-MethodName
( O
El O
Boukkouri O
et O
al O
. O
, O
2020 O
) O
, O
a O
BERT O
- O
style O
model O
whose O
layer O
- O
zero O
word O
embeddings O
are O
derived O
from O
a O
character O
CNN O
, O
following O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O
Table O
2 O
shows O
that O
the O
spelling O
- O
aware O
embeddings O
of O
CharacterBERT B-MethodName
score O
higher O
on O
the O
SpellingBee B-MethodName
probe O
when O
the O
similarity O
and O
lemma O
filters O
are O
applied O
. O
However O
, O
when O
no O
filter O
is O
applied O
, O
RoBERTa B-MethodName
's O
character O
- O
oblivious O
but O
highlytuned O
training O
process O
produces O
embeddings O
that O
score O
higher O
on O
SpellingBee B-MethodName
, O
presumably O
by O
leveraging O
implicit O
similarity O
functions O
in O
the O
embedding O
space O
. O

Although O
CharacterBERT B-MethodName
's O
embedding O
layer O
is O
better O
at O
reconstructing O
original O
words O
( O
when O
similarity O
filters O
are O
applied O
) O
, O
this O
does O
not O
mean O
that O
character O
- O
aware O
models O
are O
necessarily O
better O
downstream O
. O
El O
Boukkouri O
et O
al O
. O
( O
2020 O
) O
report O
performance O
increases O
only O
on O
the O
medical O
domain O
. O
In O
Section O
5 O
, O
we O
demonstrate O
that O
initializing O
a O
masked O
language O
model O
's O
embedding O
layer O
with O
character O
information O
has O
a O
negligible O
effect O
on O
its O
perplexity O
. O

Context O
- O
Oblivious O
Models O

The O
first O
generation O
of O
neural O
word O
representations O
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
contained O
only O
embedding O
layers O
, O
without O
any O
contextualization O
mechanism O
. O
We O
thus O
use O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
to O
estimate O
a O
lower O
bound O
on O
character O
information O
that O
can O
be O
obtained O
by O
simple O
context O
- O
oblivious O
models O
. O
We O
probe O
the O
first O
50 O
K O
words O
in O
GloVe B-MethodName
's O
vocabulary O
with O
SpellingBee O
. O
Table O
2 O
shows O
that O
GloVe B-MethodName
embeddings O
do O
contain O
a O
weak O
orthographic O
signal O
, O
better O
than O
random O
embeddings O
, O
but O
substantially O
weaker O
than O
the O
information O
stored O
in O
the O
embedding O
layer O
of O
large O
transformer O
- O
based O
language O
models O
. O
tion O
when O
trained O
on O
less O
examples O
. O
Figure O
1 O
shows O
how O
well O
SpellingBee O
can O
spell O
RoBERTa B-MethodName
- I-MethodName
Large I-MethodName
's O
vocabulary O
when O
trained O
on O
varying O
amounts O
of O
data O
, O
across O
all O
filters O
. O
We O
find O
that O
more O
data O
makes O
for O
a O
better O
probe O
, O
but O
that O
even O
a O
few O
thousand O
examples O
are O
enough O
to O
train O
SpellingBee B-MethodName
to O
extract O
significant O
character O
information O
from O
the O
embeddings O
, O
which O
can O
not O
be O
extracted O
from O
randomized O
vectors O
( O
the O
control O
) O
. O
7 O

Probing O
with O
Less O
Training O
Data O
We O
further O
examine O
whether O
SpellingBee B-MethodName
can O
extract O
informa- O

Pretraining O
Language O
Models O
to O
Spell O

Our O
probing O
experiments O
reveal O
that O
language O
models O
learn O
some O
partial O
notion O
of O
spelling O
, O
despite O
the O
lack O
of O
direct O
access O
to O
characters O
. O
Therefore O
, O
we O
hypothesize O
that O
learning O
to O
spell O
is O
beneficial O
for O
language O
models O
, O
and O
propose O
pretraining O
the O
embedding O
layer O
using O
a O
variant O
of O
the O
SpellingBee B-MethodName
probe O
described O
in O
Section O
2 O
. O
Here O
, O
the O
goal O
is O
to O
imbue O
each O
embedding O
with O
enough O
information O
for O
SpellingBee B-MethodName
to O
accurately O
generate O
its O
surface O
form O
, O
and O
then O
initialize O
the O
language O
model O
with O
the O
pretrained O
embeddings O
before O
it O
starts O
training O
on O
the O
language O
modeling O
objective O
. O
We O
apply O
this O
process O
to O
RoBERTa B-MethodName
- I-MethodName
Large I-MethodName
, O
train- O
ing O
the O
model O
's O
embedding O
layer O
with O
Spelling O
- O
Bee O
using O
the O
same O
hyperparameter O
settings O
from O
Appendix O
E O
, O
with O
the O
key O
difference O
being O
that O
the O
embeddings O
are O
now O
tunable O
parameters O
( O
not O
frozen O
) O
. O
8 O
We O
train O
RoBERTa B-MethodName
- I-MethodName
Large I-MethodName
on O
English O
Wikipedia O
using O
the O
hyperparameter O
configuration O
of O
24hBERT O
( O
Izsak O
et O
al O
. O
, O
2021 O
) O
, O
and O
cease O
training O
after O
24 O
hours O
( O
approximately O
16,000 O
steps O
) O
. O
For O
comparison O
, O
we O
train O
exactly O
the O
same O
model O
with O
a O
randomly O
- O
initialized O
embedding O
layer O
. O

Figure O
2 O
shows O
the O
masked O
language O
modeling O
loss O
with O
and O
without O
pretrained O
embeddings O
. O
We O
see O
that O
the O
curves O
quickly O
converge O
into O
one O
. O
After O
only O
1000 O
training O
steps O
, O
the O
difference O
between O
the O
validation O
losses O
never O
exceeds O
0.01 O
. O
This O
result O
indicates O
that O
in O
this O
scenario O
, O
the O
model O
does O
not O
utilize O
the O
character O
information O
injected O
into O
the O
tokens O
' O
embeddings O
. O

Although O
there O
are O
many O
possible O
ways O
to O
explicitly O
add O
orthographic O
information O
to O
tokens O
embeddings O
, O
our O
method O
is O
relatively O
straightforward O
as O
it O
gives O
the O
model O
a O
chance O
to O
utilize O
pre O
- O
stored O
character O
information O
. O
Along O
with O
the O
results O
from O
Section O
4 O
, O
we O
hypothesize O
that O
the O
implicit O
notion O
of O
spelling O
that O
the O
model O
learns O
during O
pretraining O
might O
be O
sufficient O
for O
masked O
language O
modeling O
. O

Conclusion O

This O
work O
reveals O
that O
pretrained O
language O
models O
learn O
, O
to O
some O
extent O
, O
the O
character O
composition O
of O
subword O
tokens O
. O
We O
show O
that O
our O
Spelling B-MethodName
- I-MethodName
Bee I-MethodName
probe O
can O
spell O
many O
vocabulary O
items O
using O
their O
uncontextualized O
embedding O
- O
layer O
representations O
alone O
. O
Trying O
to O
explicitly O
infuse O
character O
information O
into O
the O
model O
appears O
to O
have O
a O
minimal O
effect O
on O
the O
model O
's O
ability O
to O
optimize O
its O
8 O
To O
verify O
that O
this O
process O
does O
indeed O
encode O
the O
tokens O
' O
spellings O
into O
the O
embeddings O
, O
we O
apply O
a O
SpellingBee B-MethodName
probe O
( O
using O
a O
different O
random O
initialization O
) O
to O
the O
learned O
embeddings O
, O
which O
yields O
93.5 B-TaskName
% I-TaskName
EM B-MetricName
on O
held O
- O
out O
token O
types O
. O
language O
modeling O
objective O
, O
suggesting O
that O
the O
model O
can O
independently O
learn O
all O
the O
characterlevel O
information O
it O
needs O
for O
the O
task O
. O

A O
Levenshtein B-MetricName
Distance I-MetricName

Levenshtein B-MetricName
distance I-MetricName
( O
Levenshtein O
et O
al O
. O
, O
1966 O
) O
is O
an O
edit O
distance O
metric O
that O
, O
given O
two O
strings O
, O
calculates O
the O
minimal O
number O
of O
changes O
needed O
to O
be O
done O
in O
order O
to O
make O
the O
two O
strings O
identical O
. O
Levenshtein B-MetricName
distance I-MetricName
ratio O
is O
the O
length O
- O
normalized O
version O
, O
which O
is O
computed O
by O
adding O
the O
sum O
of O
lengths O
of O
both O
strings O
to O
the O
edit O
distance O
and O
dividing O
by O
the O
same O
sum O
of O
lengths O
. O
We O
report O
the O
main O
experiment O
's O
results O
using O
this O
ratio O
in O
Table O
3 O
. O

B O
Spelling O
Accuracy O
by O
Frequency O

We O
test O
whether O
pretrained O
models O
tend O
to O
store O
more O
spelling O
- O
related O
information O
in O
higherfrequency O
token O
types O
. O
We O
focus O
on O
RoBERTa B-MethodName
- I-MethodName
Large I-MethodName
, O
and O
assign O
each O
token O
in O
the O
test O
set O
to O
its O
frequency O
quintile O
according O
to O
the O
number O
of O
times O
it O
appeared O
in O
the O
pretraining O
corpus O
-from O
the O
10000 O
most O
frequent O
token O
types O
( O
top O
20 O
% O
) O
to O
those O
ranked O
40000 O
- O
50000 O
in O
the O
vocabulary O
( O
bottom O
20 O
% O
) O
-and O
measure O
the O
average O
performance O
of O
SpellingBee O
within O
each O
quintile O
. O
Figures O
3 O
and O
4 O
shows O
the O
results O
with O
and O
without O
the O
similarity O
filter O
. O
We O
observe O
that O
SpellingBee B-MethodName
is O
indeed O
able O
to O
extract O
more O
information O
from O
higher O
- O
frequency O
token O
types O
, O
suggesting O
that O
the O
pretrained O
model O
has O
more O
information O
about O
their O
character O
composition O
. O

C O
Spelling O
Accuracy O
by O
Length O

We O
analyze O
the O
effect O
of O
token O
length O
on O
the O
probe O
's O
ability O
to O
spell O
. O
A O
priori O
, O
it O
is O
reasonable O
to O
assume O
that O
it O
is O
easier O
for O
the O
probe O
to O
spell O
shorter O
tokens O
, O
since O
less O
information O
needs O
to O
be O
extracted O
from O
the O
embedding O
and O
there O
are O
less O
discrete O
decisions O
to O
be O
made O
while O
decoding O
. O
Indeed O
, O
Figure O
5 O
shows O
that O
with O
the O
none O
filter O
most O
vocabulary O
tokens O
with O
2 O
- O
4 O
characters O
can O
be O
accurately O
reproduced O
from O
their O
vector O
representations O
, O
while O
longer O
tokens O
are O
harder O
to O
replicate O
. O
This O
trend O
is O
particularly O
sharp O
when O
the O
similarity O
filter O
is O
applied O
, O
as O
the O
probe O
is O
hardly O
able O
to O
spell O
tokens O
with O
6 O
or O
more O
characters O
accurately O
; O
having O
said O
that O
, O
the O
probe O
is O
able O
to O
generate O
many O
partially O
correct O
spellings O
, O
as O
measured O
by O
chrF B-MetricName
( O
Figure O
6 O
) O
. O
Perhaps O
a O
less O
intuitive O
result O
is O
the O
probe O
's O
failure O
to O
spell O
single O
- O
character O
tokens O
. O
A O
closer O
look O
reveals O
that O
many O
of O
these O
examples O
are O
rare O
or O
non O
- O
alphanumeric O
characters O
( O
e.g. O
ç O
and O
$ O
) O
, O
which O
are O
probably O
very O
difficult O
for O
the O
probe O
to O
generate O
if O
it O
had O
not O
seen O
them O
during O
training O
. O
While O
these O
results O
show O
strong O
trends O
with O
respect O
to O
length O
, O
token O
length O
is O
also O
highly O
correlated O
with O
frequency O
, O
and O
it O
is O
not O
necessarily O
clear O
which O
of O
the O
two O
factors O
has O
a O
stronger O
impact O
on O
the O
amount O
and O
resolution O
of O
character O
- O
level O
information O
stored O
in O
the O
embedding O
layer O
of O
pretrained O
models O
. O

D O
Manual O
Error O
Analysis O

We O
manually O
analyze O
100 O
random O
tokens O
that O
SpellingBee B-MethodName
spelled O
incorrectly O
with O
the O
lemma O
filter O
to O
understand O
the O
nature O
of O
the O
spelling O
mistakes O
. O

Out O
of O
those O
100 O
we O
display O
20 O
mistakes O
in O
Table O
4 O
alongside O
the O
spelling O
prediction O
of O
the O
control O
baseline O
. O
SpellingBee B-MethodName
's O
mistakes O
vary O
from O
singlecharacter O
typos O
to O
completely O
different O
words O
. O
Having O
said O
that O
, O
the O
vast O
majority O
of O
mistakes O
have O
significant O
overlap O
with O
the O
correct O
spelling O
, O
such O
as O
shared O
prefixes O
and O
capitalization O
. O

E O
Hyperparameters O

We O
implement O
SpellingBee B-MethodName
with O
a O
6 B-HyperparameterValue
- O
layer B-HyperparameterName
encoderdecoder I-HyperparameterName
model I-HyperparameterName
, O
with O
512 B-HyperparameterValue
model B-HyperparameterName
dimensions I-HyperparameterName
. O

The O
model O
parameters O
are O
optimized O
with O
Adam B-HyperparameterValue
( O
Kingma O
and O
Ba O
, O
2015 O
) O
for O
1000 B-HyperparameterValue
steps B-HyperparameterName
with O
up O
to O
1024 B-HyperparameterValue
tokens B-HyperparameterName
per I-HyperparameterName
batch I-HyperparameterName
, O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-4 B-HyperparameterValue
, O
and O
a O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0.1 B-HyperparameterValue
. O
These O
are O
the O
default O
hyperpa- O
rameters O
for O
training O
a O
transformer O
language O
model O
in O
Fairseq O
. O

Acknowledgements O

This O
work O
was O
supported O
by O
the O
Tel O
Aviv O
University O
Data O
Science O
Center O
, O
Len O
Blavatnik O
and O
the O
Blavatnik O
Family O
foundation O
, O
the O
Alon O
Scholarship O
, O
Intel O
Corporation O
, O
and O
the O
Yandex O
Initiative O
for O
Machine O
Learning O
. O
We O
thank O
Avia O
Efrat O
for O
his O
valuable O
feedback O
. O

POLITICS B-MethodName
: O
Pretraining O
with O
Same O
- O
story O
Article O
Comparison O
for O
Ideology B-TaskName
Prediction I-TaskName
and I-TaskName
Stance I-TaskName
Detection I-TaskName

Ideology O
is O
at O
the O
core O
of O
political O
science O
research O
. O
Yet O
, O
there O
still O
does O
not O
exist O
generalpurpose O
tools O
to O
characterize O
and O
predict O
ideology O
across O
different O
genres O
of O
text O
. O
To O
this O
end O
, O
we O
study O
Pretrained O
Language O
Models O
using O
novel O
ideology O
- O
driven O
pretraining O
objectives O
that O
rely O
on O
the O
comparison O
of O
articles O
on O
the O
same O
story O
written O
by O
media O
of O
different O
ideologies O
. O
We O
further O
collect O
a O
large O
- O
scale O
dataset O
, O
consisting O
of O
more O
than O
3.6 O
M O
political O
news O
articles O
, O
for O
pretraining O
. O
Our O
model O
POLITICS B-MethodName
outperforms O
strong O
baselines O
and O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
on O
ideology O
prediction O
and O
stance O
detection O
tasks O
. O
Further O
analyses O
show O
that O
POLITICS B-MethodName
is O
especially O
good O
at O
understanding O
long O
or O
formally O
written O
texts O
, O
and O
is O
also O
robust O
in O
few O
- O
shot O
learning O
scenarios O
. O
* O
Equal O
contribution O
by O
the O
first O
two O
authors O
. O
News O
Story O
: O
Donald O
Trump O
tests O
positive O
for O
COVID-19 O
. O
Daily O
Kos O
( O
left O
) O
: O
It O
's O
now O
clear O
that O
Donald O
Trump O
lied O
to O
the O
nation O
about O
when O
he O
received O
a O
positive O
test O
for O
COVID-19 O
. O
. O
. O
. O
they O
're O
continuing O
to O
act O
as O
if O
nothing O
has O
changed O
- O
and O
that O
disregarding O
science O
and O
lying O
to O
the O
public O
are O
the O
only O
possible O
strategies O
. O
The O
Washington O
Times O
( O
right O
) O
: O
Trump O
says O
he O
's O
" O
doing O
very O
well O
" O
. O
. O
. O
President O
Trump O
thanked O
the O
nation O
for O
supporting O
him O
Friday O
night O
as O
he O
left O
the O
White O
House O
to O
be O
hospitalized O
for O
COVID-19 O
. O
" O
I O
want O
to O
thank O
everybody O
for O
the O
tremendous O
support O
. O
. O
. O
. O
" O
Mr. O
Trump O
said O
in O
a O
video O
recorded O
at O
the O
White O
House O
. O
Breitbart O
( O
right O
) O
: O
President O
Donald O
Trump O
thanked O
Americans O
for O
their O
support O
on O
Friday O
as O
he O
traveled O
to O
Walter O
Reed O
Military O
Hospital O
for O
further O
care O
after O
he O
was O
diagnosed O
with O
coronavirus O
. O
" O
I O
think O
I O
'm O
doing O
very O
well O
. O
. O
. O
" O
Trump O
said O
in O
a O
video O
filmed O
at O
the O
White O
House O
and O
posted O
to O
social O
media O
. O

Introduction O

Ideology O
is O
an O
ubiquitous O
factor O
in O
political O
science O
, O
journalism O
, O
and O
media O
studies O
( O
Mullins O
, O
1972 O
; O
Freeden O
, O
2006 O
; O
Martin O
, O
2015 O
) O
. O
Decades O
of O
work O
has O
gone O
into O
measuring O
ideology O
based O
on O
voting O
data O
( O
Poole O
and O
Rosenthal O
, O
1985 O
; O
Lewis O
et O
al O
. O
, O
2021 O
) O
, O
survey O
results O
( O
Preoţiuc O
- O
Pietro O
et O
al O
. O
, O
2017 O
; O
Ansolabehere O
et O
al O
. O
, O
2008 O
; O
Kim O
and O
Fording O
, O
1998 O
; O
Gabel O
and O
Huber O
, O
2000 O
) O
, O
social O
networks O
( O
Barberá O
et O
al O
. O
, O
2015 O
) O
, O
campaign O
donation O
records O
( O
Bonica O
, O
2013 O
) O
, O
and O
textual O
data O
( O
Laver O
et O
al O
. O
, O
2003 O
; O
Diermeier O
et O
al O
. O
, O
2012 O
; O
Gentzkow O
et O
al O
. O
, O
2019 O
; O
Volkens O
et O
al O
. O
, O
2021 O
) O
. O
Each O
of O
those O
approaches O
has O
its O
strengths O
and O
weaknesses O
. O
For O
instance O
, O
many O
political O
figures O
do O
not O
have O
voting O
records O
; O
surveys O
are O
expensive O
and O
politicians O
are O
often O
unwilling O
to O
disclose O
ideology O
. O
By O
contrast O
, O
political O
text O
is O
abundant O
, O
ubiquitous O
, O
yet O
challenging O
to O
work O
with O
since O
language O
is O
complex O
in O
nature O
, O
often O
domainspecific O
, O
and O
generally O
unlabeled O
. O
There O
thus O
remains O
a O
strong O
need O
for O
general O
- O
purpose O
tools O
for O
measuring O
ideology O
using O
text O
that O
can O
be O
applied O
across O
multiple O
genres O
. O
Using O
text O
as O
data O
, O
computational O
models O
for O
ideology O
measurement O
have O
rapidly O
expanded O
and O
diversified O
, O
including O
classical O
machine O
learning O
methods O
such O
as O
ideal O
point O
estimation O
( O
Groseclose O
et O
al O
. O
, O
1999 O
; O
Shor O
and O
McCarty O
, O
2011 O
) O
, O
Naive O
Bayes O
( O
Evans O
et O
al O
. O
, O
2007 O
) O
, O
support O
vector O
machines O
( O
Yu O
et O
al O
. O
, O
2008 O
) O
, O
latent O
variable O
models O
( O
Barberá O
et O
al O
. O
, O
2015 O
) O
, O
and O
regression O
( O
Peterson O
and O
Spirling O
, O
2018 O
) O
; O
and O
more O
recent O
neural O
architectures O
like O
recurrent O
neural O
networks O
( O
Iyyer O
et O
al O
. O
, O
2014 O
) O
and O
Transformers O
( O
Baly O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2021 O
) O
. O
Nonetheless O
, O
most O
of O
those O
models O
leverage O
datasets O
with O
ideology O
labels O
drawn O
from O
a O
single O
domain O
, O
and O
it O
is O
unclear O
if O
any O
of O
them O
can O
be O
generalized O
to O
diverse O
genres O
of O
text O
. O

Trained O
on O
massive O
quantities O
of O
data O
, O
Pretrained O
Language O
Models O
( O
PLMs O
) O
have O
achieved O
state O
- O
ofthe O
- O
art O
performance O
on O
many O
text O
classification O
problems O
, O
with O
an O
additional O
fine O
- O
tuning O
stage O
on O
labeled O
task O
- O
specific O
samples O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
Though O
PLMs O
suggest O
the O
promise O
of O
generalizable O
solutions O
, O
their O
ability O
to O
acquire O
the O
knowledge O
needed O
to O
detect O
complex O
features O
such O
as O
ideology O
from O
text O
across O
genres O
remains O
an O
open O
question O
. O
PLMs O
have O
been O
shown O
to O
capture O
linguistic O
structures O
with O
a O
local O
focus O
, O
such O
as O
task O
- O
specific O
words O
, O
syntactic O
agreement O
, O
and O
semantic O
compositionality O
( O
Clark O
et O
al O
. O
, O
2019 O
; O
Jawahar O
et O
al O
. O
, O
2019 O
) O
. O
Although O
word O
choice O
is O
indicative O
of O
ideology O
, O
ideological O
leaning O
and O
stance O
are O
often O
revealed O
by O
which O
entities O
and O
events O
are O
selected O
for O
presentation O
( O
Hackett O
, O
1984 O
; O
Christie O
and O
Martin O
, O
2005 O
; O
Enke O
, O
2020 O
) O
, O
with O
the O
most O
notable O
strand O
of O
work O
in O
framing O
theory O
( O
Entman O
, O
1993 O
( O
Entman O
, O
, O
2007 O
. O
One O
such O
example O
is O
demonstrated O
in O
Figure O
1 O
, O
where O
Daily O
Kos O
criticizes O
Trump O
's O
dishonesty O
while O
The O
Washington O
Times O
and O
Breitbart O
emphasize O
the O
good O
condition O
of O
his O
health O
. O

In O
this O
work O
, O
we O
propose O
to O
train O
PLMs O
for O
a O
wide O
range O
of O
ideology O
- O
related O
downstream O
tasks O
. O
We O
argue O
that O
it O
is O
critical O
for O
PLMs O
to O
consider O
the O
global O
context O
of O
a O
given O
article O
. O
For O
instance O
, O
as O
pointed O
out O
by O
Fan O
et O
al O
. O
( O
2019 O
) O
, O
one O
way O
to O
acquire O
such O
context O
is O
through O
comparison O
of O
news O
articles O
on O
the O
same O
story O
but O
reported O
by O
media O
of O
different O
ideologies O
. O
Given O
the O
lack O
of O
suitable O
datasets O
, O
we O
first O
collect O
a O
new O
large O
- O
scale O
dataset O
, O
BIGNEWS B-DatasetName
. O
1 O
It O
contains O
3,689,229 O
English O
news O
articles O
on O
politics O
, O
gathered O
from O
11 O
United O
States O
( O
US O
) O
media O
outlets O
covering O
a O
broad O
ideological O
spectrum O
. O
We O
further O
downsample O
and O
cluster O
articles O
in O
BIGNEWS B-DatasetName
by O
different O
media O
into O
groups O
, O
each O
consisting O
of O
pieces O
aligned O
on O
the O
same O
story O
. O
The O
resultant O
dataset O
, O
BIGNEWSALIGN B-DatasetName
, O
contains O
1,060,512 O
stories O
with O
aligned O
articles O
. O

Next O
we O
train O
a O
new O
PLM O
, O
POLITICS B-MethodName
, O
based O
on O
a O
Pretraining O
Objective O
Leveraging O
Inter O
- O
article O
Triplet O
- O
loss O
using O
Ideological O
Content O
and O
Story O
. O
Concretely O
, O
we O
leverage O
continued O
pretraining O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
, O
where O
we O
design O
an O
ideology O
objective O
operating O
over O
clusters O
of O
same O
- O
story O
articles O
to O
compact O
articles O
with O
similar O
ideology O
and O
contrast O
them O
with O
articles O
of O
different O
ideology O
. O
The O
learned O
representation O
can O
better O
discern O
the O
embedded O
ideological O
content O
. O
We O
further O
enhance O
it O
with O
a O
story O
objective O
that O
ensures O
the O
model O
to O
focus O
on O
meaningful O
content O
instead O
of O
overly O
relying O
on O
shortcuts O
, O
e.g. O
, O
media O
boilerplate O
. O
Both O
objectives O
are O
used O
together O
with O
our O
specialized O
masked O
language O
model O
objective O
that O
focuses O
on O
entities O
and O
sentiments O
to O
train O
POLITICS B-MethodName
. O

Our O
main O
goal O
here O
is O
to O
create O
general O
- O
purpose O
tools O
for O
analyzing O
ideological O
content O
for O
researchers O
and O
practitioners O
in O
the O
broad O
community O
. O
Furthermore O
, O
when O
experimenting O
on O
11 O
ideology O
prediction O
and O
stance O
detection O
tasks O
using O
8 O
datasets O
of O
different O
genres O
, O
including O
a O
newly O
collected O
dataset O
from O
AllSides O
, O
POLITICS B-MethodName
outperforms O
both O
a O
strong O
SVM O
baseline O
and O
previous O
PLMs O
on O
8 O
tasks O
. O
Notably O
, O
POLITICS B-MethodName
is O
particularly O
effective O
on O
long O
documents O
, O
e.g. O
, O
achieving O
10 O
% O
improvements O
on O
both O
ideology O
prediction O
and O
stance O
detection O
tasks O
over O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
We O
further O
show O
that O
our O
model O
is O
more O
robust O
in O
setups O
with O
smaller O
training O
sets O
. O

Related O
Work O

Ideology O
prediction O
is O
a O
critical O
task O
for O
quantitative O
political O
science O
( O
Mullins O
, O
1972 O
; O
Freeden O
, O
2006 O
; O
Martin O
, O
2015 O
; O
Wilkerson O
and O
Casas O
, O
2017 O
) O
. O
Both O
classical O
methods O
( O
e.g. O
, O
Naive O
Bayes O
, O
SVM O
; O
Evans O
et O
al O
. O
, O
2007 O
; O
Yu O
et O
al O
. O
, O
2008 O
; O
Sapiro O
- O
Gheiler O
, O
2019 O
) O
and O
deep O
learning O
models O
( O
e.g. O
, O
RNN O
; O
Iyyer O
et O
al O
. O
, O
2014 O
) O
have O
been O
used O
to O
predict O
ideology O
on O
a O
variety O
of O
datasets O
where O
ideology O
labels O
are O
available O
, O
such O
as O
legislative O
speeches O
( O
Laver O
et O
al O
. O
, O
2003 O
) O
and O
U.S. O
Supreme O
Court O
briefs O
( O
Evans O
et O
al O
. O
, O
2007 O
) O
. O
Notably O
, O
Liu O
et O
al O
. O
( O
2021 O
) O
pretrains O
a O
Transformer O
- O
based O
language O
generator O
to O
minimize O
the O
ideological O
bias O
in O
generated O
text O
. O
As O
generative O
models O
are O
not O
as O
effective O
as O
masked O
language O
models O
( O
MLMs O
) O
at O
text O
classification O
, O
our O
goal O
differs O
in O
that O
we O
train O
MLMs O
to O
recognize O
ideological O
contents O
in O
various O
domains O
and O
tasks O
. O

Stance O
detection O
is O
a O
useful O
task O
for O
ideology O
analysis O
because O
co O
- O
partisans O
are O
generally O
positive O
towards O
each O
other O
and O
negative O
towards O
counterpartisans O
( O
Aref O
and O
Neal O
, O
2021 O
) O
. O
There O
has O
been O
a O
large O
body O
of O
work O
on O
identifying O
individuals O
' O
stances O
towards O
specific O
targets O
from O
the O
given O
text O
( O
Thomas O
et O
al O
. O
, O
2006 O
; O
Walker O
et O
al O
. O
, O
2012 O
; O
Hasan O
and O
Ng O
, O
2013 O
) O
. O
On O
the O
methodology O
side O
, O
Mohammad O
et O
al O
. O
( O
2016b O
) O
and O
Küçük O
and O
Can O
( O
2018 O
) O
apply O
statistical O
models O
, O
e.g. O
, O
SVM O
, O
with O
handcrafted O
text O
features O
. O
Neural O
methods O
have O
also O
been O
widely O
investigated O
, O
including O
CNN O
( O
Wei O
et O
al O
. O
, O
2016 O
) O
, O
LSTM O
( O
Augenstein O
et O
al O
. O
, O
2016 O
) O
, O
hierarchical O
networks O
( O
Sun O
et O
al O
. O
, O
2018 O
) O
, O
and O
representation O
learning O
( O
Darwish O
et O
al O
. O
, O
2020 O
) O
. O

Recent O
research O
focus O
resides O
in O
leveraging O
A4 O
. O

PLMs O
for O
predicting O
stances O
, O
e.g. O
, O
incorporating O
extra O
features O
( O
Prakash O
and O
Madabushi O
, O
2020 O
) O
. O
Kawintiranon O
and O
Singh O
( O
2021 O
) O
share O
a O
similar O
spirit O
with O
our O
work O
by O
upsampling O
tokens O
to O
mask O
. O
However O
, O
they O
pre O
- O
define O
a O
list O
of O
tokens O
customized O
for O
the O
given O
targets O
, O
which O
is O
hard O
to O
generalize O
to O
new O
targets O
. O
We O
aim O
to O
train O
PLMs O
relying O
on O
general O
- O
purpose O
sentiment O
lexicons O
and O
important O
entities O
, O
to O
foster O
model O
generalizability O
. O

Domain O
- O
specific O
Pretrained O
Language O
Models O
. O

PLMs O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
have O
obtained O
stateof O
- O
the O
- O
art O
results O
on O
many O
NLP O
tasks O
. O
Inspired O
by O
the O
observation O
that O
a O
continued O
pretraining O
phase O
on O
in O
- O
domain O
data O
yields O
better O
performance O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
, O
domain O
- O
specific O
PLMs O
are O
introduced O
( O
Beltagy O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
. O
However O
, O
they O
only O
use O
the O
default O
MLM O
objective O
, O
without O
considering O
domain O
knowledge O
. O
In O
this O
work O
, O
we O
design O
ideology O
- O
driven O
pretraining O
objectives O
to O
inject O
domain O
knowledge O
to O
discern O
ideologies O
and O
related O
stances O
. O

Focusing O
on O
the O
news O
domain O
, O
PLMs O
have O
been O
primarily O
used O
for O
factuality O
prediction O
( O
Jwa O
et O
al O
. O
, O
2019 O
; O
Zellers O
et O
al O
. O
, O
2019 O
; O
Kaliyar O
et O
al O
. O
, O
2021 O
) O
and O
topic O
classification O
( O
Liu O
et O
al O
. O
, O
2020 O
; O
Büyüköz O
et O
al O
. O
, O
2020 O
; O
Gupta O
et O
al O
. O
, O
2020 O
) O
by O
fine O
- O
tuning O
on O
task O
- O
specific O
datasets O
. O
Few O
work O
has O
investigated O
PLMs O
for O
understanding O
political O
ideology O
evinced O
in O
texts O
. O
One O
exception O
is O
Baly O
et O
al O
. O
( O
2020 O
) O
, O
where O
they O
also O
leverage O
the O
triplet O
loss O
as O
the O
pretraining O
objective O
. O
However O
, O
our O
work O
is O
novel O
in O
at O
least O
three O
aspects O
. O
First O
, O
our O
triplet O
loss O
is O
designed O
to O
capture O
the O
ideological O
( O
dis O
) O
similarity O
among O
articles O
on O
the O
same O
story O
, O
while O
the O
loss O
used O
by O
Baly O
et O
al O
. O
( O
2020 O
) O
operates O
on O
articles O
of O
the O
same O
topic O
. O
As O
a O
result O
, O
their O
approach O
can O
falsely O
compact O
representations O
of O
very O
different O
news O
contents O
, O
e.g. O
, O
articles O
on O
" O
Japan O
Economics O
" O
and O
" O
Indian O
Troops O
" O
both O
belong O
to O
the O
topic O
of O
" O
Asia O
" O
. O
Moreover O
, O
our O
newly O
introduced O
story O
objective O
can O
effectively O
prevent O
the O
model O
from O
relying O
on O
media O
- O
specific O
language O
( O
e.g. O
, O
" O
for O
the O
New O
York O
Times O
" O
) O
, O
while O
their O
objective O
may O
fail O
to O
do O
so O
, O
and O
thus O
lacks O
generalizability O
to O
languages O
used O
by O
different O
media O
and O
other O
ideology O
- O
related O
tasks O
. O
Finally O
, O
we O
use O
BIGNEWS B-DatasetName
that O
contains O
more O
than O
3 O
M O
articles O
, O
which O
is O
more O
suitable O
for O
pretraining O
large O
models O
than O
the O
small O
dataset O
( O
35k O
articles O
) O
used O
by O
Baly O
et O
al O
. O
( O
2020 O
) O
. O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
systematically O
study O
and O
release O
PLMs O
for O
ideology O
- O
related O
study O
in O
the O
US O
political O
domain O
. O

3 O
Pretraining O
Datasets O

Data O
Crawling O

We O
collect O
pretraining O
datasets O
from O
online O
news O
articles O
with O
diverse O
ideological O
leanings O
and O
language O
usage O
. O
We O
select O
11 O
media O
outlets O
based O
on O
their O
ideologies O
( O
from O
far O
- O
left O
to O
far O
- O
right O
) O
and O
popularity O
. O
2 O
We O
convert O
their O
ideologies O
into O
three O
categories O
: O
left O
, O
center O
, O
and O
right O
, O
and O
crawl O
all O
pages O
published O
by O
them O
between O
January O
2000 O
and O
June O
2021 O
, O
from O
Common O
Crawl O
and O
Internet O
Archive O
. O
We O
then O
follow O
Raffel O
et O
al O
. O
( O
2020 O
) O
for O
data O
cleaning O
, O
and O
, O
additionally O
, O
only O
retain O
news O
articles O
related O
to O
US O
politics O
. O
Appendix O
A O
describes O
in O
detail O
the O
steps O
for O
removing O
non O
- O
articles O
pages O
, O
duplicates O
, O
non O
- O
US O
pages O
, O
and O
boilerplate O
. O

The O
cleaned O
data O
, O
dubbed O
BIGNEWS B-DatasetName
, O
contains O
3,689,229 O
US O
political O
news O
articles O
. O
To O
mitigate O
the O
bias O
that O
some O
media O
dominate O
the O
model O
training O
, O
we O
downsample O
the O
corpus O
so O
that O
each O
ideology O
contributes O
equally O
. O
The O
downsampled O
corpus O
, O
BIGNEWSBLN B-DatasetName
, O
contains O
2,331,552 O
news O
articles O
, O
with O
statistics O
listed O
in O
Table O
1 O
. O
We O
keep O
30 O
K O
heldout O
articles O
as O
validation O
set O
. O

Aligning O
Articles O
on O
the O
Same O
Story O

We O
compare O
how O
media O
outlets O
from O
different O
sides O
report O
the O
same O
story O
, O
which O
intuitively O
better O
captures O
ideological O
content O
. O
To O
this O
end O
, O
we O
design O
an O
algorithm O
to O
align O
articles O
in O
BIGNEWSBLN B-DatasetName
that O
cover O
the O
same O
story O
. O
We O
treat O
each O
article O
as O
an O
anchor O
, O
and O
find O
matches O
from O
other O
outlets O
based O
on O
the O
following O
similarity O
score O
: O

sim O
( O
pi O
, O
pj O
) O
= O
α O
* O
simt O
( O
pi O
, O
pj O
) O
+ O
( O
1 O
− O
α O
) O
* O
sime O
( O
pi O
, O
pj O
) O
( O
1 O
) O

where O
p O
i O
and O
p O
j O
are O
two O
articles O
, O
sim O
t O
is O
the O
cosine O
similarity O
between O
TF O
- O
IDF O
vectors O
of O
p O
i O
and O
p O
j O
, O
sim O
e O
is O
the O
weighted O
Jaccard O
similarity O
between O
the O
sets O
of O
named O
entities O
3 O
in O
p O
i O
and O
p O
j O
, O
and O
α O
= O
0.4 O
is O
a O
hyperparameter O
. O
During O
alignment O
, O
for O
an O
article O
from O
an O
outlet O
to O
be O
considered O
as O
a O
match O
, O
it O
must O
be O
published O
within O
three O
days O
before O
or O
after O
the O
anchor O
, O
has O
the O
highest O
similarity O
score O
among O
articles O
from O
the O
same O
outlet O
, O
and O
the O
score O
is O
at O
least O
θ O
= O
0.23 O
. O
Hyperparameters O
α O
and O
θ O
are O
searched O
on O
the O
Basil B-DatasetName
dataset O
( O
Fan O
et O
al O
. O
, O
2019 O
) O
, O
which O
contains O
manually O
aligned O
articles O
. O
4 O
After O
deduplicating O
articles O
in O
each O
story O
cluster O
, O
we O
obtain O
BIGNEWSALIGN B-DatasetName
, O
containing O
1,060,512 O
clusters O
with O
an O
average O
of O
4.29 O
articles O
in O
each O
. O
Appendix O
B O
details O
the O
alignment O
algorithm O
. O

POLITICS B-MethodName
via O
Continued O
Pretraining O

Here O
we O
introduce O
our O
continued O
pretraining O
methods O
based O
on O
a O
newly O
proposed O
ideology O
objective O
that O
drives O
representation O
learning O
to O
better O
discern O
ideological O
content O
by O
comparing O
same O
- O
story O
articles O
( O
§ O
4.1 O
) O
, O
which O
is O
further O
augmented O
by O
a O
story O
objective O
to O
better O
focus O
on O
content O
. O
They O
are O
combined O
with O
the O
masked O
language O
model O
objective O
, O
which O
is O
tailored O
to O
focus O
on O
entities O
and O
sentiments O
( O
§ O
4.2 O
) O
, O
to O
produce O
POLITICS B-MethodName
( O
§ O
4.3 O
) O
. O

Ideology O
- O
driven O
Pretraining O
Objectives O

To O
promote O
representation O
learning O
that O
better O
captures O
ideological O
content O
, O
we O
leverage O
BIGNEWSALIGN B-DatasetName
with O
articles O
grouped O
by O
stories O
to O
provide O
story O
- O
level O
background O
for O
model O
training O
. O
That O
is O
, O
we O
use O
triplet O
loss O
( O
Schroff O
et O
al O
. O
, O
2015 O
) O
that O
operates O
over O
triplets O
of O
< O
anchor O
, O
positive O
, O
negative O
> O
to O
encourage O
anchor O
and O
positive O
samples O
to O
have O
closer O
representations O
while O
contrasting O
anchor O
from O
negative O
samples O
. O
Our O
primary O
pretraining O
objective O
, O
i.e. O
, O
ideology O
objective O
, O
uses O
the O
triplet O
loss O
to O
teach O
the O
model O
to O
acquire O
ideology O
- O
informed O
representations O
by O
comparing O
same O
- O
story O
articles O
written O
by O
media O
of O
different O
ideologies O
. O
As O
shown O
in O
Figure O
2 O
, O
given O
a O
story O
cluster O
, O
we O
choose O
an O
article O
published O
by O
media O
on O
the O
left O
or O
right O
as O
the O
anchor O
. O
We O
then O
take O
articles O
in O
the O
same O
cluster O
with O
the O
same O
ideology O
as O
positive O
samples O
, O
and O
articles O
with O
the O
opposite O
ideology O
as O
negative O
ones O
. O
The O
ideology O
objective O
is O
formulated O
as O
follows O
: O

Lideo O
= O
t∈T O
ideo O
t O
( O
a O
) O
− O
t O
( O
p O
) O
2 O
− O
t O
( O
a O
) O
− O
t O
( O
n O
) O
2 O
+ O
δideo O
+ O
( O
2 O
) O

where O
T O
ideo O
is O
the O
set O
of O
all O
ideology O
triplets O
, O
t O
( O
a O
) O
, O
t O
( O
p O
) O
, O
and O
t O
( O
n O
) O
are O
the O
[ O
CLS O
] O
representations O
of O
anchor O
, O
positive O
, O
and O
negative O
articles O
in O
triplet O
t O
, O
δ B-HyperparameterValue
ideo O
is O
a O
hyperparameter O
, O
and O
[ O
• O
] O
+ O
is O
max O
( O
• O
, O
0 O
) O
. O

Next O
, O
we O
augment O
the O
ideology O
objective O
with O
a O
story O
objective O
to O
allow O
the O
model O
to O
focus O
on O
semantically O
meaningful O
content O
and O
to O
prevent O
the O
model O
from O
focusing O
on O
" O
shortcuts O
" O
( O
such O
as O
media O
- O
specific O
languages O
) O
to O
detect O
ideology O
. O
To O
construct O
story O
triplets O
, O
we O
use O
the O
same O
< O
anchor O
, O
positive O
> O
pairs O
as O
in O
the O
ideology O
triplet O
, O
and O
then O
treat O
articles O
from O
the O
same O
media O
outlet O
but O
on O
different O
stories O
as O
negative O
samples O
, O
as O
depicted O
in O
Figure O
2 O
. O
Similarly O
, O
our O
story O
objective O
is O
formulated O
as O
follows O
: O

Lstory O
= O
t∈Tstory O
t O
( O
a O
) O
− O
t O
( O
p O
) O
2 O
− O
t O
( O
a O
) O
− O
t O
( O
n O
) O
2 O
+ O
δstory O
+ O
( O
3 O
) O

where O
T O
story O
contains O
all O
story O
triplets O
, O
and O
δ B-HyperparameterName
story O
is O
a O
hyperparameter O
searched O
on O
the O
validation O
set O
. O

Entity O
- O
and O
Sentiment O
- O
aware O
MLM O

Here O
we O
present O
a O
specialized O
MLM O
objective O
to O
collaborate O
with O
our O
triplet O
loss O
based O
objectives O
for O
better O
representation O
learning O
. O
Notably O
, O
political O
framing O
effect O
is O
often O
reflected O
in O
which O
entities O
are O
selected O
for O
reporting O
( O
Gentzkow O
et O
al O
. O
, O
2019 O
) O
. O
Moreover O
, O
the O
occurrence O
of O
sentimental O
content O
along O
with O
the O
entities O
also O
signal O
stances O
( O
Mohammad O
et O
al O
. O
, O
2016b O
) O
. O
Therefore O
, O
we O
take O
a O
masking O
strategy O
that O
upsamples O
entity O
tokens O
( O
Sun O
et O
al O
. O
, O
2019 O
; O
Guu O
et O
al O
. O
, O
2020 O
; O
Kawintiranon O
and O
Singh O
, O
2021 O
) O
and O
sentiment O
words O
to O
be O
masked O
for O
the O
MLM O
objective O
, O
which O
improves O
from O
prior O
pretraining O
work O
that O
only O
considers O
article O
- O
level O
comparison O
( O
Baly O
et O
al O
. O
, O
2020 O
) O
. O

Concretely O
, O
we O
consider O
named O
entities O
with O
types O
of O
PERSON O
, O
NORP O
, O
ORG O
, O
GPE O
and O
EVENT O
. O
We O
detect O
sentiment O
words O
using O
lexicons O
by O
Hu O
and O
Liu O
( O
2004 O
) O
and O
Wilson O
et O
al O
. O
( O
2005 O
) O
. O
To O
allow O
MLM O
training O
to O
focus O
on O
entities O
and O
sentiment O
, O
we O
mask O
them O
with O
a O
30 O
% O
probability O
, O
and O
then O
randomly O
mask O
remaining O
tokens O
until O
15 O
% O
of O
all O
tokens O
are O
reached O
, O
as O
done O
in O
Devlin O
et O
al O
. O
( O
2019 O
) O
. O
Masked O
tokens O
are O
replaced O
with O
[ O
MASK O
] O
, O
random O
tokens O
, O
and O
original O
tokens O
with O
a O
ratio O
of O
8:1:1 O
. O

Overall O
Pretraining O
Objective O

We O
combine O
the O
aforementioned O
objectives O
as O
our O
final O
pretraining O
objective O
as O
follows O
: O

L O
= O
β O
* O
L O
ideology O
+ O
γ O
* O
L O
story O
+ O
( O
1 O
− O
β O
− O
γ O
) O
* O
L O
MLM O
( O
4 O
) O

where O
β B-HyperparameterName
= I-HyperparameterName
γ B-HyperparameterName
= O
0.25 B-HyperparameterValue
. O
Using O
L O
, O
POLITICS O
is O
produced O
via O
continued O
training O
on O
RoBERTa B-DatasetName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
5 O
We O
do O
not O
try O
to O
train O
the O
model O
from O
scratch O
since O
BIGNEWSBLN B-DatasetName
only O
has O
∼10 O
GB O
data O
, O
smaller O
than O
corpus O
for O
RoBERTa O
( O
∼160 O
GB O
) O
. O
Hyperparameters O
are O
listed O
in O
Table O
A5 O
. O

Experiments O

Given O
the O
importance O
of O
ideology O
prediction O
and O
stance O
detection O
tasks O
in O
political O
science O
( O
Thomas O
et O
al O
. O
, O
2006 O
; O
Wilkerson O
and O
Casas O
, O
2017 O
; O
Chatsiou O
and O
Mikhaylov O
, O
2020 O
) O
, O
we O
conduct O
extensive O
experiments O
on O
a O
wide O
spectrum O
of O
datasets O
with O
11 O
tasks O
( O
§ O
5.1 O
) O
. O
We O
then O
compare O
with O
both O
classical O
models O
and O
prior O
PLMs O
( O
§ O
5.2 O
) O
, O
and O
among O
our O
model O
variants O
( O
§ O
5.3 O
) O
. O
We O
present O
and O
discuss O
results O
in O
§ O
5.5 O
, O
where O
POLITICS B-MethodName
outperform O
all O
three O
baselines O
on O
8 O
out O
of O
11 O
tasks O
. O
For O
all O
models O
, O
MLM O
objectives O
are O
trained O
with O
BIGNEWSBLN B-DatasetName
, O
and O
ideology O
and O
story O
objectives O
are O
trained O
on O
BIGNEWSALIGN B-DatasetName
. O
Details O
are O
in O
Appendix O
C.1 O
. O

Datasets O
and O
Tasks O

Our O
tasks O
are O
discussed O
below O
, O
with O
statistics O
listed O
in O
Table O
2 O
and O
more O
descriptions O
in O
Appendix O
D O
. O

Ideology O
prediction O
tasks O
for O
predicting O
the O
political O
leanings O
are O
evaluated O
on O
the O
following O
datasets O
. O

• O
( O
Mohammad O
et O
al O
. O
, O
2016a O
) O
is O
a O
shared O
task O
on O
detecting O
stances O
in O
tweets O
. O
We O
consider O
two O
setups O
to O
predict O
on O
seen O
, O
i.e. O
SEval O
( O
seen O
) O
, O
and O
unseen O
, O
i.e. O
, O
SEval O
( O
unseen O
) O
, O
entities O
. O

Baselines O

We O
consider O
three O
baselines O
. O
First O
, O
we O
train O
a O
linear O
SVM O
using O
unigram O
and O
bigram O
features O
for O
each O
task O
, O
since O
it O
is O
a O
common O
baseline O
in O
political O
science O
( O
Yu O
et O
al O
. O
, O
2008 O
; O
Diermeier O
et O
al O
. O
, O
2012 O
) O
. O
Hyperparameters O
and O
feature O
selection O
are O
described O
in O
Table O
A8 O
. O
We O
further O
compare O
with O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
following O
the O
standard O
fine O
- O
tuning O
process O
for O
ideology O
prediction O
tasks O
and O
using O
the O
prompt O
described O
in O
§ O
5.4 O
for O
stance O
detection O
. O

Model O
Variants O

We O
consider O
several O
variants O
of O
POLITICS B-MethodName
. O
First O
, O
using O
triplet O
loss O
objective O
only O
, O
we O
experiment O
on O
models O
trained O
with O
ideology O
objective O
( O
Ideology O
Obj O
. O
) O
, O
story O
objective O
( O
Story O
Obj O
. O
) O
, O
or O
both O
. O
Next O
, O
we O
continue O
pretaining O
RoBERTa B-MethodName
with O
MLM O
objective O
only O
, O
using O
vanilla O
MLM O
objective O
( O
Random O
) O
, O
entity O
focused O
objective O
( O
Upsamp O
. O
Ent O
. O
) O
, O
sentiment O
focused O
objective O
( O
Upsamp O
. O
Sentiment O
) O
, O
or O
upsampling O
both O
entity O
and O
sentiment O
. O

Fine O
- O
tuning O
Procedure O

We O
fine O
- O
tune O
each O
neural O
model O
for O
up O
to O
10 O
epochs O
, O
with O
early O
stopping O
enabled O
. O
We O
select O
the O
best O
finetuned O
model O
on O
validation O
sets O
using O
F1 B-MetricName
. O
Details O
of O
experimental O
setups O
are O
in O
Table O
A7 O
. O

Ideology O
Prediction O
. O
We O
follow O
common O
practice O
of O
using O
the O
[ O
CLS O
] O
token O
for O
standard O
finetuning O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
For O
Twitter O
and O
YouTube O
User O
data O
, O
we O
encode O
them O
using O
sliding O
windows O
and O
aggregate O
by O
mean O
pooling O
. O

Stance O
Detection O
. O
We O
follow O
Schick O
and O
Schütze O
( O
2021 O
) O
on O
using O
prompts O
to O
fine O
- O
tune O
models O
for O
stance O
detection O
. O
We O
curate O
11 O
prompts O
( O
in O
Table O
A6 O
) O
and O
choose O
the O
best O
one O
based O
on O
the O
average B-MetricName
F1 I-MetricName
by O
RoBERTa B-MethodName
on O
all O
stance O
detection O
tasks O
: O

p O
[ O
SEP O
] O
The O
stance O
towards O
{ O
target O
} O
is O
[ O
MASK O
] O
. O

The O
model O
is O
trained O
to O
predict O
[ O
MASK O
] O
for O
stance O
, O
conditioned O
on O
the O
input O
p O
and O
{ O
target O
} O
. O

Main O
Results O

Table O
3 O
presents O
F1 B-MetricName
scores I-MetricName
on O
all O
tasks O
. O
POLI B-MethodName
- I-MethodName
TICS I-MethodName
achieves O
the O
best O
overall O
average B-MetricName
F1 I-MetricName
score I-MetricName
across O
the O
board O
, O
3.6 B-MetricValue
% I-MetricValue
better O
than O
the O
strongest O
baseline O
, O
RoBERTa B-MethodName
. O
More O
importantly O
, O
POLI B-MethodName
- I-MethodName
TICS I-MethodName
alone O
outperforms O
all O
three O
baselines O
listed O
in O
§ O
5.2 O
on O
8 O
out O
of O
11 O
tasks O
, O
including O
more O
than O
10 B-MetricValue
% I-MetricValue
of O
improvement O
for O
ideology O
labeling O
on O
Hyperpartisan O
and O
Youtube O
user O
- O
level O
. O
We O
attribute O
the O
performance O
gain O
to O
our O
proposed O
ideology O
- O
driven O
pretraining O
objective O
, O
which O
helps O
capture O
partisan O
content O
. O
Note O
that O
, O
on O
some O
tasks O
, O
other O
model O
variants O
lead O
POLITICS B-MethodName
by O
a O
small O
margin O
, O
and O
this O
may O
be O
of O
interest O
to O
practitioners O
performing O
specific O
tasks O
. O

We O
further O
compare O
with O
the O
model O
proposed O
by O
Baly O
et O
al O
. O
( O
2020 O
) O
, O
which O
also O
leverages O
triplet O
loss O
as O
pretraining O
objective O
but O
on O
articles O
of O
the O
same O
topics O
. O
We O
implement O
two O
versions O
of O
their O
model O
, O
using O
the O
original O
data O
released O
by O
Baly O
et O
al O
. O
( O
2020 O
) O
7 O
and O
our O
BIGNEWSBLN B-DatasetName
. O
First O
, O
pretraining O
on O
our O
BIGNEWSBLN B-DatasetName
yields O
better O
results O
on O
ideology O
prediction O
tasks O
than O
using O
the O
original O
data O
, O
indicating O
the O
value O
of O
BIGNEWSBLN B-DatasetName
. O
Second O
, O
using O
the O
triple O
construction O
method O
by O
Baly O
et O
al O
. O
( O
2020 O
) O
with O
BIGNEWSBLN B-DatasetName
does O
not O
generalize O
well O
on O
the O
stance O
detection O
task O
, O
compared O
to O
POLITICS B-MethodName
and O
its O
variants O
. O
This O
highlights O
the O
advantage O
of O
our O
objectives O
that O
enable O
content O
comparison O
among O
articles O
of O
the O
same O
stories O
. O

Moreover O
, O
our O
ideology O
- O
driven O
objectives O
help O
acquire O
knowledge O
needed O
to O
discern O
ideology O
as O
well O
as O
stance O
detection O
. O
When O
equipping O
the O
RoBERTa O
model O
with O
ideology O
and O
story O
objectives O
but O
no O
MLM O
objective O
, O
it O
achieves O
the O
second O
best O
overall O
performance O
. O

Next O
, O
focusing O
on O
entities O
better O
identifies O
stance O
. O
Simply O
continuing O
training O
RoBERTa B-MethodName
with O
vanilla O
MLM O
objective O
( O
Random O
) O
does O
not O
yield O
performance O
gain O
on O
stance O
detection O
, O
while O
our O
upsampling O
methods O
make O
a O
difference O
, O
i.e. O
, O
increasing O
sampling O
ratios O
of O
entities O
improves O
F1 B-MetricName
by O
2 B-MetricValue
% I-MetricValue
. O

Comparisons O
with O
Previous O
State O
- O
of O
- O
the O
- O
arts O
. O

Using O
the O
original O
binary O
prediction O
setup O
( O
i.e. O
, O
hyperpartisan O
or O
not O
) O
on O
Hyperpartisan O
data O
( O
Kiesel O
et O
al O
. O
, O
2019 O
) O
, O
POLITICS B-MethodName
obtains O
an O
accuracy B-MetricName
of O
85.2 B-MetricValue
, O
leading O
previous O
state O
- O
of O
- O
the O
- O
art O
results O
by O
at O
least O
3 B-MetricValue
points O
, O
as O
shown O
in O
Table O
4 O
. O

POLITICS B-MethodName
achieves O
an O
F1 B-MetricName
of O
77.0 B-MetricValue
on O
the O
origi- O
small O
sizes O
, O
showing O
the O
potential O
effectiveness O
in O
few O
- O
shot O
learning O
, O
which O
is O
echoed O
in O
§ O
6.1 O
. O

6 O
Further O
Analyses O

Few O
- O
shot O
Learning O

We O
first O
fine O
- O
tune O
all O
PLMs O
on O
small O
numbers O
of O
samples O
. O
POLITICS B-MethodName
consistently O
outperforms O
the O
two O
counterparts O
on O
both O
tasks O
, O
using O
small O
training O
sets O
( O
Figure O
4 O
) O
. O
More O
importantly O
, O
naively O
training O
RoBERTa B-MethodName
on O
the O
large O
BIGNEWSBLN B-DatasetName
does O
not O
help O
ideology O
prediction O
. O
By O
contrast O
, O
our O
ideologydriven O
objective O
can O
better O
capture O
ideology O
than O
the O
baselines O
, O
even O
when O
using O
only O
16 O
samples O
for O
fine O
- O
tuning O
on O
the O
ideology O
tasks O
. O

Ablation O
Study O
on O
POLITICS B-MethodName

We O
show O
the O
impact O
of O
removing O
each O
ideologydriven O
pretraining O
objective O
and O
upsampling O
strategy O
from O
POLITICS B-MethodName
in O
Table O
5 O
. O
First O
, O
removing O
the O
ideology O
objective O
results O
in O
the O
most O
loss O
on O
both O
tasks O
. O
This O
again O
demonstrates O
the O
effective O
- O
ness O
of O
our O
triplet O
- O
loss O
formulation O
over O
same O
- O
story O
articles O
. O
Removing O
the O
story O
objective O
also O
hurts O
the O
overall O
performance O
by O
1 B-MetricValue
% I-MetricValue
but O
improves O
the O
ideology O
prediction O
marginally O
. O
This O
shows O
that O
the O
story O
objective O
functions O
as O
an O
auxiliary O
constraint O
to O
avoid O
over O
- O
fitting O
on O
the O
" O
shortcuts O
" O
for O
discerning O
ideologies O
. O
Moreover O
, O
removing O
upsampling O
strategies O
generally O
weakens O
POLITICS B-MethodName
's O
performance O
, O
but O
only O
to O
a O
limited O
extent O
. O

We O
also O
experiment O
with O
a O
setup O
with O
hardideology O
learning O
( O
i.e. O
, O
directly O
predicting O
the O
ideology O
of O
each O
article O
without O
using O
triplet O
- O
loss O
objectives O
) O
. O
Not O
surprisingly O
, O
this O
variant O
( O
POLITICS O
+ O
Ideo O
. O
Pred O
. O
) O
outperforms O
POLITICS B-MethodName
on O
ideology O
prediction O
since O
it O
can O
directly O
learn O
ideology O
from O
the O
annotated O
labels O
. O
However O
, O
it O
has O
been O
overfitted O
to O
ideology O
prediction O
tasks O
and O
lacks O
generalizability O
, O
thus O
yields O
worse O
performance O
on O
stance O
detection O
. O

Visualizing O
Attentions O

On O
the O
Hyperpartisan O
task O
, O
we O
visualize O
the O
last O
layer O
's O
attention O
weights O
between O
the O
[ O
CLS O
] O
token O
and O
all O
other O
tokens O
by O
POLITICS B-MethodName
and O
RoBERTa B-MethodName
pretrained O
with O
vanilla O
MLM O
on O
BIGNEWSBLN B-DatasetName
( O
Random O
) O
. O
We O
randomly O
sample O
20 O
test O
articles O
, O
and O
for O
13 O
of O
them O
, O
POLITICS B-MethodName
is O
able O
to O
capture O
salient O
entities O
, O
events O
, O
and O
sentiments O
in O
the O
text O
whereas O
Random O
can O
not O
. O
We O
present O
one O
example O
in O
Figure O
5 O
where O
POLITICS B-MethodName
captures O
" O
Ashley O
Judd O
" O
, O
" O
the O
worst O
" O
, O
and O
" O
Trump O
" O
. O
More O
examples O
are O
given O
in O
Appendix O
G. O
This O
finding O
confirms O
that O
our O
ideology O
- O
driven O
objective O
and O
upsampling O
strategies O
can O
help O
the O
model O
focus O
more O
on O
entities O
of O
political O
interest O
as O
well O
as O
better O
recognize O
sentiments O
. O

Figure O
5 O
: O
Last O
layer O
attention O
scores O
between O
[ O
CLS O
] O
token O
and O
other O
input O
tokens O
( O
aggregated O
over O
all O
heads O
) O
. O
POLITICS B-MethodName
captures O
" O
Ashley O
Judd O
" O
, O
" O
worst O
" O
, O
and O
" O
Trump O
" O
. O

POLITICS B-MethodName
on O
Different O
Ideologies O

Finally O
, O
we O
measure O
whether O
PLMs O
would O
acquire O
ideological O
bias O
as O
measured O
by O
whether O
they O
fit O
with O
languages O
used O
by O
a O
specific O
ideology O
. O
Concretely O
, O
we O
follow O
Salazar O
et O
al O
. O
( O
2020 O
) O
to O
evaluate O
PLMs O
on O
30 O
K O
held O
- O
out O
articles O
of O
different O
ideologies O
from O
BIGNEWSBLN B-DatasetName
with O
pseudoperplexity O
. O
For O
efficiency O
, O
we O
estimate O
the O
pseudo O
log O
- O
likelihood O
based O
on O
200 O
random O
tokens O
in O
each O
article O
as O
used O
by O
Wang O
and O
Cho O
( O
2019 O
) O
. O
As O
illustrated O
in O
Figure O
6 O
, O
while O
MLM O
objective O
( O
Random O
) O
is O
effective O
at O
fitting O
a O
corpus O
, O
i.e. O
, O
having O
the O
lowest O
perplexities O
, O
triplet O
- O
loss O
objectives O
act O
as O
regularizers O
during O
pretraining O
, O
shown O
by O
the O
higher O
perplexity O
of O
POLITICS B-MethodName
compared O
to O
Random O
. O
Interestingly O
, O
we O
find O
center O
and O
right O
articles O
have O
lower O
perplexity O
than O
that O
of O
left O
articles O
. O
We O
hypothesize O
that O
it O
relates O
to O
political O
science O
findings O
that O
, O
over O
recent O
periods O
of O
political O
polarization O
in O
US O
, O
Republicans O
have O
become O
somewhat O
more O
coherent O
and O
similar O
than O
Democrats O
( O
Grossmann O
and O
Hopkins O
, O
2016 O
; O
Benkler O
et O
al O
. O
, O
2018 O
) O
, O
and O
are O
thus O
easier O
to O
predict O
. O

Conclusion O

We O
study O
the O
problem O
of O
training O
general O
- O
purpose O
tools O
for O
ideology O
content O
understanding O
and O
prediction O
. O
We O
present O
POLITICS B-MethodName
, O
trained O
with O
novel O
ideology O
- O
driven O
pretraining O
objectives O
based O
on O
the O
comparisons O
of O
same O
- O
story O
articles O
written O
by O
media O
outlets O
of O
different O
ideologies O
. O
To O
facilitate O
model O
training O
, O
we O
also O
collect O
a O
large O
- O
scale O
dataset O
, O
BIGNEWS B-DatasetName
, O
consisting O
of O
news O
articles O
of O
different O
ideological O
leanings O
. O
Experiments O
on O
diverse O
datasets O
for O
ideology O
prediction O
and O
stance O
detection O
tasks O
show O
that O
POLITICS B-MethodName
outperforms O
strong O
baselines O
, O
even O
with O
a O
limited O
amount O
of O
labeled O
samples O
for O
training O
, O
and O
state O
- O
of O
- O
the O
- O
art O
models O
. O
IIS-2127747 O
, O
and O
computational O
resources O
and O
services O
provided O
by O
Advanced O
Research O
Computing O
( O
ARC O
) O
, O
a O
division O
of O
Information O
and O
Technology O
Services O
( O
ITS O
) O
at O
the O
University O
of O
Michigan O
, O
Ann O
Arbor O
. O
We O
appreciate O
the O
anonymous O
reviewers O
for O
their O
helpful O
comments O
. O
We O
thank O
the O
members O
of O
the O
LAUNCH O
group O
at O
the O
University O
of O
Michigan O
for O
discussions O
and O
suggestions O
. O
We O
also O
thank O
Changyuan O
Qiu O
for O
helping O
collect O
the O
AllSides O
articles O
, O
Siqi O
Wu O
for O
giving O
us O
access O
to O
the O
Youtube O
comments O
, O
and O
Daniel O
Preotiuc O
- O
Pietro O
for O
sharing O
the O
Twitter O
user O
accounts O
used O
in O
the O
original O
study O
to O
allow O
the O
collection O
of O
the O
corresponding O
users O
' O
tweets O
in O
this O
work O
. O

Ethical O
Considerations O

BIGNEWS B-DatasetName
Collection O

All O
news O
articles O
were O
collected O
in O
a O
manner O
consistent O
with O
the O
terms O
of O
use O
of O
the O
original O
sources O
as O
well O
as O
the O
intellectual O
property O
and O
the O
privacy O
rights O
of O
the O
original O
authors O
of O
the O
texts O
, O
i.e. O
, O
source O
owners O
. O
During O
data O
collection O
, O
the O
authors O
honored O
privacy O
rights O
of O
content O
creators O
, O
thus O
did O
not O
collect O
any O
sensitive O
information O
that O
can O
reveal O
their O
identities O
. O
All O
participants O
involved O
in O
the O
process O
have O
completed O
human O
subjects O
research O
training O
at O
their O
affiliated O
institutions O
. O
We O
also O
consulted O
Section O
107 O
8 O
of O
the O
U.S. O
Copyright O
Act O
and O
ensured O
that O
our O
collection O
action O
fell O
under O
the O
fair O
use O
category O
. O

Dataset O
Usage O

All O
of O
the O
newly O
collected O
datasets O
in O
this O
work O
will O
be O
made O
available O
upon O
request O
. O
Pretraining O
corpus O
details O
are O
included O
in O
Section O
3 O
. O
The O
other O
seven O
datasets O
used O
for O
downstream O
evaluation O
are O
obtained O
in O
the O
following O
ways O
. O
CongS O
, O
HP O
, O
BASIL O
, O
VAST O
and O
SEval O
are O
acquired O
by O
direct O
download O
. O
CongS O
is O
released O
under O
the O
ODC O
- O
BY O
1.0 O
license O
( O
free O
to O
share O
, O
create O
, O
and O
adapt O
) O
. O
HP O
and O
SEval O
are O
developed O
in O
shared O
tasks O
by O
the O
NLP O
community O
, O
which O
allow O
the O
use O
of O
copyrighted O
material O
without O
permission O
from O
the O
copyright O
holder O
for O
research O
purposes O
( O
Escartín O
et O
al O
. O
, O
2017 O
) O
. O
For O
VAST O
, O
the O
author O
explicitly O
states O
" O
We O
make O
our O
dataset O
and O
models O
available O
for O
use O
" O
. O
BASIL O
is O
developed O
by O
the O
last O
author O
and O
her O
collaborators O
. O
For O
YT O
and O
TW O
, O
we O
consult O
with O
the O
corresponding O
authors O
and O
obtain O
the O
datasets O
by O
agreeing O
that O
we O
will O
not O
further O
distribute O
them O
. O
Dataset O
details O
are O
listed O
in O
Section O
5.1 O
and O
Appendix O
D O
. O

Benefit O
and O
Potential O
Misuse O

Intended O
use O
. O
The O
models O
developed O
in O
this O
work O
can O
assist O
the O
general O
public O
to O
measure O
and O
understand O
ideological O
language O
used O
in O
diverse O
genres O
of O
texts O
. O
For O
example O
, O
POLITICS B-MethodName
can O
help O
the O
general O
public O
know O
where O
their O
representatives O
stand O
on O
key O
issues O
. O
Our O
experiments O
in O
Section O
5 O
demonstrate O
how O
POLITICS B-MethodName
would O
be O
deployed O
in O
real O
life O
when O
handling O
applications O
in O
both O
ideology O
prediction O
and O
stance O
detection O
. O
We O
deem O
that O
our O
extensive O
experiments O
have O
covered O
the O
major O
usage O
of O
POLITICS B-MethodName
. O
Failure O
mode O
is O
defined O
as O
situations O
where O
POL B-MethodName
- I-MethodName
ITICS I-MethodName
fails O
to O
correctly O
predict O
the O
ideology O
of O
an O
individual O
or O
a O
given O
text O
. O
In O
such O
cases O
, O
POLI B-MethodName
- I-MethodName
TICS I-MethodName
might O
deliver O
misinformation O
or O
cause O
mis O
- O
understanding O
towards O
a O
political O
figure O
or O
a O
policy O
. O
For O
vulnerable O
populations O
( O
e.g. O
, O
people O
who O
maybe O
not O
be O
able O
to O
make O
the O
right O
judgements O
) O
, O
the O
harm O
could O
be O
tremendously O
magnified O
when O
they O
fail O
to O
interpret O
the O
model O
outputs O
or O
blindly O
trust O
machine O
responses O
. O
Ideally O
, O
the O
interpretation O
of O
our O
model O
's O
predictions O
should O
be O
carried O
out O
within O
the O
broader O
context O
of O
the O
source O
text O
. O
Misuse O
potential O
. O
Users O
may O
mistakenly O
take O
the O
machine O
prediction O
as O
a O
golden O
rule O
or O
a O
fact O
. O
We O
would O
recommend O
any O
politics O
- O
related O
machine O
learning O
models O
, O
including O
ours O
, O
put O
up O
an O
" O
use O
with O
caution O
" O
message O
to O
encourage O
users O
to O
check O
more O
sources O
or O
consult O
political O
science O
experts O
to O
reduce O
the O
risk O
of O
being O
misled O
by O
single O
source O
. O
Moreover O
, O
POLITICS B-MethodName
might O
also O
be O
misused O
to O
label O
people O
with O
a O
specific O
political O
leaning O
that O
they O
do O
not O
want O
to O
be O
associated O
with O
. O
We O
suggest O
that O
when O
in O
use O
the O
tools O
should O
be O
accompanied O
with O
descriptions O
about O
their O
limitations O
and O
imperfect O
performance O
, O
as O
well O
as O
allow O
users O
to O
opt O
out O
from O
being O
the O
subjects O
of O
measurement O
. O
Potential O
limitation O
. O
Although O
multiple O
genres O
are O
considered O
, O
the O
genre O
coverage O
is O
not O
exhaustive O
, O
and O
does O
not O
include O
other O
trending O
media O
or O
content O
of O
different O
modalities O
for O
expressing O
opinions O
, O
such O
as O
TV O
transcripts O
, O
images O
, O
and O
videos O
. O
Thus O
, O
the O
predictive O
performance O
of O
POLITICS B-MethodName
may O
still O
be O
under O
investigated O
. O
Further O
, O
in O
downstream O
evaluation O
, O
POLITICS B-MethodName
is O
only O
trained O
and O
tested O
in O
the O
same O
domain O
, O
so O
its O
cross O
- O
genre O
ability O
needs O
further O
evaluation O
. O
Bias O
Mitigation O
. O
During O
data O
preprocessing O
, O
we O
create O
BIGNEWSBLN B-DatasetName
to O
ensure O
that O
all O
ideologies O
have O
almost O
equal O
presence O
to O
minimize O
potential O
bias O
. O
POLITICS B-MethodName
is O
not O
designed O
to O
encode O
bias O
. O
In O
Figure O
6 O
, O
the O
discrepancy O
in O
perplexities O
among O
different O
ideologies O
is O
more O
related O
to O
the O
greater O
coherence O
among O
Republicans O
than O
Democrats O
, O
rather O
than O
POLITICS B-MethodName
encoding O
biased O
knowledge O
. O

In O
conclusion O
, O
there O
is O
no O
greater O
than O
minimal O
risk O
/ O
harm O
introduced O
by O
either O
BIGNEWSBLN B-DatasetName
or O
POLITICS B-MethodName
. O
However O
, O
to O
discourage O
the O
misuse O
, O
we O
will O
always O
warn O
users O
that O
model O
predictions O
are O
for O
informational O
purpose O
only O
and O
users O
should O
always O
resort O
to O
the O
broader O
context O
to O
reduce O
the O
risk O
of O
absorbing O
biased O
information O
. O

Removing O
Non O
- O
article O
Pages O
. O
Online O
news O
websites O
also O
post O
non O
- O
news O
content O
. O
We O
remove O
such O
pages O
by O
checking O
their O
page O
titles O
and O
URLs O
based O
on O
a O
list O
of O
patterns O
. O
Sample O
patterns O
are O
shown O
in O
Table O
A1 O
. O

Removing O
Duplicate O
Pages O
. O
We O
use O
characterlevel O
edit O
distance O
to O
identify O
duplicate O
pages O
. O
Specifically O
, O
we O
use O
the O
following O
formula O
to O
calculate O
the O
difference O
between O
page O
a O
and O
page O
b O
: O

diff O
( O
a O
, O
b O
) O
= O
dist O
( O
a O
, O
b O
) O
/ O
max O
( O
len O
( O
a O
) O
, O
len O
( O
b O
) O
) O
( O
5 O
) O

where O
dist O
( O
a O
, O
b O
) O
is O
the O
Levenshtein B-MetricName
distance I-MetricName
between O
a O
and O
b. O
If O
the O
value O
is O
less O
than O
0.1 B-MetricValue
, O
we O
consider O
two O
pages O
as O
duplicates O
and O
we O
only O
keep O
the O
one O
with O
earlier O
publication O
date O
. O
Following O
this O
procedure O
, O
we O
remove O
duplicated O
pages O
within O
each O
media O
outlet O
. O
A3 O
. O

Removing O
Media O
Leaking O
Phrases O
. O
To O
prevent O
the O
model O
from O
learning O
features O
specific O
to O
individual O
media O
outlets O
, O
we O
perform O
a O
two O
- O
step O
cleaning O
. O
First O
, O
we O
mask O
phrases O
that O
mention O
the O
media O
outlet O
itself O
( O
e.g. O
, O
New O
York O
Times O
, O
NYTimes O
, O
and O
nytimes.com O
) O
. O
Second O
, O
we O
create O
a O
list O
of O
patterns O
for O
frequently O
appearing O
sentences O
( O
more O
than O
100 O
times O
) O
, O
for O
each O
media O
outlet O
. O
For O
example O
, O
as O
in O
" O
author O
currently O
serves O
as O
a O
senior O
political O
analyst O
for O
[ O
MASK O
] O
Channel O
and O
contributes O
to O
all O
major O
political O
coverage O
" O
, O
both O
the O
author O
name O
and O
the O
sentence O
itself O
can O
leak O
media O
outlet O
information O
. O
Since O
sentences O
with O
media O
leaking O
information O
usually O
appear O
at O
the O
beginning O
or O
end O
of O
the O
article O
, O
we O
remove O
any O
of O
the O
first O
and O
last O
two O
are O
enabled O
, O
we O
adopt O
alternating O
training O
strategy O
as O
in O
Ganin O
et O
al O
. O
( O
2016 O
) O
to O
apply O
these O
two O
objectives O
for O
parameter O
updates O
in O
an O
alternating O
manner O
. O

C.2 O
Fine O
- O
tuning O

For O
both O
ideology O
prediction O
and O
stance O
detection O
tasks O
, O
we O
fine O
- O
tune O
each O
model O
for O
up O
to O
10 O
epochs O
. O
We O
use O
early O
stopping O
and O
select O
the O
best O
checkpoint O
on O
validation O
set O
among O
10 O
epochs O
. O
For O
ideology O
prediction O
tasks O
, O
we O
follow O
standard O
practice O
of O
using O
[ O
CLS O
] O
token O
and O
feed O
- O
forward O
neural O
networks O
( O
FFNN O
) O
for O
classification O
. O
For O
stance O
detection O
tasks O
, O
we O
use O
prompts O
to O
fine O
- O
tune O
PLMs O
. O
We O
curate O
11 O
prompts O
as O
shown O
in O
Table O
A6 O
, O
and O
select O
the O
best O
prompt O
based O
on O
the O
performance O
of O
RoBERTa B-MethodName
. O
Fine O
- O
tuning O
hyperparameters O
are O
listed O
in O
Table O
A7 O
. O

For O
the O
SVM O
classifier O
, O
we O
use O
the O
implementation O
of O
TF O
- O
IDF O
feature O
extractor O
and O
linear O
SVM O
classifier O
in O
scikit O
- O
learn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
. O
The O
classifier O
's O
hyperparameters O
are O
listed O
in O
Table O
A8 O
. O

Models O
VAST O
F O
f O
avor O
F O
against O
F O
avg O
BERT O
- O
joint O
( O
Allaway O
and O
McKeown O
, O
2020 O
) O
54.5 O
59.1 O
65.3 O
TGA O
Net O
( O
Allaway O
and O
McKeown O
, O
2020 O
) O
57.3 O
59.0 O
66.5 O
BERT O
- O
base O
( O
Jayaram O
and O
Allaway O
, O
2021 O
) O
64 O
. O

Appendix O
D O
Downstream O
Evaluation O
Datasets O

This O
section O
lists O
more O
details O
of O
the O
eight O
datasets O
used O
in O
our O
downstream O
evaluation O
as O
well O
as O
their O
processing O
steps O
. O

D.1 O
Ideology O
Prediction O

• O
Congress O
Speech O
11 O
( O
CongS O
; O
Gentzkow O
et O
al O
. O
, O
2018 O
) O
: O
We O
filter O
out O
speeches O
with O
less O
than O
80 O
words O
and O
use O
the O
speaker O
's O
party O
affiliation O
as O
the O
ideology O
of O
the O
speech O
. O
• O
AllSides O
12 O
( O
AllS O
) O
: O
We O
crawl O
articles O
from O
AllSides O
and O
use O
the O
media O
outlet O
's O
annotated O
ideology O
as O
that O
of O
the O
article O
. O
• O
Hyperpartisan O
13 O
( O
HP O
; O
Kiesel O
et O
al O
. O
, O
2019 O
) O
: O

We O
convert O
the O
benchmark O
into O
a O
3 O
- O
way O
classification O
task O
by O
projecting O
media O
- O
level O
ideology O
annotations O
to O
articles O
. O
• O
YouTube O
( O
Wu O
and O
Resnick O
, O
2021 O
) O
contains O
cross O
- O
partisan O
discussions O
between O
liberals O
and O
conservatives O
on O
YouTube O
. O
In O
our O
experiments O
. O
we O
only O
keep O
controversial O
comments O
: O
1 O
) O
A O
video O
must O
have O
at O
least O
1,500 O
comments O
and O
150,000 O
views O
; O
2 O
) O
A O
comment O
must O
have O
at O
least O
20 O
replies O
. O
The O
original O
dataset O
annotates O
users O
' O
ideology O
on O
a O
7 O
- O
point O
scale O
. O
We O
further O
convert O
it O
into O
a O
3 O
- O
way O
classification O
task O
for O
left O
, O
center O
, O
and O
right O
ideologies O
. O
For O
the O
comment O
- O
level O
prediction O
task O
on O
YT O
( O
cmt O
. O
) O
, O
we O
use O
the O
provided O
user O
- O
level O
ideology O
annotation O
. O
For O
userlevel O
prediction O
on O
YT O
( O
user O
) O
, O
we O
concatenate O
all O
comments O
by O
a O
user O
. O
• O
Twitter O
( O
TW O
; O
Preoţiuc O
- O
Pietro O
et O
al O
. O
, O
2017 O
) O
: O

We O
crawl O
recent O
tweets O
by O
each O
user O
and O
remove O
replies O
and O
non O
- O
English O
tweets O
. O
We O
as-11 O
https O
: O
/ O
/ O
data.stanford.edu O
/ O
congress O
_ O
text O
. O

12 O
https O
: O
/ O
/ O
www.allsides.com O
. O

13 O
https O
: O
/ O
/ O
webis.de O
/ O
data O
/ O
pan O
- O
semeval O
- O
hyperpartisan O
- O
news O
- O
detection-19 O
. O
html O
. O
( O
Zarrella O
and O
Marsh O
, O
2016 O
) O
59.32 O
76.33 O
67.82 O
pkudblab O
( O
Wei O
et O
al O
. O
, O
2016 O
) O
61.98 O
72.67 O
67.33 O
SVM O
- O
ngrams O
( O
Mohammad O
et O
al O
. O
, O
2016a O
) O
62.98 O
74.98 O
68.98 O
Majority O
class O
( O
Mohammad O
et O
al O
. O
, O
2016a O
) O
sume O
users O
' O
ideologies O
do O
not O
change O
after O
their O
self O
- O
report O
since O
prior O
work O
has O
shown O
that O
people O
's O
ideology O
is O
less O
likely O
to O
change O
across O
the O
political O
spectrum O
( O
Fiorina O
and O
Abrams O
, O
2008 O
) O
. O
We O
sort O
all O
tweets O
from O
a O
user O
chronologically O
and O
concatenate O
them O
. O

D.2 O
Stance O
Detection O

• O
BASIL B-DatasetName
14 O
( O
Fan O
et O
al O
. O
, O
2019 O
) O
: O
We O
convert O
the O
original O
dataset O
such O
that O
the O
new O
tasks O
are O
to O
predict O
the O
stance O
towards O
a O
target O
at O
two O
granularities O
: O
article O
( O
art O
. O
) O
and O
sentence O
( O
sent O
. O
) O
levels O
. O
The O
targets O
in O
the O
dataset O
can O
be O
a O
person O
( O
e.g. O
, O
Donald O
Trump O
) O
or O
an O
organization O
( O
e.g. O
, O
Justice O
Department O
) O
. O
• O
VAST B-DatasetName
15 O
( O
Allaway O
and O
McKeown O
, O
2020 O
) O
predicts O
the O
stance O
of O
a O
comment O
towards O
a O
target O
. O
The O
targets O
in O
the O
dataset O
are O
noun O
phrases O
covering O
a O
broad O
range O
of O
topics O
( O
e.g. O
, O
immigration O
, O
home O
schoolers O
) O
. O
We O
notice O
the O
original O
dataset O
contains O
contradictory O
samples O
, O
where O
the O
same O
comment O
- O
target O
pair O
is O
annotated O
with O
opposite O
stances O
, O
and O
therefore O
remove O
duplicate O
and O
contradictory O
samples O
. O
• O
SemEval B-DatasetName
16 O
( O
SEval O
; O
Mohammad O
et O
al O
. O
, O
2016a O
) O
predicts O
a O
tweet O
's O
stance O
towards O
a O
target O
. O
The O
dataset O
contains O
six O
targets O
: O
Atheism O
, O
Climate O
Change O
, O
Feminist O
, O
Hillary O
Clinton O
, O
Abortion O
, O
and O
Donald O
Trump O
. O
Notably O
, O
the O
last O
target O
is O
not O
seen O
during O
training O
, O
and O
only O
appears O
in O
testing O
. O

Acknowledgments O

This O
work O
is O
supported O
in O
part O
through O
National O
Science O
Foundation O
under O
grants O
IIS-2100885 O
and O

Appendix O
A O
BIGNEWS B-DatasetName
Cleaning O
Steps O

In O
this O
section O
, O
we O
provide O
the O
details O
of O
our O
data O
cleaning O
steps O
for O
BIGNEWS B-DatasetName
. O
We O
adopt O
the O
following O
cleaning O
steps O
to O
only O
keep O
news O
articles O
that O
relate O
to O
US O
politics O
. O
paragraphs O
, O
if O
they O
contain O
a O
sentence O
that O
matches O
such O
pattern O
. O

Appendix O
B O
News O
Story O
Alignment O

As O
shown O
in O
Equation O
1 O
, O
we O
combine O
text O
similarity O
and O
entity O
similarity O
to O
be O
the O
final O
story O
similarity O
score O
. O
Only O
title O
and O
the O
first O
five O
sentences O
are O
considered O
in O
the O
calculation O
. O
We O
further O
require O
aligned O
articles O
a O
and O
b O
to O
satisfy O
two O
constraints O
: O

• O
The O
difference O
in O
publication O
dates O
of O
a O
and O
b O
is O
at O
most O
three O
days O
. O
• O
a O
and O
b O
must O
contain O
at O
least O
one O
common O
named O
entity O
in O
the O
title O
or O
in O
the O
first O
three O
sentences O
. O

We O
use O
CoreNLP O
to O
extract O
named O
entities O
in O
arti O
- O
cles O
( O
Manning O
et O
al O
. O
, O
2014 O
) O
. O
For O
the O
second O
constraint O
, O
we O
further O
apply O
Crosswikis O
to O
map O
each O
entity O
to O
a O
unique O
concept O
in O
Wikipedia O
( O
Spitkovsky O
and O
Chang O
, O
2012 O
) O
. O
When O
calculating O
entity O
similarity O
, O
we O
split O
each O
entity O
into O
single O
words O
and O
remove O
stop O
words O
. O
After O
alignment O
, O
we O
use O
the O
procedure O
described O
in O
Appendix O
A O
to O
remove O
duplicate O
articles O
in O
the O
same O
story O
cluster O
. O
The O
hyperparameters O
are O
α B-HyperparameterName
= O
0.4 B-HyperparameterValue
and O
θ B-HyperparameterName
= O
0.23 B-HyperparameterValue
. O

Evaluating O
Alignment O
Algorithm O
. O
We O
search O
the O
hyperparameters O
on O
the O
Basil O
dataset O
( O
Fan O
et O
al O
. O
, O
2019 O
) O
and O
test O
the O
algorithm O
on O
the O
Allsides O
dataset O
collected O
in O
Cao O
and O
Wang O
( O
2021 O
) O
. O
The O
Allsides O
dataset O
consists O
of O
manually O
aligned O
news O
articles O
from O
251 O
media O
outlets O
. O
After O
removing O
media O
outlets O
not O
in O
BIGNEWSBLN B-DatasetName
, O
we O
obtain O
2 O
, O
904 O
articles O
on O
1 O
, O
316 O
stories O
. O

To O
evaluate O
the O
performance O
of O
the O
alignment O
algorithm O
, O
we O
add O
the O
evaluation O
dataset O
into O
BIGNEWSBLN B-DatasetName
and O
treat O
each O
evaluation O
article O
as O
the O
anchor O
article O
for O
the O
alignment O
algorithm O
. O
We O
use O
the O
remaining O
evaluation O
articles O
in O
the O
same O
story O
as O
relevant O
articles O
, O
which O
becomes O
the O
target O
to O
be O
identified O
. O
The O
algorithm O
achieves O
0.612 B-MetricValue
mean B-MetricName
reciprocal I-MetricName
rank I-MetricName
( O
MRR B-MetricName
) O
on O
the O
Basil B-DatasetName
dataset O
and O
0.679 B-MetricValue
MRR B-MetricName
on O
the O
Allsides B-DatasetName
dataset O
. O

Appendix O
C O
Continued O
Pretraining O
and O

Fine O
- O
tuning O

C.1 O
Continued O
Pretraining O

We O
initialize O
all O
variants O
of O
POLITICS B-MethodName
with O
a O
RoBERTa B-MethodName
- I-MethodName
base I-MethodName
model O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
which O
contains O
about O
125 B-HyperparameterValue
M I-HyperparameterValue
parameters B-HyperparameterName
. O
Our O
implemen- O

Appendix O
E O
Task O
Property O

This O
section O
introduces O
detailed O
definitions O
of O
four O
properties O
, O
based O
on O
which O
we O
divide O
tasks O
into O
two O
categories O
reported O
in O
Figure O
3 O
. O

• O
Formality O
: O
Speech O
and O
news O
genres O
are O
considered O
as O
formal O
, O
and O
others O
are O
informal O
. O
• O
Training O
set O
size O
: O
Datasets O
with O
more O
than O
2,000 O
training O
samples O
are O
categorized O
as O
large O
, O
and O
small O
otherwise O
. O
• O
Document O
length O
: O
Datasets O
with O
average O
document O
length O
larger O
than O
500 O
are O
treated O
as O
" O
long O
" O
, O
and O
others O
are O
short O
. O
• O
Aggregation O
level O
: O
If O
a O
dataset O
is O
a O
collection O
of O
single O
articles O
/ O
posts O
/ O
tweets O
, O
then O
it O
is O
in O
the O
category O
of O
" O
Single O
" O
. O
If O
posts O
are O
concatenated O
and O
aggregated O
at O
user O
level O
, O
then O
it O
is O
marked O
as O
" O
User O
" O
. O
Specifically O
, O
only O
YouTube O
User O
and O
Twitter O
in O
Table O
2 O
are O
in O
the O
" O
User O
" O
category O
. O

Appendix O
F O
Comparison O
with O
Previous O
State O
- O
of O
- O
the O
- O
art O
Models O

Here O
we O
compare O
POLITICS B-MethodName
with O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
on O
three O
selected O
datasets O
: O
Hyperpartisan O
( O
§ O
5.5 O
) O
, O
VAST O
( O
§ O
F.1 O
) O
, O
and O
SemEval O
( O
§ O
F.2 O
) O
. O
§ O
F.3 O
discusses O
why O
direct O
comparisons O
are O
not O
applicable O
on O
the O
other O
5 O
datasets O
. O

F.1 O
VAST O

POLITICS B-MethodName
outperforms O
all O
previous O
state O
- O
of O
- O
theart O
models O
in O
the O
literature O
as O
well O
as O
the O
strong O
RoBERTa B-MethodName
baseline I-MethodName
, O
as O
can O
be O
seen O
in O
Table O
A9 O
. O
Following O
Allaway O
and O
McKeown O
( O
2020 O
) O
and O
Jayaram O
and O
Allaway O
( O
2021 O
) O
, O
F B-MetricName
avg I-MetricName
is O
defined O
as O
the O
macro B-MetricName
- I-MetricName
averaged I-MetricName
F1 I-MetricName
over O
all O
three O
classes O
( O
favor O
, O
against O
, O
and O
neutral O
) O
. O
The O
results O
are O
reported O
on O
the O
original O
VAST B-DatasetName
dataset O
which O
contains O
contradictory O
samples O
, O
where O
the O
same O
comment O
- O
target O
pairs O
are O
annotated O
with O
opposite O
stances O
so O
they O
are O
counted O
in O
both O
categories O
. O

F.2 O
SemEval O

Table O
A10 O
show O
the O
results O
of O
state O
- O
of O
- O
the O
- O
art O
models O
and O
POLITICS B-MethodName
on O
SemEval O
. O
Following O
Mohammad O
et O
al O
. O
( O
2016a O
) O
, O
F O
avg O
is O
defined O
as O
the O
macro B-MetricName
- I-MetricName
averaged I-MetricName
F1 I-MetricName
over O
favor O
and O
against O
classes O
. O
State O
- O
of O
- O
the O
- O
art O
models O
train O
separate O
classifiers O
, O
one O
for O
each O
target O
, O
thus O
yield O
better O
results O
than O
POLITICS B-MethodName
. O
Similar O
observation O
is O
made O
by O
Mohammad O
et O
al O
. O
( O
2016a O
) O
, O
where O
one O
single O
SVM O
that O
is O
trained O
on O
all O
five O
targets O
performs O
worse O
than O
five O
one O
- O
versus O
- O
rest O
SVM O
classifiers O
. O

F.3 O
Reasons O
for O
Inapplicable O
Comparisons O

We O
are O
unable O
to O
directly O
compare O
with O
existing O
models O
on O
datasets O
other O
than O
Hyperpartisan O
, O
VAST B-DatasetName
, O
and O
SemEval B-DatasetName
for O
the O
following O
reasons O
: O

• O
The O
original O
dataset O
either O
is O
used O
for O
different O
tasks O
that O
are O
not O
ideology O
prediction O
or O
stance O
detection O
: O
Congress O
Speech O
( O
Gentzkow O
et O
al O
. O
, O
2018 O
) O
, O
BASIL O
( O
Fan O
et O
al O
. O
, O
2019 O
) O
, O
and O
YouTube O
( O
Wu O
and O
Resnick O
, O
2021 O
) O
. O
• O
The O
dataset O
is O
newly O
collected O
( O
AllSides B-DatasetName
) O
or O
contains O
newly O
collected O
samples O
( O
Twitter O
; O
Preoţiuc O
- O
Pietro O
et O
al O
. O
, O
2017 O
) O
. O

Appendix O
G O
Visualize O
Attention O
Weights O

UNISUMM B-MethodName
and O
SUMMZOO B-DatasetName
: O
Unified O
Model O
and O
Diverse O
Benchmark O
for O
Few B-TaskName
- I-TaskName
Shot I-TaskName
Summarization I-TaskName

The O
high O
annotation O
costs O
and O
diverse O
demands O
of O
various O
summarization O
tasks O
motivate O
the O
development O
of O
few O
- O
shot O
summarization O
. O
However O
, O
despite O
the O
emergence O
of O
many O
summarization O
tasks O
and O
datasets O
, O
the O
current O
training O
paradigm O
for O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
systems O
ignores O
potentially O
shareable O
knowledge O
in O
heterogeneous O
datasets O
. O
To O
this O
end O
, O
we O
propose O
UNISUMM B-MethodName
, O
a O
unified O
few O
- O
shot O
summarization O
model O
pre O
- O
trained O
with O
multiple O
summarization O
tasks O
and O
can O
be O
prefix O
- O
tuned O
to O
excel O
at O
any O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
task O
. O
Meanwhile O
, O
to O
better O
evaluate O
few O
- O
shot O
summarizers O
, O
under O
the O
principles O
of O
diversity O
and O
robustness O
, O
we O
assemble O
and O
release O
a O
new O
benchmark O
SUMM B-DatasetName
- I-DatasetName
ZOO I-DatasetName
. O
It O
consists O
of O
8 O
summarization O
tasks O
with O
multiple O
sets O
of O
few O
- O
shot O
samples O
for O
each O
task O
, O
covering O
diverse O
domains O
. O
Experimental O
results O
and O
analysis O
show O
that O
UNISUMM B-MethodName
outperforms O
strong O
baselines O
by O
a O
large O
margin O
across O
all O
sub O
- O
tasks O
in O
SUMMZOO B-DatasetName
under O
both O
automatic O
and O
human O
evaluations O
and O
achieves O
comparable O
results O
in O
human B-MetricName
evaluation I-MetricName
compared O
with O
a O
GPT-3.5 B-MethodName
model O
. O

Introduction O

There O
has O
been O
a O
recent O
surge O
of O
interest O
in O
summarizers O
based O
on O
large O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
Yang O
et O
al O
. O
, O
2020 O
; O
Zhong O
et O
al O
. O
, O
2020 O
; O
Yu O
et O
al O
. O
, O
2022 O
; O
Wang O
et O
al O
. O
, O
2023 O
) O
, O
where O
various O
summarization O
tasks O
( O
the O
term O
task O
later O
in O
this O
paper O
refers O
to O
a O
specific O
summarization O
task O
, O
e.g. O
, O
query O
- O
focused O
meeting O
summarization O
, O
which O
is O
usually O
associated O
with O
a O
corresponding O
dataset O
, O
e.g. O
, O
QMSum O
, O
unless O
otherwise O
specified O
. O
) O
have O
been O
proposed O
to O
meet O
different O
practical O
demands O
, O
such O
as O
comprehending O
different O
inputs O
( O
e.g. O
, O
news O
( O
Fabbri O
et O
al O
. O
, O
2019 O
) O
and O
dialogue O
( O
Zhong O
et O
al O
. O
, O
2022a O
) O
) O
and O
generating O
different O
outputs O
( O
e.g. O
, O
headlines O
( O
Zhang O
and O
* O
Yulong O
Chen O
completed O
this O
work O
during O
his O
internship O
at O
Microsoft O
. O
† O
Yang O
Liu O
is O
the O
corresponding O
author O
. O

How O
to O
summarize O
their O
talk O
? O

Attention O
is O
All O
You O
Need O

Paper O
Summ O

Figure O
1 O
: O
The O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
scenario O
in O
this O
paper O
. O
We O
are O
interested O
in O
how O
to O
re O
- O
use O
previous O
datasets O
( O
e.g. O
, O
CNNDM B-DatasetName
) O
to O
improve O
the O
few O
- O
shot O
performance O
on O
unseen O
target O
tasks O
( O
e.g. O
, O
DIALOGSUM B-DatasetName
) O
. O

Tetreault O
, O
2019 O
) O
and O
paragraphs O
( O
Perez O
- O
Beltrachini O
and O
Lapata O
, O
2021 O
) O
) O
. O
Because O
annotating O
gold O
summaries O
for O
newly O
- O
proposed O
summarization O
tasks O
is O
costly O
( O
Sen O
et O
al O
. O
, O
2008 O
; O
, O
fewshot B-TaskName
summarization I-TaskName
, O
the O
task O
of O
building O
a O
model O
for O
a O
specific O
summarization O
scenario O
using O
very O
limited O
ground O
- O
truth O
data O
( O
Chen O
and O
Shuai O
, O
2021 O
) O
, O
has O
gained O
increasing O
attention O
from O
the O
research O
community O
( O
Fabbri O
et O
al O
. O
, O
2021 O
; O
Logan O
IV O
et O
al O
. O
, O
2022 O
; O
He O
et O
al O
. O
, O
2022 O
) O
. O

Recently O
, O
prefix O
- O
tuning O
( O
Li O
and O
Liang O
, O
2021 O
) O
has O
established O
strong O
baselines O
on O
many O
few O
- O
shot O
natural O
language O
generation O
tasks O
, O
including O
summarization O
. O
The O
main O
idea O
is O
to O
extract O
knowledge O
from O
PLMs O
by O
prepending O
and O
tuning O
additional O
parameters O
( O
prefixes O
) O
before O
each O
layer O
of O
the O
PLM O
. O
Work O
has O
been O
done O
to O
improve O
the O
performance O
by O
designing O
more O
sophisticated O
prefixes O
( O
Ghazvininejad O
et O
al O
. O
, O
2022 O
; O
. O
Despite O
being O
effective O
, O
PLMs O
can O
have O
limited O
summarization O
knowledge O
due O
to O
the O
salient O
gap O
between O
pre O
- O
training O
objectives O
( O
e.g. O
, O
language O
modeling O
) O
and O
summarization O
objectives O
( O
Aribandi O
et O
al O
. O
, O
2022 O
) O
. O
In O
addition O
, O
existing O
summarization O
datasets O
can O
provide O
relevant O
knowledge O
to O
newly O
- O
proposed O
summarization O
tasks O
, O
and O
therefore O
benefit O
sum O
- O
marization O
tasks O
, O
especially O
under O
the O
few O
- O
shot O
scenario O
. O
However O
, O
existing O
work O
tends O
to O
tune O
PLMs O
directly O
on O
a O
new O
task O
, O
without O
exploiting O
cross O
- O
task O
knowledge O
from O
summarization O
datasets O
, O
which O
may O
limit O
the O
generalization O
and O
adaptation O
abilities O
of O
models O
( O
Zhong O
et O
al O
. O
, O
2019 O
; O
. O

We O
address O
these O
issues O
by O
proposing O
a O
unified O
few O
- O
shot O
summarization O
framework O
, O
UNISUMM B-MethodName
. O
The O
idea O
is O
to O
combine O
multi O
- O
task O
pre O
- O
training O
( O
Chen O
and O
Shuai O
, O
2021 O
) O
on O
existing O
summarization O
datasets O
with O
few O
- O
shot O
prefixtuning O
( O
Li O
and O
Liang O
, O
2021 O
) O
on O
target O
tasks O
. O
To O
this O
end O
, O
we O
first O
build O
a O
multi O
- O
task O
model O
based O
on O
a O
Transformer O
- O
based O
language O
model O
as O
the O
backbone O
and O
equip O
it O
with O
task O
- O
specific O
prefix O
vectors O
, O
and O
then O
pre O
- O
train O
the O
multi O
- O
task O
model O
on O
diverse O
summarization O
datasets O
. O
In O
this O
stage O
, O
we O
optimize O
the O
summarization O
model O
together O
with O
task O
- O
specific O
prefixes O
and O
also O
a O
universal O
prefix O
, O
using O
an O
asymmetrical O
weight B-HyperparameterName
decay I-HyperparameterName
strategy O
. O
Using O
prefixes O
in O
the O
multi O
- O
task O
pre O
- O
training O
stage O
leads O
to O
two O
advantages O
: O
First O
, O
the O
mixture O
of O
shared O
summarization O
parameters O
and O
unique O
task O
- O
specific O
parameters O
helps O
to O
leverage O
natural O
benefits O
across O
datasets O
( O
Ruder O
, O
2017 O
) O
. O
Second O
, O
the O
pre O
- O
trained O
prefixes O
can O
be O
tuned O
to O
serve O
as O
a O
knob O
for O
the O
second O
stage O
of O
prefix O
- O
tuning O
on O
unseen O
tasks O
. O
When O
facing O
an O
unseen O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
task O
, O
we O
freeze O
the O
multi O
- O
task O
learned O
backbone O
model O
and O
use O
the O
universal O
prefix O
as O
initialization O
for O
prefix O
- O
tuning O
. O

A O
data O
obstacle O
for O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
research O
is O
the O
lack O
of O
a O
benchmark O
for O
fair O
comparison O
. O
Previous O
studies O
either O
focus O
on O
one O
type O
of O
data O
, O
e.g. O
, O
news O
text O
, O
or O
train O
their O
systems O
on O
non O
- O
public O
few O
- O
shot O
samples O
. O
However O
, O
because O
few O
- O
shot O
models O
can O
be O
highly O
sensitive O
to O
training O
data O
, O
the O
selection O
of O
different O
few O
- O
shot O
samples O
in O
different O
papers O
can O
lead O
to O
ambiguous O
comparisons O
( O
a.k.a O
. O
Sample O
Selection O
Bias O
( O
Cortes O
et O
al O
. O
, O
2008 O
) O
) O
. O
To O
address O
these O
issues O
, O
we O
assemble O
and O
release O
a O
new O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
benchmark O
, O
SUMMZOO B-DatasetName
, O
following O
two O
principles O
, O
namely O
diversity O
of O
tasks O
and O
robustness O
of O
evaluation O
. O
SUMMZOO B-DatasetName
collects O
summarization O
data O
from O
8 O
existing O
datasets O
, O
which O
are O
diverse O
in O
terms O
of O
domain O
( O
news O
, O
academic O
papers O
, O
meetings O
, O
etc O
. O
) O
, O
format O
( O
single O
- O
document O
and O
multi O
- O
document O
) O
, O
and O
length O
on O
both O
source O
and O
target O
sides O
. O
For O
more O
robust O
evaluation O
, O
for O
each O
task O
, O
SUMMZOO B-DatasetName
provides O
5 O
different O
( O
randomly O
sampled O
) O
few O
- O
shot O
training O
sets O
, O
and O
requires O
all O
systems O
to O
report O
their O
averaged O
results O
. O
Finally O
, O
SUMMZOO B-DatasetName
includes O
10 O
- O
shot O
and O
100 O
- O
shot O
settings O
. O

We O
compare O
UNISUMM B-MethodName
against O
several O
strong O
baselines O
, O
including O
a O
GPT-3.5 B-MethodName
model O
( O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
) O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Ouyang O
et O
al O
. O
, O
2022 O
) O
, O
on O
SUMMZOO B-DatasetName
and O
conduct O
thorough O
analysis O
. O
Experimental O
results O
of O
automatic O
evaluation O
metrics O
show O
that O
UNISUMM B-MethodName
outperforms O
baselines O
across O
all O
sub O
- O
stasks O
and O
human O
evaluation O
shows O
that O
UNISUMM B-MethodName
achieves O
better O
performance O
than O
baselines O
of O
similar O
sizes O
and O
comparable O
performance O
compared O
with O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
. O
Additionally O
, O
UNISUMM B-MethodName
is O
empirically O
found O
to O
be O
more O
stable O
and O
robust O
when O
facing O
different O
few O
- O
shot O
samples O
. O
Analysis O
shows O
that O
combining O
multi O
- O
task O
pre O
- O
training O
and O
few O
- O
shot O
prefix O
- O
tuning O
is O
essential O
to O
the O
performance O
of O
UNISUMM B-MethodName
and O
other O
techniques O
, O
such O
as O
universal O
prefix O
and O
asymmetrical O
weight O
decay O
strategy O
, O
can O
all O
improve O
its O
generalization O
ability O
. O
We O
release O
our O
code O
, O
model O
and O
benchmark O
at O
https O
: O
/ O
/ O
github.com O
/ O
microsoft O
/ O
UniSumm B-MethodName
. O

Related O
Work O

Few B-TaskName
- I-TaskName
shot I-TaskName
Summarization I-TaskName
A O
critical O
challenge O
for O
neural O
summarizers O
is O
that O
they O
are O
data O
- O
hungry O
and O
require O
large O
- O
scale O
annotated O
data O
. O
To O
alleviate O
the O
data O
sparsity O
issue O
, O
Fabbri O
et O
al O
. O
( O
2021 O
) O
extract O
characteristics O
of O
the O
target O
dataset O
and O
build O
pseudo O
summaries O
from O
the O
Wikipedia O
corpus O
. O
Small O
plug O
- O
in O
networks O
( O
Bražinskas O
et O
al O
. O
, O
2020 O
) O
are O
injected O
into O
PLMs O
to O
predict O
the O
properties O
of O
the O
target O
dataset O
with O
only O
a O
small O
amount O
of O
labeled O
instances O
. O
To O
close O
the O
gap O
between O
pretraining O
and O
fine O
- O
tuning O
, O
propose O
a O
second O
stage O
of O
pre O
- O
training O
before O
fine O
- O
tuning O
with O
large O
- O
scale O
generative O
models O
. O
Such O
challenges O
of O
summarization O
have O
also O
been O
explored O
in O
the O
cross O
- O
lingual O
setting O
( O
Wang O
et O
al O
. O
, O
2022 O
; O
Chen O
et O
al O
. O
, O
2022b O
) O
. O
Although O
transfer O
learning O
methods O
make O
use O
of O
external O
data O
, O
one O
still O
needs O
to O
carefully O
select O
source O
domains O
and O
tasks O
to O
avoid O
negative O
transfer O
( O
Gururangan O
et O
al O
. O
, O
2020 O
; O
Pilault O
et O
al O
. O
, O
2020 O
) O
. O
Compared O
with O
them O
, O
UNISUMM B-MethodName
can O
be O
easily O
prefix O
- O
tuned O
to O
any O
target O
tasks O
without O
the O
effort O
of O
building O
large O
pseudo O
data O
or O
selecting O
relevant O
data O
. O
To O
our O
knowledge O
, O
we O
are O
the O
first O
to O
combine O
prefix O
- O
tuning O
and O
multi O
- O
task O
learning O
for O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
, O
showing O
very O
positive O
2 O
12834 O
results O
. O

Existing O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
evaluation O
suffers O
from O
two O
data O
- O
related O
problems O
. O
First O
, O
previous O
studies O
usually O
focus O
on O
only O
one O
type O
of O
summarization O
tasks O
in O
their O
experiments O
( O
Bražinskas O
et O
al O
. O
, O
2020 O
; O
. O
Thus O
, O
it O
is O
difficult O
to O
evaluate O
their O
generalization O
ability O
. O
Second O
, O
the O
few O
- O
shot O
settings O
and O
selections O
of O
few O
- O
shot O
samples O
are O
miscellaneous O
, O
which O
makes O
evaluations O
from O
different O
research O
papers O
not O
comparable O
with O
each O
other O
( O
Cortes O
et O
al O
. O
, O
2008 O
) O
. O
Therefore O
, O
in O
this O
work O
, O
we O
propose O
SUMMZOO B-DatasetName
for O
better O
benchmarking O
future O
research O
on O
few O
- O
shot O
summarization O
. O
To O
our O
knowledge O
, O
SUMMZOO B-DatasetName
is O
the O
first O
public O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
benchmark O
that O
covers O
a O
set O
of O
diverse O
summarization O
tasks O
. O

Prompt O
Learning O
for O
Text O
Generation O
The O
idea O
of O
prompt O
learning O
is O
first O
proposed O
in O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
where O
it O
aims O
to O
guide O
PLMs O
to O
do O
different O
tasks O
without O
further O
fine O
- O
tuning O
by O
prepending O
task O
- O
related O
examples O
to O
the O
input O
and O
has O
shown O
positive O
results O
on O
many O
text O
generation O
tasks O
, O
including O
summarization O
( O
Goyal O
et O
al O
. O
, O
2022 O
) O
. O
Prefix O
- O
tuning O
extends O
this O
idea O
from O
discrete O
tokens O
to O
continuous O
vectors O
( O
Li O
and O
Liang O
, O
2021 O
) O
. O
It O
adds O
continuous O
embeddings O
( O
prefixes O
) O
to O
each O
Transformer O
layer O
as O
external O
value O
and O
key O
vectors O
. O
During O
training O
, O
only O
prefixes O
are O
updated O
while O
the O
other O
parameters O
are O
unchanged O
. O
Logan O
IV O
et O
al O
. O
( O
2022 O
) O
and O
Gu O
et O
al O
. O
( O
2022 O
) O
propose O
to O
use O
pre O
- O
training O
to O
boost O
the O
low O
performance O
for O
few O
- O
shot O
learning O
. O
combines O
the O
transfer O
learning O
and O
prompt O
learning O
for O
text O
generation O
. O
Compared O
with O
them O
, O
we O
are O
interested O
in O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
and O
propose O
multi O
- O
task O
pre O
- O
training O
as O
an O
effective O
strategy O
to O
make O
use O
of O
data O
from O
related O
tasks O
to O
improve O
performance O
of O
diverse O
target O
tasks O
, O
which O
suits O
real O
- O
life O
scenarios O
. O

Method O

Following O
Chen O
and O
Shuai O
( O
2021 O
) O
, O
the O
task O
of O
fewshot B-TaskName
text I-TaskName
summarization I-TaskName
is O
defined O
as O
follows O
. O
For O
an O
unseen O
target B-TaskName
summarization I-TaskName
task O
u O
, O
few B-TaskName
- I-TaskName
shot I-TaskName
text I-TaskName
summarization I-TaskName
is O
to O
generate O
a O
summary O
Y O
, O
given O
an O
input O
text O
X O
, O
by O
learning O
from O
a O
limited O
number O
k O
( O
k O
≤ O
100 O
typically O
) O
of O
labeled O
training O
instances O
of O
u O
, O
with O
the O
help O
of O
general O
knowledge O
K O
. O

The O
overall O
framework O
of O
UNISUMM B-MethodName
is O
shown O
in O
Figure O
2 O
. O
It O
consists O
of O
2 O
phases O
: O
1 O
) O
Learning O
general O
knowledge O
by O
multi O
- O
task O
pre O
- O
training O
on O
existing O
summarization O
datasets O
( O
§ O
3.1 O
) O
and O
; O
2 O
) O
Learning O
target O
task O
knowledge O
by O
prefix O
- O
tuning O
on O
each O
target O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
dataset O
( O
§ O
3.2 O
) O
. O

Multi O
- O
Task O
Pre O
- O
Training O
with O
Prefix O

As O
shown O
in O
Figure O
2 O
( O
a O
) O
, O
in O
the O
first O
stage O
, O
we O
take O
a O
Transformer O
- O
based O
pre O
- O
trained O
language O
encoderdecoder O
model O
( O
for O
example O
, O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
) O
M O
= O
[ O
M O
en O
; O
M O
de O
] O
as O
the O
summarization O
model O
, O
parameterized O
by O
θ O
. O
We O
further O
pre O
- O
train O
this O
model O
on O
a O
set O
of O
popular O
summarization O
datasets O
( O
e.g. O
, O
CNNDM B-DatasetName
, O
PubMed B-DatasetName
and O
XWikis B-DatasetName
) O
to O
learn O
general O
summarization O
knowledge O
. O
For O
each O
task O
t O
, O
we O
inject O
task O
- O
specific O
prefix O
vectors O
of O
encoder O
( O
P O
t O
en O
) O
and O
decoder O
( O
P O
t O
de O
) O
, O
P O
t O
= O
[ O
P O
t O
en O
; O
P O
t O
de O
] O
, O
into O
the O
model O
, O
parameterized O
by O
θ O
p O
t O
. O
Following O
( O
Li O
and O
Liang O
, O
2021 O
) O
, O
the O
prefix O
vectors O
are O
prepended O
to O
each O
Transformer O
layer O
of O
M O
as O
additional O
key O
and O
value O
vectors O
as O
: O
[ O
P O
t O
en O
; O
M O
en O
; O
P O
t O
de O
; O
M O
de O
] O
. O

12835 O

For O
all O
pre O
- O
training O
tasks O
, O
given O
input O
text O
X O
, O
the O
multi O
- O
task O
optimization O
objective O
is O
to O
minimize O
the O
negative O
log O
- O
likelihood O
of O
generating O
the O
target O
summary O
Y O
= O
{ O
y O
1 O
, O
y O
2 O
, O
... O
y O
|Y O
| O
} O
: O

L O
( O
θ O
, O
θ O
p O
t O
) O
= O
|Y O
| O
i O
log O
P O
( O
y O
i O
|X O
, O
y O
1 O
, O
• O
• O
• O
, O
y O
i−1 O
) O
. O
( O
1 O
) O

In O
the O
multi O
- O
task O
pre O
- O
training O
stage O
, O
we O
optimize O
θ O
and O
θ O
p O
t O
together O
. O

Prefix O
- O
Tuning O

Through O
multi O
- O
task O
pre O
- O
training O
, O
we O
obtain O
the O
UNISUMM B-MethodName
model O
with O
diverse O
summarization O
knowledge O
. O
As O
shown O
in O
Figure O
2 O
( O
b O
) O
, O
for O
an O
unseen O
summarization O
task O
u O
( O
for O
example O
, O
Wikihow B-DatasetName
or O
MultiNews B-DatasetName
) O
, O
given O
only O
k O
training O
samples O
, O
we O
conduct O
prefix O
- O
tuning O
( O
Li O
and O
Liang O
, O
2021 O
) O
on O
the O
UNISUMM B-MethodName
model O
. O
A O
new O
- O
task O
prefix O
P O
u O
= O
[ O
P O
u O
en O
; O
P O
u O
de O
] O
is O
created O
, O
parameterized O
by O
θ O
p O
u O
, O
which O
can O
be O
either O
initialized O
randomly O
or O
from O
a O
prefix O
of O
pre O
- O
training O
tasks O
. O
We O
then O
freeze O
the O
parameters O
θ O
of O
the O
shared O
summarization O
model O
and O
only O
tune O
θ O
p O
u O
using O
the O
objective O
defined O
in O
Equation O
1 O
. O
By O
doing O
this O
, O
we O
can O
maximize O
the O
learned O
summarization O
knowledge O
in O
UNISUMM B-MethodName
and O
also O
avoid O
over O
- O
fitting O
the O
model O
to O
very O
few O
samples O
. O

Universal O
Prefix O

Empirically O
, O
given O
a O
target O
task O
, O
initializing O
newtask O
prefix O
from O
the O
most O
related O
pre O
- O
training O
tasks O
can O
be O
helpful O
. O
However O
, O
for O
a O
brand O
new O
task O
, O
selecting O
meta O
tasks O
can O
be O
a O
complicated O
process O
, O
which O
requires O
large O
efforts O
of O
feature O
engineering O
( O
Chen O
and O
Shuai O
, O
2021 O
) O
. O
Therefore O
, O
during O
multi O
- O
task O
pre O
- O
training O
, O
we O
also O
pre O
- O
train O
a O
universal O
prefix O
, O
which O
can O
be O
used O
as O
a O
stable O
initialization O
for O
few O
- O
shot O
prefix O
- O
tuning O
. O

In O
particular O
, O
during O
multi O
- O
task O
pretraining O
( O
§ O
3.1 O
) O
, O
we O
initialize O
a O
universal O
encoder O
and O
decoder O
prefix O
vector O
P O
* O
= O
[ O
P O
* O
en O
; O
P O
* O
de O
] O
, O
parameterized O
by O
θ O
p O
* O
. O
For O
each O
training O
instance O
from O
task O
t O
, O
it O
has O
a O
15 O
% O
probability O
to O
be O
coupled O
with O
this O
universal O
prefix O
vector O
instead O
of O
its O
task O
- O
specific O
prefix O
P O
t O
. O
The O
parameters O
θ O
p O
* O
are O
optimized O
together O
with O
θ O
. O
Then O
in O
prefix O
- O
tuning O
, O
we O
use O
this O
universal O
vector O
as O
initialization O
for O
the O
unseen O
task O
parameter O
θ O
p O
u O
( O
§ O
3.2 O
) O
. O

Asymmetrical O
Weight B-HyperparameterName
Decay I-HyperparameterName

A O
potential O
problem O
in O
multi O
- O
task O
learning O
is O
the O
negative O
transfer O
among O
different O
pre O
- O
training O
tasks O
. O

To O
alleviate O
this O
, O
inspired O
by O
previous O
work O
( O
Evgeniou O
and O
Pontil O
, O
2004 O
; O
Bengio O
, O
2012 O
; O
, O
we O
set O
different O
weight B-HyperparameterName
decay I-HyperparameterName
regularizations O
on O
different O
parameters O
of O
UNISUMM B-MethodName
. O
Specifically O
, O
we O
separate O
optimizers O
of O
the O
prefixes O
and O
the O
summarization O
model O
in O
pre O
- O
training O
. O
We O
assign O
a O
lower O
weight B-HyperparameterName
decay I-HyperparameterName
value O
d B-HyperparameterName
p I-HyperparameterName
= O
0.01 B-HyperparameterValue
on O
the O
prefix O
optimizer O
, O
enabling O
prefixes O
to O
flexibly O
learn O
task O
- O
specific O
knowledge O
, O
and O
a O
higher O
weight B-HyperparameterName
decay I-HyperparameterName
value O
d B-HyperparameterName
l I-HyperparameterName
= O
0.05 B-HyperparameterValue
on O
the O
summarization O
model O
optimizer O
, O
enforcing O
it O
to O
learn O
a O
broader O
generalization O
across O
different O
tasks O
. O

Formally O
, O
at O
training O
step O
i O
: O

θ O
i+1 O
= O
( O
1 O
− O
d B-HyperparameterName
l I-HyperparameterName
) O
θ O
i O
− O
α B-HyperparameterName
i I-HyperparameterName
∇f O
i O
( O
θ O
i O
) O
, O
θ O
i+1 O
p O
= O
( O
1 O
− O
d B-HyperparameterName
p I-HyperparameterName
) O
θ O
i O
p O
− O
α B-HyperparameterName
i I-HyperparameterName
p O
∇f O
i O
p O
( O
θ O
i O
p O
) O
, O
( O
2 O
) O

where O
α B-HyperparameterName
i I-HyperparameterName
and O
α B-HyperparameterName
i I-HyperparameterName
p I-HyperparameterName
are O
the O
learning B-HyperparameterName
rates I-HyperparameterName
for O
summarization O
model O
and O
prefix O
, O
and O
∇f O
i O
( O
θ O
i O
) O
and O
∇f O
i O
p O
( O
θ O
i O
p O
) O
are O
the O
batch O
gradient O
for O
summarization O
model O
and O
prefix O
. O

The O
SUMMZOO B-DatasetName
Benchmark O

SUMMZOO B-DatasetName
is O
sourced O
from O
existing O
summarization B-TaskName
benchmark O
based O
on O
the O
principles O
of O
diversity O
and O
robustness O
, O
where O
we O
assemble O
each O
dataset O
into O
few O
- O
shot O
evaluation O
settings O
. O

Diversity O
of O
Tasks O
As O
a O
major O
goal O
, O
we O
ensure O
that O
SUMMZOO B-DatasetName
can O
include O
a O
diversity O
of O
different O
summarization O
tasks O
, O
covering O
multiple O
domains O
, O
text O
styles O
and O
compression O
ratios O
. O
Thus O
, O
we O
carefully O
select O
8 O
summarization B-TaskName
tasks O
including O
monologue B-TaskName
/ I-TaskName
dialogue I-TaskName
texts I-TaskName
and I-TaskName
single I-TaskName
/ I-TaskName
multi I-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
tasks O
. O
Their O
domains O
also O
span O
an O
assorted O
set O
such O
as O
news O
, O
scientific O
papers O
, O
instructions O
, O
online O
forums O
and O
meetings O
. O

Robustness O
of O
Evaluation O

Our O
second O
goal O
is O
to O
ensure O
that O
experiments O
on O
SUMMZOO B-DatasetName
can O
be O
compared O
with O
each O
other O
in O
a O
robust O
manner O
. O
Also O
, O
we O
want O
to O
reduce O
the O
randomness O
from O
different O
selections O
of O
few O
- O
shot O
samples O
. O
Therefore O
, O
for O
each O
task O
, O
we O
provide O
5 O
sets O
of O
few O
- O
shot O
training O
samples O
, O
and O
we O
ask O
all O
models O
to O
train O
on O
these O
5 O
sets O
respectively O
and O
report O
their O
averaged O
results O
and O
standard O
deviations O
. O
We O
also O
formulate O
two O
fewshot O
training O
settings O
with O
the O
number O
of O
shots O
k O
set O
to O
10 O
or O
100 O
, O
where O
the O
first O
can O
be O
considered O
as O
a O
more O
extreme O
low O
- O
resource O
scenario O
while O
the O
second O
is O
a O
more O
commonly O
tested O
setting O
. O
Table O
1 O
summarizes O
the O
statistics O
of O
sub O
- O
datasets O
in O
SUMMZOO B-DatasetName
. O
The O
detailed O
descriptions O
of O
each O
dataset O
can O
be O
found O
in O
Appendix O
A O
. O

Experimental O
Setup O

Training O
Datasets O

For O
multi O
- O
task O
pre O
- O
training O
( O
§ O
3.1 O
) O
, O
we O
use O
a O
combination O
of O
seven O
summarization O
datasets O
: O
CN B-DatasetName
- I-DatasetName
NDM I-DatasetName
( O
Nallapati O
et O
al O
. O
, O
2016 O
) O
, O
BillSum B-DatasetName
( O
Kornilova O
and O
Eidelman O
, O
2019 O
) O
, O
PubMed B-DatasetName
( O
Cohan O
et O
al O
. O
, O
2018 O
) O
, O
GovReport B-DatasetName
( O
Huang O
et O
al O
. O
, O
2021 O
) O
, O
MediaSum B-DatasetName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
SummScreen B-DatasetName
( O
Chen O
et O
al O
. O
, O
2022a O
) O
and O
XWikis B-DatasetName
( O
Perez O
- O
Beltrachini O
and O
Lapata O
, O
2021 O
) O
. O

To O
balance O
the O
training O
data O
size O
of O
different O
datasets O
, O
we O
perform O
down O
- O
sampling O
on O
over O
- O
sized O
datasets O
and O
up O
- O
sampling O
on O
low O
- O
resource O
datasets O
respectively O
. O
The O
detailed O
descriptions O
of O
each O
dataset O
and O
statistics O
of O
resulting O
data O
for O
pretraining O
are O
shown O
in O
Appendix O
B O
and O
Table O
8 O
. O

Baseline O
Models O

PEGASUS B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
is O
a O
large O
pretrained O
encoder O
- O
decoder O
model O
, O
which O
is O
particularly O
designed O
for O
text B-TaskName
summarization I-TaskName
. O
The O
model O
is O
trained O
using O
the O
gap O
sentence O
generation O
task O
. O
We O
use O
PEGASUS B-MethodName
LARGE I-MethodName
( O
C4+HugeNews B-DatasetName
) O
1 O
for O
comparison O
, O
which O
improves O
upon O
the O
results O
reported O
in O
the O
original O
paper O
. O

BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
is O
a O
pre O
- O
trained O
encoder O
- O
decoder O
language O
model O
using O
selfdenoising O
tasks O
. O
We O
compare O
with O
the O
BARTlarge B-MethodName
model O
2 O
with O
two O
tuning O
strategies O
on O
fewshot B-TaskName
summarization I-TaskName
tasks O
, O
namely O
standard O
finetuning O
( O
BART B-MethodName
- I-MethodName
FT I-MethodName
) O
and O
prefix O
- O
tuning O
( O
BART B-MethodName
- I-MethodName
PT I-MethodName
) O
. O

1 O
https O
: O
/ O
/ O
huggingface.co O
/ O
google O
/ O
pegasus-large B-MethodName
2 O
https O
: O
/ O
/ O
huggingface.co O
/ O
facebook O
/ O
bart-large B-MethodName
In O
BART B-MethodName
- I-MethodName
PT I-MethodName
, O
the O
prefix O
vector O
is O
added O
in O
the O
same O
way O
as O
in O
UNISUMM B-MethodName
. O

MultiBART B-MethodName
is O
a O
variant O
of O
BART B-MethodName
- I-MethodName
large I-MethodName
. O
Similar O
to O
UNISUMM B-MethodName
, O
it O
is O
first O
multi O
- O
task O
pre O
- O
trained O
on O
the O
same O
data O
( O
§ O
5.1 O
) O
but O
without O
prefixes O
. O
And O
it O
can O
also O
be O
fine O
- O
tuned O
or O
prefix O
- O
tuned O
to O
fit O
fewshot B-TaskName
summarization I-TaskName
tasks O
. O
We O
only O
show O
the O
results O
of O
prefix O
- O
tuned O
MultiBART B-MethodName
because O
we O
find O
finetuning O
the O
entire O
MultiBART B-MethodName
model O
always O
leads O
to O
worse O
performance O
in O
the O
few O
- O
shot O
setting O
. O
This O
strong O
baseline O
can O
be O
considered O
as O
an O
indicator O
to O
verify O
the O
effectiveness O
of O
using O
prefixes O
in O
both O
multi O
- O
task O
pre O
- O
training O
and O
few O
- O
shot O
tuning O
. O

Text B-MethodName
- I-MethodName
davinci-002 I-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Ouyang O
et O
al O
. O
, O
2022 O
) O
is O
a O
large O
language O
model O
( O
175B O
) O
from O
the O
GPT-3.5 B-MethodName
family O
, O
3 O
using O
instruction O
tuning O
, O
and O
has O
shown O
great O
zero- O
/ O
few O
- O
shot O
performance O
on O
many O
NLP O
tasks O
, O
including O
summarization B-TaskName
. O
Specifically O
, O
recent O
work O
finds O
that O
GPT-3.5 B-MethodName
models O
can O
show O
much O
better O
performance O
with O
the O
technique O
of O
in O
- O
context O
learning O
( O
ICL O
) O
( O
Brown O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2022a O
) O
. O
We O
use O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
with O
ICL O
for O
experiments O
, O
and O
only O
show O
the O
performance O
of O
1 O
- O
shot O
ICL O
because O
of O
its O
input O
length O
limitation O
. O
4 O
All O
baseline O
models O
and O
UNISUMM B-MethodName
are O
evaluated O
on O
SUMMZOO B-DatasetName
( O
Appendix O
C O
shows O
the O
implementation O
details O
) O
. O
We O
conduct O
both O
automatic O
and O
human O
evaluation O
. O
As O
described O
, O
SUMMZOO B-DatasetName
requires O
models O
to O
report O
averaged O
results O
and O
their O
standard O
deviations O
over O
5 O
sets O
of O
different O
few O
- O
shot O
samples O
( O
except O
for O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
) O
. O
We O
use O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
Table O
3 O
: O
R2 B-MetricName
scores O
of O
1 B-MethodName
- I-MethodName
shot I-MethodName
text I-MethodName
- I-MethodName
davinci-002 I-MethodName
( O
GPT-3.5 B-MethodName
) O
using O
ICL O
compared O
with O
10 B-MethodName
- I-MethodName
shot I-MethodName
UNISUMM I-MethodName
and O
100 B-MethodName
- I-MethodName
shot I-MethodName
UNISUMM I-MethodName
. O

tion O
5 O
, O
which O
evaluates O
the O
n O
- O
gram O
overlap O
in O
the O
model O
- O
generated O
summary O
against O
the O
reference O
summary O
. O
We O
report O
the O
F B-MetricName
-1 I-MetricName
scores O
of O
ROUGE-1 B-MetricName
( O
R1 B-MetricName
) O
, O
ROUGE-2 B-MetricName
( O
R2 B-MetricName
) O
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
( O
RL B-MetricName
) O
. O

6 O
Automatic O
Evaluation O

Main O
Results O

The O
main O
results O
are O
shown O
in O
Table O
2 O
and O
3 O
. O
First O
, O
compared O
with O
PEGASUS B-MethodName
, O
UNISUMM B-MethodName
outperforms O
it O
across O
all O
tasks O
except O
100 B-TaskName
- I-TaskName
shot I-TaskName
XSum I-TaskName
, O
and O
shows O
the O
best O
averaged O
scores O
in O
both O
10 O
- O
shot O
and O
100 O
- O
shot O
settings O
. O
We O
also O
find O
that O
10 B-MethodName
- I-MethodName
shot I-MethodName
UNISUMM I-MethodName
can O
outperform O
100 B-MethodName
- I-MethodName
shot I-MethodName
PEGASUS I-MethodName
on O
MultiNews B-DatasetName
, O
Arxiv B-DatasetName
and O
QMSum B-DatasetName
by O
a O
large O
mar- O
5 O
We O
use O
the O
files2rouge O
for O
evaluation O
. O

gin O
, O
suggesting O
that O
UNISUMM B-MethodName
can O
benefit O
from O
diverse O
training O
data O
and O
effectively O
adapt O
indirect O
knowledge O
to O
unseen O
tasks O
. O
It O
is O
notable O
that O
although O
the O
foundation O
BART B-MethodName
model O
is O
inferior O
to O
PEGASUS B-MethodName
, O
the O
BART B-MethodName
- I-MethodName
based I-MethodName
UNISUMM I-MethodName
can O
still O
outperform O
PEGASUS B-MethodName
with O
the O
learned O
summarization O
knowledge O
. O
Overall O
, O
UNISUMM B-MethodName
surpasses O
both O
BART B-MethodName
- I-MethodName
FT I-MethodName
and O
BART B-MethodName
- I-MethodName
PT I-MethodName
by O
a O
large O
margin O
on O
all O
tasks O
in O
all O
settings O
, O
which O
suggests O
the O
equipment O
of O
multi O
- O
task O
learning O
can O
substantially O
improve O
model O
performance O
on O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
tasks O
, O
in O
particular O
in O
the O
10 O
- O
shot O
setting O
. O

UNISUMM B-MethodName
also O
outperforms O
MultiBART B-MethodName
by O
a O
large O
margin O
, O
especially O
in O
the O
10 O
- O
shot O
setting O
( O
Avg O
. O
2.86 O
R1 O
improvements O
) O
. O
Considering O
that O
Multi B-MethodName
- I-MethodName
BART I-MethodName
is O
multi O
- O
task O
pre O
- O
trained O
on O
the O
exact O
same O
data O
as O
UNISUMM B-MethodName
does O
, O
the O
main O
difference O
from O
UNISUMM B-MethodName
is O
whether O
to O
use O
prefixes O
in O
both O
multitask O
pre O
- O
training O
and O
few O
- O
shot O
tuning O
. O
The O
result O
verifies O
the O
effectiveness O
of O
UNISUMM B-MethodName
framework O
, O
in O
particular O
the O
prefix O
addition O
in O
the O
multi O
- O
task O
pre O
- O
training O
phrase O
( O
§ O
3.1 O
) O
. O

The O
comparison O
between O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
and O
UNISUMM B-MethodName
is O
shown O
in O
Table O
3 O
. O
Generally O
, O
100 B-MethodName
- I-MethodName
shot I-MethodName
UNISUMM I-MethodName
achieves O
higher O
ROUGE B-MetricName
scores O
than O
1 B-MethodName
- I-MethodName
shot I-MethodName
text I-MethodName
- I-MethodName
davinci-002 I-MethodName
on O
all O
tasks O
and O
overall O
performance O
and O
10 B-MethodName
- I-MethodName
shot I-MethodName
UNISUMM I-MethodName
shows O
better O
performance O
compared O
with O
1 B-MethodName
- I-MethodName
shot I-MethodName
text I-MethodName
- I-MethodName
davinci-002 I-MethodName
except O
for O
XSum B-TaskName
and O
Reddit B-TaskName
. O
Such O
improvements O
can O
be O
attributed O
to O
the O
fact O
that O
UNISUMM B-MethodName
is O
few O
- O
shot O
trained O
on O
more O
samples O
. O
It O
is O
also O
worth O
noting O
that O
UNISUMM B-MethodName
is O
based O
on O
BART O
- O
large O
( O
400 O
M O
) O
, O
while O
GPT-3.5 B-MethodName
is O
orders O
of O
magnitude O
larger O
( O
175B O
) O
. O
Also O
, O
we O
note O
that O
10 B-MethodName
- I-MethodName
shot I-MethodName
UNISUMM I-MethodName
can O
achieve O
higher O
ROUGE B-MetricName
scores O
on O
some O
tasks O
such O
as O
MultiNews B-TaskName
and O
Arxiv B-TaskName
compared O
with O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
. O
Besides O
UNISUMM B-MethodName
is O
multi O
- O
task O
trained O
on O
relevant O
data O
, O
one O
possible O
reason O
is O
that O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
is O
only O
presented O
with O
1 O
- O
shot O
summary O
as O
ICL O
context O
, O
due O
to O
the O
length O
limitation O
. O
However O
, O
given O
the O
previous O
finding O
( O
Goyal O
et O
al O
. O
, O
2022 O
) O
that O
GPT-3.5 B-MethodName
generated O
summaries O
can O
be O
favored O
by O
human O
evaluators O
with O
even O
lower O
ROUGE B-MetricName
scores O
, O
we O
also O
conduct O
human O
evaluation O
in O
§ O
7 O
. O

Model O
Robustness O

The O
sample O
selection O
bias O
( O
Cortes O
et O
al O
. O
, O
2008 O
) O
has O
been O
a O
major O
problem O
for O
few O
- O
shot O
tasks O
, O
where O
model O
performance O
is O
strongly O
correlated O
with O
the O
selection O
of O
few O
- O
shot O
samples O
. O
And O
a O
sound O
system O
should O
be O
robust O
and O
stable O
when O
taking O
different O
few O
- O
shot O
samples O
. O
To O
demonstrate O
the O
robustness O
and O
stability O
of O
different O
few O
- O
shot O
summarization O
models O
, O
we O
report O
their O
standard B-MetricName
deviations I-MetricName
of I-MetricName
ROUGE-1 I-MetricName
scores O
on O
5 O
different O
sets O
of O
few O
- O
shot O
samples O
provided O
in O
SUMMZOO B-DatasetName
in O
Table O
4 O
. O

Overall O
, O
the O
standard B-MetricName
deviations I-MetricName
of O
UNISUMM B-MethodName
are O
lower O
than O
all O
other O
baselines O
on O
most O
tasks O
in O
both O
settings O
, O
suggesting O
that O
UNISUMM B-MethodName
is O
most O
stable O
and O
robust O
when O
facing O
different O
few O
- O
shot O
samples O
. O
Also O
, O
MultiBART B-MethodName
outperforms O
BART B-MethodName
- I-MethodName
PT I-MethodName
and O
shows O
better O
averaged O
results O
than O
PEGASUS B-MethodName
in O
the O
100 O
- O
shot O
, O
showing O
that O
reusing O
related O
summarization O
datasets O
is O
valuable O
. O
However O
, O
it O
can O
still O
be O
unstable O
in O
the O
10 O
- O
shot O
setting O
. O
In O
contrast O
, O
UNISUMM B-MethodName
shows O
the O
least O
averaged O
standard O
deviations O
across O
all O
tasks O
in O
both O
settings O
. O
This O
suggests O
that O
the O
two O
- O
phase O
training O
with O
prefixes O
in O
the O
UNISUMM B-MethodName
framework O
is O
essential O
for O
enhancing O
the O
model O
robustness O
. O

We O
present O
the O
full O
table O
, O
including O
standard O
deviations O
of O
R2 B-MetricName
and O
RL B-MetricName
scores O
, O
in O
Appendix O
D. O
Overall O
, O
we O
find O
that O
UNISUMM B-MethodName
is O
most O
robust O
and O
stable O
towards O
different O
training O
samples O
. O

Human B-MetricName
Evaluation I-MetricName

To O
better O
understand O
the O
outputs O
of O
different O
fewshot O
summarization O
systems O
, O
following O
Kryscinski O
et O
al O
. O
( O
2019Kryscinski O
et O
al O
. O
( O
, O
2020 O
, O
we O
conduct O
a O
human O
evaluation O
from O
four O
dimensions O
: O
Fluency B-MetricName
, O
Consistency B-MetricName
, O
Coherence B-MetricName
and O
Relevance B-MetricName
. O
We O
select O
30 O
samples O
from O
QMSum B-DatasetName
, O
WikiHow B-DatasetName
and O
MultiNews B-DatasetName
, O
respectively O
, O
covering O
both O
monologue O
and O
dialogue O
texts O
. O
Then O
, O
for O
each O
sample O
, O
we O
ask O
a O
judge O
with O
experience O
in O
human B-MetricName
evaluation I-MetricName
for O
summarization O
tasks O
, O
to O
give O
scores O
from O
1 O
to O
5 O
( O
higher O
score O
indicates O
better O
quality O
) O
along O
each O
evaluation O
dimen- O
Avg O
. O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
3 O
- O
Task O
15.3 O
15.8 O
4.8 O
10.9 O
15.0 O
15.7 O
9.2 O
11.9 O
5.7 O
6.1 O
12.6 O
15.6 O
17.1 O
19.8 O
11.5 O
13.5 O
11.4 O
13.6 O
7 O
- O
Task O
15.2 O
15.9 O
7.2 O
11.4 O
15.4 O
16.4 O
9.4 O
11.7 O
5.6 O
6.2 O
13.4 O
15.7 O
18.5 O
20.7 O
12.1 O
13.9 O
12.1 O
14.0 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
10 O
100 O
Random O
15.6 O
16.0 O
4.4 O
11.1 O
16.2 O
16.3 O
9.4 O
11.6 O
6.0 O
6.1 O
13.3 O
15.7 O
18.1 O
21.0 O
11.9 O
13.7 O
11.9 O
13.9 O
CNNDM O
15.1 O
15.8 O
6.3 O
11.1 O
14.8 O
15.8 O
9.4 O
11.7 O
5.6 O
6.1 O
13.1 O
15.5 O
18.7 O
20.7 O
11.9 O
13.7 O
11.9 O
13.8 O
Universal O
15.2 O
15.9 O
7.2 O
11.4 O
15.4 O
16.4 O
9.4 O
11.7 O
5.6 O
6.2 O
13.4 O
15.6 O
18.5 O
20.7 O
12.1 O
13.9 O
12.1 O
14.0 O
sion O
. O
Candidate O
outputs O
are O
from O
gold O
summaries O
, O
1 O
- O
shot O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
, O
100 O
- O
shot O
PEGASUS B-MethodName
, O
BART B-MethodName
- I-MethodName
PT I-MethodName
and O
UNISUMM B-MethodName
respectively O
. O
In O
total O
, O
we O
have O
450 O
summaries O
to O
evaluate O
and O
the O
results O
are O
reported O
in O
Table O
5 O
. O
Appendix O
E O
gives O
detailed O
description O
of O
evaluation O
dimensions O
. O

In O
human B-MetricName
evaluation I-MetricName
, O
UNISUMM B-MethodName
outperforms O
PEGASUS B-MethodName
and O
BART B-MethodName
- I-MethodName
PT I-MethodName
on O
all O
datasets O
regarding O
all O
dimensions O
, O
achieving O
a O
higher O
fluency B-MetricName
score I-MetricName
than O
gold B-MethodName
summaries O
on O
QMSum B-DatasetName
and O
a O
comparable O
score O
on O
MultiNews B-DatasetName
and O
WikiHow B-DatasetName
, O
suggesting O
that O
UNISUMM B-MethodName
can O
generate O
very O
fluent O
sentences O
which O
can O
be O
comparable O
with O
human O
annotated O
summaries O
. O
A O
challenge O
of O
QMSum B-DatasetName
is O
that O
models O
are O
asked O
to O
generate O
summaries O
focusing O
on O
the O
input O
queries O
. O
Thus O
, O
Relevance B-MetricName
is O
a O
very O
important O
metric O
for O
this O
task O
. O
However O
, O
Relevance B-MetricName
sees O
very O
low O
score O
for O
PEGASUS B-MethodName
( O
3.27 B-MetricValue
) O
and O
BART B-MethodName
- I-MethodName
PT I-MethodName
( O
2.80 B-MetricValue
) O
, O
suggesting O
they O
are O
weak O
in O
extracting O
relevant O
information O
based O
on O
user O
queries O
. O
In O
contrast O
, O
UNISUMM B-MethodName
achieves O
a O
higher O
score O
( O
3.97 B-MetricValue
) O
. O
Text B-MethodName
- I-MethodName
davinci-002 I-MethodName
also O
performs O
very O
well O
on O
this O
task O
, O
even O
outperforming O
the O
gold B-MethodName
summaries O
on O
Fluency B-MetricName
, O
but O
UNISUMM B-MethodName
still O
achieves O
comparable O
results O
with O
limited O
training O
samples O
and O
much O
lower O
cost O
. O

On O
MultiNews B-DatasetName
, O
since O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
is O
only O
input O
with O
1 O
- O
shot O
summary O
as O
ICL O
example O
due O
to O
length O
limitation O
, O
although O
it O
can O
generate O
very O
fluent O
( O
4.97 B-MetricValue
) O
and O
coherent O
( O
4.73 B-MetricValue
) O
summaries O
, O
it O
is O
less O
preferred O
by O
human O
annotators O
w.r.t O
. O
Consistency B-MetricName
and O
Relevance B-MetricName
. O
UNISUMM B-MethodName
still O
outperforms O
other O
systems O
and O
only O
loses O
to O
gold B-MethodName
summaries O
on O
this O
two O
metrics O
. O
Similar O
results O
are O
also O
observed O
on O
WikiHow B-DatasetName
, O
where O
text B-MethodName
- I-MethodName
davinci-002 I-MethodName
tends O
to O
generate O
very O
long O
summaries O
, O
which O
can O
con O
- O
tain O
some O
hallucination O
and O
less O
important O
content O
, O
and O
UNISUMM B-MethodName
shows O
comparable O
performance O
on O
Consistency B-MetricName
and O
Relevance B-MetricName
. O

We O
show O
case O
studies O
and O
their O
analysis O
, O
including O
an O
error O
case O
where O
UNISUMM B-MethodName
fails O
, O
in O
Appendix O
F O
. O

Analysis O

Task O
Scale O
in O
Multi O
- O
task O
Training O

One O
common O
concern O
about O
multi O
- O
task O
training O
is O
that O
: O
when O
multiple O
tasks O
are O
combined O
, O
will O
newly O
added O
tasks O
hurt O
or O
help O
the O
performance O
? O
To O
verify O
this O
, O
we O
add O
one O
variant O
of O
UNISUMM B-MethodName
for O
comparison O
, O
whose O
phase-1 O
is O
multi O
- O
task O
pretrained O
on O
3 O
tasks O
instead O
of O
all O
7 O
tasks O
in O
Table O
8 O
. O
For O
the O
3 O
tasks O
, O
we O
use O
the O
combination O
of O
CN B-DatasetName
- I-DatasetName
NDM I-DatasetName
, O
PubMed B-DatasetName
and O
MediaSum B-DatasetName
, O
which O
are O
typical O
datasets O
for O
news B-TaskName
summarization I-TaskName
( O
MultiNews B-DatasetName
and O
Xsum B-DatasetName
) O
, O
academic B-TaskName
paper I-TaskName
summarization I-TaskName
( O
ArXiv B-DatasetName
) O
and O
dialogue B-TaskName
summarization I-TaskName
( O
DIALOGSUM B-DatasetName
, O
SAM B-DatasetName
- I-DatasetName
Sum I-DatasetName
and O
QMSum B-DatasetName
) O
. O

Results O
in O
Table O
6 O
show O
that O
when O
extending O
the O
multi O
- O
task O
pre O
- O
training O
datasets O
from O
3 O
to O
7 O
, O
UNISUMM B-MethodName
achieves O
better O
results O
on O
multiple O
datasets O
. O
For O
example O
, O
taking O
ArXiv B-TaskName
as O
the O
target O
task O
, O
7 B-MethodName
- I-MethodName
Task I-MethodName
UNISUMM I-MethodName
outperforms O
3 B-MethodName
- I-MethodName
Task I-MethodName
UNISUMM I-MethodName
in O
both O
10 O
and O
100 O
- O
shot O
settings O
. O
It O
suggests O
that O
7 B-MethodName
- I-MethodName
Task I-MethodName
UNISUMM I-MethodName
can O
benefit O
from O
GovReport B-DatasetName
, O
XWikis B-DatasetName
, O
SummScreen B-DatasetName
and O
BillSum B-DatasetName
for O
scientific O
text O
summarization O
. O
On O
average O
, O
the O
R2 B-MetricName
score O
improves O
by O
0.4 B-MetricValue
for O
the O
10 O
- O
shot O
setting O
and O
0.7 B-MetricValue
for O
the O
100 O
- O
shot O
setting O
. O
This O
shows O
that O
negative O
transfer O
is O
minor O
in O
UNISUMM B-MethodName
and O
suggests O
that O
by O
training O
UNISUMM B-MethodName
on O
even O
more O
datasets O
, O
its O
generalization O
can O
potentially O
be O
improved O
by O
learning O
more O
indirect O
summarization O
knowledge O
. O
8 O
12840 O

Different O
Prefix O
Initializations O

UNISUMM B-MethodName
is O
equipped O
with O
a O
universal O
prefix O
that O
was O
randomly O
( O
15 O
% O
) O
picked O
by O
all O
tasks O
during O
multi O
- O
task O
pre O
- O
training O
( O
§ O
3.3 O
) O
. O
In O
Table O
7 O
, O
we O
show O
the O
ablation O
study O
of O
using O
different O
prefix O
initialization O
strategies O
in O
few O
- O
shot O
prefix O
- O
tuning O
. O
Due O
to O
space O
limitation O
, O
we O
show O
R-2 B-MetricName
scores O
here O
. O
We O
compare O
three O
strategies O
: O
initialized O
the O
prefix O
randomly O
, O
using O
CNNDM B-DatasetName
prefix O
or O
using O
universal O
prefix O
. O
The O
CNNDM O
prefix O
is O
selected O
to O
be O
compared O
here O
because O
it O
is O
considered O
as O
a O
general O
summarization O
task O
and O
has O
been O
proved O
helpful O
to O
many O
tasks O
, O
e.g. O
, O
SAMSum B-DatasetName
( O
Gliwa O
et O
al O
. O
, O
2019 O
) O
. O

We O
see O
that O
using O
universal O
prefix O
yields O
the O
best O
results O
on O
most O
tasks O
. O
Also O
, O
the O
universal O
prefix O
is O
particularly O
useful O
for O
the O
10 O
- O
shot O
setting O
, O
bringing O
a O
0.23 B-MetricValue
improvement O
for O
R2 B-MetricName
score O
. O
In O
addition O
, O
we O
find O
that O
using O
task O
- O
specific O
prefix O
( O
CNNDM O
) O
shows O
the O
worst O
performance O
on O
some O
tasks O
, O
such O
as O
QMSum B-TaskName
and O
ArXiv B-TaskName
, O
and O
has O
the O
lowest O
average O
score O
. O
This O
can O
be O
explained O
by O
that O
the O
taskspecific O
prefix O
( O
CNNDM O
) O
stores O
abundant O
task O
specific O
knowledge O
, O
which O
however O
can O
be O
harmful O
to O
unseen O
target O
tasks O
, O
especially O
when O
the O
target O
task O
is O
very O
different O
from O
the O
pre O
- O
training O
task O
. O

We O
show O
more O
analysis O
in O
Appendix O
G O
. O

Conclusion O

We O
introduced O
UNISUMM B-MethodName
, O
a O
novel O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
system O
that O
can O
be O
easily O
prefix O
- O
tuned O
to O
excel O
at O
and O
generalize O
on O
a O
diversity O
of O
summarization O
tasks O
. O
We O
propose O
to O
combine O
multitask O
learning O
and O
prefix O
- O
tuning O
by O
jointly O
training O
the O
prefixes O
and O
the O
summarizer O
on O
multiple O
existing O
summarization O
datasets O
. O
By O
only O
tuning O
the O
prefix O
parameters O
, O
UNISUMM B-MethodName
shows O
superior O
performance O
over O
strong O
baseline O
systems O
, O
yielding O
fluent O
and O
faithful O
summaries O
across O
tasks O
. O
In O
addition O
, O
we O
assembled O
and O
released O
a O
new O
benchmark O
, O
SUMMZOO B-DatasetName
, O
for O
fairly O
and O
effectively O
evaluating O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
models O
. O
It O
covers O
an O
assorted O
set O
of O
summarization O
tasks O
and O
provides O
multiple O
few O
- O
shot O
sets O
for O
a O
more O
robust O
and O
fairer O
comparison O
. O

Limitations O

The O
limitation O
of O
UNISUMM B-MethodName
can O
be O
stated O
from O
three O
perspectives O
. O
First O
, O
the O
multi O
- O
task O
pre O
- O
training O
of O
UNISUMM B-MethodName
can O
be O
time O
and O
cost O
consuming O
, O
which O
requires O
large O
GPU O
resources O
. O
Second O
, O
the O
current O
framework O
uses O
prefixes O
of O
a O
fixed O
length O
for O
both O
multi O
- O
task O
training O
and O
few O
- O
shot O
prefixtuning O
. O
However O
, O
different O
summarization O
task O
may O
prefer O
different O
size O
of O
prefixes O
. O
Third O
, O
in O
this O
work O
, O
we O
focus O
on O
summarization O
tasks O
in O
English O
. O
The O
performance O
of O
UNISUMM B-MethodName
for O
languages O
that O
have O
a O
different O
morphology O
or O
syntactic O
structures O
from O
English O
needs O
further O
exploration O
. O

Ethics O
Statement O

Copyright O
and O
Citation O
Issue O
The O
copyright O
of O
individual O
datasets O
in O
SUMMZOO B-DatasetName
belongs O
to O
the O
original O
authors O
. O
The O
usage O
license O
of O
each O
dataset O
also O
applies O
to O
SUMMZOO B-DatasetName
. O
To O
ensure O
fair O
credit O
, O
when O
using O
SUMMZOO B-DatasetName
for O
evaluation O
, O
please O
also O
cite O
original O
papers O
, O
where O
individual O
datasets O
are O
introduced O
. O

Data O
Availability O
and O
Safety O
Pre O
- O
training O
and O
fine O
- O
tuning O
summarization O
data O
studied O
in O
this O
paper O
are O
mostly O
publicly O
available O
, O
otherwise O
we O
will O
provide O
links O
to O
the O
access O
application O
. O
Although O
filtering O
has O
been O
conducted O
in O
building O
the O
original O
datasets O
, O
some O
contents O
can O
contain O
uncomfortable O
descriptions O
, O
e.g. O
, O
news O
coverage O
of O
violent O
crimes O
and O
events O
. O

Usage O
of O
Large O
PLM O
The O
GPT-3.5 B-MethodName
model O
is O
used O
to O
generate O
text O
( O
summaries O
) O
for O
input O
documents O
of O
summarization O
tasks O
. O
The O
generated O
text O
is O
only O
used O
for O
experiments O
and O
analysis O
, O
which O
are O
presented O
in O
corresponding O
sections O
. O
No O
further O
usage O
, O
e.g. O
, O
generating O
content O
for O
manuscripts O
, O
of O
GPT-3.5 B-MethodName
or O
its O
family O
, O
is O
included O
in O
this O
paper O
. O

Human B-MetricName
Evaluation I-MetricName

We O
conduct O
human B-MetricName
evaluation I-MetricName
with O
the O
help O
of O
one O
judge O
, O
who O
obtained O
their O
postgraduate O
degree O
in O
the O
United O
Kingdom O
and O
has O
a O
solid O
experience O
in O
evaluating O
summarization O
tasks O
. O
They O
were O
compensated O
through O
a O
payment O
of O
around O
400 O
USD O
for O
450 O
instances O
( O
§ O
7 O
) O
. O

C O
Implementation O
Details O

We O
use O
BART B-MethodName
- I-MethodName
large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
to O
initialize O
the O
summarization O
model O
of O
UNISUMM B-MethodName
. O
All O
experiments O
are O
conducted O
on O
NVIDIA O
A100 O
GPU O
with O
PyTorch O
1.11 O
. O
The O
max B-HyperparameterName
input I-HyperparameterName
length I-HyperparameterName
and O
target B-HyperparameterName
length I-HyperparameterName
are O
set O
to O
2,048 B-HyperparameterValue
and O
400 B-HyperparameterValue
. O
The O
hyperparameter O
choice O
is O
based O
on O
previous O
few B-TaskName
- I-TaskName
shot I-TaskName
summarization I-TaskName
work O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O
Fabbri O
et O
al O
. O
, O
2021 O
; O
Chen O
and O
Shuai O
, O
2021 O
) O
and O
empirical O
consideration O
. O
For O
multi O
- O
task O
pre O
- O
training O
, O
we O
initialize O
from O
BART B-MethodName
- I-MethodName
large I-MethodName
, O
and O
train O
the O
model O
on O
16 O
GPUs O
with O
300,000 O
steps O
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1.5e-5 B-HyperparameterValue
, O
and O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
with O
4,000 B-HyperparameterValue
steps O
. O
For O
few O
- O
shot O
tuning O
, O
we O
prefix O
- O
tune O
the O
model O
on O
4 O
GPUs O
with O
100 O
and O
1000 O
steps O
for O
10 O
- O
shot O
and O
100shot O
, O
respectively O
, O
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1.5e-4 B-HyperparameterValue
, O
and O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
with O
10 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
steps O
. O
For O
XSum B-TaskName
, O
the O
training B-HyperparameterName
steps I-HyperparameterName
are O
set O
to O
10 B-HyperparameterValue
and O
100 B-HyperparameterValue
for O
10 O
- O
shot O
and O
100 O
- O
shot O
, O
respectively O
, O
while O
other O
configurations O
are O
unchanged O
. O

D O
Model O
Robustness O

E O
Human B-MetricName
Evaluation I-MetricName

Following O
Kryscinski O
et O
al O
. O
( O
2019Kryscinski O
et O
al O
. O
( O
, O
2020 O
, O
we O
conduct O
human O
evaluation O
from O
4 O
dimensions O
, O
which O
can O
offer O
a O
more O
robust O
and O
holistic O
perspective O
to O
understand O
summarization B-TaskName
system O
) O
: O

• O
Fluency B-MetricName
evaluates O
the O
quality O
of O
individually O
generated O
sentences O
, O
including O
grammar O
, O
word O
order O
, O
etc O
; O

• O
Coherence B-MetricName
evaluates O
the O
collective O
quality O
of O
generated O
summaries O
; O

• O
Relevance B-MetricName
evaluates O
the O
importance O
of O
information O
in O
the O
generated O
summaries O
; O

• O
Consistency B-MetricName
evaluates O
the O
factual O
alignment O
of O
the O
generated O
summary O
against O
the O
input O
document O
. O

We O
ask O
a O
judge O
to O
give O
scores O
from O
1 O
to O
5 O
along O
these O
4 O
dimensions O
. O
Higher O
score O
indicates O
better O
quality O
. O
The O
judge O
is O
a O
postgraduate O
student O
, O
who O
studied O
in O
the O
United O
Kingdom O
and O
has O
solid O
experience O
in O
evaluating O
summarization B-TaskName
tasks O
. O

F O
Case O
Study O

We O
qualitatively O
demonstrate O
the O
advantages O
of O
UNISUMM B-MethodName
( O
100 O
- O
shot O
) O
using O
cases O
from O
MultiNews B-DatasetName
and O
QMSum B-DatasetName
, O
and O
present O
an O
error O
analysis O
using O
case O
from O
WikiHow B-DatasetName
. O

As O
shown O
in O
Table O
11 O
( O
MultiNews B-DatasetName
) O
, O
we O
see O
that O
the O
UNISUMM B-MethodName
generates O
a O
summary O
with O
similar O
events O
and O
faithful O
descriptions O
compared O
with O
the O
gold O
summary O
. O
However O
, O
PEGASUS B-MethodName
generated O
summary O
contains O
factual O
errors O
( O
" O
... O
was O
last O
seen O
in O
a O
package O
shipped O
to O
the O
us O
from O
belgium O
. O
" O
) O
while O
the O
summary O
generated O
by O
UNISUMM B-MethodName
( O
" O
... O
unearthed O
... O
shipment O
from O
belgium O
to O
newark O
" O
) O
is O
consistent O
with O
the O
gold O
summary O
and O
input O
( O
" O
... O
turned O
up O
... O
shipped O
from O
belgium O
. O
" O
) O
. O
This O
shows O
that O
UNISUMM B-MethodName
has O
the O
ability O
to O
collect O
important O
information O
from O
multiple O
news O
reports O
and O
generate O
high O
- O
quality O
summaries O
, O
which O
is O
a O
task O
that O
the O
model O
has O
never O
seen O
during O
multi O
- O
task O
pre O
- O
training O
. O

Also O
, O
as O
shown O
in O
Table O
12 O
( O
QMSum B-DatasetName
) O
, O
compared O
with O
gold B-MethodName
summary O
, O
although O
the O
summary O
generated O
by O
UNISUMM B-MethodName
is O
longer O
, O
it O
is O
highly O
relevant O
to O
the O
query O
. O
And O
UNISUMM B-MethodName
properly O
rephrases O
the O
key O
utterance O
from O
the O
source O
meeting O
into O
an O
objective O
description O
, O
which O
suits O
the O
characteristic O
of O
conversation O
summarization O
. O
In O
contrast O
, O
the O
summary O
generated O
by O
PEGASUS B-MethodName
misses O
im O
- O
portant O
contents O
and O
contains O
irrelevant O
sentences O
compared O
with O
UNISUMM B-MethodName
and O
human O
annotation O
. O
This O
evidence O
shows O
that O
UNISUMM B-MethodName
successfully O
learns O
important O
characters O
of O
query O
- O
based O
meeting O
summarization O
task O
with O
only O
100 O
samples O
. O

An O
error O
case O
where O
UNISUMM B-MethodName
fails O
can O
be O
found O
in O
Table O
14 O
( O
WikiHow B-DatasetName
) O
. O
UniSumm B-DatasetName
mistakenly O
generates O
" O
... O
matches O
the O
text O
of O
the O
letter O
... O
" O
, O
where O
the O
ground O
truth O
should O
be O
the O
" O
... O
matches O
. O
. O
. O
the O
one O
( O
address O
) O
... O
on O
the O
envelope O
" O
. O
Moreoever O
, O
the O
summary O
generated O
by O
UniSumm B-MethodName
is O
a O
bit O
repetitive O
in O
wording O
, O
e.g. O
, O
serveral O
repeated O
phrases O
" O
... O
on O
the O
inside O
of O
the O
letter O
... O
" O
. O

We O
present O
more O
cases O
in O
Table O
13 O
( O
ArXiv B-DatasetName
and O
DIALOGSUM B-DatasetName
) O
, O
Table O
14 O
( O
XSum B-DatasetName
) O
and O
Table O
15 O
( O
SAMSum B-DatasetName
and O
Reddit B-DatasetName
) O
. O
Overall O
, O
we O
find O
that O
UNISUMM B-MethodName
is O
capable O
of O
generating O
very O
fluent O
, O
relevant O
, O
faithful O
and O
human O
- O
like O
summaries O
on O
diverse O
unseen O
tasks O
. O
This O
verifies O
UNISUMM B-MethodName
's O
great O
generalization O
ability O
in O
the O
few O
- O
shot O
scenario O
. O

G O
Influence O
of O
Weight B-HyperparameterName
Decay I-HyperparameterName

In O
§ O
3.4 O
, O
we O
design O
a O
separated O
weight B-HyperparameterName
decay I-HyperparameterName
strategy O
to O
circumvent O
negative O
transfer O
in O
multi O
- O
task O
learning O
. O
In O
Table O
10 O
, O
we O
examine O
whether O
the O
combination O
of O
different O
weight B-HyperparameterName
decay I-HyperparameterName
rates O
( O
d B-HyperparameterName
p I-HyperparameterName
for O
prefixes O
and O
d B-HyperparameterName
l I-HyperparameterName
for O
the O
summarization O
model O
) O
is O
beneficial O
. O
Specifically O
, O
we O
report O
ROUGE-2 B-MetricName
scores O
on O
SUMMZOO B-DatasetName
with O
different O
combinations O
of O
weight B-HyperparameterName
decay I-HyperparameterName
rates O
. O
We O
can O
see O
that O
the O
model O
performs O
the O
best O
with O
d B-HyperparameterName
p I-HyperparameterName
= O
0.05 B-HyperparameterValue
and O
d B-HyperparameterName
l I-HyperparameterName
= O
0.01 B-HyperparameterValue
. O
And O
this O
asymmetrical O
weight B-HyperparameterName
decay I-HyperparameterName
is O
especially O
helpful O
to O
10 B-TaskName
- I-TaskName
shot I-TaskName
XSum I-TaskName
, O
which O
is O
more O
distinct O
from O
pretraining O
summarization O
tasks O
and O
relies O
more O
on O
general O
summarization O
knowledge O
. O

12848 O

MultiNews B-DatasetName
Input O
text O
paris O
( O
ap O
) O
the O
pompidou O
centre O
in O
paris O
hopes O
to O
display O
a O
long O
- O
vanished O
picasso O
painting O
in O
may O
, O
now O
that O
it O
has O
been O
recovered O
by O
u.s O
. O
customs O
authorities O
. O
this O
undated O
photo O
provided O
by O
the O
united O
states O
department O
of O
justice O
, O
shows O
a O
cubist O
painting O
entitled O
the O
hairdresserby O
pablo O
picasso O
. O
authorities O
say O
the O
painting O
worth O
millions O
of O
dollars O
was O
... O
||||| O
a O
stolen O
picasso O
worth O
millions O
of O
dollars O
was O
shipped O
to O
the O
u.s O
. O
in O
a O
package O
that O
described O
it O
as O
a O
$ O
37 O
" O
art O
craft O
" O
but O
it O
will O
soon O
be O
on O
its O
way O
back O
to O
france O
... O
PEGASUS O
-a O
1911 O
cubist O
painting O
worth O
millions O
of O
dollars O
has O
been O
recovered O
after O
being O
mysteriously O
transported O
from O
france O
to O
the O
us O
nearly O
a O
decade O
ago O
. O
the O
pablo O
picasso O
painting O
, O
titled O
la O
coiffeuse O
, O
was O
reported O
missing O
from O
a O
pompidou O
storeroom O
in O
paris O
in O
2001 O
and O
was O
last O
seen O
in O
a O
package O
shipped O
to O
the O
us O
from O
belgium O
, O
the O
ap O
reports O
. O
federal O
prosecutors O
in O
brooklyn O
filed O
papers O
thursday O
to O
seize O
the O
painting O
, O
which O
was O
hidden O
in O
a O
climate O
- O
controlled O
storage O
facility O
in O
queens O
, O
ny O
, O
and O
will O
soon O
be O
returned O
to O
france O
, O
reports O
the O
new O
york O
times O
. O
the O
center O
for O
art O
recovery O
in O
paris O
says O
it O
hopes O
to O
have O
the O
painting O
back O
in O
may O
, O
at O
a O
time O
when O
the O
cultural O
world O
is O
reeling O
from O
islamic O
state O
videos O
showing O
the O
destruction O
of O
statues O
in O
iraq O
. O
" O
this O
is O
a O
true O
comfort O
for O
us O
at O
a O
time O
when O
the O
cultural O
world O
is O
reeling O
from O
the O
islamic O
state O
video O
showing O
the O
destruction O
of O
statues O
in O
iraq O
, O
" O
says O
alain O
seban O
, O
the O
director O
of O
the O
pompidou O
centre O
in O
paris O
. O
" O
it O
is O
a O
work O
of O
art O
that O
represents O
the O
genius O
of O
pablo O
picasso O
and O
was O
bequeathed O
to O
us O
in O
2001 O
by O
the O
master O
of O
france O
's O
national O
museum O
. O
" O

GPT-3.5 B-DatasetName
-the O
new O
york O
times O
has O
a O
front O
- O
page O
story O
friday O
detailing O
how O
former O
fbi O
chief O
james O
comey O
tried O
to O
instruct O
president O
trump O
about O
the O
proper O
boundaries O
that O
should O
exist O
between O
them O
. O

UNISUMM B-DatasetName
-a O
century O
- O
old O
picasso O
painting O
missing O
from O
paris O
for O
more O
than O
a O
decade O
resurfaced O
in O
the O
united O
states O
under O
false O
pretenses O
as O
a O
$ O
37 O
holiday O
- O
themed O
" O
art O
craft O
, O
" O
the O
ap O
reports O
. O
the O
1911 O
painting O
, O
called O
" O
la O
coiffeuse O
, O
" O
was O
unearthed O
in O
december O
in O
a O
fedex O
shipment O
from O
belgium O
to O
newark O
. O
the O
canvas O
had O
been O
smuggled O
out O
of O
a O
storeroom O
of O
the O
centre O
georges O
pompidou O
, O
the O
paris O
museum O
and O
arts O
center O
. O
officials O
there O
thought O
it O
was O
safe O
and O
sound O
until O
they O
received O
a O
loan O
request O
for O
it O
in O
2001 O
. O
they O
searched O
the O
storerooms O
and O
could O
not O
find O
it O
. O
on O
dec O
. O
17 O
, O
someone O
going O
by O
" O
robert O
" O
with O
an O
address O
in O
belgium O
brought O
the O
painting O
to O
a O
climate O
- O
controlled O
warehouse O
in O
long O
island O
city O
and O
was O
seized O
, O
the O
ap O
reports O
. O
the O
us O
attorney O
for O
the O
eastern O
district O
of O
new O
york O
filed O
a O
civil O
complaint O
thursday O
to O
forfeit O
the O
picasso O
and O
return O
it O
to O
france O
. O
the O
canvas O
, O
titled O
" O
la O
coiffeuse O
, O
" O
was O
bequeathed O
to O
the O
national O
museums O
of O
france O
in O
1998 O
. O
the O
pompidou O
director O
alain O
seban O
said O
in O
a O
statement O
friday O
that O
he O
hopes O
the O
work O
can O
be O
exhibited O
again O
publicly O
in O
may O
. O
no O
arrests O
have O
been O
made O
in O
the O
case O
. O

Gold B-MethodName
-a O
picasso O
painting O
that O
was O
found O
to O
have O
vanished O
from O
a O
paris O
museum O
more O
than O
a O
decade O
ago O
has O
turned O
up O
2014 O
in O
the O
us O
, O
in O
a O
package O
shipped O
from O
belgium O
. O
its O
papers O
identified O
it O
as O
a O
$ O
37 O
" O
art O
craft O
/ O
toy O
" O
and O
also O
included O
the O
line O
" O
joyeux O
noel O
, O
" O
or O
merry O
christmas O
, O
the O
new O
york O
times O
reports O
. O
a O
man O
named O
" O
robert O
" O
attempted O
to O
send O
the O
package O
to O
a O
climate O
- O
controlled O
warehouse O
in O
queens O
, O
new O
york O
, O
in O
december O
, O
but O
custom O
officials O
at O
the O
port O
of O
newark O
seized O
what O
turned O
out O
to O
be O
picasso O
's O
1911 O
la O
coiffeuse O
( O
the O
hairdresser O
) O
. O
french O
museum O
officials O
traveled O
to O
new O
york O
last O
month O
and O
confirmed O
the O
find O
is O
indeed O
the O
missing O
picasso O
work O
, O
which O
the O
centre O
georges O
pompidou O
realized O
was O
missing O
from O
its O
storerooms O
in O
2001 O
following O
a O
loan O
request O
; O
it O
was O
then O
valued O
at O
more O
than O
$ O
2.5 O
million O
. O
court O
documents O
do O
n't O
specify O
whether O
the O
sender O
or O
would O
- O
be O
recipient O
have O
been O
identified O
, O
nbc O
news O
reports O
, O
but O
" O
a O
lost O
treasure O
has O
been O
found O
, O
" O
us O
attorney O
loretta O
lynch O
said O
in O
a O
statement O
. O
she O
filed O
a O
civil O
complaint O
yesterday O
that O
will O
have O
the O
painting O
returned O
to O
france O
. O
the O
pompidou O
's O
director O
, O
who O
called O
the O
painting O
's O
rediscovery O
a O
" O
true O
comfort O
, O
" O
hopes O
the O
painting O
can O
go O
on O
display O
at O
the O
museum O
as O
early O
as O
may O
, O
the O
ap O
reports O
. O
( O
another O
picasso O
work O
was O
stolen O
days O
before O
this O
one O
was O
found O
. O
) O

Table O
11 O
: O
Case O
study O
( O
a O
) O
: O
Gold B-MethodName
and O
generated O
summaries O
by O
1 B-MethodName
- I-MethodName
shot I-MethodName
text I-MethodName
- I-MethodName
davinci-002 I-MethodName
( O
GPT3.5 B-MethodName
) O
, O
100 B-MethodName
- I-MethodName
shot I-MethodName
PEGASUS I-MethodName
and O
UNISUMM B-MethodName
on O
MultiNews B-DatasetName
, O
respectively O
. O
" O
||||| O
" O
seperates O
different O
reports O
in O
the O
input O
. O

12849 O

QMSum B-DatasetName

Input O
text O

what O
did O
barry O
hughes O
think O
about O
draft O
changes O
to O
cps O
guidance O
when O
talking O
about O
the O
divergence O
between O
the O
law O
in O
england O
and O
wales O
? O
suzy O
davies O
am O
: O
thank O
you O
. O
i O
was O
going O
to O
ask O
this O
question O
later O
, O
but O
i O
think O
it O
fits O
in O
. O
i O
've O
forgotten O
what O
i O
was O
going O
to O
ask O
; O
this O
is O
terrible O
. O
oh O
no O
, O
that O
's O
right O
obviously O
, O
this O
act O
is O
on O
a O
particular O
trajectory O
at O
the O
moment O
, O
and O
it O
's O
due O
to O
be O
introduced O
before O
the O
end O
of O
this O
assembly O
. O
do O
you O
think O
it O
would O
be O
better O
for O
us O
as O
scrutinisers O
of O
this O
act O
if O
we O
could O
see O
the O
draft O
changes O
to O
cps O
guidance O
on O
the O
public O
interest O
test O
before O
we O
make O
our O
final O
decision O
? O
barry O
hughes O
: O
i O
honestly O
do O
n't O
think O
that O
would O
necessarily O
be O
helpful O
. O
i O
've O
had O
some O
discussions O
with O
kwame O
, O
who O
would O
have O
an O
involvement O
in O
this O
. O
... O

PEGASUS B-MethodName
barry O
hughes O
thought O
it O
would O
be O
better O
for O
the O
members O
to O
see O
the O
draft O
changes O
to O
cps O
guidance O
on O
the O
public O
interest O
test O
before O
making O
their O
final O
decision O
. O
he O
added O
that O
they O
would O
need O
to O
work O
that O
up O
as O
they O
went O
along O
, O
and O
it O
would O
be O
a O
risk O
of O
putting O
the O
cart O
before O
the O
horse O
. O

GPT-3.5 B-MethodName

Barry O
Hughes O
thinks O
that O
it O
would O
not O
be O
helpful O
for O
the O
public O
to O
see O
the O
draft O
changes O
to O
the O
CPS O
guidance O
on O
the O
public O
interest O
test O
before O
the O
final O
decision O
is O
made O
on O
the O
bill O
. O
He O
believes O
that O
it O
would O
be O
better O
to O
take O
the O
general O
principles O
set O
out O
in O
the O
code O
for O
crown O
prosecutors O
and O
provide O
more O
specific O
details O
around O
them O
that O
relate O
to O
the O
issues O
being O
discussed O
in O
the O
bill O
. O

UNISUMM B-MethodName
barry O
hughes O
thought O
that O
the O
draft O
changes O
to O
cps O
guidance O
would O
not O
necessarily O
be O
helpful O
. O
he O
thought O
that O
they O
would O
simply O
want O
to O
take O
the O
present O
public O
interest O
factors O
set O
out O
very O
clearly O
in O
the O
code O
for O
crown O
prosecutors O
, O
and O
provide O
a O
degree O
of O
detail O
around O
those O
that O
relates O
more O
specifically O
to O
the O
issues O
that O
they O
were O
discussing O
. O
so O
, O
it O
would O
be O
taking O
principles O
of O
generality O
and O
according O
them O
a O
degree O
of O
specificity O
. O
and O
they O
'd O
need O
to O
work O
that O
up O
as O
they O
go O
along O
. O

Gold B-MethodName
barry O
hughes O
thought O
draft O
changes O
to O
cps O
guidance O
would O
not O
necessarily O
be O
helpful O
to O
the O
scrutinisers O
, O
and O
simply O
wanted O
to O
take O
the O
present O
interest O
factors O
. O
barry O
thought O
they O
needed O
to O
work O
for O
the O
principles O
of O
generality O
and O
a O
degree O
of O
specificity O
. O

Table O
12 O
: O
Case O
study O
( O
b O
) O
: O
Gold B-MethodName
and O
generated O
summaries O
by O
1 B-MethodName
- I-MethodName
shot I-MethodName
text I-MethodName
- I-MethodName
davinci-002 I-MethodName
( O
GPT3.5 B-MethodName
) O
, O
100 B-MethodName
- I-MethodName
shot I-MethodName
PEGASUS I-MethodName
and O
UNISUMM B-MethodName
on O
QMSum B-TaskName
, O
respectively O
. O
The O
blod O
is O
the O
query O
. O

12850 O

ArXiv B-DatasetName
Input O
text O
rapid O
progress O
in O
the O
design O
and O
manufacture O
of O
optical O
fiber O
systems O
is O
a O
result O
of O
worldwide O
demand O
for O
ultra O
-high O
bit O
-rate O
optical O
communications O
. O
this O
explains O
the O
growing O
interest O
of O
the O
soliton O
community O
in O
soliton O
-based O
optical O
fiber O
communication O
systems O
. O
this O
area O
of O
research O
was O
considerably O
advanced O
in O
recent O
years O
@ O
xcite O
. O
the O
most O
remarkable O
results O
include O
the O
application O
of O
the O
concept O
of O
the O
dispersion O
management O
to O
_ O
temporal O
optical O
solitons O
_ O
and O
soliton O
-based O
optical O
transmission O
systems O
, O
and O
the O
discovery O
of O
the O
so O
-called O
_ O
dispersion O
managed O
soliton O
_ O
. O
high O
-speed O
optical O
communications O
require O
effective O
components O
such O
as O
high O
-performance O
broadband O
computer O
networks O
that O
can O
be O
developed O
by O
employing O
the O
concept O
of O
the O
bit O
-parallel O
-wavelength O
( O
bpw O
) O
pulse O
transmission O
that O
offers O
many O
of O
the O
advantages O
of O
both O
parallel O
fiber O
ribbon O
cable O
and O
conventional O
wavelength O
-division O
... O
PEGASUS B-MethodName
the O
physical O
properties O
of O
multi O
-frequency O
solitary O
waves O
propagating O
in O
optical O
fibers O
and O
waveguides O
are O
discussed O
in O
the O
context O
of O
the O
soliton O
-based O
optical O
dispersion O
management O
. O
the O
emphasis O
is O
placed O
on O
the O
role O
of O
the O
spatial O
dispersion O
in O
the O
propagation O
of O
such O
solitary O
waves O
. O
it O
is O
shown O
that O
the O
propagation O
of O
such O
waves O
in O
optical O
fibers O
and O
waveguides O
is O
affected O
not O
only O
by O
the O
parametric O
difference O
between O
the O
pulse O
and O
the O
beam O
velocity O
, O
but O
also O
by O
the O
spatial O
variation O
of O
the O
dispersion O
profile O
. O
as O
a O
consequence O
, O
we O
argue O
that O
the O
role O
of O
the O
spatial O
dispersion O
in O
the O
propagation O
of O
solitary O
waves O
in O
optical O
fibers O
and O
waveguides O
is O
not O
limited O
to O
the O
parametric O
difference O
between O
the O
pulse O
and O
the O
beam O
velocity O
, O
but O
also O
includes O
the O
role O
of O
the O
spatial O
dispersion O
itself O
. O
This O
article O
discusses O
the O
various O
types O
of O
solitary O
waves O
that O
can O
occur O
in O
nonlinear O
optical O
systems O
, O
with O
a O
focus O
on O
how O
to O
extract O
information O
about O
the O
propagation O
characteristics O
from O
experimental O
data O
. O
The O
examples O
of O
temporal O
and O
spatial O
solitons O
are O
given O
, O
along O
with O
a O
discussion O
of O
the O
differences O
between O
them O
. O

The O
difficulties O
associated O
with O
experimental O
measurements O
are O
also O
addressed O
, O
along O
with O
suggestions O
for O
ways O
to O
overcome O
them O
. O

UNISUMM B-MethodName
the O
aim O
of O
the O
present O
paper O
is O
to O
present O
a O
versatile O
scheme O
for O
the O
study O
of O
spatial O
and O
temporal O
solitary O
waves O
. O
this O
approach O
is O
based O
on O
the O
theory O
of O
spatio O
-temporal O
wave O
propagation O
, O
where O
the O
spatial O
beam O
profile O
along O
the O
transverse O
direction O
is O
similar O
to O
the O
temporal O
pulse O
profile O
in O
fibers O
. O
in O
all O
such O
systems O
, O
the O
nonlinear O
interaction O
between O
the O
waves O
of O
two O
( O
or O
more O
) O
frequencies O
is O
the O
major O
physical O
effect O
that O
can O
support O
coupled O
-mode O
multi O
-frequency O
solitary O
waves O
. O
we O
will O
illustrate O
three O
common O
examples O
: O
multi O
-wavelength O
solitary O
waves O
in O
bit O
-parallel O
-wavelength O
optical O
fiber O
links O
, O
multi O
-colour O
spatial O
solitons O
due O
to O
multistep O
cascading O
in O
optical O
waveguides O
with O
quadratic O
nonlinearities O
, O
and O
quasiperiodic O
solitons O
in O
the O
fibonacci O
superlattices O
. O

Gold B-MethodName
we O
discuss O
several O
novel O
types O
of O
multi O
-component O
( O
temporal O
and O
spatial O
) O
envelope O
solitary O
waves O
that O
appear O
in O
fiber O
and O
waveguide O
nonlinear O
optics O
. O
in O
particular O
, O
we O
describe O
multi O
-channel O
solitary O
waves O
in O
bit O
-parallel O
-wavelength O
fiber O
transmission O
systems O
for O
high O
performance O
computer O
networks O
, O
multi O
-colour O
parametric O
spatial O
solitary O
waves O
due O
to O
cascaded O
nonlinearities O
of O
quadratic O
materials O
, O
and O
quasiperiodic O
envelope O
solitons O
due O
to O
quasi O
-phase O
-matching O
in O
fibonacci O
optical O
superlattices O
. O
2 O

DIALOGSUM B-DatasetName

Input O
text O
person1 O
: O
ms O
. O
dawson O
, O
i O
need O
you O
to O
take O
a O
dictation O
for O
me O
. O
person2 O
: O
yes O
, O
sir O
... O
person1 O
: O
this O
should O
go O
out O
as O
an O
intra O
- O
office O
memorandum O
to O
all O
employees O
by O
this O
afternoon O
. O
are O
you O
ready O
? O
person2 O
: O
yes O
, O
sir O
. O
go O
ahead O
. O
person1 O
: O
attention O
all O
staff O
... O
effective O
immediately O
, O
all O
office O
communications O
are O
restricted O
to O
email O
correspondence O
and O
official O
memos O
. O
the O
use O
of O
instant O
message O
programs O
by O
employees O
during O
working O
hours O
is O
strictly O
prohibited O
. O
... O
PEGASUS B-MethodName
person1 O
advises O
dawson O
to O
sign O
an O
intra O
- O
office O
memorandum O
restricting O
office O
communications O
to O
email O
correspondence O
and O
official O
memos O
. O
person2 O
asks O
if O
this O
applies O
to O
internal O
and O
external O
communications O
. O
person1 O
tells O
them O
so O
. O

GPT-3.5 B-MethodName
person1 O
needs O
Ms. O
Dawson O
to O
take O
a O
dictation O
for O
an O
intra O
- O
office O
memorandum O
that O
is O
to O
be O
distributed O
to O
all O
employees O
by O
4 O
pm O
. O
The O
memo O
is O
to O
inform O
all O
staff O
that O
effective O
immediately O
, O
all O
office O
communications O
are O
restricted O
to O
email O
correspondence O
and O
official O
memos O
; O
the O
use O
of O
instant O
message O
programs O
by O
employees O
during O
working O
hours O
is O
strictly O
prohibited O
. O
At O
first O
offense O
, O
the O
employee O
will O
receive O
a O
warning O
and O
be O
placed O
on O
probation O
. O
The O
second O
offense O
will O
result O
in O
termination O
. O

UNISUMM B-MethodName
person1 O
asks O
ms O
. O
dawson O
to O
take O
a O
dictation O
for O
person1 O
. O
person1 O
restricts O
office O
communications O
to O
email O
correspondence O
and O
official O
memos O
and O
prohibits O
the O
use O
of O
instant O
message O
programs O
by O
employees O
during O
working O
hours O
. O

Gold B-MethodName
ms O
. O
dawson O
helps O
person1 O
to O
write O
a O
memo O
to O
inform O
every O
employee O
that O
they O
have O
to O
change O
the O
communication O
method O
and O
should O
not O
use O
instant O
messaging O
anymore O
. O

12851 O

WikiHow B-DatasetName
Input O
text O
before O
folding O
the O
letter O
, O
make O
sure O
that O
the O
address O
on O
the O
inside O
of O
the O
letter O
matches O
with O
the O
one O
that O
you O
have O
written O
or O
printed O
on O
the O
envelope.this O
will O
help O
avoid O
any O
mixups O
. O
do O
n't O
forget O
to O
check O
that O
you O
've O
signed O
the O
letter O
. O
make O
sure O
that O
the O
text O
is O
facing O
up O
and O
towards O
you O
. O
this O
is O
your O
last O
chance O
to O
proofread O
and O
double O
check O
that O
you O
have O
n't O
forgotten O
anything.for O
example O
, O
have O
you O
dated O
the O
letter O
? O
are O
there O
any O
spelling O
or O
grammar O
mistakes O
? O
, O
take O
the O
bottom O
of O
the O
letter O
and O
fold O
it O
so O
that O
it O
is O
about O
half O
an O
inch O
( O
about O
1 O
cm O
) O
from O
the O
top O
of O
the O
page.you O
can O
place O
the O
envelope O
underneath O
the O
letter O
to O
use O
as O
a O
guide O
. O
make O
sure O
that O
when O
you O
have O
folded O
the O
letter O
up O
, O
it O
is O
small O
enough O
to O
fit O
inside O
the O
envelope O
. O
make O
sure O
to O
line O
up O
all O
the O
outside O
edges O
of O
the O
paper O
before O
creasing O
so O
that O
you O
do O
n't O
end O
up O
with O
a O
crooked O
fold O
. O
if O
the O
fold O
is O
crooked O
, O
your O
letter O
may O
not O
fit O
into O
the O
envelope.use O
a O
ruler O
to O
make O
the O
crease O
nice O
and O
sharp O
. O
you O
can O
do O
this O
by O
holding O
a O
ruler O
sideways O
and O
using O
the O
thin O
edge O
. O
slide O
the O
edge O
of O
the O
ruler O
along O
the O
letter O
on O
the O
creases O
so O
they O
are O
nice O
and O
flat O
. O
you O
will O
now O
take O
the O
right O
part O
of O
the O
letter O
and O
fold O
it O
about O
one O
third O
of O
the O
way O
in.line O
up O
the O
top O
and O
bottom O
edges O
of O
the O
letter O
and O
then O
crease O
the O
fold O
. O
take O
the O
other O
side O
of O
the O
letter O
and O
fold O
it O
one O
third O
of O
the O
way O
in O
. O
this O
will O
be O
the O
same O
thing O
that O
you O
did O
with O
the O
right O
side.line O
up O
the O
top O
and O
bottom O
of O
the O
edges O
so O
they O
are O
perfectly O
straight O
before O
creasing O
. O
the O
last O
crease O
you O
made O
should O
go O
into O
the O
bottom O
of O
the O
envelope O
first O
. O
place O
it O
so O
that O
the O
folds O
face O
the O
backside O
of O
the O
envelope.this O
will O
make O
it O
easy O
for O
your O
recipient O
to O
find O
where O
to O
begin O
unfolding O
the O
letter O
. O

PEGASUS B-MethodName
take O
the O
top O
of O
the O
letter O
and O
fold O
it O
so O
that O
it O
is O
about O
half O
an O
inch O
( O
about O
1 O
cm O
) O
from O
the O
top O
of O
the O
page O
. O
place O
the O
envelope O
underneath O
the O
letter O
. O
make O
sure O
that O
the O
envelope O
is O
flat O
on O
the O
inside O
. O
crease O
the O
folded O
side O
of O
the O
letter O
. O

GPT-3.5 B-MethodName

The O
document O
explains O
how O
to O
fold O
a O
letter O
so O
that O
it O
will O
fit O
into O
an O
envelope O
. O
First O
, O
make O
sure O
that O
the O
address O
on O
the O
letter O
matches O
the O
address O
on O
the O
envelope O
. O
Next O
, O
fold O
the O
bottom O
of O
the O
letter O
up O
to O
the O
top O
of O
the O
page O
, O
making O
sure O
to O
crease O
the O
fold O
. O
Then O
, O
fold O
the O
right O
side O
of O
the O
letter O
in O
to O
the O
middle O
, O
and O
repeat O
with O
the O
left O
side O
. O
Finally O
, O
insert O
the O
letter O
into O
the O
envelope O
so O
that O
the O
folds O
are O
facing O
the O
back O
. O

UNISUMM B-MethodName
check O
that O
the O
address O
on O
the O
inside O
of O
the O
letter O
matches O
the O
text O
of O
the O
letter O
to O
make O
sure O
that O
the O
address O
on O
the O
inside O
of O
the O
letter O
matches O
with O
the O
one O
on O
the O
page O
. O
make O
sure O
that O
the O
text O
matches O
with O
the O
one O
on O
the O
inside O
of O
the O
letter O
. O
make O
sure O
that O
the O
envelope O
is O
small O
enough O
to O
fit O
into O
the O
envelope O
. O

Gold B-MethodName
check O
the O
address O
. O
lay O
the O
letter O
on O
a O
table O
. O
fold O
the O
bottom O
half O
up O
. O
crease O
the O
fold O
. O
fold O
the O
right O
half O
of O
the O
letter O
inwards O
. O
fold O
the O
left O
half O
of O
the O
letter O
inwards O
. O
turn O
the O
letter O
sideways O
and O
insert O
into O
the O
envelope O
. O

XSum B-DatasetName

Input O
text O
the O
sunday O
times O
says O
the O
missile O
veered O
off O
course O
during O
a O
test O
in O
june O
last O
year O
-weeks O
before O
the O
commons O
voted O
to O
spend O
40bn O
renewing O
trident O
. O
questioned O
by O
andrew O
marr O
, O
the O
pm O
refused O
to O
say O
four O
times O
if O
she O
had O
known O
about O
the O
test O
ahead O
of O
the O
vote O
. O
the O
snp O
's O
nicola O
sturgeon O
called O
for O
a O
' O
full O
disclosure O
' O
of O
what O
happened O
. O
according O
to O
the O
sunday O
times O
, O
an O
unarmed O
trident O
ii O
d5 O
missile O
veered O
off O
in O
the O
wrong O
direction O
towards O
the O
us O
-instead O
of O
towards O
africa O
-when O
it O
was O
launched O
from O
a O
british O
submarine O
off O
the O
coast O
of O
florida O
. O
in O
july O
-days O
after O
mrs O
may O
had O
become O
prime O
minister O
-mps O
voted O
overwhelmingly O
in O
favour O
of O
replacing O
trident O
. O
during O
the O
debate O
, O
mrs O
may O
told O
mps O
it O
would O
be O
' O
an O
act O
of O
gross O
irresponsibility O
' O
for O
the O
uk O
to O
abandon O
its O
nuclear O
weapons O
. O
mps O
backed O
its O
renewal O
by O
472 O
votes O
to O
117 O
. O
however O
, O
all O
52 O
snp O
mps O
voted O
against O
it O
-as O
did O
labour O
leader O
jeremy O
corbyn O
. O
when O
asked O
on O
the O
bbc O
's O
andrew O
marr O
show O
whether O
she O
had O
known O
then O
that O
a O
misfire O
had O
happened O
, O
mrs O
may O
said O
: O
' O
i O
have O
absolute O
faith O
in O
our O
trident O
missiles O
. O
' O
when O
i O
made O
that O
speech O
in O
the O
house O
of O
commons O
, O
what O
we O
were O
talking O
about O
was O
whether O
or O
not O
we O
should O
renew O
our O
trident O
. O
' O
she O
was O
asked O
a O
further O
three O
times O
-but O
did O
not O
answer O
the O
questions O
. O
the O
ministry O
of O
defence O
did O
not O
give O
details O
of O
the O
test O
process O
but O
said O
it O
had O
been O
a O
success O
. O
scottish O
first O
minister O
, O
mrs O
sturgeon O
-a O
long O
- O
standing O
opponent O
of O
trident O
, O
whose O
submarines O
are O
based O
at O
faslane O
, O
on O
the O
river O
clyde O
-said O
the O
apparent O
misfire O
was O
a O
' O
hugely O
serious O
issue O
' O
. O
she O
tweeted O
: O
' O
there O
should O
be O
full O
disclosure O
of O
what O
happened O
, O
who O
knew O
what O
/ O
when O
, O
and O
why O
the O
house O
of O
commons O
was O
n't O
told O
. O
' O
meanwhile O
, O
mr O
corbyn O
said O
the O
reports O
called O
for O
' O
a O
serious O
discussion O
' O
. O
... O
PEGASUS B-MethodName
the O
prime O
minister O
has O
been O
accused O
of O
" O
side O
- O
stepping O
" O
questions O
about O
a O
submarine O
- O
launched O
nuclearcapable O
missile O
which O
misfired O
during O
a O
test O
. O

GPT-3.5 B-MethodName
trident O
is O
the O
uk O
's O
nuclear O
weapons O
system O
, O
consisting O
of O
submarines O
, O
missiles O
and O
warheads O
. O
it O
is O
operated O
by O
the O
royal O
navy O
and O
has O
been O
in O
use O
since O
the O
1990s O
. O
reports O
that O
a O
missile O
test O
in O
june O
2016 O
went O
wrong O
have O
raised O
questions O
about O
the O
system O
's O
effectiveness O
and O
whether O
it O
was O
deliberately O
kept O
quiet O
ahead O
of O
a O
key O
parliamentary O
vote O
on O
its O
renewal O
. O

UNISUMM B-MethodName
a O
nuclear O
missile O
launched O
a O
british O
submarine O
failed O
during O
a O
test O
, O
according O
to O
a O
report O
in O
the O
sunday O
times O
. O

Gold B-MethodName
theresa O
may O
is O
coming O
under O
pressure O
to O
say O
whether O
she O
knew O
about O
a O
reported O
misfire O
of O
the O
uk O
's O
nuclear O
weapons O
system O
before O
a O
crucial O
commons O
vote O
. O
well O
, O
there O
was O
a O
meet O
and O
greet O
with O
james O
charles O
in O
one O
of O
the O
malls O
in O
birmingham O
and O
about O
8000 O
fans O
showed O
up O
for O
it O
. O
cora O
: O
it O
cause O
a O
gridlock O
around O
the O
mall O
and O
-of O
course O
-british O
media O
had O
to O
make O
some O
( O
quite O
negative O
) O
comments O
on O
it O
. O
ellie O
: O
they O
came O
for O
sister O
james O
? O
! O
> O
: O
( O
ellie O
: O
i O
sister O
snapped O
! O
! O
: O
p O
: O
d O
cora O
: O
haha O
: O
d O
cora O
: O
you O
should O
n't O
watch O
so O
much O
youtube O
, O
you O
're O
getting O
weirder O
and O
weirder O
. O
: O
d O
ellie O
: O
sister O
shut O
up O
: O
p O
so O
, O
what O
did O
they O
say O
? O
cora O
: O
; O
) O
: O
* O
daily O
mail O
" O
was O
surprised O
that O
a O
meet O
and O
greet O
with O
a O
" O
virtually O
unknown O
" O
youtuber O
gathered O
8000 O
people O
. O
: O
p O
cora O
: O
a O
host O
from O
lbc O
tried O
to O
find O
an O
answer O
to O
an O
unanswerable O
question O
: O
" O
who O
is O
james O
charles O
? O
" O
... O
PEGASUS B-MethodName
ellie O
's O
meet O
and O
greet O
with O
james O
charles O
in O
birmingham O
got O
some O
negative O
comments O
from O
british O
media O
. O
cora O
thinks O
it O
's O
ridiculous O
that O
a O
man O
can O
wear O
makeup O
. O

GPT-3.5 B-MethodName
cora O
and O
ellie O
were O
discussing O
the O
meet O
and O
greet O
with O
james O
charles O
in O
birmingham O
. O
8000 O
fans O
showed O
up O
and O
it O
caused O
a O
gridlock O
. O
british O
media O
was O
surprised O
and O
some O
hosts O
could O
n't O
wrap O
their O
heads O
around O
the O
fact O
that O
a O
guy O
can O
wear O
makeup O
. O

UNISUMM B-MethodName
about O
8000 O
fans O
showed O
up O
for O
the O
meet O
and O
greet O
with O
james O
charles O
in O
birmingham O
. O

Gold B-MethodName
there O
was O
a O
meet O
- O
and O
- O
greet O
with O
james O
charles O
in O
birmingham O
which O
gathered O
8000 O
people O
. O

Reddit B-DatasetName

Input O
Text O
tifu O
making O
lunch O
. O
made O
myself O
a O
lush O
sandwich O
for O
lunch O
today O
. O
smoked O
ham O
, O
cheddar O
cheese O
, O
a O
dash O
of O
bbq O
sauce O
on O
thick O
cut O
wholemeal O
bread O
. O
popped O
it O
in O
a O
small O
tupperware O
box O
, O
for O
freshness O
. O
i O
found O
some O
bn O
type O
biscuits O
in O
the O
cupboard O
, O
so O
i O
pop O
them O
in O
the O
box O
on O
top O
of O
my O
sandwiches O
. O
think O
nothing O
of O
it O
. O
eventually O
lunch O
time O
comes O
around O
. O
i O
discover O
, O
to O
my O
horror O
, O
that O
the O
biscuits O
have O
absorbed O
the O
moisture O
from O
the O
sandwich O
. O
and O
now O
i O
have O
a O
dried O
out O
sandwich O
and O
soggy O
biscuits O
PEGASUS O
made O
a O
luch O
sandwich O
for O
lunch O
, O
only O
to O
have O
it O
ruined O
by O
soggy O
biscuits O
. O
i O
'm O
completely O
fucked O
. O

GPT-3.5 O
made O
a O
sandwich O
and O
it O
turned O
out O
terrible O
because O
the O
biscuits O
absorbed O
all O
the O
moisture O
. O

UNISUMM B-MethodName
made O
a O
big O
sandwich O
, O
with O
cheddar O
cheese O
, O
bbq O
sauce O
and O
bn O
type O
biscuits O
. O
popped O
them O
in O
a O
tupperware O
box O
, O
and O
discover O
they O
've O
dried O
out O
Gold B-MethodName
do O
n't O
put O
your O
sandwich O
and O
biscuits O
in O
same O
box O
, O
unless O
you O
like O
dry O
bread O
and O
soggy O
biscuits O
. O
B2 O
. O
Did O
you O
discuss O
the O
license O
or O
terms O
for O
use O
and O
/ O
or O
distribution O
of O
any O
artifacts O
? O

Ethics O
Statement O

B3 O
. O
Did O
you O
discuss O
if O
your O
use O
of O
existing O
artifact O
( O
s O
) O
was O
consistent O
with O
their O
intended O
use O
, O
provided O
that O
it O
was O
specified O
? O
For O
the O
artifacts O
you O
create O
, O
do O
you O
specify O
intended O
use O
and O
whether O
that O
is O
compatible O
with O
the O
original O
access O
conditions O
( O
in O
particular O
, O
derivatives O
of O
data O
accessed O
for O
research O
purposes O
should O
not O
be O
used O
outside O
of O
research O
contexts O
) O
? O
Section O
3 O
, O
Section O
4 O
, O
Section O
5 O
, O
Appendix O
A O
, O
Appendix O
B O
and O
Appendix O
C. O
B4 O
. O
Did O
you O
discuss O
the O
steps O
taken O
to O
check O
whether O
the O
data O
that O
was O
collected O
/ O
used O
contains O
any O
information O
that O
names O
or O
uniquely O
identifies O
individual O
people O
or O
offensive O
content O
, O
and O
the O
steps O
taken O
to O
protect O
/ O
anonymize O
it O
? O
Ethics O
Statement O
B5 O
. O
Did O
you O
provide O
documentation O
of O
the O
artifacts O
, O
e.g. O
, O
coverage O
of O
domains O
, O
languages O
, O
and O
linguistic O
phenomena O
, O
demographic O
groups O
represented O
, O
etc O
. O
? O
Section O
3 O
, O
Section O
4 O
, O
Section O
5 O
, O
Appendix O
A O
, O
Appendix O
B O
and O
Appendix O
C. O
B6 O
. O
Did O
you O
report O
relevant O
statistics O
like O
the O
number O
of O
examples O
, O
details O
of O
train O
/ O
test O
/ O
dev O
splits O
, O
etc O
. O
for O
the O
data O
that O
you O
used O
/ O
created O
? O
Even O
for O
commonly O
- O
used O
benchmark O
datasets O
, O
include O
the O
number O
of O
examples O
in O
train O
/ O
validation O
/ O
test O
splits O
, O
as O
these O
provide O
necessary O
context O
for O
a O
reader O
to O
understand O
experimental O
results O
. O
For O
example O
, O
small O
differences O
in O
accuracy O
on O
large O
test O
sets O
may O
be O
significant O
, O
while O
on O
small O
test O
sets O
they O
may O
not O
be O
. O
Appendix O
A O
and O
Appendix O
B O
. O

C O
Did O
you O
run O
computational O
experiments O
? O

Section O
5 O
, O
Section O
6 O
and O
Appendix O
C O
C1 O
. O
Did O
you O
report O
the O
number O
of O
parameters O
in O
the O
models O
used O
, O
the O
total O
computational O
budget O
( O
e.g. O
, O
GPU O
hours O
) O
, O
and O
computing O
infrastructure O
used O
? O
Appendix O
C O

The O
Responsible O
NLP O
Checklist O
used O
at O
ACL O
2023 O
is O
adopted O
from O
NAACL O
2022 O
, O
with O
the O
addition O
of O
a O
question O
on O
AI O
writing O
assistance O
. O

C2 O
. O
Did O
you O
discuss O
the O
experimental O
setup O
, O
including O
hyperparameter O
search O
and O
best O
- O
found O
hyperparameter O
values O
? O
Appendix O
C O
C3 O
. O
Did O
you O
report O
descriptive O
statistics O
about O
your O
results O
( O
e.g. O
, O
error O
bars O
around O
results O
, O
summary O
statistics O
from O
sets O
of O
experiments O
) O
, O
and O
is O
it O
transparent O
whether O
you O
are O
reporting O
the O
max O
, O
mean O
, O
etc O
. O
or O
just O
a O
single O
run O
? O
Section O
5 O
and O
Section O
6 O
C4 O
. O
If O
you O
used O
existing O
packages O
( O
e.g. O
, O
for O
preprocessing O
, O
for O
normalization O
, O
or O
for O
evaluation O
) O
, O
did O
you O
report O
the O
implementation O
, O
model O
, O
and O
parameter O
settings O
used O
( O
e.g. O
, O
NLTK O
, O
Spacy O
, O
ROUGE O
, O
etc O
. O
) O
? O
Section O
3 O
, O
Section O
4 O
, O
Section O
5 O
, O
Appendix O
A O
, O
Appendix O
B O
and O
Appendix O
C. O
D O
Did O
you O
use O
human O
annotators O
( O
e.g. O
, O
crowdworkers O
) O
or O
research O
with O
human O
participants O
? O
Section O
7 O
D1 O
. O
Did O
you O
report O
the O
full O
text O
of O
instructions O
given O
to O
participants O
, O
including O
e.g. O
, O
screenshots O
, O
disclaimers O
of O
any O
risks O
to O
participants O
or O
annotators O
, O
etc O
. O
? O
Section O
7 O
and O
Appendix O
E O
D2 O
. O
Did O
you O
report O
information O
about O
how O
you O
recruited O
( O
e.g. O
, O
crowdsourcing O
platform O
, O
students O
) O
and O
paid O
participants O
, O
and O
discuss O
if O
such O
payment O
is O
adequate O
given O
the O
participants O
' O
demographic O
( O
e.g. O
, O
country O
of O
residence O
) O
? O
Ethics O
Statement O
D3 O
. O
Did O
you O
discuss O
whether O
and O
how O
consent O
was O
obtained O
from O
people O
whose O
data O
you O
're O
using O
/ O
curating O
? O
For O
example O
, O
if O
you O
collected O
data O
via O
crowdsourcing O
, O
did O
your O
instructions O
to O
crowdworkers O
explain O
how O
the O
data O
would O
be O
used O
? O
Section O
7 O
and O
Appendix O
E O
D4 O
. O
Was O
the O
data O
collection O
protocol O
approved O
( O
or O
determined O
exempt O
) O
by O
an O
ethics O
review O
board O
? O
Section O
7 O
and O
Appendix O
E O
D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
Appendix O
E O

Acknowledgements O

We O
appreciate O
all O
reviewers O
and O
chairs O
from O
ACL O
2023 O
for O
their O
valuable O
suggestions O
. O
We O
thank O
Dan O
Iter O
, O
Hiteshi O
Sharma O
, O
Zicheng O
Liu O
, O
Sen O
Yang O
and O
Leyang O
Cui O
for O
their O
proofreading O
and O
inspiring O
discussion O
. O
This O
publication O
has O
emanated O
from O
research O
conducted O
with O
the O
financial O
support O
of O
the O
Pioneer O
and O
" O
Leading O
Goose O
" O
R O
& O
D O
Program O
of O
Zhejiang O
under O
Grant O
Number O
2022SDX O
- O
HDX0003 O
. O

A O
Datasets O
in O
SummZoo B-DatasetName

The O
final O
SummZoo B-DatasetName
contains O
following O
sub O
- O
tasks O
: O

MultiNews B-DatasetName
( O
Fabbri O
et O
al O
. O
, O
2019 O
) O
is O
a O
large O
- O
scale O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
dataset O
. O
The O
task O
is O
to O
generate O
a O
summary O
given O
multiple O
news O
articles O
. O

XSum B-DatasetName
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
is O
an O
extreme B-TaskName
text I-TaskName
summarization I-TaskName
dataset O
. O
Given O
a O
news O
article O
, O
the O
task O
is O
to O
generate O
a O
one O
- O
sentence O
summary O
. O

Reddit B-DatasetName
- I-DatasetName
TIFU I-DatasetName
( O
Kim O
et O
al O
. O
, O
2019 O
) O
is O
a O
social B-TaskName
post I-TaskName
summarization I-TaskName
dataset O
. O
The O
task O
is O
to O
generate O
a O
short O
summary O
for O
posts O
from O
the O
online O
discussion O
forum O
Reddit O
. O
6 O
Compared O
with O
news O
text O
, O
the O
text O
in O
Reddit B-DatasetName
- I-DatasetName
TIFU I-DatasetName
is O
less O
formal O
and O
structured O
. O

ArXiv B-DatasetName
( O
Cohan O
et O
al O
. O
, O
2018 O
) O
is O
a O
long O
scientific O
paper O
summarization O
dataset O
collected O
from O
ArXiv O
, O
including O
articles O
of O
multiple O
domains O
, O
such O
as O
physics O
, O
computer O
science O
, O
etc O
. O

WikiHow B-DatasetName
( O
Koupaee O
and O
Wang O
, O
2018 O
) O
is O
a O
largescale O
instruction O
summarization O
dataset O
. O
The O
task O
is O
to O
generate O
a O
short O
summary O
given O
the O
multiplestep O
instruction O
. O

SAMSum B-DatasetName
( O
Gliwa O
et O
al O
. O
, O
2019 O
) O
is O
a O
written O
conversation B-TaskName
summarization I-TaskName
dataset O
for O
Messengerstyle O
chit O
- O
chats O
. O
Both O
dialogue O
and O
summary O
are O
annotated O
by O
experts O
. O

DIALOGSUM B-DatasetName
) O
is O
a O
real O
- O
life O
scenario O
dialogue O
summarization O
dataset O
that O
covers O
a O
wide O
range O
of O
daily O
life O
dialogues O
, O
including O
diverse O
task O
- O
oriented O
dialogues O
. O
The O
testset O
of O
DI B-DatasetName
- I-DatasetName
ALOGSUM I-DatasetName
provides O
three O
reference O
summaries O
for O
each O
dialogue O
, O
we O
report O
the O
averaged O
results O
. O

QMSum B-DatasetName
( O
Zhong O
et O
al O
. O
, O
2021 O
) O
is O
a O
query B-TaskName
- I-TaskName
based I-TaskName
meeting I-TaskName
summarization I-TaskName
dataset O
that O
is O
derived O
from O
Augmented O
Multi O
- O
party O
Interaction O
( O
AMI O
) O
corpus O
( O
Kraaij O
et O
al O
. O
, O
2005 O
) O
, O
the O
International O
Computer O
Science O
Institute O
( O
ICSI O
) O
( O
Shriberg O
et O
al O
. O
, O
2004 O
) O
and O
Committee O
Meetings O
. O
The O
task O
is O
to O
generate O
a O
summary O
given O
a O
meeting O
and O
a O
query O
. O

B O
Multi O
- O
Task O
Pre O
- O
Training O
Datasets O

We O
use O
the O
following O
datasets O
for O
multi O
- O
task O
pretraining O
: O

Semantic B-MethodName
Framework I-MethodName
based I-MethodName
Query I-MethodName
Generation I-MethodName
for I-MethodName
Temporal I-MethodName
Question I-MethodName
Answering I-MethodName
over O
Knowledge O
Graphs O

Answering B-TaskName
factual I-TaskName
questions I-TaskName
with I-TaskName
temporal I-TaskName
intent I-TaskName
over I-TaskName
knowledge I-TaskName
graphs I-TaskName
( O
temporal B-TaskName
KGQA I-TaskName
) O
attracts O
rising O
attention O
in O
recent O
years O
. O
In O
the O
generation O
of O
temporal O
queries O
, O
existing O
KGQA B-TaskName
methods O
ignore O
the O
fact O
that O
some O
intrinsic O
connections O
between O
events O
can O
make O
them O
temporally O
related O
, O
which O
may O
limit O
their O
capability O
. O
We O
systematically O
analyze O
the O
possible O
interpretation O
of O
temporal O
constraints O
and O
conclude O
the O
interpretation O
structures O
as O
the O
Semantic O
Framework O
of O
Temporal O
Constraints O
, O
SF O
- O
TCons O
. O
Based O
on O
the O
semantic O
framework O
, O
we O
propose O
a O
temporal B-MethodName
question I-MethodName
answering I-MethodName
method O
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
, O
which O
generates O
query O
graphs O
by O
exploring O
the O
relevant O
facts O
of O
mentioned O
entities O
, O
where O
the O
exploring O
process O
is O
restricted O
by O
SF O
- O
TCons O
. O
Our O
evaluations O
show O
that O
SF B-MethodName
- I-MethodName
TQA I-MethodName
significantly O
outperforms O
existing O
methods O
on O
two O
benchmarks O
over O
different O
knowledge O
graphs O
. O

Introduction O

With O
the O
rapid O
growth O
of O
knowledge O
graphs O
, O
temporal B-TaskName
question I-TaskName
answering I-TaskName
over I-TaskName
knowledge I-TaskName
graphs I-TaskName
( O
temporal B-TaskName
KGQA I-TaskName
) O
is O
attracting O
rising O
attention O
in O
recent O
years O
( O
Jia O
et O
al O
. O
, O
2018b O
( O
Jia O
et O
al O
. O
, O
, O
2021 O
. O
In O
temporal B-TaskName
KGQA I-TaskName
, O
a O
common O
phenomenon O
is O
that O
questions O
express O
temporal O
relations O
between O
events O
or O
time O
expressions O
, O
while O
knowledge O
graphs O
describe O
the O
facts O
resulting O
from O
each O
event O
. O
Existing O
methods O
handle O
the O
heterogeneity O
between O
natural O
language O
and O
knowledge O
graph O
representation O
in O
two O
ways O
. O
Some O
systems O
express O
temporal O
intents O
by O
constructing O
executable O
queries O
, O
some O
apply O
time O
- O
sensitive O
neural O
models O
to O
rank O
candidate O
answers O
. O
Considering O
that O
neural O
models O
are O
difficult O
to O
characterize O
the O
clear O
boundaries O
of O
concepts O
( O
e.g. O
, O
exactly O
filter O
all O
events O
that O
occur O
" O
before O
2022 O
" O
) O
, O
this O
paper O
focuses O
on O
generating O
queries O
that O
correspond O
to O
the O
meaning O
of O
questions O
. O

From O
the O
logic O
perspective O
, O
formulated O
queries O
are O
actually O
logical O
restrictions O
about O
KG O
facts O
. O
The O
answers O
to O
a O
question O
is O
a O
set O
of O
KG O
objects O
, O
each O
of O
which O
satisfies O
the O
corresponding O
logical O
restrictions O
. O
In O
previous O
studies O
( O
e.g. O
, O
Jia O
et O
al O
. O
, O
2018b O
) O
, O
temporal O
intents O
are O
converted O
into O
restrictions O
over O
KG O
facts O
with O
quantitative O
time O
values O
. O
Example O
1 O
illustrates O
a O
typical O
conversion O
from O
a O
temporal O
question O
to O
such O
restriction O
. O
Example O
1 O
. O
" O
Who O
was O
the O
president O
of O
the O
U.S. O
when O
John O
Lennon O
was O
shot O
? O
" O

The O
corresponding O
query O
on O
Wikidata O
can O
be O
formulated O
as O
the O
following O
logical O
restriction O
: O

T O
1 O
= O
time O
( O
position_held O
( O
AN O
S O
, O
U.S._president O
) O
) O
∧ O
T O
2 O
= O
time O
( O
Murder_of_John_Lennon O
) O
∧ O
OVERLAPS O
( O
T O
1 O
, O
T O
2 O
) O
. O

However O
, O
the O
idea O
of O
constructing O
queries O
with O
quantitative O
restrictions O
can O
not O
exhaust O
all O
possible O
scenarios O
. O
As O
illustrated O
in O
Example O
2 O
, O
facts O
with O
time O
values O
are O
not O
a O
necessary O
premise O
to O
introduce O
a O
temporal O
relation O
. O

Example O
2 O
. O
" O
Where O
was O
John O
Lennon O
standing O
when O
he O
was O
shot O
? O
" O

To O
construct O
a O
comparison O
restriction O
, O
we O
need O
to O
enumerate O
the O
" O
standing O
" O
of O
J.L. O
( O
i.e. O
all O
the O
experiences O
of O
his O
life O
) O
. O
The O
enumeration O
is O
hard O
to O
implement O
and O
might O
introduce O
errors O
. O
1 O
In O
fact O
, O
the O
temporal O
intent O
does O
not O
rely O
on O
any O
time O
value O
. O
The O
two O
events O
occur O
simultaneously O
just O
because O
they O
are O
different O
aspects O
of O
the O
same O
entity O
( O
wd O
: O
Q2341090 O
) O
, O
the O
murder O
of O
John O
Lennon O
. O

The O
above O
example O
reveals O
that O
intrinsic O
connections O
can O
also O
make O
events O
temporally O
related O
. O
We O
argue O
that O
the O
neglect O
of O
such O
cases O
may O
limit O
the O
capability O
of O
existing O
methods O
. O
Therefore O
, O
the O
possible O
temporal O
constraints O
, O
especially O
those O
that O
do O
not O
rely O
on O
explicit O
time O
values O
, O
need O
to O
be O
specifically O
studied O
. O
The O
main O
challenges O
in O
concluding O
such O
constraints O
come O
from O
the O
complexity O
of O
natural O
language O
and O
the O
lack O
of O
supervision O
signals O
. O
Practical O
KGQA B-TaskName
tasks O
often O
provide O
only O
questionanswer O
pairs O
. O
i.e. O
, O
the O
constraints O
on O
the O
relevant O
facts O
are O
unknown O
. O
Manually O
enumerating O
all O
possible O
constraint O
structures O
in O
a O
huge O
search O
space O
will O
be O
cumbersome O
or O
even O
infeasible O
. O
Thus O
, O
there O
is O
a O
need O
for O
a O
lightweight O
method O
to O
model O
the O
various O
constraints O
that O
correspond O
to O
possible O
temporal O
intents O
. O

Inspired O
by O
the O
basic O
idea O
of O
frame O
semantics O
that O
" O
one O
can O
not O
understand O
the O
meaning O
of O
a O
word O
without O
access O
to O
all O
the O
encyclopedic O
knowledge O
that O
relates O
to O
that O
word O
. O
" O
( O
Fillmore O
et O
al O
. O
, O
2006 O
) O
, O
we O
assume O
that O
temporal O
intents O
are O
expressed O
as O
certain O
constraints O
about O
corresponding O
knowledge O
and O
could O
be O
interpreted O
by O
some O
structures O
over O
KG O
facts O
. O
Specifically O
, O
the O
events O
involved O
in O
a O
temporal O
constraint O
should O
provide O
certain O
KG O
facts O
, O
which O
support O
a O
possible O
interpretation O
of O
it O
. O
We O
conclude O
the O
temporal O
constraints O
and O
their O
corresponding O
interpretation O
structures O
as O
the O
Semantic O
Framework O
of O
Temporal O
Constraints O
, O
SF O
- O
TCons O
. O
SF O
- O
TCons O
describes O
what O
kinds O
of O
knowledge O
are O
needed O
and O
how O
they O
are O
composed O
in O
the O
potential O
interpretations O
. O
It O
consists O
of O
6 O
interpretation O
structures O
, O
which O
will O
be O
presented O
in O
Section O
2 O
. O
To O
the O
best O
of O
our O
knowledge O
, O
SF O
- O
TCons O
is O
the O
first O
work O
to O
systematically O
summarize O
the O
interpretation O
structures O
for O
temporal B-TaskName
KGQA I-TaskName
tasks O
. O

Based O
on O
SF O
- O
TCons O
, O
we O
propose O
a O
semanticframework B-MethodName
- I-MethodName
based I-MethodName
question I-MethodName
answering I-MethodName
method I-MethodName
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
, O
to O
convert O
SF O
- O
TCons O
into O
executable O
queries O
. O
SF B-MethodName
- I-MethodName
TQA I-MethodName
generates O
query O
graphs O
by O
exploring O
the O
relevant O
facts O
of O
mentioned O
entities O
, O
where O
the O
query O
graph O
is O
a O
graph O
representation O
of O
executable O
logical O
queries O
that O
resembles O
subgraphs O
of O
KG O
( O
Yih O
et O
al O
. O
, O
2015 O
) O
. O
SF B-MethodName
- I-MethodName
TQA I-MethodName
improves O
the O
accuracy O
of O
query O
generation O
by O
regarding O
SF O
- O
TCons O
as O
restrictions O
in O
the O
exploration O
. O
SF B-MethodName
- I-MethodName
TQA I-MethodName
firstly O
evokes O
possible O
interpretations O
of O
temporal O
intents O
according O
to O
TimeML O
( O
Pustejovsky O
et O
al O
. O
, O
2010 O
) O
annotations O
. O
It O
then O
grounds O
the O
temporal O
elements O
in O
corresponding O
interpretation O
structures O
by O
the O
relevant O
KG O
facts O
. O
The O
grounding O
phase O
will O
generate O
multiple O
candidate O
queries O
, O
the O
best O
candidate O
will O
be O
distinguished O
by O
ranking O
the O
pairs O
of O
questions O
and O
serialized O
queries O
with O
a O
BERT O
model O
. O
The O
rest O
of O
this O
paper O
is O
organized O
as O
follows O
: O
Section O
2 O
discusses O
the O
SF O
- O
TCons O
in O
detail O
. O
Section O
3 O
presents O
SF B-MethodName
- I-MethodName
TQA I-MethodName
. O
Section O
4 O
evaluates O
the O
SF B-MethodName
- I-MethodName
TQA I-MethodName
with O
two O
benchmarks O
over O
different O
knowledge O
graphs O
. O
Section O
5 O
summarizes O
the O
related O
work O
. O
The O
last O
section O
concludes O
this O
paper O
. O

Semantic O
Framework O
of O
Temporal O
Constraints O

As O
previously O
introduced O
, O
temporal O
intents O
reflect O
constraints O
on O
events O
and O
time O
expressions O
. O
We O
argue O
that O
what O
really O
supports O
the O
constraints O
is O
the O
essential O
knowledge O
underlying O
the O
involved O
elements O
. O
For O
example O
, O
in O
a O
comparison O
like O
" O
before O
WWI O
" O
, O
what O
is O
needed O
is O
its O
start O
time O
" O
1914 O
" O
rather O
than O
the O
named O
entity O
wd O
: O
Q361 O
in O
KG O
. O
Therefore O
, O
temporal O
constraints O
can O
be O
interpreted O
by O
describing O
what O
kind O
of O
knowledge O
is O
needed O
and O
how O
they O
are O
composed O
. O
The O
interpretation O
structures O
of O
the O
constraints O
are O
presented O
as O
SF O
- O
TCons O
, O
the O
Semantic O
Framework O
of O
Temporal O
Constraints O
. O

Temporal O
Constraints O
in O
Questions O

Depending O
on O
whether O
the O
constraints O
concern O
quantitative O
attributes O
of O
a O
single O
event O
or O
the O
relations O
between O
events O
, O
we O
classify O
the O
temporal O
constraints O
as O
follows O
. O

Value O
Constraints O
. O
The O
intentions O
about O
quantitative O
values O
are O
often O
expressed O
with O
time O
values O
or O
ordinals O
( O
e.g. O
, O
" O
first O
president O
" O
) O
. O
They O
require O
certain O
events O
to O
have O
corresponding O
temporal O
or O
ordinal O
attributes O
. O
Thus O
, O
they O
could O
be O
denoted O
as O
follow O
. O

HASVALUE O
( O
E O
1 O
, O
T O
1 O
) O
, O
( O
VC-1 O
) O

HASVALUE O
( O
E O
1 O
, O
O O
1 O
) O
, O
( O
VC-2 O
) O

where O
E O
, O
T O
, O
O O
denotes O
events O
, O
time O
expressions O
and O
ordinals O
respectively O
. O
As O
an O
example O
, O
the O
intent O
" O
first O
president O
" O
could O
be O
denoted O
as O
HASVALUE O
( O
" O
president O
" O
, O
" O
first O
" O
) O
. O
Specifically O
, O
temporal O
interrogatives O
( O
e.g. O
, O
" O
when O
did O
sth O
. O
happen O
? O
" O
) O
are O
denoted O
as O
HASVALUE O
( O
E O
1 O
, O
T O
? O
) O
, O
which O
declare O
the O
existence O
of O
the O
temporal O
attributes O
but O
has O
no O
restrict O
on O
the O
specific O
value O
. O

Relation O
Constraints O
. O
The O
possible O
relations O
between O
time O
and O
events O
have O
been O
well O
studied O
in O
the O
AI O
area O
. O
We O
follow O
TimeML O
, O
the O
most O
commonly O
used O
annotation O
specification O
, O
to O
model O
the O
relation O
constraints O
. O
when O
] O
he O
was O
[ O
Event O
2 O
shot O
] O
? O
⟨TLINK O
reltype O
= O
SIMULTANEOUS O
target O
= O
EVENT O
1 O
relatedTo O
= O
EVENT O
2 O
signal O
= O
SIGNAL O
1 O
/ O
⟩ O
As O
illustrated O
in O
Example O
3 O
, O
temporal O
relations O
are O
triggered O
by O
certain O
signals O
( O
e.g. O
, O
" O
when O
" O
) O
and O
classified O
into O
pre O
- O
defined O
reltypes O
. O
For O
the O
practical O
demand O
of O
QA O
tasks O
, O
we O
formalized O
the O
relation O
constraints O
as O

RELATION O
( O
T O
R O
, O
E O
1 O
, O
T O
1 O
) O
, O
( O
RC-1 O
) O
RELATION O
( O
T O
R O
, O
E O
1 O
, O
E O
2 O
) O
, O
( O
RC-2 O
) O

where O
T O
R O
denotes O
the O
13 O
temporal O
reltypes O
in O
TimeML O
( O
Pustejovsky O
et O
al O
. O
, O
2003 O
) O
, O
E O
and O
T O
denotes O
events O
and O
time O
expressions O
respectively O
. O
The O
TimeML O
- O
style O
annotation O
in O
the O
example O
question O
corresponds O
the O
following O
RC-2 O
constraint O
: O
RELATION O
( O
SIMULTANEOUS O
, O
" O
standing O
" O
, O
" O
shot O
" O
) O

Interpretation O
Structure O
for O
Temporal O
Constraints O

As O
previously O
mentioned O
, O
one O
temporal O
constraint O
could O
be O
supported O
by O
various O
interpretations O
. O
We O
summarize O
6 O
interpretation O
structures O
( O
IS O
) O
according O
to O
whether O
the O
involved O
event O
expressions O
are O
intrinsically O
connected O
and O
what O
connector O
between O
them O
can O
correspond O
to O
the O
expected O
meanings O
. O
In O
order O
to O
enhance O
the O
generality O
of O
the O
IS O
as O
much O
as O
possible O
, O
we O
do O
not O
restrict O
the O
specific O
semantic O
representations O
of O
involved O
events O
, O
but O
only O
focus O
on O
the O
key O
knowledge O
that O
they O
can O
provide O
. O
The O
6 O
IS O
are O
presented O
as O
follows O
. O

IS-1 O
Comparison O
structure O

HASVALUE O
( O
E O
1 O
, O
T O
1 O
) O
| O
RELATION O
( O
T O
r O
, O
E O
1 O
, O
T O
1 O
|E O
2 O
) O
⇒ O
COMPARE⟨• O
, O
time O
( O
E O
1 O
) O
, O
time O
( O
T O
1 O
|E O
2 O
) O
⟩ O

This O
structure O
interprets O
VC-1 O
and O
RC O
, O
where O
• O
denotes O
algebraic O
predicate O
for O
time O
values O
( O
Allen O
, O
1983 O
; O
Jia O
et O
al O
. O
, O
2018b O
) O
. O
Specifically O
, O
the O
predicate O
• O
is O
required O
to O
be O
EQUAL O
in O
VC-1 O
and O
is O
determined O
according O
to O
the O
identified O
type O
T O
r O
in O
RC O
. O
This O
structure O
supposes O
that O
the O
involved O
events O
provides O
certain O
time O
values O
. O

For O
example O
, O
the O
question O
: O
" O
Which O
movie O
did O
Alfred O
Hitchcock O

[ O
Event O
1 O
direct O
] O
[ O
Signal O
1 O
in O
] O
[ O
Time O
1 O

1960 O
] O
? O
" O
corresponds O
to O
the O
following O
constraint O
and O
KG O
facts O
, O
where O
the O
" O
direct O
" O
event O
provides O
the O
value O
" O
1960 O
- O
10 O
- O
7 O
" O
. O
COMPARE⟨INCLUDES O
, O
time O
( O
" O
direct O
" O
) O
, O
" O
1960 O
" O
⟩ O

Alfred_Hitchcock O
Psycho O
( O
ANS O
) O
director O
( O
E1 O
) O
" O
1960 O
- O
10 O
- O
7 O
" O
in_time O
" O
1960 O
" O
( O
T1 O
) O
−INCLUDES→ O
IS-2 O
Ordering O
Structure O
HASVALUE O
( O
E O
1 O
, O
O O
1 O
) O
⇒ O
ORDER⟨attr O
( O
E O
1 O
) O
, O
O O
1 O
⟩ O

This O
structure O
interprets O
VC-2 O
by O
ordering O
entities O
( O
or O
facts O
) O
that O
are O
described O
by O
E O
1 O
. O
It O
supposes O
that O
E O
1 O
describes O
a O
common O
attribute O
of O
certain O
objects O
to O
be O
ordered O
. O
For O
example O
, O
the O
question O
: O
" O
When O
did O
Henry O
the O
VIII O
[ O
Event O
1 O
marry O
] O
his O
[ O
Ordinal O
1 O
first O
] O
wife O
? O
" O
corresponds O
to O
ORDER⟨attr O
( O
" O
marry O
" O
) O
, O
" O
first O
" O
⟩ O

Henry_VIII_of_England O
. O
. O
. O
Cathe O
. O
. O
. O
Aragon O
( O
T=1506 O
, O
1st O
( O
O1 O
) O
) O
spouse O
( O
E1 O
) O
Cathe O
. O
. O
. O
Parr O
( O
T=1543 O
, O
6th O
) O
spouse O
( O
E1 O
) O
IS-3 O
Direct O
Query O
Structure O
HASVALUE O
( O
E O
1 O
, O
X O
) O
⇒ O
FIND⟨ent O
( O
E O
1 O
) O
, O
attr O
( O
X O
) O
⟩ O

In O
some O
cases O
, O
the O
expected O
values O
are O
directly O
represented O
in O
KG O
facts O
. O
This O
structure O
interprets O
VC O
by O
directly O
finding O
the O
expected O
value O
X O
in O
certain O
attributes O
of O
some O
related O
entity O
. O
It O
supposes O
that O
the O
entity O
is O
related O
to O
the O
mentioned O
event O
E O
1 O
. O

For O
example O
, O
the O
description O
: O
" O
. O
. O
. O
did O
the O

IS-4 O
Same O
Entity O
Structure O

RELATION O
( O
T O
r O
, O
E O
1 O
, O
E O
2 O
) O
⇒ O
SAMEENTITY⟨e O
, O
attr O
( O
E O
1 O
) O
, O
attr O
( O
E O
2 O
) O
⟩ O

This O
structure O
interprets O
simultaneous O
cases O
of O
RC-2 O
. O
It O
supposes O
that O
the O
events O
should O
be O
attributes O
of O
a O
certain O
entity O
e O
. O

RELATION O
( O
T O
r O
, O
E O
1 O
, O
E O
2 O
) O
⇒ O
PARTOF⟨r O
p O
, O
E O
1 O
, O
E O
2 O
⟩ O

This O
structure O
interprets O
including O
cases O
of O
RC-2 O
. O
It O
does O
not O
restrict O
the O
representation O
of O
events O
E O
1 O
and O
E O
2 O
in O
KG O
, O
but O
requires O
that O
their O
representation O
must O
be O
connected O
by O
a O
relation O
r O
p O
which O
implies O
" O
part O
- O
of O
" O
. O

For O
example O
, O
the O
question O
" O
What O
award O
did O
Laurence O
Fishburne O
[ O
Event O
1 O
received O
] O
[ O
Signal O
1 O
at O
] O
[ O
Event O
2 O
the O
46th O
Tony O
Awards O
] O
? O
" O
corresponds O
to O
the O
following O
representation O
and O
KG O
facts O
, O
where O
E O
1 O
corresponds O
to O
a O
statement O
2 O
and O
E O
2 O
corresponds O
to O
a O
named O
entity O
. O
PARTOF⟨r O
p O
, O
" O
received O
" O
, O
" O
. O
. O
. O
Awards O
" O
⟩ O

Laurence_Fishburne O
Best_Featured_Actor O
( O
ANS O
) O
award_received O
E O
1 O
46th_Tony_Awards O
( O
E2 O
) O
. O
. O
. O
subject_of O
( O
rp O
) O
IS-6 O
Sequent O
Structure O
RELATION O
( O
T O
r O
, O
E O
1 O
, O
E O
2 O
) O
⇒ O
SEQUENT⟨r O
≺ O
, O
ent O
( O
E O
1 O
) O
, O
ent O
( O
E O
2 O
) O
⟩ O

This O
structure O
interprets O
before O
/ O
after O
cases O
of O
RC-2 O
. O
It O
supposes O
the O
events O
make O
a O
pair O
of O
related O
entities O
to O
be O
sequential O
in O
KG O
, O
where O
the O
entities O
are O
involved O
in O
E O
1 O
and O
E O
2 O
respectively O
and O
they O
must O
be O
connected O
by O
a O
relation O
r O
≺ O
which O
indicates O
a O
preceding O
( O
or O
succeeding O
) O
relation O
. O
In O
summary O
, O
IS O
1 O
to O
3 O
interpret O
the O
temporal O
constraints O
via O
temporal O
facts O
with O
explicit O
quantitative O
values O
. O
IS O
4 O
to O
6 O
model O
the O
intrinsic O
connections O
that O
can O
make O
events O
temporally O
related O
. O
It O
is O
worth O
noting O
that O
SF O
- O
TCons O
only O
expresses O
the O
expected O
form O
of O
corresponding O
knowledge O
, O
how O
to O
obtain O
the O
specific O
knowledge O
is O
left O
to O
the O
implementation O
of O
question O
- O
answering O
systems O
. O

3 O
Semantic B-MethodName
- I-MethodName
Framework I-MethodName
- I-MethodName
Based I-MethodName
Temporal I-MethodName
Question I-MethodName
Answering I-MethodName

Figure O
1 O
illustrates O
the O
question B-TaskName
- I-TaskName
answering I-TaskName
process O
of O
the O
semantic B-MethodName
- I-MethodName
framework I-MethodName
- I-MethodName
Based I-MethodName
temporal I-MethodName
question I-MethodName
answering I-MethodName
method O
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
. O
The O
query O
generation O
consists O
of O
two O
steps O
, O
1 O
) O
evoking O
the O
constraints O
and O
their O
possible O
interpretations O
( O
i.e. O
, O
constraint O
evocation O
) O
and O
2 O
) O
grounding O
the O
constraints O
by O
exploring O
the O
relevant O
KG O
facts O
( O
i.e. O
, O
constraint O
grounding O
) O
. O
The O
generated O
candidate O
queries O
will O
be O
ranked O
by O
a O
BERT O
model O
, O
and O
the O
execution O
results O
of O
the O
highest O
- O
scored O
query O
will O
be O
considered O
as O
answers O
. O

Constraint O
Evocation O

The O
first O
step O
of O
SF B-MethodName
- I-MethodName
TQA I-MethodName
is O
to O
determine O
the O
possible O
constraints O
. O
We O
fine O
- O
tune O
a O
BERT O
model O
to O
annotate O
the O
temporal O
elements O
. O
The O
corresponding O
constraints O
and O
interpretation O
structures O
are O
evoked O
according O
to O
recognized O
signals O
. O
The O
elements O
that O
involve O
certain O
constraints O
are O
determined O
by O
TimeML O
relations O
or O
by O
simply O
taking O
the O
temporal O
elements O
that O
are O
directly O
described O
by O
the O
signals O
( O
i.e. O
, O
the O
nearest O
neighbor O
of O
the O
corresponding O
signals O
) O
. O
The O
algebraic O
predicates O
in O
the O
comparison O
structure O
are O
determined O
by O
normalizing O
the O
TimeML O
relation O
types O
, O
while O
other O
implicit O
elements O
are O
left O
to O
the O
grounding O
phase O
. O

Constraint O
Grounding O

In O
general O
query O
graph O
generation O
, O
basic O
query O
graphs O
are O
constructed O
as O
1 B-HyperparameterValue
or O
2 B-HyperparameterValue
hop O
paths O
from O
mentioned O
entities O
to O
answers O
, O
and O
they O
are O
extended O
by O
pre O
- O
defined O
expanding O
action O
( O
Yih O
et O
al O
. O
, O
2015 O
) O
or O
fixed O
interpretation O
structures O
of O
constraints O
( O
Bao O
et O
al O
. O
, O
2016 O
) O
. O
In O
temporal B-TaskName
KGQA I-TaskName
, O
the O
main O
issue O
is O
that O
events O
could O
have O
various O
representations O
in O
KG O
. O
As O
illustrated O
by O
the O
examples O
in O
Section O
2.2 O
, O
they O
could O
be O
represented O
by O
named O
entities O
, O
triplet O
facts O
, O
or O
attributes O
of O
their O
participants O
. O
Therefore O
, O
we O
treat O
the O
generation O
of O
query O
graphs O
as O
grounding O
the O
temporal O
elements O
in O
the O
interpretations O
of O
SF O
- O
TCons O
. O
We O
divide O
the O
descriptions O
of O
events O
into O
nominal O
and O
predicative O
. O
We O
suppose O
that O
nominal O
descriptions O
could O
be O
the O
event O
themselves O
, O
and O
predicative O
descriptions O
reflect O
certain O
aspects O
of O
the O
events O
, O
such O
as O
their O
participants O
or O
their O
post O
- O
effect O
. O
Therefore O
, O
nominal O
events O
could O
be O
linked O
entities O
and O
others O
correspond O
to O
the O
neighboring O
nodes O
or O
facts O
of O
the O
explored O
subgraph O
( O
s O
) O
or O
linked O
entities O
. O
The O
corresponding O
nodes O
or O
facts O
must O
provide O
the O
knowledge O
required O
by O
corresponding O
interpolation O
structures O
. O

We O
illustrate O
the O
above O
process O
by O
the O
example O
in O
Figure O
2 O
. O
In O
this O
example O
, O
the O
entity O
linking O
module O
will O
provide O
John_Lennon O
as O
a O
linked O
entity O
, O
and O
the O
grounding O
start O
with O
the O
" O
shot O
" O
event O
which O
contains O
the O
only O
linked O
entity O
. O
We O
will O
explore O
all O
the O
neighboring O
facts O
of O
John_Lennon O
( O
as O
illustrated O
in O
Figure O
2 O
) O
as O
candidates O
for O
the O
event O
. O
Since O
" O
shot O
" O
is O
a O
predicative O
event O
and O
the O
SAME_ENTITY O
constraint O
requires O
it O
to O
provide O
an O
attribute O
, O
we O
will O
find O
a O
triplet O
that O
contains O
John_Lennon O
and O
take O
the O
other O
entity O
in O
it O
( O
i.e. O
, O
Murder_of_John_Lennon O
) O
as O
the O
expected O
e. O
Similarly O
, O
we O
explore O
the O
neighboring O
facts O
and O
select O
one O
relation O
that O
matches O
with O
the O
question O
meaning O
( O
i.e. O
, O
location O
for O
" O
standing O
" O
) O
. O
When O
there O
are O
multiple O
candidate O
relations O
, O
we O
will O
rank O
the O
candidates O
by O
scoring O
their O
serializations O
with O
a O
BERT O
model O
. O
The O
highest O
- O
scored O
one O
will O
be O
filled O
in O
the O
corresponding O
slot O
. O

In O
the O
specific O
implementation O
, O
which O
candidates O
satisfy O
the O
question O
meanings O
best O
are O
determined O
by O
neural O
models O
. O
In O
the O
training O
process O
, O
we O
take O
relations O
that O
appear O
on O
shortest O
paths O
between O
mentioned O
entities O
and O
answers O
as O
positive O
samples O
. O
In O
particular O
, O
the O
relation O
that O
entails O
part O
of O
or O
precedes O
are O
filtered O
according O
to O
the O
KG O
schema O
in O
the O
training O
process O
and O
are O
predicted O
by O
neural O
models O
during O
the O
test O
process O
. O
Queries O
for O
the O
questions O
of O
multiple O
constraints O
are O
the O
conjunction O
of O
the O
grounding O
result O
of O
each O
constraint O
and O
queries O
for O
the O
questions O
with O
no O
temporal O
constraints O
are O
unrestricted O
basic O
query O
graphs O
. O

Query O
Ranking O

SF B-MethodName
- I-MethodName
TQA I-MethodName
usually O
generates O
multiple O
candidate O
queries O
for O
one O
question O
. O
We O
select O
one O
of O
the O
candidates O
via O
neural O
ranking O
models O
. O
Specifically O
, O
we O
express O
the O
generated O
queries O
via O
SPARQL O
3 O
and O
serialize O
the O
queries O
by O
dropping O
auxiliary O
symbols O
( O
e.g. O
, O
" O
{ O
" O
) O
. O
We O
use O
the O
BERT O
model O
with O
crossentropy O
loss O
to O
score O
the O
pair O
of O
the O
input O
question O
and O
serialized O
queries O
. O
For O
each O
question O
, O
we O
use O
the O
candidate O
queries O
with O
the O
highest O
F B-MetricName
1 I-MetricName
score I-MetricName
as O
the O
positive O
samples O
and O
select O
k B-HyperparameterName
others O
as O
negative O
samples O
. O
In O
order O
to O
make O
our O
model O
more O
robust O
, O
we O
classify O
the O
negatives O
samples O
as O
confusing O
queries O
and O
irrelevant O
queries O
. O
Confusing O
queries O
are O
those O
that O
can O
find O
partial O
answers O
but O
of O
lower O
F B-MetricName
1 I-MetricName
scores I-MetricName
than O
the O
positive O
samples O
. O
Irrelevant O
queries O
are O
those O
whose O
outputs O
have O
no O
intersection O
with O
the O
correct O
answers O
. O
The O
ratio O
of O
confusing O
queries O
to O
irrelevant O
queries O
is O
1 O
: O
1 O
. O
The O
necessity O
of O
classifying O
the O
negative O
sample O
is O
presented O
in O
Appendix O
A O
. O

Evaluation O

Datasets O

We O
evaluate O
our O
method O
on O
TempQuestions B-DatasetName
( O
Jia O
et O
al O
. O
, O
2018a O
) O

Evaluation O
Metrics O

We O
report O
the O
Hit B-MetricName
@ I-MetricName
1 I-MetricName
( O
denoted O
as O
H B-MetricName
@ I-MetricName
1 I-MetricName
) O
, O
Precision B-MetricName
( O
denoted O
as O
P B-MetricName
r I-MetricName
) O
, O
Recall B-MetricName
( O
denoted O
as O
Re O
) O
and O
F B-MetricName
1 I-MetricName
score I-MetricName
of O
the O
evaluation O
results O
. O
Our O
computation O
follows O
Jia O
et O
al O
. O
's O
( O
2021 O
) O
6 O
, O
where O
the O
precision B-MetricName
is O
considered O
1 B-MetricValue
if O
the O
output O
of O
a O
question O
is O
empty O
and O
the O
F B-MetricName
1 I-MetricName
score I-MetricName
on O
a O
dataset O
is O
computed O
as O
the O
average O
of O
the O
scores O
of O
each O
question O
. O

Compared O
Methods O

On O
TempQuestions B-DatasetName
, O
we O
compare O
our O
results O
with O
general B-TaskName
KGQA I-TaskName
methods O
AQQU B-MethodName
( O
Bast O
and O
Haussmann O
, O
2015 O
) O
, O
QUINT B-MethodName
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
and O
their O
improved O
version O
( O
Jia O
et O
al O
. O
, O
2018b O
) O
by O
incorporating O
the O
temporal O
question O
decomposition O
method O
TEQUILA B-MethodName
, O
QUINT+TEQUILA B-MethodName
and O
AQQU+TEQUILA B-MethodName
. O
On O
TimeQuestions B-DatasetName
, O
we O
compare O
our O
results O
with O
general B-TaskName
KGQA I-TaskName
methods O
Pull B-MethodName
- I-MethodName
Net I-MethodName
( O
Sun O
et O
al O
. O
, O
2019 O
) O
, O
GraftNet B-MethodName
( O
Sun O
et O
al O
. O
, O
2018 O
) O
, O
5 O
https O
: O
/ O
/ O
archive.org O
/ O
download O
/ O
wikibase O
- O
wikidatawiki-20190128 O

6 O
Their O
script O
could O
be O
downloaded O
from O
here O
. O

UNIQORN B-MethodName
and O
the O
temporal B-TaskName
KGQA I-TaskName
method O
EXAQT B-MethodName
( O
Jia O
et O
al O
. O
, O
2021 O
) O
. O

Implementation O
Details O

Our O
results O
are O
obtained O
on O
a O
workstation O
with O
an O
Intel O
Xeon O
Gold O
5222 O
CPU O
, O
32 O
GB O
of O
RAM O
, O
and O
NVIDIA O
RTX3090 O
GPUs O
. O
The O
hyper O
- O
parameters O
of O
the O
ranking O
models O
are O
listed O
in O
Table O
2 O
. O
They O
are O
determined O
according O
to O
the O
F B-MetricName
1 I-MetricName
scores I-MetricName
on O
the O
development O
sets O
. O
We O
use O
ELQ O
( O
Li O
et O
al O
. O
, O
2020 O
) O
results O
on O
both O
two O
benchmarks O
. O
Specially O
, O
we O
improve O
the O
F B-MetricName
1 I-MetricName
scores I-MetricName
by O
+3.6 B-MetricValue
and O
+7.1 B-MetricValue
points O
on O
TempQuestions B-DatasetName
and O
TimeQuestions B-DatasetName
respectively O
. O
On O
TempQuestions B-DatasetName
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
improves O
the O
Hit B-MetricName
@ I-MetricName
1 I-MetricName
and O
precision B-MetricName
by O
+5.0 B-MetricValue
and O
+1.9 B-MetricValue
respectively O
. O
On O
TimeQuestions B-DatasetName
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
achieves O
better O
recall B-MetricName
( O
5.2 B-MetricValue
points O
higher O
) O
while O
EXAQT B-MethodName
achieves O
better O
precision B-MetricName
( O
4.2 B-MetricValue
points O
higher O
) O
. O
The O
reason O
could O
be O
the O
different O
strategies O
when O
dealing O
with O
unsolvable O
problems O
. O
EXAQT B-MethodName
tends O
to O
output O
empty O
answers O
( O
which O
correspond O
to O
1 O
in O
precision O
) O
and O
SF B-MethodName
- I-MethodName
TQA I-MethodName
degrades O
to O
the O
unrestricted O
generation O
of O
basic O
query O
graphs O
( O
which O
capture O
incomplete O
question O
meanings O
) O
. O
The O
Hit B-MetricName
@ I-MetricName
1 I-MetricName
of O
SF B-MethodName
- I-MethodName
TQA I-MethodName
is O
2.6 B-MetricValue
points O
lower O
than O
EXAQT B-MethodName
might O
because O
EXAQT B-MethodName
ranks O
all O
candidate O
answers O
while O
SF B-MetricValue
- I-MetricValue
TQA I-MetricValue
just O
randomly O
returns O
one O
candidate O
that O
satisfies O
the O
generated O
query O
. O

Main O
Results O

Ablation O
Studies O

We O
conduct O
ablations O
on O
the O
necessity O
of O
interpretations O
for O
intrinsic O
connections O
( O
i.e. O
, O
IS-4 O
to O
6 O
) O
. O
We O
analyze O
the O
result O
on O
questions O
with O
only O
relation O
constraints O
. O
The O
ablation O
results O
are O
illustrated O
in O
Table O
5 O
. O
The O
2nd O
row O
shows O
that O
without O
IS O
4 O
to O
6 O
the O
F B-MetricName
1 I-MetricName
scores I-MetricName
drop O
1.5 B-MetricValue
and O
3.8 B-MetricValue
points O
respectively O
on O
the O
benchmarks O
. O
The O
3rd O
row O
shows O
that O
results O
obtained O
by O
generating O
only O
basic O
query O
and O
the O
reported O
results O
of O
compared O
methods O
on O
TimeQuestions B-DatasetName
are O
provided O
by O
the O
authors O
of O
EXAQT B-MethodName
( O
Jia O
et O
al O
. O
, O
2021 O
) O
. O

graphs O
without O
any O
restriction O
will O
decrease O
the O
F B-MetricName
1 I-MetricName
scores I-MetricName
by O
14.1 B-MetricValue
and O
4.7 B-MetricValue
points O
respectively O
. O
The O
differences O
between O
the O
results O
on O
the O
two O
benchmarks O
might O
reflect O
the O
differences O
between O
underlying O
KGs O
. O
SF B-MethodName
- I-MethodName
TQA I-MethodName
without O
IS O
4 O
to O
6 O
achieves O
acceptable O
results O
on O
TempQuestions B-DatasetName
, O
which O
might O
indicate O
that O
Freebase O
can O
provide O
sufficient O
temporal O
facts O
for O
comparisons O
. O
SF B-MethodName
- I-MethodName
TQA I-MethodName
with O
only O
basic O
query O
graphs O
on O
TimeQuestions B-DatasetName
performs O
much O
better O
than O
on O
TempQuestions B-DatasetName
, O
which O
might O
indicate O
that O
Wikidata O
provides O
richer O
and O
finer O
relations O
between O
entities O
, O
thus O
the O
connections O
between O
mentioned O
entities O
and O
answers O
are O
more O
likely O
to O
be O
satisfied O
via O
simple O
relation O
paths O
. O

Method O
TempQ. B-DatasetName
TimeQ B-DatasetName
. O

Error O
Analysis O

We O
analyze O
the O
main O
errors O
of O
100 O
questions O
of O
which O
the O
F B-MetricName
1 I-MetricName
scores I-MetricName
are O
less O
than O
1 B-MetricValue
. O
The O
results O
are O
illustrated O
in O
Table O
6 O
. O

Main O
Error O

TempQ. B-DatasetName
TimeQ. B-DatasetName
The O
1st O
row O
counts O
the O
questions O
with O
incorrectly O
recognized O
entities O
or O
temporal O
constraints O
, O
which O
reveals O
that O
SF B-MethodName
- I-MethodName
TQA I-MethodName
severely O
suffers O
from O
error O
propagations O
on O
TimeQuestions B-DatasetName
. O
The O
2nd O
row O
counts O
the O
questions O
whose O
meaning O
can O
not O
be O
perfectly O
expressed O
by O
generated O
constraints O
( O
e.g. O
, O
questions O
with O
multi O
- O
hop O
non O
- O
temporal O
property O
paths O
like O
" O
wife O
of O
the O
actor O
who O
played O
in O
the O
movie O
pinball O
wizard O
" O
) O
. O
The O
3rd O
row O
shows O
that O
our O
ranking O
model O
is O
hard O
to O
train O
with O
limited O
data O
( O
TempQuestions B-DatasetName
contains O
less O
than O
1,000 B-HyperparameterValue
training B-HyperparameterName
samples I-HyperparameterName
) O
. O
Besides O
, O
data O
quality O
appears O
to O
be O
an O
important O
issue O
. O
In O
about O
10 O
% O
of O
the O
sample O
questions O
, O
the O
provided O
answers O
are O
inconsistent O
with O
the O
knowledge O
in O
the O
given O
KG O
. O
For O
example O
, O
TimeQuestions B-DatasetName
annotate O
2010_F1_Championship O
( O
wd O
: O
Q69934 O
) O
as O
the O
answer O
of O
" O
Who O
won O
the O
2010 O
f1 O
championship O
? O
" O
. O
For O
over O
1 O
/ O
3 O
of O
the O
sampled O
questions O
, O
KG O
can O
not O
provide O
sufficient O
evidence O
( O
e.g. O
, O
occurrence O
times O
of O
the O
corresponding O
facts O
are O
not O
provided O
) O
for O
obtaining O
all O
answers O
. O

Related O
work O

Temporal O
Information O
in O
Natural O
Language O
. O
Temporal O
information O
has O
attracted O
the O
attention O
of O
AI O
and O
linguistics O
communities O
for O
a O
long O
time O
. O
Allen O
presents O
an O
interval O
- O
based O
temporal O
logic O
for O
reasoning O
the O
relation O
between O
time O
duration O
( O
Allen O
, O
1983 O
) O
and O
a O
computation O
theory O
for O
action O
and O
time O
( O
Allen O
, O
1984 O
) O
. O
He O
concludes O
13 O
possible O
interval O
relations O
with O
their O
transitivity O
table O
. O
Mani O
and O
Wilson O
( O
2000 O
) O
introduces O
an O
annotation O
scheme O
for O
temporal O
expression O
in O
news O
and O
discusses O
its O
possible O
application O
in O
event O
ordering O
and O
event O
time O
alignment O
. O
TimeML O
( O
Pustejovsky O
et O
al O
. O
, O
2003 O
( O
Pustejovsky O
et O
al O
. O
, O
, O
2010 O
) O
is O
a O
specification O
for O
annotating O
temporal O
information O
from O
narratives O
. O
TimeML O
has O
become O
the O
de O
facto O
standard O
in O
the O
NLP O
community O
. O
It O
annotates O
time O
expressions O
, O
events O
, O
the O
relations O
between O
them O
, O
and O
the O
signals O
that O
trigger O
the O
relations O
in O
XML O
form O
. O

Temporal B-TaskName
KGQA I-TaskName
. O
Early O
KGQA B-TaskName
systems O
usually O
do O
not O
handle O
temporal O
constraints O
( O
e.g. O
, O
Berant O
and O
Liang O
, O
2014 O
) O
or O
apply O
simple O
heuristics O
about O
their O
surface O
forms O
( O
Berant O
et O
al O
. O
, O
2013 O
; O
Bast O
and O
Haussmann O
, O
2015 O
; O
Bao O
et O
al O
. O
, O
2016 O
) O
. O
Some O
benchmarks O
that O
specifically O
focus O
on O
temporal O
intents O
in O
KGQA B-TaskName
emerge O
in O
recent O
years O
, O
including O
Tem O
- O
pQuestions O
( O
Jia O
et O
al O
. O
, O
2018b O
) O
, O
TimeQuestions B-DatasetName
( O
Jia O
et O
al O
. O
, O
2021 O
) O
and O
TempQA B-DatasetName
- I-DatasetName
WD I-DatasetName
( O
Neelam O
et O
al O
. O
, O
2022 O
) O
. O
In O
terms O
of O
the O
technologies O
for O
temporal B-TaskName
KGQA I-TaskName
, O
Jia O
et O
al O
. O
( O
2018a O
) O
proposes O
TEQUILA B-MethodName
. O
It O
relies O
on O
limited O
hand O
- O
crafted O
rules O
to O
decompose O
complex O
temporal O
relations O
and O
solves O
composed O
simple O
questions O
via O
underlying O
general B-TaskName
KGQA I-TaskName
systems O
. O
EXAQT B-MethodName
( O
Jia O
et O
al O
. O
, O
2021 O
) O
uses O
Group O
Steiner O
Trees O
to O
anchor O
a O
KG O
sub O
- O
graph O
for O
each O
question O
, O
retrieving O
answers O
in O
the O
sub O
- O
graph O
with O
augmented O
temporal O
facts O
by O
an O
RGCN O
model O
. O
Besides O
, O
there O
are O
also O
some O
researches O
specifically O
focus O
on O
question O
event O
- O
centric O
or O
temporal O
knowledge O
graphs O
. O
Costa O
et O
al O
. O
( O
2020 O
) O
proposes O
a O
question O
answering O
benchmark O
Event O
- O
QA O
over O
EventKG O
Demidova O
, O
2018 O
, O
2019 O
) O
. O
Saxena O
et O
al O
. O
( O
2021 O
) O
proposes O
CronQuestion O
over O
a O
sub O
- O
graph O
of O
Wikidata O
with O
a O
limited O
subset O
of O
relations O
for O
evaluating O
temporal O
KG O
embeddings O
. O

In O
summary O
, O
existing O
temporal B-TaskName
KGQA I-TaskName
methods O
either O
analyze O
only O
the O
surface O
form O
of O
temporal O
constraints O
or O
rely O
on O
end O
- O
to O
- O
end O
neural O
models O
. O
While O
neural O
models O
might O
be O
robust O
to O
diversified O
representations O
of O
knowledge O
, O
they O
are O
hard O
to O
characterize O
the O
clear O
boundaries O
of O
temporal O
constraints O
( O
e.g. O
, O
accurately O
filtering O
all O
events O
that O
occur O
before O
2022 O
) O
. O

KGQA O
via O
Query O
Graph O
Generation O
. O
Constructing O
queries O
via O
exploring O
the O
relevant O
facts O
of O
mentioned O
entities O
is O
a O
common O
practice O
in O
KGQA O
, O
especially O
in O
the O
situations O
where O
only O
questionanswers O
pairs O
are O
provided O
. O
Yih O
et O
al O
. O
( O
2015 O
) O
defines O
query O
graphs O
that O
can O
be O
straightforwardly O
mapped O
to O
an O
executable O
logical O
query O
. O
They O
model O
the O
generation O
of O
query O
graphs O
as O
a O
staged O
search O
problem O
, O
where O
the O
query O
graphs O
are O
expanded O
by O
exploring O
legitimate O
predicate O
sequences O
starting O
from O
mentioned O
entities O
. O
Bao O
et O
al O
. O
( O
2016 O
) O
expands O
basic O
query O
graphs O
with O
6 O
kinds O
of O
manually O
designed O
constraints O
including O
quantitative O
temporal O
and O
ordinal O
constraints O
. O
Luo O
et O
al O
. O
( O
2018 O
) O
encodes O
query O
graphs O
of O
complex O
structures O
into O
a O
uniform O
vector O
representation O
for O
complex O
questions O
. O
Lan O
and O
Jiang O
( O
2020 O
) O
prunes O
the O
search O
space O
via O
early O
incorporation O
of O
constraints O
. O

The O
existing O
query O
graph O
generation O
methods O
are O
not O
specifically O
designed O
for O
temporal O
constraints O
, O
they O
simply O
suppose O
that O
temporal O
or O
ordinal O
signals O
correspond O
to O
quantitative O
constraints O
. O
Specifically O
, O
Bao O
et O
al O
. O
( O
2016 O
) O
, O
Luo O
et O
al O
. O
( O
2018 O
) O
, O
and O
Lan O
and O
Jiang O
( O
2020 O
) O
recognize O
time O
constraints O
via O
syntax O
signals O
and O
simply O
interpret O
them O
as O
general O
aggregation O
functions O
( O
e.g. O
, O
greater O
than O
X O
, O
max O
at O
N O
) O
, O
i.e. O
, O
their O
interpretations O
of O
temporal O
constraints O
are O
similar O
to O
" O
SF B-MethodName
- I-MethodName
TQA I-MethodName
w O
/ O
o O
IS-4 O
to O
6 O
" O
( O
referring O
to O
Table O
5 O
) O
. O
In O
contrast O
, O
we O
systematically O
analyze O
the O
interpretation O
structure O
of O
temporal O
constraints O
, O
including O
the O
analysis O
of O
what O
kind O
of O
intrinsic O
connection O
can O
make O
events O
temporally O
related O
. O

Conclusion O
and O
Future O
work O

In O
this O
paper O
, O
we O
study O
the O
logical O
constraint O
that O
corresponds O
to O
temporal O
intents O
in O
questions O
. O
Our O
main O
contributions O
can O
be O
summarized O
as O
follows O
: O

• O
We O
propose O
the O
idea O
of O
analyzing O
temporal O
intents O
via O
possible O
interpretation O
structures O
. O
We O
conclude O
the O
interpretation O
structures O
as O
SF O
- O
TCons O
, O
which O
allows O
one O
constraint O
expression O
has O
various O
interpretations O
. O

• O
We O
propose O
the O
semantic O
- O
framework O
- O
based O
temporal O
question O
answering O
method O
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
. O
SF B-MethodName
- I-MethodName
TQA I-MethodName
mitigates O
the O
heterogeneity O
between O
expressions O
of O
temporal O
intents O
and O
KG O
facts O
. O
It O
enhances O
the O
query O
generation O
via O
structural O
restrictions O
provided O
by O
SF O
- O
TCons O
. O

• O
Our O
implementation O
of O
SF B-MethodName
- I-MethodName
TQA I-MethodName
establishes O
new O
SOTAs O
on O
two O
benchmarks O
and O
improves O
the O
F B-MetricName
1 I-MetricName
scores I-MetricName
by O
+3.6 B-MetricValue
and O
+7.1 B-MetricValue
points O
respectively O
. O

In O
the O
near O
future O
, O
it O
is O
worth O
exploring O
to O
alleviate O
the O
possible O
knowledge O
incompleteness O
in O
practical O
KG O
by O
developing O
a O
hybrid O
questionanswering O
method O
on O
both O
knowledge O
graphs O
and O
web O
texts O
. O
In O
addition O
, O
this O
paper O
focuses O
only O
on O
temporal O
intent O
, O
while O
problems O
in O
real O
configurations O
may O
contain O
both O
complex O
non O
- O
temporal O
and O
temporal O
intents O
. O
Therefore O
, O
it O
would O
be O
helpful O
to O
combine O
SF O
- O
TCons O
with O
general B-TaskName
KGQA I-TaskName
systems O
for O
complex O
questions O
. O

Limitations O

• O
Due O
to O
the O
compositionality O
of O
natural O
language O
, O
a O
temporal O
question O
could O
be O
very O
complex O
, O
which O
is O
beyond O
the O
ability O
of O
our O
implemented O
QA O
system O
. O
For O
example O
, O
The O
following O
question O
is O
syntactically O
legitimate O
but O
can O
not O
be O
handled O
by O
SF B-MethodName
- I-MethodName
TQA I-MethodName
: O
" O
What O
year O
did O
the O
second O
president O
of O
the O
United O
States O
, O
elected O
after O
the O
last O
spouse O
of O
the O
author O
of O
' O
Wish O
Tree O
for O
Washington O
, O
DC O
' O
was O
shot O
, O
marry O
his O
wife O
? O
" O

• O
While O
the O
linguistic O
and O
entity O
annotations O
help O
SF B-MethodName
- I-MethodName
TQA I-MethodName
alleviate O
the O
lack O
of O
structured O
supervision O
, O
they O
make O
it O
hard O
to O
apply O
SF B-MethodName
- I-MethodName
TQA I-MethodName
to O
low O
- O
resource O
languages O
or O
questions O
with O
no O
named O
entities O
( O
e.g. O
, O
" O
What O
are O
the O
important O
events O
that O
will O
happen O
at O
the O
turn O
of O
the O
century O
? O
" O
) O
. O
Besides O
, O
as O
a O
pipeline O
method O
, O
SF B-MethodName
- I-MethodName
TQA I-MethodName
suffers O
from O
possible O
error O
propagations O
. O

Acknowledgements O

This O
work O
was O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
under O
Grant O
No O
. O
62072224 O
. O
and O
the O
Program O
B O
for O
Outstanding O
PhD O
candidate O
of O
Nanjing O
University O
. O
The O
authors O
would O
like O
to O
thank O
all O
the O
participants O
of O
this O
work O
and O
anonymous O
reviewers O
. O

A O
Appendix O
for O
Different O
Training O
Strategies O

We O
also O
evaluated O
the O
effects O
of O
different O
training O
strategies O
as O
illustrated O
in O
Table O
7 O
. O
The O
results O
in O
2 O
to O
4 O
rows O
are O
obtained O
by O
simply O
using O
the O
irrelevant O
negatives O
, O
confusing O
negatives O
or O
randomly O
sampling O
negatives O
without O
classification O
respectively O
. O
The O
results O
show O
that O
both O
of O
the O
two O
types O
of O
negative O
sample O
are O
needed O
for O
training O
. O
The O
balanced O
sampling O
of O
the O
two O
types O
effectively O
improves O
SF B-MethodName
- I-MethodName
TQA I-MethodName
on O
the O
smaller O
dataset O
, O
TempQuestions B-DatasetName
. O

Method O
TempQ. B-DatasetName
TimeQ B-DatasetName
. O

Full O
System O
41.2 B-MetricValue
41.1 B-MetricValue
53.9 B-MetricValue
52.7 B-MetricValue
w O
/ O
o O
confusing O
34.9 B-MetricValue
35.9 B-MetricValue
49.5 B-MetricValue
49.3 B-MetricValue
w O
/ O
o O
irrelevant O
10.6 B-MetricValue
10.4 B-MetricValue
37.5 B-MetricValue
36.0 B-MetricValue
random O
neg O
. O

37.3 B-MetricValue
37.4 B-MetricValue
53.5 B-MetricValue
52.6 B-MetricValue

Exploring O
Representation O
- O
Level O
Augmentation O
for O
Code O
Search O

Code B-TaskName
search I-TaskName
, O
which O
aims O
at O
retrieving O
the O
most O
relevant O
code O
fragment O
for O
a O
given O
natural O
language O
query O
, O
is O
a O
common O
activity O
in O
software O
development O
practice O
. O
Recently O
, O
contrastive O
learning O
is O
widely O
used O
in O
code B-TaskName
search I-TaskName
research O
, O
where O
many O
data O
augmentation O
approaches O
for O
source O
code O
( O
e.g. O
, O
semantic O
- O
preserving O
program O
transformation O
) O
are O
proposed O
to O
learn O
better O
representations O
. O
However O
, O
these O
augmentations O
are O
at O
the O
raw O
- O
data O
level O
, O
which O
requires O
additional O
code O
analysis O
in O
the O
preprocessing O
stage O
and O
additional O
training O
costs O
in O
the O
training O
stage O
. O
In O
this O
paper O
, O
we O
explore O
augmentation O
methods O
that O
augment O
data O
( O
both O
code O
and O
query O
) O
at O
representation O
level O
which O
does O
not O
require O
additional O
data O
processing O
and O
training O
, O
and O
based O
on O
this O
we O
propose O
a O
general B-MethodName
format I-MethodName
of I-MethodName
representationlevel I-MethodName
augmentation I-MethodName
that O
unifies O
existing O
methods O
. O
Then O
, O
we O
propose O
three O
new O
augmentation O
methods O
( O
linear B-MethodName
extrapolation I-MethodName
, O
binary B-MethodName
interpolation I-MethodName
, O
and O
Gaussian B-MethodName
scaling I-MethodName
) O
based O
on O
the O
general O
format O
. O
Furthermore O
, O
we O
theoretically O
analyze O
the O
advantages O
of O
the O
proposed O
augmentation O
methods O
over O
traditional O
contrastive O
learning O
methods O
on O
code O
search O
. O
We O
experimentally O
evaluate O
the O
proposed O
representationlevel O
augmentation O
methods O
with O
state O
- O
of O
- O
theart O
code O
search O
models O
on O
a O
large O
- O
scale O
public O
dataset O
consisting O
of O
six O
programming O
languages O
. O
The O
experimental O
results O
show O
that O
our O
approach O
can O
consistently O
boost O
the O
performance O
of O
the O
studied O
code O
search O
models O
. O
Our O
source O
code O
is O
available O
at O
https O
: O
/ O
/ O
github O
. O
com O
/ O
Alex O
- O
HaochenLi O
/ O
RACS O
. O

Introduction O

In O
software O
development O
, O
developers O
often O
search O
and O
reuse O
commonly O
used O
functionalities O
to O
improve O
their O
productivity O
( O
Nie O
et O
al O
. O
, O
2016 O
; O
Shuai O
et O
al O
. O
, O
2020 O
) O
. O
With O
the O
growing O
size O
of O
large O
- O
scale O
codebases O
such O
as O
GitHub O
, O
retrieving O
semantically O
* O
Corresponding O
author O
. O
relevant O
code O
fragments O
accurately O
becomes O
increasingly O
important O
in O
this O
field O
( O
Allamanis O
et O
al O
. O
, O
2018 O
; O
. O

Traditional O
approaches O
( O
Nie O
et O
al O
. O
, O
2016 O
; O
Yang O
and O
Huang O
, O
2017 O
; O
Rosario O
, O
2000 O
; O
Hill O
et O
al O
. O
, O
2011 O
; O
Satter O
and O
Sakib O
, O
2016 O
; O
Lv O
et O
al O
. O
, O
2015 O
; O
Van O
Nguyen O
et O
al O
. O
, O
2017 O
) O
leverage O
information O
retrieval O
techniques O
to O
treat O
code O
snippets O
as O
natural O
language O
text O
and O
match O
certain O
terms O
in O
code O
with O
queries O
, O
hence O
suffering O
from O
the O
vocabulary O
mismatch O
problem O
( O
McMillan O
et O
al O
. O
, O
2011 O
; O
Robertson O
et O
al O
. O
, O
1995 O
) O
. O
Deep O
siamese O
neural O
networks O
first O
embed O
queries O
and O
code O
fragments O
into O
a O
joint O
embedding O
space O
, O
then O
measure O
similarity O
by O
calculating O
dot O
product O
or O
cosine O
distance O
( O
Lv O
et O
al O
. O
, O
2015 O
; O
Cambronero O
et O
al O
. O
, O
2019 O
; O
Gu O
et O
al O
. O
, O
2021 O
) O
. O
Recently O
, O
with O
the O
popularity O
of O
large O
scale O
pre O
- O
training O
techniques O
, O
some O
big O
models O
for O
source O
code O
( O
Guo O
et O
al O
. O
, O
2021 O
; O
Feng O
et O
al O
. O
, O
2020 O
; O
Guo O
et O
al O
. O
, O
2022 O
; O
Jain O
et O
al O
. O
, O
2021 O
; O
Li O
et O
al O
. O
, O
2022 O
) O
with O
various O
pre O
- O
training O
tasks O
are O
proposed O
and O
significantly O
outperform O
previous O
models O
. O

Contrastive B-MethodName
learning I-MethodName
is O
widely O
adopted O
by O
the O
above O
- O
mentioned O
models O
. O
It O
is O
suitable O
for O
code O
search O
because O
the O
learning O
objective O
aims O
to O
push O
apart O
negative O
query O
- O
code O
pairs O
and O
pull O
together O
positive O
pairs O
at O
the O
same O
time O
. O
In O
contrastive O
learning O
, O
negative O
pairs O
are O
usually O
generated O
by O
In O
- O
Batch O
Augmentation O
( O
Huang O
et O
al O
. O
, O
2021 O
) O
. O
For O
positive O
pairs O
, O
besides O
labeled O
ones O
, O
some O
researchers O
proposed O
augmentation O
approaches O
to O
generate O
more O
positive O
pairs O
( O
Bui O
et O
al O
. O
, O
2021 O
; O
Jain O
et O
al O
. O
, O
2021 O
; O
Fang O
et O
al O
. O
, O
2020 O
; O
. O
The O
main O
hypothesis O
behind O
these O
approaches O
is O
that O
augmentations O
do O
not O
change O
the O
original O
semantics O
. O
However O
, O
these O
approaches O
are O
resource O
- O
consuming O
( O
Yin O
et O
al O
. O
, O
2021 O
; O
Jeong O
et O
al O
. O
, O
2022 O
) O
. O
Models O
have O
to O
embed O
the O
data O
again O
for O
the O
augmented O
data O
. O

To O
solve O
this O
problem O
, O
some O
researchers O
proposed O
representation O
- O
level O
augmentation O
, O
which O
augments O
the O
representations O
of O
the O
original O
data O
. O
For O
example O
, O
linear B-MethodName
interpolation I-MethodName
, O
a O
representationlevel B-MethodName
augmentation I-MethodName
method O
, O
is O
adopted O
by O
many O
classification O
tasks O
in O
NLP O
( O
Guo O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2020 O
; O
Du O
et O
al O
. O
, O
2021 O
) O
. O
The O
augmented O
representation O
captures O
the O
structure O
of O
the O
data O
manifold O
and O
hence O
could O
force O
model O
to O
learn O
better O
features O
, O
as O
argued O
by O
Verma O
et O
al O
. O
( O
2021 O
) O
. O
These O
augmentation O
approaches O
are O
also O
considered O
to O
be O
semantic O
- O
preserving O
. O

The O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
methods O
are O
not O
investigated O
on O
the O
code B-TaskName
search I-TaskName
task O
before O
. O
To O
the O
best O
of O
our O
knowledge O
, O
Jeong O
et O
al O
. O
( O
2022 O
) O
is O
the O
only O
work O
to O
bring O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
approaches O
to O
a O
retrieval O
task O
. O
Besides O
linear B-MethodName
interpolation I-MethodName
, O
it O
also O
proposes O
another O
approach O
called O
stochastic B-MethodName
perturbation I-MethodName
for I-MethodName
document I-MethodName
retrieval I-MethodName
. O
Although O
these O
augmentation O
methods O
bring O
improvements O
to O
model O
performance O
, O
they O
are O
not O
yet O
fully O
investigated O
. O
The O
relationships O
between O
the O
existing O
methods O
and O
how O
they O
affect O
model O
performance O
remain O
to O
be O
explored O
. O

In O
this O
work O
, O
we O
first O
unify O
linear B-MethodName
interpolation I-MethodName
and O
stochastic B-MethodName
perturbation I-MethodName
into O
a O
general O
format O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
. O
We O
further O
propose O
three O
augmentation O
methods O
( O
linear B-MethodName
extrapolation I-MethodName
, O
binary B-MethodName
interpolation I-MethodName
, O
and O
Gaussian B-MethodName
scaling I-MethodName
) O
based O
on O
the O
general O
format O
. O
Then O
we O
theoretically O
analyze O
the O
advantages O
of O
the O
proposed O
augmentation O
methods O
based O
on O
the O
most O
commonly O
used O
InfoNCE O
loss O
( O
Van O
den O
Oord O
et O
al O
. O
, O
2018 O
) O
. O
As O
optimizing O
InfoNCE O
loss O
equals O
to O
maximizing O
the O
mutual O
information O
between O
positive O
pairs O
, O
applying O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
leads O
to O
tighter O
lower O
bounds O
of O
mutual O
information O
. O
We O
evaluate O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
on O
several O
Siamese O
networks O
across O
several O
large O
- O
scale O
datasets O
. O
Experimental O
results O
show O
the O
effectiveness O
of O
the O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
methods O
in O
boosting O
the O
performance O
of O
these O
code O
search O
models O
. O
To O
verify O
the O
generalization O
ability O
of O
our O
method O
to O
other O
tasks O
, O
we O
also O
conduct O
experiments O
on O
the O
paragraph O
retrieval O
task O
, O
and O
the O
results O
show O
that O
our O
method O
can O
also O
improve O
the O
performance O
of O
several O
paragraph O
retrieval O
models O
. O

In O
summary O
, O
our O
contributions O
of O
this O
work O
are O
as O
follows O
: O

• O
We O
unify O
previous O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
methods O
to O
propose O
a O
general O
format O
. O
Based O
on O
this O
general O
format O
, O
we O
propose O
three O
novel O
augmentation O
methods O
. O

• O
We O
conduct O
theoretical O
analysis O
to O
show O
that O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
has O
tighter O
lower O
bounds O
of O
mutual O
information O
between O
positive O
pairs O
. O

• O
We O
apply O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
on O
several O
code O
search O
models O
and O
evaluate O
them O
on O
the O
public O
CodeSearchNet B-DatasetName
dataset O
with O
six O
programming O
languages O
. O
Improvement O
of O
MRR B-MetricName
( O
Mean B-MetricName
Reciprocal I-MetricName
Rank I-MetricName
) O
demonstrates O
the O
effectiveness O
of O
the O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
methods O
. O

The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O
We O
introduce O
related O
work O
of O
code B-TaskName
search I-TaskName
and O
data O
augmentation O
in O
Section O
2 O
. O
Section O
3 O
introduces O
the O
main O
part O
, O
including O
the O
general O
format O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
, O
new O
augmentation O
methods O
, O
and O
their O
application O
on O
code B-TaskName
search I-TaskName
. O
In O
Section O
4 O
, O
we O
analyze O
the O
theoretical O
lower O
bounds O
of O
mutual O
information O
and O
study O
why O
our O
approach O
works O
. O
In O
Section O
5 O
and O
Section O
6 O
, O
we O
conduct O
extensive O
experiments O
to O
show O
the O
effectiveness O
of O
our O
approach O
. O
Then O
we O
discuss O
the O
generality O
of O
our O
approach O
in O
Section O
7 O
and O
Section O
8 O
concludes O
this O
paper O
. O

Related O
Work O

Code B-TaskName
search I-TaskName

As O
code B-TaskName
search I-TaskName
can O
significantly O
improve O
the O
productivity O
of O
software O
developers O
by O
reusing O
functionalities O
in O
large O
codebases O
, O
finding O
the O
semanticrelevant O
code O
fragments O
precisely O
is O
one O
of O
the O
key O
challenges O
in O
code O
search O
. O

Traditional O
approaches O
leverage O
information O
retrieval O
techniques O
that O
try O
to O
match O
some O
keywords O
between O
queries O
and O
codes O
( O
McMillan O
et O
al O
. O
, O
2011 O
; O
Robertson O
et O
al O
. O
, O
1995 O
) O
. O
These O
approaches O
suffer O
from O
vocabulary O
mismatch O
problem O
where O
models O
fail O
to O
retrieve O
the O
relevant O
codes O
due O
to O
the O
difference O
in O
semantics O
. O

Later O
, O
deep O
neural O
models O
for O
code B-TaskName
search I-TaskName
are O
proposed O
. O
They O
could O
be O
divided O
into O
two O
categories O
, O
early O
fusion O
and O
late O
fusion O
. O
Late O
fusion O
approaches O
( O
Gu O
et O
al O
. O
, O
2018 O
; O
Husain O
et O
al O
. O
, O
2019 O
) O
use O
a O
siamese O
network O
to O
embed O
queries O
and O
codes O
into O
a O
shared O
vector O
space O
separately O
, O
then O
calculate O
dot O
product O
or O
cosine O
distance O
to O
measure O
the O
semantic O
similarity O
. O
Recently O
, O
following O
the O
idea O
of O
late O
fusion O
, O
some O
transformer O
- O
based O
models O
with O
specifically O
designed O
pre O
- O
training O
tasks O
are O
proposed O
( O
Feng O
et O
al O
. O
, O
2020 O
; O
Guo O
et O
al O
. O
, O
2021Guo O
et O
al O
. O
, O
, O
2022 O
. O
They O
significantly O
outperform O
previous O
models O
by O
improving O
the O
understanding O
of O
code O
semantics O
. O
Instead O
of O
calculating O
representations O
of O
queries O
and O
codes O
independently O
, O
early O
fusion O
approaches O
model O
the O
correlations O
between O
queries O
and O
codes O
during O
the O
embedding O
process O
( O
Li O
et O
al O
. O
, O
2020 O
) O
. O
Li O
et O
al O
. O
( O
2020 O
) O
argues O
that O
early O
fusion O
makes O
it O
easier O
to O
capture O
implicit O
similarities O
. O
For O
applications O
of O
an O
online O
code B-TaskName
search I-TaskName
system O
, O
late O
fusion O
approach O
facilitates O
the O
use O
of O
neural O
models O
because O
the O
code O
representations O
can O
be O
calculated O
and O
stored O
in O
advance O
. O
During O
run O
time O
, O
only O
query O
representations O
need O
to O
be O
computed O
. O
Thus O
, O
in O
this O
work O
, O
we O
focus O
on O
late O
fusion O
approaches O
. O

Data O
augmentation O

Data O
augmentation O
has O
long O
been O
considered O
crucial O
for O
learning O
better O
representations O
in O
contrastive O
learning O
. O
The O
augmented O
data O
are O
considered O
to O
have O
the O
same O
semantics O
with O
the O
original O
data O
. O
For O
the O
augmentation O
of O
queries O
, O
synonym O
replacement O
, O
random O
insertion O
, O
random O
swap O
, O
random O
deletion O
, O
back O
- O
translation O
, O
spans O
technique O
and O
word O
perturbation O
can O
be O
potentially O
used O
to O
generate O
individual O
augmentations O
( O
Wei O
and O
Zou O
, O
2019 O
; O
Giorgi O
et O
al O
. O
, O
2021 O
; O
Fang O
et O
al O
. O
, O
2020 O
) O
. O
For O
the O
augmentation O
of O
code O
fragments O
, O
Bui O
et O
al O
. O
( O
2021 O
) O
proposed O
six O
semantic O
- O
preserving O
transformations O
: O
Variable O
Renaming O
, O
Permute O
Statement O
, O
Unused O
Statement O
, O
Loop O
Exchange O
, O
Switch O
to O
If O
and O
Boolean O
Exchange O
. O
These O
query O
and O
code O
augmentation O
approaches O
have O
one O
thing O
in O
common O
, O
that O
is O
, O
the O
transformation O
is O
applied O
to O
the O
original O
input O
data O
. O
Another O
category O
is O
augmenting O
during O
the O
embedding O
process O
. O
Models O
can O
generate O
different O
representations O
of O
the O
same O
data O
by O
leveraging O
time O
- O
varying O
mechanisms O
. O
MoCo O
encodes O
data O
twice O
by O
the O
same O
model O
with O
different O
parameters O
. O
SimCSE O
leverages O
the O
property O
of O
dropout O
layers O
by O
randomly O
deactivating O
different O
neurons O
for O
the O
same O
input O
. O
Methods O
described O
in O
this O
paragraph O
are O
resource O
- O
consuming O
because O
models O
embed O
twice O
to O
get O
representations O
of O
original O
data O
and O
augmented O
one O
. O

For O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
on O
NLP O
tasks O
, O
linear B-MethodName
interpolation I-MethodName
is O
widely O
used O
on O
classification O
tasks O
in O
previous O
work O
( O
Guo O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2020 O
; O
Du O
et O
al O
. O
, O
2021 O
) O
. O
They O
take O
the O
interpo O
- O
lation O
result O
as O
noised O
data O
and O
want O
models O
to O
classify O
the O
noised O
one O
into O
the O
original O
class O
. O
Verma O
et O
al O
. O
( O
2021 O
) O
theoretically O
analyzed O
how O
interpolation O
noise O
benefits O
classification O
tasks O
and O
why O
it O
is O
better O
than O
Gaussian O
noise O
. O
Jeong O
et O
al O
. O
( O
2022 O
) O
is O
the O
first O
to O
introduce O
linear B-MethodName
interpolation I-MethodName
and O
perturbation O
to O
the O
document O
retrieval O
task O
. O
However O
, O
the O
effect O
and O
intrinsic O
relationship O
of O
these O
two O
methods O
are O
not O
fully O
investigated O
. O

Approach O

In O
this O
section O
, O
we O
unify O
the O
linear B-MethodName
interpolation I-MethodName
and O
stochastic B-MethodName
perturbation I-MethodName
into O
a O
general O
format O
. O
Based O
on O
it O
, O
we O
propose O
three O
other O
augmentation O
methods O
for O
the O
code O
retrieval O
task O
, O
linear B-MethodName
extrapolation I-MethodName
, O
binary B-MethodName
interpolation I-MethodName
and O
Gaussian B-MethodName
scaling I-MethodName
. O
Then O
, O
we O
explain O
how O
to O
apply O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
with O
InfoNCE O
loss O
in O
code O
retrieval O
. O

General O
format O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName

For O
simplicity O
, O
we O
take O
code O
augmentation O
as O
an O
example O
to O
elaborate O
the O
details O
. O
The O
calculation O
process O
is O
similar O
when O
applying O
to O
query O
augmentations O
. O
Given O
a O
data O
distribution O

D O
= O
{ O
x O
i O
} O
K O
i=1 O

where O
x O
i O
is O
a O
code O
snippet O
, O
K B-HyperparameterName
is O
the O
size O
of O
the O
dataset O
. O
We O
use O
an O
encoder O
function O
h O
: O
D O
→ O
H O
to O
map O
codes O
to O
representations O
H O
. O

Linear B-MethodName
interpolation I-MethodName
Linear B-MethodName
interpolation I-MethodName
randomly O
interpolate O
h O
i O
with O
another O
chosen O
sample O
h O
j O
from O
H O
: O

h O
+ O
i O
= O
λh B-HyperparameterName
i O
+ O
( O
1 O
− O
λ B-HyperparameterName
) O
h O
j O
( O
1 O
) O

where O
λ B-HyperparameterName
is O
a O
coefficient O
sampled O
from O
a O
random O
distribution O
. O
For O
example O
, O
λ B-HyperparameterName
can O
be O
sampled O
from O
a O
uniform O
distribution O
λ B-HyperparameterName
∼ O
U O
( O
α O
, O
1.0 O
) O
with O
high O
values O
of O
α O
to O
make O
sure O
that O
the O
augmented O
data O
has O
similar O
semantics O
with O
the O
original O
code O
x O
i O
. O

Stochastic B-MethodName
perturbation I-MethodName

Stochastic B-MethodName
perturbation I-MethodName
aims O
at O
randomly O
deactivating O
some O
features O
of O
representation O
vectors O
. O
In O
order O
to O
do O
so O
, O
masks O
are O
sampled O
from O
a O
Bernoulli O
distribution O
B O
( O
e O
, O
p O
) O
, O
where O
e O
is O
the O
embedding O
dimension O
. O
p O
is O
a O
low O
probability O
value O
since O
we O
only O
deactivate O
a O
small O
proportion O
of O
features O
. O
For O
implementation O
, O
we O
could O
use O
Dropout O
layers O
. O

General O
format O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
We O
revisit O
the O
above O
two O
augmentation O
approaches O
and O
unify O
them O
into O
a O
general O
format O
, O
which O
could O
be O
described O
as O
: O

h O
+ O
= O
α O
⊙ O
h O
+ O
β O
⊙ O
h O
′ O
( O
2 O
) O

where O
h O
, O
h O
′ O
∈ O
H O
, O
α O
and O
β O
are O
coefficient O
vectors O
. O
For O
linear B-MethodName
interpolation I-MethodName
, O
α O
= O
λ B-HyperparameterName
, O
β O
= O
1 O
− O
λ B-HyperparameterName
, O
h O
, O
h O
′ O
∈ O
H O
, O
and O
h O
̸ O
= O
h O
′ O
. O
For O
stochastic B-MethodName
perturbation I-MethodName
, O
α O
∈ O
{ O
0 O
, O
1 O
1−p B-HyperparameterName
} O
e O
where O
elements O
of O
α O
are O
sampled O
from O
a O
Bernoulli O
distribution O
B O
( O
p B-HyperparameterName
) O
, O
β O
= O
1 O
1−p B-HyperparameterName
− O
α O
, O
h O
∈ O
H O
, O
and O
h O
′ O
= O
0 O
. O

New O
augmentation O
methods O

Based O
on O
the O
general O
format O
, O
we O
provide O
three O
new O
augmentation O
methods O
for O
the O
code O
retrieval O
task O
. O

Binary B-MethodName
interpolation I-MethodName
Binary B-MethodName
interpolation I-MethodName
randomly O
swaps O
some O
features O
with O
another O
chosen O
sample O
. O
The O
difference O
between O
binary B-MethodName
interpolation I-MethodName
and O
stochastic B-MethodName
perturbation I-MethodName
is O
that O
the O
former O
swaps O
with O
other O
samples O
while O
the O
latter O
swaps O
with O
zero O
vector O
. O
Specifically O
, O
for O
binary B-MethodName
interpolation I-MethodName
, O
α O
∈ O
{ O
0 O
, O
1 O
} O
e O
where O
elements O
of O
α O
are O
sampled O
from O
a O
Bernoulli O
distribution O
B O
( O
p O

) O
, O
β O
= O
1 O
− O
α O
, O
h O
, O
h O
′ O
∈ O
H O
, O
and O
h O
̸ O
= O
h O
′ O
. O

Linear B-MethodName
extrapolation I-MethodName
Wang O
and O
Isola O
( O
2020 O
) O
concludes O
that O
optimizing O
contrastive O
loss O
makes O
feature O
vectors O
roughly O
uniformly O
distributed O
on O
the O
hypersphere O
. O
As O
linear B-MethodName
interpolation I-MethodName
generates O
augmented O
data O
inside O
the O
hypersphere O
, O
linear B-MethodName
extrapolation I-MethodName
oppositely O
generates O
outside O
ones O
. O
λ B-HyperparameterName
is O
sampled O
from O
a O
uniform O
distribution O
λ B-HyperparameterName
∼ O
U O
( O
1.0 O
, O
α O
) O
with O
small O
values O
of O
α O
. O
Other O
settings O
are O
the O
same O
as O
linear B-MethodName
interpolation I-MethodName
. O

Gaussian B-MethodName
scaling I-MethodName
Gaussian B-MethodName
scaling I-MethodName
generates O
scaling O
coefficients O
for O
each O
feature O
in O
the O
representation O
, O
which O
can O
be O
considered O
as O
a O
type O
of O
perturbation O
noise O
. O
Compared O
with O
directly O
adding O
Gaussian O
noise O
, O
the O
proposed O
scaling O
noise O
captures O
the O
structure O
of O
the O
data O
manifold O
, O
which O
may O
force O
networks O
to O
learn O
better O
representations O
. O
If O
we O
describe O
Gaussian B-MethodName
scaling I-MethodName
in O
general O
format O
, O
then O
α O
= O
1 O
, O
β O
∼ O
N O
( O
0 O
, O
σ O
) O
with O
small O
values O
of O
σ O
, O
h O
= O
h O
′ O
∈ O
H O
. O

Contrastive O
learning O
with O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
for O
code B-TaskName
search I-TaskName

Contrastive O
learning O
seeks O
to O
satisfy O
that O
similarities O
between O
positive O
pairs O
are O
greater O
than O
that O
between O
negative O
pairs O
, O
which O
is O
suitable O
for O
code B-TaskName
search I-TaskName
. O
In O
previous O
works O
, O
in O
order O
to O
optimize O
the O
objective O
, O
several O
loss O
functions O
are O
proposed O
, O
including O
triplet O
loss O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
maxmargin O
loss O
( O
Hadsell O
et O
al O
. O
, O
2006 O
) O
, O
and O
logistic O
loss O
( O
Weinberger O
and O
Saul O
, O
2009 O
) O
. O
In O
this O
work O
, O
we O
consider O
InfoNCE O
loss O
( O
Van O
den O
Oord O
et O
al O
. O
, O
2018 O
) O
because O
of O
its O
better O
performance O
hence O
dominant O
adaption O
in O
current O
contrastive O
learning O
models O
. O

For O
the O
effect O
of O
representation O
- O
level O
augmentation O
on O
other O
loss O
functions O
, O
we O
empirically O
analyze O
it O
in O
Appendix O
B. O
We O
start O
from O
the O
vanilla O
InfoNCE O
loss O
. O
Suppose O
we O
have O
a O
batch O
of O
B B-HyperparameterName
samples O
consisting O
of O
queries O
and O
codes O
. O
We O
encode O
queries O
and O
codes O
to O
get O
query O
representations O
Q O
= O
{ O
q O
i O
} O
B O
i=1 O
and O
code O
representations O
C O
= O
{ O
c O
j O
} O
B O
j=1 O
using O
the O
encoder O
function O
h. O
( O
q O
i O
, O
c O
j O
) O
are O
positive O
pairs O
when O
i O
= O
j O
and O
negative O
pairs O
otherwise O
. O
Therefore O
, O
for O
each O
query O
, O
we O
could O
generate O
1 O
positive O
pair O
and O
B B-HyperparameterName
− O
1 O
negative O
pairs O
. O
The O
InfoNCE O
loss O
tries O
to O
maximize O
the O
similarity O
between O
one O
positive O
pair O
and O
minimize O
the O
similarity O
between O
other O
negative O
pairs O
. O
Here O
we O
use O
dot O
product O
as O
the O
measurement O
of O
similarity O
and O
in O
Section O
6.4 O
we O
will O
discuss O
the O
effect O
of O
using O
dot O
product O
compared O
to O
cosine O
distance O
. O
The O
loss O
can O
be O
described O
as O
: O

L O
= O
−E O
log O
exp O
( O
q O
i O
• O
c O
i O
) O
exp O
( O
q O
i O
• O
c O
i O
) O
+ O
B O
j̸ O
= O
i O
exp O
( O
q O
i O
• O
c O
j O
) O

( O
3 O
) O
Then O
, O
we O
conduct O
representation O
- O
level O
augmentation O
. O
We O
randomly O
choose O
one O
out O
of O
the O
five O
augmentation O
methods O
and O
augment O
the O
original O
queries O
Q O
and O
original O
codes O
C O
for O
N B-HyperparameterName
times O
. O
In O
each O
augmentation O
, O
the O
augmentation O
approach O
is O
fixed O
, O
but O
the O
coefficients O
α B-HyperparameterName
and O
β B-HyperparameterName
are O
randomized O
hence O
different O
. O
After O
augmentation O
, O
we O
get O
augmented O
query O
and O
code O
sets O
, O

Q O
+ O
= O
{ O
q O
ni O
} O
n O
= O
N O
, O
i O
= O
B O
n=1 O
, O
i=1 O
and O
C O
+ O
= O
{ O
c O
nj O
} O
n O
= O
N O
, O
j O
= O
B O
n=1 O
, O
j=1 O

. O
We O
follow O
the O
hypothesis O
of O
other O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
methods O
that O
the O
augmented O
representation O
still O
preserves O
or O
is O
similar O
to O
the O
original O
semantics O
. O
Therefore O
, O
for O
a O
certain O
query O
q O
i O
, O
we O
consider O
q O
i O
and O
q O
ni O
, O
∀n O
∈ O
[ O
1 O
, O
N O
] O
share O
the O
same O
semantic O
meaning O
. O
And O
this O
similarly O
applies O
to O
c O
j O
and O
c O
nj O
, O
∀n O
∈ O
[ O
1 O
, O
N O
] O
. O
Since O
( O
q O
i O
, O
c O
j O
) O
is O
labeled O
as O
a O
positive O
pair O
when O
i O
= O
j O
, O
∀n O
∈ O
[ O
1 O
, O
N O
] O
( O
q O
i O
, O
c O
nj O
) O
and O
( O
q O
ni O
, O
c O
j O
) O
are O
also O
naturally O
labeled O
as O
positive O
pairs O
. O
Similarly O
, O
we O
get O
( O
q O
ni O
, O
c O
nj O
, O
j̸ O
= O
i O
) O
as O
negative O
pairs O
. O
Thus O
, O
compared O
with O
vanilla O
InfoNCE O
loss O
, O
we O
now O
have O
( O
N B-HyperparameterName
+1 O
) O
2 O
B B-HyperparameterName
positive O
pairs O
in O
total O
and O
for O
each O
query O
q O
∈ O
Q O
∪ O
Q O
+ O
we O
can O
generate O
( O
B B-HyperparameterName
− O
1 O
) O
( O
N B-HyperparameterName
+ O
1 O
) O
negative O
pairs O
. O

Theoretical O
Analysis O

In O
this O
section O
, O
we O
mathematically O
analyze O
the O
effect O
of O
InfoNCE O
loss O
with O
or O
without O
representationlevel O
augmentation O
and O
prove O
that O
optimizing O
In O
- O
foNCE O
loss O
with O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
leads O
to O
mutual O
information O
with O
tighter O
lower O
bounds O
between O
positive O
pairs O
. O
Here O
we O
take O
linear B-MethodName
interpolation I-MethodName
as O
an O
example O
to O
demonstrate O
the O
benefits O
. O
Other O
forms O
of O
representation O
- O
level O
augmentation O
are O
left O
to O
future O
work O
. O
The O
mutual O
information O
between O
a O
query O
q O
and O
a O
code O
fragment O
c O
is O
: O

I O
( O
q O
, O
c O
) O
= O
E O
q O
, O
c O
log O
p O
( O
c|q O
) O
p O
( O
c O
) O
( O
4 O
) O

Theorem O
1 O
Optimizing O
InfoNCE O
loss O
L O
N O
improves O
lower O
bounds O
of O
mutual O
information O
I O
( O
q O
, O
c O
) O
for O
a O
positive O
pair O
: O

I O
( O
q O
, O
c O
) O
≥ O
log O
( O
B O
) O
− O
L O
N O
( O
5 O
) O

where O
q O
∈ O
Q O
, O
c O
∈ O
C O
, O
and O
B B-HyperparameterName
is O
the O
size O
of O
sets O
. O

Proof O
of O
Theorem O
1 O
is O
presented O
in O
Appendix O
A.1 O
. O
This O
is O
proved O
in O
the O
original O
paper O
of O
In O
- O
foNCE O
loss O
( O
Van O
den O
Oord O
et O
al O
. O
, O
2018 O
) O
. O
Here O
, O
to O
better O
extend O
the O
proof O
of O
Theorem O
2 O
, O
we O
prove O
it O
in O
another O
way O
. O

I O
( O
q O
, O
c O
) O
≥ O
1 O
α O
2 O
( O
log O
( O
N O
B O
) O
− O
L O
N O
− O
αβ O
• O
I O
( O
q O
, O
c O
− O
) O
− O
αβ O
• O
I O
( O
q O
− O
, O
c O
) O
− O
β O
2 O
• O
I O
( O
q O
− O
, O
c O
− O
) O
) O
( O
6 O
) O

where O
q O
, O
q O
− O
∈ O
Q O
, O
c O
, O
c O
− O
∈ O
C O
, O
( O
q O
, O
c O
− O
) O
, O
( O
q O
− O
, O
c O
) O
and O
( O
q O
− O
, O
c O
− O
) O
are O
all O
negative O
pairs O
, O
α B-HyperparameterName
and O
β B-HyperparameterName
are O
coefficients O
in O
Equation O
2 O
, O
B B-HyperparameterName
is O
the O
size O
of O
sets O
, O
and O
N B-HyperparameterName
is O
the O
augmentation O
time O
. O

Proof O
of O
Theorem O
2 O
is O
presented O
in O
Appendix O
A.2 O
. O
Since O
we O
interpolate O
q O
with O
other O
samples O
q O
− O
in O
the O
batch O
( O
c O
with O
c O
− O
) O
, O
the O
mutual O
information O
between O
( O
q O
, O
c O
− O
) O
, O
( O
q O
− O
, O
c O
) O
and O
( O
q O
− O
, O
c O
− O
) O
are O
also O
incorporated O
into O
the O
optimizing O
process O
. O
As O
defined O
in O
Eq.1 O
, O
β B-HyperparameterName
is O
a O
small O
value O
that O
close O
to O
0 O
. O
According O
to O
Eq.4 O
, O
for O
negative O
pairs O
p O
( O
c|q O
) O
can O
be O
expressed O
as O
p O
( O
c O
) O
p O
( O
q O
) O
the O
optimal O
mutual O
information O
between O
negative O
pairs O
is O
also O
0 O
. O
Note O
that O
we O
interpolate O
q O
and O
c O
with O
different O
samples O
to O
make O
sure O
that O
( O
q O
− O
, O
c O
− O
) O
is O
a O
negative O
pair O
. O
Thus O
, O
the O
last O
three O
terms O
in O
Eq.6 O
can O
be O
ignored O
. O
Comparing O
1 O
α O
2 O
( O
log O
( O
N O
B O
) O
− O
L O
N O
) O
with O
log O
( O
B O
) O
− O
L O
N O
, O
we O
can O
see O
that O
representationlevel B-MethodName
augmentation I-MethodName
improves O
the O
lower O
bounds O
of O
mutual O
information O
. O

Experimental O
Setup O

In O
this O
section O
, O
we O
describe O
datasets O
, O
baselines O
, O
and O
implementation O
details O
. O

Datasets O

To O
evaluate O
the O
effectiveness O
of O
representationlevel B-MethodName
augmentation I-MethodName
, O
we O
use O
a O
large O
- O
scale O
benchmark O
dataset O
CodeSearchNet B-DatasetName
( O
CSN B-DatasetName
) O
( O
Husain O
et O
al O
. O
, O
2019 O
) O
that O
is O
widely O
used O
in O
previous O
studies O
( O
Guo O
et O
al O
. O
, O
2021 O
; O
Feng O
et O
al O
. O
, O
2020 O
; O
Guo O
et O
al O
. O
, O
2022 O
) O
. O
It O
contains O
six O
programming O
languages O
including O
Ruby O
, O
JavaScript O
, O
Go O
, O
Python O
, O
Java O
, O
and O
PHP O
. O
The O
statistics O
of O
the O
dataset O
are O
shown O
in O
Table O
1 O
. O
For O
the O
training O
set O
, O
it O
contains O
positive O
- O
only O
querycode O
pairs O
. O
For O
validation O
and O
test O
sets O
, O
they O
only O
have O
queries O
and O
the O
model O
retrieves O
the O
correct O
code O
fragments O
from O
a O
fixed O
codebase O
. O
Here O
we O
follow O
( O
Guo O
et O
al O
. O
, O
2021 O
) O
to O
filter O
out O
low O
- O
quality O
examples O
( O
such O
as O
code O
that O
can O
not O
be O
successfully O
parsed O
into O
Abstract O
Syntax O
Trees O
) O
. O

We O
measure O
the O
performance O
using O
Mean B-MetricName
Reciprocal I-MetricName
Rank I-MetricName
( O
MRR B-MetricName
) O
which O
is O
widely O
adopted O
in O
previous O
studies O
. O
MRR B-MetricName
is O
the O
average O
of O
reciprocal O
ranks O
of O
a O
true O
code O
fragment O
for O
a O
given O
query O
Q. O
It O
is O
calculated O
as O
: O

M O
RR O
= O
1 O
|Q| O
|Q| O
i=1 O
1 O
Rank O
i O
( O
7 O
) O

where O
Rank O
i O
is O
the O
rank O
of O
the O
correct O
code O
fragment O
that O
is O
related O
to O
the O
i O
- O
th O
query O
. O

Baselines O

Since O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
is O
orthogonal O
to O
siamese O
networks O
, O
we O
apply O
it O
to O
several O
models O
: O

• O
RoBERTa B-MethodName
( I-MethodName
code I-MethodName
) I-MethodName
is O
pre O
- O
trained O
with O
mask O
language O
modeling O
( O
MLM O
) O
task O
on O
code O
corpus O
( O
Husain O
et O
al O
. O
, O
2019 O
) O
. O

• O
CodeBERT B-MethodName
is O
a O
bi O
- O
modal O
pre O
- O
trained O
model O
pre O
- O
trained O
on O
two O
tasks O
: O
MLM O
and O
replaced O
token O
detection O
( O
Feng O
et O
al O
. O
, O
2020 O
) O
. O
Note O
that O
in O
this O
work O
we O
refer O
CodeBERT B-MethodName
to O
the O
siamese O
network O
architecture O
described O
in O
the O
appendix O
of O
the O
original O
paper O
. O

• O
GraphCodeBERT B-MethodName
takes O
the O
structure O
information O
of O
codes O
into O
account O
. O
( O
Guo O
et O
al O
. O
, O
2021 O
) O
develops O
two O
structure O
- O
based O
pretraining O
tasks O
: O
data O
flow O
edge O
prediction O
and O
node O
alignment O
. O

• O
UniXCoder B-MethodName
leverages O
cross O
- O
model O
contents O
like O
AST O
and O
comments O
to O
enhance O
code O
representation O
( O
Guo O
et O
al O
. O
, O
2022 O
) O
. O

Implementation O
details O

For O
all O
the O
settings O
of O
these O
models O
except O
the O
training O
epoch O
, O
we O
follow O
the O
original O
paper O
. O
For O
representation O
- O
level O
augmentation O
, O
since O
linear O
extrapolation O
and O
linear B-MethodName
interpolation I-MethodName
is O
similar O
, O
we O
implement O
it O
as O
one O
approach O
. O
During O
training O
, O
we O
randomly O
choose O
one O
augmentation O
approach O
out O
of O
four O
with O
equal O
probability O
for O
a O
batch O
and O
augment O
data O
5 B-HyperparameterValue
times O
. O
We O
re O
- O
sample O
data O
and O
coefficients O
to O
augment O
the O
original O
data O
in O
each O
augmentation O
. O
Specifically O
, O
for O
linear B-MethodName
interpolation I-MethodName
and O
extrapolation O
, O
we O
sample O
α O
from O
a O
uniform O
distribution O
U O
∼ O
( O
0.9 O
, O
1.1 O
) O
. O
For O
perturbation B-MethodName
, O
we O
set O
the O
probability O
p B-HyperparameterName
of O
the O
Dropout O
layer O
as O
0.1 B-HyperparameterValue
. O
For O
binary B-MethodName
interpolation I-MethodName
, O
we O
sample O
α O
from O
a O
Bernoulli O
distribution O
B O
( O
p B-HyperparameterName
= O
0.25 B-HyperparameterValue
) O
. O
For O
Gaussian B-MethodName
scaling I-MethodName
, O
we O
sample O
β B-HyperparameterName
from O
a O
normal O
distribution O
N O
( O
0 O
, O
0.1 O
) O
. O

We O
set O
the O
training B-HyperparameterName
epoch I-HyperparameterName
as O
30 B-HyperparameterValue
. O
All O
experiments O
are O
conducted O
on O
a O
GeForce O
RTX O
A6000 O
GPU O
. O

Results O

In O
this O
section O
, O
we O
first O
show O
the O
overall O
performance O
on O
code O
search O
when O
applying O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
, O
and O
then O
individually O
analyze O
each O
augmentation O
method O
. O
Then O
, O
we O
demonstrate O
the O
relationship O
between O
loss O
and O
MRR B-MetricName
and O
the O
effect O
of O
augmentation O
on O
vector O
distribution O
. O
Finally O
, O
we O
take O
an O
ablation O
study O
to O
analyze O
the O
impact O
of O
augmentation B-HyperparameterName
times I-HyperparameterName
. O

Overall O
results O

The O
overall O
performance O
evaluation O
results O
are O
shown O
in O
Table O
2 O
. O
In O
each O
iteration O
, O
we O
randomly O
choose O
one O
augmentation O
method O
. O
We O
can O
see O
that O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
is O
a O
universal O
approach O
that O
can O
consistently O
improve O
the O
code O
search O
performance O
. O
Optimizing O
a O
tighter O
lower O
bound O
of O
mutual O
information O
brings O
about O
2 B-MetricValue
% I-MetricValue
gain O
of O
MRR B-MetricName
on O
average O
. O
The O
robust O
improvements O
have O
no O
relationships O
with O
certain O
models O
or O
certain O
programming O
languages O
. O

Effectiveness O
of O
individual O
augmentation O
approach O

To O
evaluate O
the O
effectiveness O
of O
the O
five O
augmentation O
approaches O
, O
we O
apply O
them O
alone O
and O
test O
them O
on O
the O
CSN B-DatasetName
- I-DatasetName
Python I-DatasetName
dataset O
, O
as O
shown O
in O
Table O
3 O
. O

Note O
that O
we O
follow O
the O
same O
experimental O
settings O
described O
in O
Section O
5 O
. O
As O
the O
results O
show O
, O
every O
augmentation O
approach O
can O
improve O
baselines O
and O
the O
improvements O
brought O
by O
these O
augmentation O
approaches O
are O
stable O
. O
The O
combination O
of O
these O
approaches O
does O
not O
boost O
the O
performance O
compared O
with O
individually O
applying O
one O
of O
these O
augmentations O
. O
Since O
all O
these O
approaches O
can O
be O
derived O
from O
the O
general O
format O
, O
they O
have O
no O
distinct O
difference O
and O
hence O
share O
the O
similar O
effect O
on O
improving O
the O
mutual O
information O
between O
positive O
pairs O
. O

I O
( O
q O
, O
c O
) O
≥ O
1 O
α O
2 O
( O
log O
( O
N O
B O
) O
− O
L O
N O
) O

. O
Thus O
, O
we O
argue O
that O
the O
effect O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
is O
improving O
the O
lower O
bounds O
of O
mutual O
information O
. O
However O
, O
this O
only O
comes O
true O
when O
loss O
can O
decrease O
to O
similar O
values O
under O
such O
two O
conditions O
. O
To O
prove O
that O
, O
after O
each O
epoch B-HyperparameterName
, O
we O
save O
the O
model O
parameters O
, O
record O
the O
loss O
of O
the O
training O
set O
, O
test O
models O
on O
the O
CSN B-DatasetName
- I-DatasetName
Python I-DatasetName
test O
set O
, O
and O
plot O
the O
relationship O
between O
loss O
and O
MRR B-MetricName
with O
or O
without O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
, O
as O
shown O
in O
Figure O
1 O
. O
We O
can O
see O
that O
when O
the O
loss O
is O
the O
same O
, O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
always O
leads O
to O
better O
performance O
( O
GraphCode B-MethodName
- I-MethodName
BERT I-MethodName
) O
. O
Even O
when O
loss O
can O
not O
decrease O
to O
the O
same O
value O
without O
augmentation O
, O
it O
outperforms O
the O
vanilla O
contrastive O
learning O
( O
CodeBERT B-MethodName
) O
. O
We O
believe O
that O
other O
than O
improving O
the O
lower O
bounds O
, O
capturing O
the O
explicit O
relations O
between O
augmented O
data O
and O
the O
original O
one O
can O
also O
lead O
to O
higher O
mutual O
information O
between O
positive O
pairs O
. O

Impact O
on O
vector O
distribution O

In O
code B-TaskName
search I-TaskName
, O
we O
measure O
the O
similarity O
by O
calculating O
the O
dot O
product O
between O
query O
representations O
and O
code O
representations O
. O
Here O
we O
analyze O
the O
influence O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
by O
visualizing O
the O
code O
vector O
distribution O
. O
We O
add O
a O
linear O
layer O
to O
embed O
the O
representations O
to O
a O
twodimensional O
vector O
, O
as O
shown O
in O
Figure O
2 O
. O
Besides O
, O
we O
plot O
the O
two O
- O
norms O
of O
vectors O
with O
Gaussian O
kernel O
density O
estimation O
. O

As O
Wang O
and O
Isola O
( O
2020 O
) O
concluded O
, O
the O
optimization O
of O
InfoNCE O
loss O
makes O
vectors O
evenly O
distributed O
, O
which O
corresponds O
to O
the O
left O
image O
of O
Figure O
2 O
. O
Vectors O
roughly O
have O
the O
same O
twonorm O
. O
After O
applying O
augmentation O
, O
vectors O
still O
follow O
a O
circular O
pattern O
but O
their O
two O
- O
norms O
are O
changed O
. O
We O
argue O
that O
the O
InfoNCE O
loss O
degrades O
dot O
product O
to O
cosine O
distance O
since O
all O
vectors O
have O
similar O
norms O
while O
representation O
- O
level O
augmentation O
leverages O
the O
norms O
of O
vectors O
to O
distinguish O
some O
hard O
examples O
. O
We O
can O
see O
that O
in O
the O
middle O
image O
of O
Figure O
2 O
, O
vector O
norms O
are O
much O
more O
uniformly O
distributed O
than O
that O
without O
augmentation O
. O

However O
, O
this O
impact O
also O
reveals O
a O
limitation O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
. O
Some O
previous O
studies O
argue O
the O
necessity O
of O
using O
vector O
normalization O
, O
otherwise O
, O
the O
Softmax O
distribution O
will O
be O
made O
arbitrarily O
sharp O
. O
For O
example O
, O
the O
last O
step O
of O
UniXCoder B-MethodName
is O
normalization O
. O
We O
evaluate O
UniX B-MethodName
- I-MethodName
Coder I-MethodName
on O
CSN B-DatasetName
- I-DatasetName
Python I-DatasetName
, O
as O
shown O
in O
Table O
4 O
. O
When O
we O
apply O
augmentations O
to O
UniXCoder B-MethodName
with O
normalization O
, O
the O
performance O
is O
worse O
. O
However O
, O
if O
we O
remove O
the O
normalization O
step O
, O
augmentations O
can O
boost O
performance O
just O
like O
for O
other O
models O
. O

Impact O
of O
the O
number B-HyperparameterName
of I-HyperparameterName
augmentation I-HyperparameterName
times I-HyperparameterName

To O
analyze O
the O
impact B-HyperparameterName
of I-HyperparameterName
augmentation I-HyperparameterName
times I-HyperparameterName
N B-HyperparameterName
, O
we O
conduct O
experiments O
on O
CodeBERT B-MethodName
with O
N B-HyperparameterName
= O
5 B-HyperparameterValue
, O
15 B-HyperparameterValue
, O
25 B-HyperparameterValue
, O
respectively O
. O
We O
take O
10 B-HyperparameterValue
epochs B-HyperparameterName
as O
an O
interval O
, O
save O
the O
best O
model O
in O
each O
interval O
and O
test O
them O
on O
the O
test O
set O
, O
as O
shown O
in O
Figure O
3 O
. O
We O
can O
see O
that O
augmenting O
more O
times O
leads O
to O
a O
relatively O
better O
performance O
that O
is O
even O
greater O
than O
the O
result O
reported O
in O
15 B-HyperparameterValue
- O
times B-HyperparameterName
augmentation O
takes O
50 B-HyperparameterValue
epochs B-HyperparameterName
. O
We O
train O
CodeBERT B-MethodName
with O
N B-HyperparameterName
= O
25 B-HyperparameterValue
for O
60 B-HyperparameterValue
epochs B-HyperparameterName
and O
MRR B-MetricName
is O
still O
increasing O
. O
Therefore O
, O
we O
argue O
that O
for O
the O
application O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
, O
we O
should O
find O
a O
balance O
between O
performance O
and O
time O
cost O
. O

Discussion O

According O
to O
the O
proof O
of O
Theorem O
2 O
, O
the O
advantage O
of O
applying O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
should O
be O
task O
- O
agnostic O
. O
To O
show O
its O
generalization O
ability O
, O
we O
evaluate O
our O
proposed O
approaches O
on O
two O
passage O
retrieval O
benchmark O
datasets O
, O
NFCorpus B-DatasetName
( O
Boteva O
et O
al O
. O
, O
2016 O
) O
and O
FiQA-2018 B-DatasetName
1 O
. O
The O
difference O
between O
code B-TaskName
search I-TaskName
and O
passage O
retrieval O
is O
that O
the O
retrieved O
items O
are O
changed O
from O
code O
snippets O
written O
in O
programming O
language O
to O
passages O
written O
in O
English O
. O
We O
take O
DistilBERT B-MethodName
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
for O
experiments O
. O
We O
implement O
our O
approach O
based O
on O
an O
open O
- O
sourced O
framework O
BEIR O
( O
Thakur O
et O
al O
. O
, O
2021 O
) O
. O
The O
two O
models O
are O
fine O
- O
tuned O
on O
two O
datasets O
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
, O
respectively O
. O
The O
settings O
of O
augmentation O
are O
the O
same O
as O
those O
in O
the O
code O
search O
task O
. O
For O
other O
hyper O
- O
parameters O
, O
we O
follow O
the O
settings O
that O
are O
provided O
by O
the O
framework O
. O

Results O
are O
shown O
in O
Table O
5 O
, O
which O
confirms O
that O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
also O
improves O
the O
performance O
of O
passage O
retrieval O
models O
. O

Conclusion O

In O
this O
work O
, O
we O
unify O
existing O
approaches O
to O
propose O
a O
general O
format O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
in O
code B-TaskName
search I-TaskName
. O
Based O
on O
the O
general O
format O
, O
we O
propose O
three O
other O
augmentation O
methods O
. O
We O
further O
theoretically O
analyze O
the O
effect O
of O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
by O
proving O
that O
it O
helps O
optimize O
a O
tighter O
lower O
bound O
of O
mutual O
information O
between O
positive O
pairs O
. O
We O
evaluate O
our O
approach O
on O
several O
models O
and O
datasets O
and O
the O
results O
demonstrate O
the O
effectiveness O
of O
the O
proposed O
approach O
. O

Limitations O

As O
discussed O
in O
Section O
6 O
, O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
mainly O
has O
two O
limitations O
. O
First O
, O
it O
can O
not O
boost O
the O
performance O
for O
models O
with O
vector O
normalization O
. O
We O
find O
that O
representation O
- O
level O
augmentation O
improves O
performance O
by O
leveraging O
the O
norms O
of O
vectors O
. O
With O
normalization O
, O
the O
norm O
of O
vectors O
is O
fixed O
and O
hence O
augmentation O
can O
not O
bring O
performance O
gains O
. O
Second O
, O
as O
discussed O
in O
Section O
6.5 O
, O
although O
augmenting O
more O
times O
can O
lead O
to O
better O
performance O
, O
the O
time O
spent O
on O
training O
also O
increases O
. O
Balancing O
performance O
and O
time O
- O
cost O
will O
be O
our O
future O
work O
. O

Acknowledgement O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
helpful O
comments O
and O
suggestions O
. O

A O
Proof O

A.1 O
Proof O
of O
Theorem O
1 O

LN O
= O
−E O
log O
exp O
( O
qi O
• O
ci O
) O
exp O
( O
qi O
• O
ci O
) O
+ O
B O
j̸ O
= O
i O
exp O
( O
qi O
• O
cj O
) O
= O
E O
log O
1 O
+ O
B O
j̸ O
= O
i O
exp O
( O
qi O
• O
cj O
) O
exp O
( O
qi O
• O
ci O
) O
≥ O
E O
log O
B O
j̸ O
= O
i O
exp O
( O
qi O
• O
cj O
) O
exp O
( O
qi O
• O
ci O
) O
= O
E O
 O
 O
log O
 O
 O
B O
j̸ O
= O
i O
exp O
( O
qi O
• O
cj O
) O
 O
 O
− O
log O
exp O
( O
qi O
• O
ci O
) O
 O
 O
≈ O
E O
log O
[ O
B O
• O
E O
[ O
exp O
( O
qi O
• O
cj O
) O
] O
] O
− O
E O
log O
exp O
( O
qi O
• O
ci O
) O
( O
8 O
) O

According O
to O
the O
original O
paper O
of O
InfoNCE O
loss O
( O
Van O
den O
Oord O
et O
al O
. O
, O
2018 O
) O
, O
the O
optimal O
value O
for O
exp O
( O
q O
• O
c O
) O
is O
given O
by O
p O
( O
c|q O
) O
p O
( O
c O
) O
, O
by O
substituting O
it O
, O
we O
can O
get O
: O

L O
N O
≥ O
E O
log O
B O
• O
E O
[ O
p O
( O
c O
j O
|q O
i O
) O
p O
( O
c O
j O
) O
) O
] O
− O
E O
log O
p O
( O
c O
i O
|q O
i O
) O
p O
( O
c O
i O
) O
( O
9 O
) O

Since O
q O
i O
and O
c O
j O
are O
negative O
pairs O
and O
sampled O
independently O
, O
E O
[ O

p O
( O
c O
j O
|q O
i O
) O
p O
( O
c O
j O
) O
] O
= O
E O
[ O
p O
( O
c O
j O
, O
q O
i O
) O
p O
( O
c O
j O
) O
p O
( O
q O
i O
) O
] O
= O
E O
[ O
p O
( O
c O
j O
) O
p O
( O
q O
i O
) O
p O
( O
c O
j O
) O
p O
( O
q O
i O
) O
] O
= O
1 O
. O

According O
to O
the O
definition O
of O
mutual O
information O
described O
in O
Eq.4 O
, O
the O
mutual O
information O
I O
( O
q O
i O
, O
c O
i O
) O
is O
the O
second O
term O
. O
Thus O
, O
we O
get O
: O

L O
N O
≥ O
log O
B O
− O
I O
( O
q O
i O
, O
c O
i O
) O
( O
10 O
) O

Therefore O
, O
I O
( O
q O
, O
c O
) O
≥ O
log O
( O
B O
) O
− O
L O
N O
. O

A.2 O
Proof O
of O
Theorem O
2 O

By O
applying O
augmentation O
, O
the O
expectation O
of O
loss O
can O
be O
divided O
into O
four O
parts O
that O
correspond O
to O
four O
types O
of O
pairs O
: O
original O
query O
and O
original O
code O
, O
augmented O
query O
and O
original O
code O
, O
original O
query O
and O
augmented O
code O
, O
and O
augmented O
query O
and O
augmented O
code O
. O
It O
can O
be O
expressed O
as O
: O

LN O
= O
E O
− O
E O
log O
exp O
( O
qi O
• O
ci O
) O
exp O
( O
qi O
• O
ci O
) O
+ O
BN O
j̸ O
= O
i O
exp O
( O
qi O
• O
cj O
) O
− O
E O
log O
exp O
( O
qi O
• O
c O
* O
i O
) O
exp O
( O
qi O
• O
c O
* O
i O
) O
+ O
BN O
j̸ O
= O
i O
exp O
( O
qi O
• O
cj O
) O
− O
E O
log O
exp O
( O
q O
* O
i O
• O
ci O
) O
exp O
( O
q O
* O
i O
• O
ci O
) O
+ O
BN O
j̸ O
= O
i O
exp O
( O
q O
* O
i O
• O
cj O
) O
− O
E O
log O
exp O
( O
q O
* O
i O
• O
c O
* O
i O
) O
exp O
( O
q O
* O
i O
• O
c O
* O
i O
) O
+ O
BN O
j̸ O
= O
i O
exp O
( O
q O
* O
i O
• O
cj O
) O
( O
11 O
) O

where O
q O
i O
∈ O
Q O
, O
c O
i O
∈ O
C O
, O
q O
* O
i O
∈ O
Q O
+ O
, O
c O
* O
i O
∈ O
C O
+ O
, O
and O
c O
j O
∈ O
C O
∪ O
C O
+ O
. O
Next O
, O
we O
will O
analyze O
the O
four O
terms O
individually O
. O
For O
the O
first O
term O
, O
original O
query O
and O
original O
code O
, O
it O
is O
the O
same O
as O
proved O
in O
Appendix O
A.1 O
. O
For O
the O
second O
term O
, O
original O
query O
and O
augmented O
code O
, O
the O
derivation O
can O
be O
formulated O
as O
: O

− O
E O
log O
exp O
( O
q O
i O
• O
c O
* O
i O
) O
exp O
( O
q O
i O
• O
c O
* O
i O
) O
+ O
BN O
j̸ O
= O
i O
exp O
( O
q O
i O
• O
c O
j O
) O
= O
E O
log O
1 O
+ O
BN O
j̸ O
= O
i O
exp O
( O
q O
i O
• O
c O
j O
) O
exp O
( O
q O
i O
• O
c O
* O
i O
) O
≥ O
E O
log O
BN O
j̸ O
= O
i O
exp O
( O
q O
i O
• O
c O
j O
) O
exp O
( O
q O
i O
• O
( O
α O
• O
c O
i O
+ O
β O
• O
c O
j O
) O
) O
= O
E O
log O
BN O
j̸ O
= O
i O
exp O
( O
q O
i O
• O
c O
j O
) O
exp O
( O
q O
i O
• O
c O
i O
• O
α O
) O
• O
exp O
( O
q O
i O
• O
c O
j O
• O
β O
) O
) O
= O
E O
log O
BN O
j̸ O
= O
i O
exp O
( O
q O
i O
• O
c O
j O
) O
− O
α O
• O
log O
exp O
( O
q O
i O
• O
c O
i O
) O
− O
β O
• O
log O
exp O
( O
q O
i O
• O
c O
j O
) O
( O
12 O
) O

Similar O
to O
Eq.9 O
, O
we O
substitute O
exponential O
function O
with O
probability O
, O
and O
we O
could O
get O
that O
the O
second O
term O
is O
greater O
than O
log O
N O
B O
− O
αI O
( O
q O
i O
, O
c O
i O
) O
− O
βI O
( O
q O
i O
, O
c O
j O
) O
. O
The O
only O
difference O
between O
the O
second O
term O
and O
the O
third O
term O
is O
that O
in O
the O
derivation O
of O
second O
term O
we O
decompose O
c O
* O
i O
into O
α O
• O
c O
i O
+ O
β O
• O
c O
j O
while O
for O
the O
third O
one O
we O
decompose O
q O
* O
i O
into O
α O
• O
q O
i O
+ O
β O
• O
q O
j O
. O
Therefore O
, O
the O
third O
term O
is O
greater O
than O
log O
N O
B O
− O
αI O
( O
q O
i O
, O
c O
i O
) O
− O
βI O
( O
q O
j O
, O
c O
i O
) O
. O

For O
the O
fourth O
term O
, O
it O
can O
be O
described O
as O
: O

− O
E O
log O
exp O
( O
q O
* O
i O
• O
c O
* O
i O
) O
exp O
( O
q O
* O
i O
• O
c O
* O
i O
) O
+ O
BN O
j̸ O
= O
i O
exp O
( O
q O
* O
i O
• O
cj O
) O
≥ O
E O
log O
BN O
j̸ O
= O
i O
exp O
( O
q O
* O
i O
• O
cj O
) O
exp O
( O
( O
α O
• O
qi O
+ O
β O
• O
qj O
) O
) O
• O
( O
α O
• O
ci O
+ O
β O
• O
c O
′ O
j O
) O
) O
= O
E O
log O
BN O
j̸ O
= O
i O
exp O
( O
q O
* O
i O
• O
cj O
) O
− O
α O
2 O
• O
log O
exp O
( O
qi O
• O
ci O
) O
− O
αβ O
• O
log O
exp O
( O
qi O
• O
c O
′ O
j O
) O
− O
αβ O
• O
log O
exp O
( O
qj O
• O
ci O
) O
− O
β O
2 O
• O
log O
exp O
( O
qj O
• O
c O
′ O
j O
) O

≥ O
log O
N O
B O
− O
α O
2 O
I O
( O
qi O
, O
ci O
) O
− O
αβI O
( O
qi O
, O
c O
′ O
j O
) O
− O
αβI O
( O
qj O
, O
ci O
) O
− O
β O
2 O
I O
( O
qj O
, O
c O
′ O
j O
) O

Then O
we O
remove O
the O
expectation O
of O
Eq O
. O
A.2 O
by O
multiplying O
the O
corresponding O
probabilities O
of O
four O
terms O
, O
we O
get O
: O

LN O
≥ O
B O
2 O
( O
N O
+ O
1 O
) O
2 O
B O

Therefore O
, O
we O
get O
: O

I O
( O
q O
i O
, O
c O
i O
) O
≥ O
1 O
α O
2 O
log O
( O
N O
B O
) O
− O
L O
N O
− O
αβ O
• O
I O
( O
q O
i O
, O
c O
′ O
j O
) O
− O
αβ O
• O
I O
( O
q O
j O
, O
c O
i O
) O
− O
β O
2 O
• O
I O
( O
q O
j O
, O
c O
′ O
j O
) O
( O
15 O
) O

B O
Representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
on O
other O
loss O
functions O

Here O
, O
we O
investigate O
whether O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
also O
benefits O
other O
loss O
functions O
. O
We O
apply O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
on O
triplet O
loss O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
logistic O
loss O
( O
Weinberger O
and O
Saul O
, O
2009 O
) O
, O
because O
these O
two O
loss O
functions O
are O
also O
used O
prior O
to O
InfoNCE O
loss O
. O
Triplet O
loss O
learns O
to O
maximize O
the O
distances O
between O
negative O
pairs O
while O
minimizing O
the O
distances O
between O
positive O
pairs O
, O
which O
can O
be O
written O
as O
: O

L O
triplet O
= O
− O
1 O
N O
N O
i=1 O
max O
( O
0 O
, O
||q O
i O
− O
c O
i O
|| O
2 O
2 O
− O
||q O
i O
− O
c O
j O
, O
j̸ O
= O
i O
|| O
2 O
2 O
+ O
ϵ O
) O
( O
16 O
) O

And O
logistic O
loss O
is O
also O
called O
NCE O
loss O
, O
which O
can O
be O
described O
as O
: O

L O
logistic O
= O
− O
1 O
N O
N O
i=1 O
log O
σ O
( O
q O
i O
• O
c O
i O
) O
− O
1 O
N O
− O
1 O
j̸ O
= O
i O
log O
σ O
( O
q O
i O
• O
c O
j O
) O
( O
17 O
) O

where O
definitions O
of O
variables O
follow O
Equation O
3 O
, O
σ O
represents O
sigmoid O
function O
, O
and O
we O
set O
ϵ O
= O
5 O
in O
our O
experiments O
. O

We O
finetune O
CodeBERT B-MethodName
and O
GraphCodeBERT B-MethodName
with O
the O
two O
loss O
functions O
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
on O
Ruby O
and O
Javascript O
dataset O
. O
Results O
of O
triplet O
loss O
and O
logistic O
loss O
are O
shown O
in O
Table O
6 O
and O
Table O
7 O
, O
As O
we O
can O
see O
, O
representation B-MethodName
- I-MethodName
level I-MethodName
augmentation I-MethodName
can O
also O
improve O
performance O
when O
other O
loss O
functions O
are O
used O
. O
We O
believe O
this O
is O
because O
the O
augmentation O
makes O
the O
model O
more O
robust O
. O

Generating B-TaskName
Natural I-TaskName
Language I-TaskName
Proofs I-TaskName
with O
Verifier O
- O
Guided O
Search O

Reasoning O
over O
natural O
language O
is O
a O
challenging O
problem O
in O
NLP O
. O
In O
this O
work O
, O
we O
focus O
on O
proof B-TaskName
generation I-TaskName
: O
Given O
a O
hypothesis O
and O
a O
set O
of O
supporting O
facts O
, O
the O
model O
generates O
a O
proof O
tree O
indicating O
how O
to O
derive O
the O
hypothesis O
from O
supporting O
facts O
. O
Compared O
to O
generating O
the O
entire O
proof O
in O
one O
shot O
, O
stepwise O
generation O
can O
better O
exploit O
the O
compositionality O
and O
generalize O
to O
longer O
proofs O
but O
has O
achieved O
limited O
success O
on O
real O
- O
world O
data O
. O
Existing O
stepwise O
methods O
struggle O
to O
generate O
proof O
steps O
that O
are O
both O
logically O
valid O
and O
relevant O
to O
the O
hypothesis O
. O
Instead O
, O
they O
tend O
to O
hallucinate O
invalid O
steps O
given O
the O
hypothesis O
. O
In O
this O
paper O
, O
we O
present O
a O
novel O
stepwise O
method O
, O
NLProofS B-MethodName
( O
Natural B-MethodName
Language I-MethodName
Proof I-MethodName
Search I-MethodName
) O
, O
which O
learns O
to O
generate O
relevant O
steps O
conditioning O
on O
the O
hypothesis O
. O
At O
the O
core O
of O
our O
approach O
, O
we O
train O
an O
independent O
verifier O
to O
check O
the O
validity O
of O
the O
proof O
steps O
to O
prevent O
hallucination O
. O
Instead O
of O
generating O
steps O
greedily O
, O
we O
search O
for O
proofs O
maximizing O
a O
global O
proof O
score O
judged O
by O
the O
verifier O
. O
NL B-MethodName
- I-MethodName
ProofS I-MethodName
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
EntailmentBank B-DatasetName
and O
RuleTaker B-DatasetName
. O
Specifically O
, O
it O
improves O
the O
correctness B-MetricName
of O
predicted O
proofs O
from O
27.7 B-MetricValue
% I-MetricValue
to O
33.3 B-MetricValue
% I-MetricValue
in O
the O
distractor O
setting O
of O
EntailmentBank B-DatasetName
, O
demonstrating O
the O
effectiveness O
of O
NLProofS B-MethodName
in O
generating O
challenging O
human O
- O
authored O
proofs O
. O
1 O

Introduction O

A O
fundamental O
goal O
of O
AI O
since O
its O
inception O
is O
automated O
reasoning O
( O
McCarthy O
et O
al O
. O
, O
1960 O
) O
: O
given O
explicitly O
provided O
knowledge O
as O
assumptions O
, O
we O
want O
the O
system O
to O
draw O
logically O
valid O
conclusions O
. O
Research O
in O
automated O
reasoning O
has O
traditionally O
focused O
on O
structured O
domains O
such O
as O
formal O
logic O
( O
Robinson O
and O
Voronkov O
, O
2001 O
) O
. O
On O
the O
other O
hand O
, O
recent O
work O
suggests O
that O
free O
- O
form O
natural O
language O
can O
be O
a O
suitable O
vehicle O
for O
reasoning O
( O
Clark O
et O
al O
. O
, O
2020 O
; O
, O
because O
natural O
language O
represents O
knowledge O
without O
requiring O
labour O
- O
intensive O
formalization O
. O
However O
, O
reasoning O
in O
natural O
language O
is O
challenging O
, O
as O
it O
requires O
compositional O
generalization O
to O
novel O
examples O
( O
Ruis O
et O
al O
. O
, O
2020 O
) O
-a O
capability O
that O
state O
- O
of O
- O
the O
- O
art O
large O
language O
models O
struggle O
with O
( O
Rae O
et O
al O
. O
, O
2021 O
) O
. O

In O
this O
work O
, O
we O
focus O
on O
proof B-TaskName
generation I-TaskName
in O
natural O
language O
( O
Fig O
. O
1 O
) O
: O
given O
a O
hypothesis O
and O
a O
set O
of O
supporting O
facts O
in O
natural O
language O
, O
the O
model O
generates O
a O
proof O
tree O
indicating O
how O
the O
hypothesis O
is O
derived O
from O
a O
subset O
of O
the O
supporting O
facts O
. O
The O
proof O
tree O
may O
contain O
intermediate O
conclusions O
, O
which O
need O
to O
be O
generated O
by O
the O
model O
. O
Existing O
methods O
generate O
the O
proof O
either O
in O
a O
single O
shot O
or O
step O
by O
step O
. O

Stepwise O
methods O
leverage O
the O
compositionality O
of O
proofs O
, O
making O
it O
easier O
for O
the O
model O
to O
learn O
and O
generalize O
to O
longer O
proofs O
. O

However O
, O
existing O
stepwise O
methods O
suffer O
from O
a O
trade O
- O
off O
between O
generating O
valid O
steps O
and O
relevant O
steps O
. O
Prior O
works O
( O
Sanyal O
et O
al O
. O
, O
2022 O
; O
Bostrom O
et O
al O
. O
, O
2022 O
) O
have O
observed O
that O
, O
given O
the O
hypothesis O
, O
the O
model O
often O
learns O
to O
hallucinate O
invalid O
proof O
steps O
leading O
to O
the O
hypothesis O
, O
instead O
of O
performing O
valid O
logical O
inference O
( O
see O
examples O
in O
Table O
3 O
) O
. O
To O
mitigate O
this O
issue O
, O
previous O
attempts O
have O
restricted O
the O
model O
from O
accessing O
the O
hypothesis O
, O
forcing O
it O
to O
generate O
conclusions O
based O
solely O
on O
known O
premises O
. O
However O
, O
without O
the O
hypothesis O
, O
the O
model O
tends O
to O
generate O
many O
valid O
but O
irrelevant O
steps O
. O
This O
problem O
is O
especially O
prominent O
for O
real O
- O
world O
natural O
language O
proofs O
. O
Due O
to O
the O
inherent O
ambiguity O
of O
natural O
language O
, O
the O
search O
space O
for O
each O
proof O
step O
is O
much O
larger O
than O
that O
of O
simple O
synthetic O
tasks O
. O
That O
may O
explain O
why O
stepwise O
methods O
have O
demonstrated O
superior O
performance O
on O
the O
simple O
, O
synthetic O
RuleTaker B-DatasetName
dataset O
( O
Clark O
et O
al O
. O
, O
2020 O
) O
but O
not O
on O
the O
more O
realistic O
, O
human O
- O
authored O
EntailmentBank B-DatasetName
dataset O
, O
which O
is O
the O
gap O
we O
aim O
to O
bridge O
. O

We O
introduce O
NLProofS B-MethodName
, O
a O
novel O
method O
for O
stepwise B-MethodName
proof I-MethodName
generation I-MethodName
. O
It O
generates O
proof O
steps O
conditioning O
on O
the O
hypothesis O
, O
enabling O
the O
model O
to O
learn O
to O
generate O
only O
relevant O
steps O
. O
To O
prevent O
hallucination O
, O
it O
trains O
an O
independent O
verifier O
based O
on O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
which O
takes O
a O
single O
step O
( O
including O
multiple O
premises O
and O
one O
conclusion O
) O
as O
input O
and O
produces O
a O
score O
indicating O
its O
validity O
. O
During O
inference O
, O
instead O
of O
generating O
steps O
greedily O
, O
NLProofS B-MethodName
searches O
for O
proofs O
h O
: O
solar O
is O
a O
kind O
of O
renewable O
energy O
for O
heating O
homes O
sent1 O
: O
homes O
are O
buildings O
sent2 O
: O
solar O
is O
renewable O
sent3 O
: O
wind O
is O
a O
kind O
of O
energy O
sent4 O
: O
solar O
is O
a O
kind O
of O
energy O
sent5 O
: O
energy O
is O
used O
for O
heating O
buildings O
sent6 O
: O
coal O
is O
nonrenewable O
… O
… O
Hypothesis O
( O
ℎ O
) O
: O
Supporting O
facts O
( O
) O
: O

sent1 O
: O
homes O
are O
buildings O
h O
: O
solar O
is O
a O
kind O
of O
renewable O
energy O
for O
heating O
homes O
sent5 O
: O
energy O
is O
used O
for O
heating O
buildings O
int1 O
: O
energy O
is O
used O
for O
heating O
homes O
sent2 O
: O
solar O
is O
renewable O
sent4 O
: O
solar O
is O
a O
kind O
of O
energy O
int2 O
: O
solar O
is O
a O
kind O
of O
renewable O
energy O

Proof O
tree O
( O
) O
: O

Input O
Output O
$ O
hypothesis O
$ O
= O
solar O
is O
a O
kind O
of O
renewable O
energy O
for O
heating O
homes O
; O
$ O
facts O
$ O
= O
sent1 O
: O
homes O
are O
buildings O
sent2 O
: O
solar O
is O
renewable O
… O
; O

sent1 O
& O
sent5 O
- O
> O
int1 O
: O
energy O
is O
used O
for O
heating O
homes O
; O
sent2 O
& O
sent4 O
- O
> O
int2 O
: O
solar O
is O
a O
kind O
of O
renewable O
energy O
; O
int1 O
& O
int2 O
- O
> O
hypothesis O
; O

$ O
hypothesis O
$ O
= O
solar O
is O
a O
kind O
of O
renewable O
energy O
for O
heating O
homes O
; O
$ O
facts O
$ O
= O
sent1 O
: O
homes O
are O
buildings O
sent2 O
: O
solar O
is O
renewable O
… O
; O
$ O
partial_proof O
$ O
= O
sent1 O
& O
sent5 O
- O
> O
int1 O
: O
energy O
is O
used O
for O
heating O
homes O
; O
sent2 O
& O
sent4 O
- O
> O
int2 O
: O
solar O
is O
a O
kind O
of O
renewable O
energy O
; O

Task O
: O

Single O
- O
shot O
generation O
: O

Stepwise B-MethodName
generation I-MethodName
( O
2nd O
step O
) O
: O

Figure O
1 O
: O
Top O
: O
In O
proof O
generation O
, O
given O
a O
hypothesis O
and O
multiple O
supporting O
facts O
( O
potentially O
with O
redundant O
facts O
) O
, O
the O
model O
generates O
a O
proof O
tree O
, O
including O
both O
the O
tree O
structure O
and O
the O
intermediate O
conclusions O
( O
int1 O
and O
int2 O
) O
. O
A O
common O
approach O
encodes O
the O
input O
/ O
output O
as O
text O
sequences O
and O
generates O
the O
proof O
in O
a O
single O
- O
shot O
( O
Middle O
) O
or O
generates O
the O
proof O
step O
by O
step O
( O
Bottom O
, O
showing O
only O
one O
of O
the O
three O
steps O
) O
using O
text O
- O
to O
- O
text O
models O
. O

that O
maximize O
a O
proof O
score O
aggregating O
the O
validity O
scores O
of O
all O
steps O
. O
We O
evaluate O
NLProofS B-MethodName
on O
two O
benchmarks O
: O
Rule B-DatasetName
- I-DatasetName
Taker I-DatasetName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
and O
EntailmentBank B-DatasetName
. O
RuleTaker B-DatasetName
consists O
of O
simple O
, O
synthetic O
English O
sentences O
generated O
from O
templates O
. O
In O
contrast O
, O
proofs O
in O
EntailmentBank B-DatasetName
are O
authored O
by O
human O
experts O
and O
are O
more O
challenging O
. O
They O
are O
in O
unconstrained O
natural O
language O
and O
exhibit O
considerable O
fuzziness O
and O
ambiguity O
, O
as O
is O
typical O
for O
reasoning O
in O
natural O
language O
. O
NLProofS B-MethodName
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
both O
datasets O
. O
On O
EntailmentBank B-DatasetName
, O
it O
outperforms O
previous O
best O
results O
by O
a O
large O
margin O
. O
For O
example O
, O
in O
the O
distractor O
task O
setting O
, O
it O
improves O
the O
accuracy B-MetricName
of O
generating O
complete O
proofs O
from O
27.7 B-MetricValue
% I-MetricValue
to O
33.3 B-MetricValue
% I-MetricValue
and O
the O
accuracy B-MetricName
of O
identifying O
relevant O
supporting O
facts O
from O
46.1 B-MetricValue
% I-MetricValue
to O
58.8 B-MetricValue
% I-MetricValue
, O
which O
demonstrates O
the O
effectiveness O
of O
our O
method O
in O
generating O
challenging O
human O
- O
authored O
proofs O
. O

In O
addition O
, O
we O
conduct O
extensive O
ablations O
to O
gain O
insights O
. O
First O
, O
we O
show O
that O
the O
verifier O
plays O
a O
crucial O
role O
in O
generating O
proofs O
by O
providing O
well O
- O
calibrated O
validity O
scores O
. O
Without O
the O
verifier O
, O
our O
method O
performs O
worse O
on O
EntailmentBank B-DatasetName
and O
fails O
completely O
on O
RuleTaker B-DatasetName
. O
Second O
, O
while O
generating O
long O
proofs O
remains O
a O
major O
challenge O
, O
NLProofS B-MethodName
leads O
to O
large O
improvements O
for O
long O
proofs O
. O
Third O
, O
there O
is O
still O
a O
large O
room O
for O
future O
improvement O
, O
e.g. O
, O
by O
generating O
more O
accurate O
and O
diverse O
proof O
steps O
as O
candidates O
for O
the O
search O
algorithm O
to O
explore O
. O

Contributions O
. O
In O
summary O
, O
our O
contributions O
are O
two O
- O
fold O
. O
First O
, O
we O
introduce O
NLProofS B-MethodName
, O
a O
stepwise O
proof B-TaskName
generation I-TaskName
method O
that O
searches O
for O
proofs O
whose O
validity O
is O
scored O
by O
a O
verifier O
. O
It O
substantially O
advances O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
the O
challenging O
Entail B-DatasetName
- I-DatasetName
mentBank I-DatasetName
dataset O
. O
Second O
, O
through O
extensive O
analyses O
and O
ablations O
, O
we O
shed O
light O
on O
the O
performance O
improvement O
and O
reveal O
the O
current O
bottleneck O
. O
Our O
work O
is O
a O
first O
step O
exploring O
the O
interplay O
between O
verifiers O
and O
proof O
search O
in O
generating B-TaskName
natural I-TaskName
language I-TaskName
proofs I-TaskName
, O
and O
we O
expect O
further O
advancements O
down O
the O
road O
. O

Related O
Work O

Proof B-TaskName
generation I-TaskName
in I-TaskName
natural I-TaskName
language I-TaskName
. O
Table O
1 O
summarizes O
existing O
methods O
for O
generating B-TaskName
natural I-TaskName
language I-TaskName
proofs I-TaskName
, O
including O
single O
- O
shot O
and O
stepwise O
methods O
. O
Single O
- O
shot O
methods O
generate O
the O
entire O
proof O
tree O
in O
one O
shot O
, O
enforcing O
structural O
constraints O
explicitly O
via O
linear O
integer O
programming O
( O
Saha O
et O
al O
. O
, O
2020 O
; O
Sun O
et O
al O
. O
, O
2021 O
) O
or O
implicitly O
via O
pretrained O
text O
- O
to O
- O
text O
transformers O
( O
Gontier O
et O
al O
. O
, O
2020 O
; O
( O
Fig O
. O
1 O
Middle O
) O
. O
In O
contrast O
, O
stepwise O
methods O
generate O
the O
proof O
as O
individual O
proof O
steps O
, O
forward O
Sanyal O
et O
al O
. O
, O
2022 O
; O
Bostrom O
et O
al O
. O
, O
2022 O
) O
, O
backward O
( O
Liang O
et O
al O
. O
, O
2021 O
; O
Qu O
et O
al O
. O
, O
2022 O
; O
Dalvi O
et O
al O
. O
, O
2022 O
) O
, O
or O
both O
( O
Hong O
et O
al O
. O
, O
2022 O
) O
. O
Our O
method O
generates O
proofs O
stepwise O
, O
in O
the O
forward O
direction O
. O

When O
generating O
a O
proof O
step O
, O
prior O
work O
has O
observed O
that O
if O
the O
hypothesis O
is O
available O
, O
the O
model O
often O
uses O
it O
to O
hallucinate O
the O
intermediate O
conclusion O
instead O
of O
drawing O
valid O
logical O
inferences O
( O
Table O
3 O
) O
. O
Therefore O
, O
ProofWriter B-MethodName
, O
FaiRR B-MethodName
( O
Sanyal O
et O
al O
. O
, O
2022 O
) O
, O
and O
SCSearch B-MethodName
( O
Bostrom O
et O
al O
. O
, O
2022 O
) O

✗ O
N O
/ O
A O
No O
intermediates O
✗ O
N O
/ O
A O
✗ O
✓ O
EntailmentWriter B-DatasetName
✗ O
N O
/ O
A O
✓ O
✗ O
N O
/ O
A O
✓ O
✓ O
ProofWriter B-MethodName
✓ O
→ O
✗ O
✗ O
✗ O
✗ O
✓ O
FaiRR B-MethodName
✓ O
→ O
✗ O
✗ O
✗ O
✗ O
✓ O
SCSearch B-MethodName
✓ O
→ O
✗ O
✗ O
✗ O
✗ O
✗ O
MetGen B-MethodName
✓ O
Both O
✓ O
✗ O
✗ O
✓ O
✗ O
Dalvi O
et O
al O
. O
( O
2022 O
) O
† O
✓ O
← O
✓ O
✓ O
✗ O
✗ O
✗ O
NLProofS B-MethodName
( O
ours O
) O
✓ O
→ O
✓ O
✓ O
✓ O
✓ O
✓ O

Table O
1 O
: O
A O
comparison O
of O
NLProofS B-MethodName
with O
existing O
methods O
for O
proof O
generation O
: O
PRover B-MethodName
( O
Saha O
et O
al O
. O
, O
2020 O
) O
, O
Entail B-MethodName
- I-MethodName
mentWriter I-MethodName
, O
ProofWriter B-MethodName
, O
FaiRR B-MethodName
( O
Sanyal O
et O
al O
. O
, O
2022 O
) O
, O
SCSearch B-MethodName
( O
Bostrom O
et O
al O
. O
, O
2022 O
) O
, O
MetGen B-MethodName
( O
Hong O
et O
al O
. O
, O
2022 O
) O
, O
and O
a O
concurrent O
work O
( O
Dalvi O
et O
al O
. O
, O
2022 O
) O
marked O
with O
†. O
→ O
and O
← O
denote O
forward O
/ O
backward O
stepwise O
proof O
generation O
. O

inference O
from O
known O
premises O
only O
. O
However O
, O
without O
the O
hypothesis O
, O
the O
model O
may O
generate O
many O
valid O
proof O
steps O
irrelevant O
to O
the O
hypothesis O
. O
Unlike O
other O
forward O
stepwise O
methods O
, O
our O
model O
has O
access O
to O
the O
hypothesis O
but O
relies O
on O
a O
verifier O
to O
check O
the O
validity O
of O
proof O
steps O
and O
prevent O
hallucination O
. O
Dalvi O
et O
al O
. O
( O
2022 O
) O
is O
a O
concurrent O
work O
that O
also O
uses O
a O
verifier O
to O
score O
multiple O
candidate O
proof O
steps O
generated O
by O
the O
model O
. O
However O
, O
they O
use O
the O
scores O
to O
make O
a O
greedy O
local O
decision O
, O
selecting O
the O
best O
step O
and O
discarding O
others O
, O
whereas O
we O
search O
for O
proofs O
with O
the O
maximum O
aggregated O
scores O
. O
Besides O
, O
they O
train O
the O
verifier O
on O
additionally O
annotated O
negative O
examples O
, O
whereas O
we O
train O
on O
pseudo O
- O
negative O
examples O
generated O
automatically O
without O
additional O
annotation O
efforts O
( O
Sec O
. O
4.2 O
) O
. O
Other O
stepwise O
methods O
in O
Table O
1 O
do O
not O
have O
verifiers O
, O
and O
they O
make O
local O
decisions O
. O

PRover B-MethodName
( O
Saha O
et O
al O
. O
, O
2020 O
) O
, O
ProofWriter B-MethodName
, O
and O
FaiRR B-MethodName
have O
only O
evaluated O
on O
the O
simple O
RuleTaker B-DatasetName
dataset O
( O
Clark O
et O
al O
. O
, O
2020 O
) O
. O
And O
it O
is O
nontrivial O
to O
extend O
them O
to O
real O
- O
world O
data O
. O
For O
example O
, O
FaiRR B-MethodName
assumes O
sentences O
fall O
into O
two O
categories O
: O
rules O
and O
facts O
, O
which O
are O
tailored O
for O
RuleTaker B-DatasetName
. O
introduce O
EntailmentBank B-DatasetName
, O
a O
challenging O
benchmark O
of O
proofs O
authored O
by O
human O
experts O
, O
which O
is O
used O
to O
evaluate O
EntailmentWriter B-DatasetName
, O
their O
method O
for O
single O
- O
shot O
proof O
generation O
. O
SCSearch B-MethodName
and O
Dalvi O
et O
al O
. O
( O
2022 O
) O
also O
use O
EntailmentBank B-DatasetName
but O
focus O
on O
different O
task O
settings O
that O
do O
not O
quantitatively O
evaluate O
the O
generated O
proofs O
. O

Reasoning O
in O
other O
NLP O
tasks O
. O
Multi O
- O
hop O
reasoning O
can O
also O
be O
found O
in O
open O
- O
domain O
QA O
, O
fact O
verification O
( O
Jiang O
et O
al O
. O
, O
2020 O
) O
, O
and O
reading O
comprehension O
( O
Min O
et O
al O
. O
, O
2019 O
; O
Sinha O
et O
al O
. O
, O
2019 O
; O
Jiang O
et O
al O
. O
, O
2019 O
) O
. O
Compared O
to O
proof B-TaskName
generation I-TaskName
, O
reasoning O
chains O
in O
these O
tasks O
are O
much O
simpler O
, O
often O
consisting O
of O
only O
2 O
- O
3 O
supporting O
facts O
. O
Also O
, O
they O
are O
more O
coarse O
- O
grained O
, O
involving O
large O
chunks O
of O
texts O
such O
as O
passages O
instead O
of O
simple O
, O
short O
sentences O
. O
Bostrom O
et O
al O
. O
( O
2021 O
) O
generate O
conclusions O
from O
premises O
. O
Their O
method O
can O
potentially O
be O
a O
component O
in O
proof B-TaskName
generation I-TaskName
but O
does O
not O
consider O
whether O
the O
generated O
conclusions O
are O
relevant O
. O
In O
math O
word O
problems O
, O
Cobbe O
et O
al O
. O
( O
2021 O
) O
demonstrate O
the O
benefits O
of O
using O
a O
verifier O
to O
re O
- O
rank O
the O
model O
's O
predicted O
solutions O
. O
However O
, O
these O
solutions O
are O
unconstrained O
texts O
, O
whereas O
proofs O
in O
our O
task O
are O
structured O
trees O
/ O
graphs O
. O
Further O
, O
we O
use O
the O
verifier O
during O
proof O
generation O
rather O
than O
merely O
to O
rank O
the O
solutions O
post O
hoc O
. O
Our O
verifier O
is O
also O
related O
to O
natural O
language O
inference O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
especially O
the O
multipremises O
setting O
in O
Lai O
et O
al O
. O
( O
2017 O
) O
. O
Recently O
, O
large O
language O
models O
have O
shown O
the O
ability O
to O
solve O
multi O
- O
step O
reasoning O
through O
chain O
- O
of O
- O
thought O
prompting O
Kojima O
et O
al O
. O
, O
2022 O
) O
on O
arithmetic O
, O
symbolic O
and O
commonsense O
reasoning O
tasks O
. O

Symbolic O
reasoning O
. O
Classical O
AI O
has O
invested O
significant O
efforts O
in O
reasoning O
in O
symbolic O
domains O
, O
e.g. O
, O
automated O
theorem O
proving O
( O
ATP O
) O
( O
Kovács O
and O
Voronkov O
, O
2013 O
; O
Yang O
and O
Deng O
, O
2019 O
; O
Polu O
and O
Sutskever O
, O
2020 O
) O
. O
Researchers O
have O
attempted O
to O
apply O
ATP O
to O
natural O
language O
through O
semantic O
parsing O
( O
Mineshima O
et O
al O
. O
, O
2015 O
; O
Saparov O
and O
Mitchell O
, O
2022 O
) O
. O
However O
, O
it O
is O
challenging O
( O
if O
not O
impossible O
) O
for O
semantic O
parsers O
to O
cover O
the O
full O
complexity O
of O
natural O
language O
. O
Therefore O
, O
researchers O
have O
developed O
reasoning O
approaches O
bypassing O
semantic O
parsing O
( O
Angeli O
et O
al O
. O
, O
2016 O
; O
Kalyanpur O
et O
al O
. O
, O
2022 O
; O
Yang O
and O
Deng O
, O
2021 O
) O
. O

One O
promising O
example O
is O
neurosymbolic O
reasoning O
. O
It O
uses O
neural O
networks O
to O
handle O
the O
complexity O
of O
natural O
language O
but O
incorporates O
inductive O
biases O
inspired O
by O
symbolic O
reasoning O
( O
Weber O
et O
al O
. O
, O
2019 O
; O
Smolensky O
, O
1990 O
; O
Kathryn O
and O
Mazaitis O
, O
2018 O
; O
Lee O
et O
al O
. O
, O
2016 O
) O
. O
Our O
method O
also O
falls O
into O
this O
broad O
category O
. O
It O
uses O
large O
language O
models O
to O
generate O
individual O
reasoning O
steps O
but O
chains O
the O
steps O
together O
into O
a O
coherent O
, O
tree O
- O
structured O
proof O
using O
symbolic O
search O
algorithms O
. O

Generating B-TaskName
Natural I-TaskName
Language I-TaskName
Proofs I-TaskName

Task O
definition O
. O
Now O
we O
define O
the O
proof B-TaskName
generation I-TaskName
task O
. O
As O
in O
Fig O
. O
1 O
( O
Top O
) O
, O
the O
input O
consists O
of O
a O
hypothesis O
h O
and O
a O
set O
of O
supporting O
facts O
C O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
. O
. O
. O
, O
sent O
n O
} O
. O
Both O
h O
and O
sent O
i O
are O
natural O
language O
sentences O
. O
h O
can O
be O
derived O
from O
a O
subset O
of O
C O
through O
reasoning O
of O
one O
or O
multiple O
steps O
. O

The O
output O
is O
a O
proof O
tree O
T O
specifying O
how O
h O
is O
derived O
from O
C. O
The O
tree O
has O
h O
as O
its O
root O
and O
sent O
i O
as O
leaf O
nodes O
. O
The O
intermediate O
nodes O
are O
intermediate O
conclusions O
generated O
by O
the O
model O
. O
Each O
non O
- O
leaf O
node O
u O
corresponds O
to O
a O
reasoning O
step O
with O
u O
as O
the O
conclusion O
and O
its O
children O
as O
premises O
. O
To O
successfully O
perform O
the O
task O
, O
the O
model O
must O
select O
relevant O
sentences O
from O
C O
, O
use O
them O
as O
leaf O
nodes O
to O
compose O
a O
valid O
proof O
tree O
leading O
to O
h O
, O
and O
fill O
in O
all O
the O
intermediate O
conclusions O
. O

Singles O
- O
shot O
vs. O
stepwise O
generation O
. O
A O
simple O
and O
effective O
method O
for O
proof O
generation O
, O
popularized O
by O
ProofWriter B-MethodName
, O
is O
to O
finetune O
a O
pretrained O
T5 B-MethodName
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
to O
map O
the O
input O
( O
h O
and O
C O
) O
to O
the O
output O
( O
T O
) O
, O
either O
in O
a O
single O
shot O
or O
stepwise O
. O
To O
that O
end O
, O
the O
input O
/ O
output O
must O
be O
encoded O
as O
text O
sequences O
, O
e.g. O
, O
encoding O
the O
input O
by O
concatenating O
h O
and O
C O
as O
illustrated O
in O
Fig O
. O
1 O
. O
3 O
The O
output O
proof O
tree O
can O
be O
encoded O
by O
post O
- O
order O
traversal O
. O
As O
in O
Fig O
. O
1 O
, O
nodes O
are O
labeled O
with O
identifiers O
: O
sent O
* O
for O
leaf O
nodes O
, O
int O
* O
for O
intermediate O
nodes O
, O
and O
hypothesis O
for O
the O
root O
. O
The O
output O
sequence O
is O
produced O
by O
traversing O
the O
tree O
in O
post O
- O
order O
, O
generating O
one O
proof O
step O
at O
each O
non O
- O
leaf O
node O
, O
using O
& O
for O
" O
and O
" O
and O
- O
> O
for O
" O
entails O
" O
. O
The O
tree O
may O
correspond O
to O
multiple O
valid O
sequences O
due O
to O
different O
ordering O
between O
proof O
steps O
and O
between O
premises O
within O
a O
step O
. O
Nevertheless O
, O
the O
evaluation O
metric O
can O
be O
calculated O
from O
the O
reconstructed O
trees O
instead O
of O
the O
raw O
text O
sequences O
. O

In O
single O
- O
shot O
generation O
, O
the O
model O
generates O
the O
output O
sequence O
of O
the O
entire O
proof O
( O
Fig O
. O
1 O
Middle O
) O
, O
whereas O
in O
stepwise O
generation O
, O
each O
time O
the O
model O
takes O
the O
current O
partial O
proof O
as O
input O
( O
besides O
h O
and O
C O
) O
and O
generates O
only O
the O
next O
step O
( O
Fig O
. O
1 O
Bottom O
) O
. O

Our O
Method O
: O
NLProofS B-MethodName

Now O
we O
present O
NLProofS B-MethodName
, O
our O
method O
for O
generating B-TaskName
natural I-TaskName
language I-TaskName
proofs I-TaskName
. O
It O
has O
three O
main O
components O
: O

( O
1 O
) O
a O
stepwise O
prover O
for O
generating O
candidate O
proof O
steps O
; O
( O
2 O
) O
a O
verifier O
for O
scoring O
the O
validity O
of O
proofs O
; O

( O
3 O
) O
an O
algorithm O
for O
searching O
for O
proofs O
that O
have O
high O
aggregated O
proof O
scores O
. O

Stepwise O
Prover O

Like O
prior O
work O
, O
we O
implement O
the O
stepwise O
prover O
by O
finetuning O
a O
pretrained O
T5 B-MethodName
model O
. O
The O
training O
data O
is O
extracted O
from O
the O
steps O
in O
ground O
truth O
proofs O
. O
Let O
T O
be O
a O
proof O
tree O
and O
u O
∈ O
T O
be O
a O
nonleaf O
node O
corresponding O
to O
a O
step O
we O
want O
to O
extract O
. O
Take O
node O
int1 O
in O
Fig O
. O
1 O
as O
an O
example O
of O
u. O
Non O
- O
leaf O
nodes O
in O
T O
can O
be O
categorized O
into O
( O
1 O
) O
u O
's O
descendants O
, O
e.g. O
, O
none O
in O
Fig O
. O
1 O
; O
( O
2 O
) O
u O
itself O
and O
its O
ancestors O
, O
e.g. O
, O
int1 O
and O
h O
in O
Fig O
. O
1 O
; O
( O
3 O
) O
neither O
, O
e.g. O
, O
int2 O
in O
Fig O
. O
1 O
. O
The O
partial O
proof O
must O
include O
all O
of O
( O
1 O
) O
but O
none O
of O
( O
2 O
) O
. O
It O
may O
or O
may O
not O
include O
nodes O
in O
( O
3 O
) O
. O
Therefore O
, O
for O
this O
particular O
example O
, O
the O
partial O
proof O
can O
not O
include O
int1 O
or O
h O
but O
has O
a O
free O
choice O
about O
whether O
to O
include O
int2 O
. O
When O
preprocessing O
the O
training O
data O
, O
we O
make O
these O
choices O
randomly O
as O
a O
form O
of O
data O
augmentation O
. O

During O
inference O
, O
the O
prover O
may O
generate O
syntactically O
ill O
- O
formed O
proof O
steps O
. O
For O
the O
example O
in O
Fig O
. O
1 O
( O
Bottom O
) O
, O
" O
int1 O
& O
int2 O
- O
> O
hypothesis O
; O
" O
is O
ill O
- O
formed O
, O
since O
the O
premise O
int2 O
is O
not O
available O
in O
the O
current O
partial O
proof O
. O
We O
mitigate O
the O
issue O
by O
generating O
multiple O
proof O
steps O
from O
the O
model O
via O
beam O
search O
and O
using O
heuristics O
to O
filter O
out O
ill O
- O
formed O
ones O
, O
e.g. O
, O
those O
with O
syntactical O
errors O
or O
unavailable O
premises O
. O

Verifier O

Scoring O
a O
proof O
step O
. O
We O
introduce O
an O
independent O
verifier O
, O
which O
is O
trained O
to O
check O
the O
validity O
of O
proof O
steps O
and O
prevent O
the O
prover O
from O
hallucinating O
invalid O
steps O
based O
on O
the O
hypothesis O
. O
A O
proof O
step O
has O
multiple O
premises O
and O
one O
conclusion O
. O
The O
verifier O
takes O
them O
as O
input O
and O
produces O
a O
continuous O
validity O
score O
in O
[ O
0 O
, O
1 O
] O
. O

We O
implement O
the O
verifier O
by O
finetuning O
a O
pretrained O
RoBERTa B-MethodName
model O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
Aggregating O
scores O
for O
the O
entire O
proof O
. O

Step B-MetricName
scores I-MetricName
are O
aggregated O
to O
produce O
the O
score O
of O
the O
entire O
proof O
tree O
. O
We O
associate O
scores O
with O
all O
nodes O
in O
the O
tree O
recursively O
. O
All O
leaves O
have O
a O
score O
of O
1.0 B-MetricValue
, O
as O
they O
are O
explicitly O
provided O
assumptions O
that O
always O
hold O
. O
Each O
non O
- O
leaf O
node O
u O
corresponds O
to O
a O
proof O
step O
s O
from O
its O
children O
v O
1 O
, O
v O
2 O
, O
. O
. O
. O
, O
v O
l O
and O
has O
a O
score O
defined O
as O

scr O
n O
( O
u O
) O
= O
min O
scr O
s O
( O
s O
) O
, O
scr O
n O
( O
v O
1 O
) O
, O
. O
. O
. O
, O
scr O
n O
( O
v O
l O
) O
, O
( O
1 O
) O

where O
scr O
s O
( O
s O
) O
is O
the O
step B-MetricName
score I-MetricName
, O
e.g. O
, O
produced O
by O
a O
verifier O
. O
Intuitively O
, O
scr O
n O
( O
u O
) O
reflects O
our O
confidence O
in O
u O
, O
and O
it O
is O
monotonically O
non O
- O
increasing O
w.r.t O
. O
the O
step O
score O
and O
the O
scores O
of O
its O
children O
. O
Eqn O
. O
1 O
is O
just O
one O
simple O
way O
of O
defining O
scr O
n O
( O
u O
) O
, O
and O
we O
leave O
a O
more O
thorough O
exploration O
of O
scoring O
options O
for O
future O
work O
. O
Finally O
, O
the O
proof O
score O
is O
scr O
n O
( O
h O
) O
: O
the O
root O
's O
score O
. O

Proof B-TaskName
Search I-TaskName

Now O
we O
combine O
the O
prover O
and O
the O
verifier O
in O
our O
proof O
search O
algorithm O
, O
which O
looks O
for O
proofs O
with O
optimal O
proof O
scores O
. O
Our O
method O
is O
inspired O
by O
automated O
reasoning O
in O
formal O
logic O
( O
Russell O
and O
Norvig O
, O
2002 O
) O
, O
where O
proofs O
are O
found O
by O
searching O
in O
a O
large O
space O
efficiently O
. O
Instead O
of O
greedy O
stepwise O
proof B-TaskName
generation I-TaskName
, O
we O
search O
for O
proof O
trees O
in O
a O
large O
proof O
graph O
( O
Fig O
. O
2 O
) O
, O
allowing O
the O
model O
to O
explore O
different O
paths O
, O
recover O
from O
errors O
, O
and O
ultimately O
find O
better O
proofs O
. O
Proof O
trees O
correspond O
to O
paths O
in O
proof O
graphs O
( O
treating O
S O
as O
" O
and O
" O
nodes O
in O
and O
- O
or O
graphs O
) O
. O
Therefore O
, O
our O
task O
is O
to O
search O
for O
a O
path O
from O
C O
to O
h O
that O
maximizes O
scr O
n O
( O
h O
) O
. O
The O
search O
algorithm O
is O
outlined O
in O
Fig O
. O
2 O
and O
Algorithm O
1 O
. O
Proof B-TaskName
search I-TaskName
takes O
place O
only O
in O
inference O
. O

In O
training O
, O
we O
train O
a O
stepwise O
prover O
P O
and O
a O
verifier O
V. O
In O
inference O
, O
we O
use O
them O
to O
iteratively O
expand O
the O
proof O
graph O
and O
update O
the O
node O
scores O
until O
the O
graph O
can O
no O
longer O
be O
updated O
. O
At O
that O
point O
, O
we O
extract O
the O
best O
proof O
of O
h O
found O
so O
far O
. O

Initialization O
( O
line O
1 O
- O
3 O
in O
Algorithm O
1 O
) O
. O
We O
initialize O
the O
proof O
graph O
using O
the O
greedy O
proof O
generated O
by O
P. O
We O
could O
also O
start O
from O
scratch O
, O
i.e. O
, O
I O
= O
S O
= O
∅ O
and O
scr O
( O
h O
) O
= O
0 O
, O
but O
the O
initialization O
accelerates O
proof O
search O
by O
providing O
a O
non O
- O
zero O
initial O
score O
for O
h O
, O
which O
can O
be O
used O
to O
prune O
unpromising O
paths O
during O
search O
. O

Iteration O
( O
line O
5 O
- O
13 O
in O
Algorithm O
1 O
) O
. O
We O
use O
P O
to O
generate O
proof O
steps O
for O
updating O
the O
graph O
. O
P O
is O
trained O
on O
partial O
proof O
trees O
rather O
than O
graphs O
. O
So O
in O
each O
iteration O
, O
we O
first O
sample O
a O
new O
partial O
proof O
tree O
from O
the O
graph O
as O
the O
candidate O
for O
expansion O
( O
details O
in O
Appendix O
E O
) O
. O
Then O
, O
we O
use O
P O
to O
generate O
multiple O
proof O
Algorithm O
1 O
: O
Proof O
search O
. O

Input O
: O
Hypothesis O
h O
, O
supporting O
facts O
C O
, O
stepwise O
prover O
P O
, O
verifier O
V O
Output O
: O
Proof O
tree O
T O
steps O
s O
1 O
, O
s O
2 O
, O
. O
. O
. O
, O
s O
k O
through O
beam O
search O
followed O
by O
filtering O
as O
discussed O
in O
Sec O
. O
4.1 O
. O
We O
calculate O
step O
scores O
scr O
s O
( O
s O
1 O
) O
, O
scr O
s O
( O
s O
2 O
) O
, O
. O
. O
. O
, O
scr O
s O
( O
s O
k O
) O
by O
averaging O
verifier O
scores O
v_scrs O
from O
V O
( O
Sec O
. O
4.2 O
) O
and O
prover O
scores O
p_scrs O
from O
P O
, O
which O
are O
the O
likelihood O
scores O
in O
beam O
search O
. O

1 O
G O
← O
generate_greedy O
( O
P O
, O
h O
, O
C O
) O
2 O
PG O
← O
initialize_graph O
( O
G O
) O
3 O
explored O
← O
∅ O

Then O
we O
try O
to O
update O
the O
proof O
graph O
by O
executing O
these O
steps O
. O
Assume O
a O
step O
s O
i O
has O
premises O
v O
1 O
, O
. O
. O
. O
, O
v O
l O
and O
a O
conclusion O
u. O
First O
, O
we O
use O
Eqn O
. O
1 O
to O
calculate O
a O
tentative O
score O
scr O
n O
( O
u O
) O
. O
If O
u O
is O
an O
existing O
node O
in O
the O
graph O
with O
scr O
n O
( O
u O
) O
≥ O
scr O
n O
( O
u O
) O
, O
the O
step O
becomes O
a O
noop O
, O
and O
we O
do O
not O
perform O
any O
update O
. O
Otherwise O
, O
there O
are O
two O
cases O
: O
( O
1 O
) O
If O
u O
is O
not O
in O
the O
graph O
( O
Fig O
. O
2 O
) O
, O
we O
just O
create O
a O
new O
node O
for O
it O
with O
scr O
n O
( O
u O
) O
= O
scr O
n O
( O
u O
) O
; O
( O
2 O
) O
If O
u O
is O
in O
the O
graph O
and O
scr O
n O
( O
u O
) O
< O
scr O
n O
( O
u O
) O
, O
we O
update O
u O
by O
replacing O
the O
existing O
proof O
step O
leading O
to O
it O
with O
the O
new O
step O
s O
i O
with O
scr O
n O
( O
u O
) O
= O
scr O
n O
( O
u O
) O
. O
According O
to O
Eqn O
. O
1 O
, O
the O
score O
change O
may O
affect O
u O
's O
successors O
, O
so O
we O
propagate O
the O
change O
to O
all O
of O
them O
. O

Proof O
extraction O
( O
line O
14 O
in O
Algorithm O
1 O
) O
. O
When O
all O
proof O
steps O
in O
an O
iteration O
are O
no O
- O
op O
, O
we O
stop O
and O
extract O
the O
best O
proof O
of O
h O
found O
so O
far O
, O
which O
simply O
consists O
of O
all O
predecessors O
of O
h. O
The O
result O
is O
guaranteed O
to O
be O
a O
tree O
, O
as O
we O
prove O
in O
Appendix O
D O
. O

Main O
Results O

Experimental O
Setup O

We O
evaluate O
NLProofS B-MethodName
on O
proof O
generation O
using O
two O
benchmarks O
: O
a O
real O
- O
world O
benchmark O
Entailment B-DatasetName
- I-DatasetName
Bank I-DatasetName
and O
a O
synthetic O
benchmark O
RuleTaker B-DatasetName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
. O
Training O
and O
inference O
details O
are O
in O
Appendix O
F. O
Bostrom O
et O
al O
. O
( O
2022 O
) O
also O
evaluated O
on O
EntailmentBank B-DatasetName
but O
deviated O
from O
the O
original O
setting O
, O
instead O
formulating O
the O
task O
as O
distinguishing O
verifiable O
hypotheses O
from O
unverifiable O
ones O
. O
In O
order O
to O
have O
a O
fair O
comparison O
with O
their O
work O
, O
we O
also O
evaluate O
under O
their O
setting O
in O
Appendix O
G O
. O

EntailmentBank B-DatasetName
. O
EntailmentBank B-DatasetName
consists O
of O
1,840 O
proof O
trees O
constructed O
by O
expert O
annotators O
( O
1,313 B-HyperparameterValue
for O
training O
, O
187 B-HyperparameterValue
for O
validation O
, O
and O
340 B-HyperparameterValue
for O
testing O
) O
. O
It O
comes O
with O
three O
variants O
of O
the O
proof B-TaskName
generation I-TaskName
task O
( O
Sec O
. O
3 O
) O
with O
varying O
numbers O
of O
distractors O
in O
supporting O
facts O
C. O
Task O
1 O
does O
not O
have O
any O
distractor O
, O
i.e. O
, O
C O
consists O
of O
exactly O
the O
leaf O
nodes O
of O
the O
ground O
truth O
proof O
tree O
. O
In O
Task O
2 O
, O
C O
always O
has O
25 O
sentences O
, O
including O
ground O
truth O
supporting O
facts O
as O
well O
as O
distractors O
. O
In O
Task O
3 O
, O
C O
is O
a O
large O
corpus O
of O
12 O
K O
sentences O
derived O
from O
WorldTree O
V2 O
( O
Xie O
et O
al O
. O
, O
2020 O
) O
, O
requiring O
the O
model O
to O
retrieve O
relevant O
supporting O
facts O
from O
the O
corpus O
. O
We O
evaluate O
on O
all O
three O
tasks O
. O
Our O
method O
is O
directly O
applicable O
to O
Task O
1 O
and O
Task O
2 O
. O
For O
Task O
3 O
, O
retrieve O
25 O
supporting O
facts O
for O
each O
hypothesis O
. O
We O
use O
the O
same O
retrieved O
supporting O
facts O
and O
focus O
solely O
on O
proof O
generation O
. O
4 O
And O
following O
their O
practice O
, O
we O
train O
the O
model O
on O
Task O
2 O
and O
evaluate O
its O
zero O
- O
shot O
performance O
on O
Task O
3 O
. O

A O
generated O
proof O
treeT O
is O
compared O
against O
the O
ground O
truth O
T O
using O
official O
metrics O
developed O
by O
En B-DatasetName
- I-DatasetName
tailmentBank I-DatasetName
. O
In O
summary O
, O
the O
leaves O
, O
proof O
steps O
, O
and O
intermediate O
conclusions O
inT O
are O
compared O
against O
those O
in O
T O
to O
produce O
two O
metrics O
: O
the O
F1 B-MetricName
score I-MetricName
, O
and O
the O
AllCorrect B-MetricName
score I-MetricName
which O
evaluates O
exact O
matches O
. O
5 O
In O
addition O
, O
the O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
metric O
measures O
whether O
T O
is O
identical O
to O
T O
. O
As O
a O
caveat O
, O
these O
metrics O
do O
not O
account O
for O
the O
existence O
of O
multiple O
valid O
proof O
trees O
. O
Metrics O
for O
evaluating O
leaves O
are O
less O
impacted O
by O
this O
issue O
, O
as O
multiple O
valid O
trees O
often O
have O
the O
same O
set O
of O
leaves O
. O
Please O
refer O
to O
Appendix O
A O
and O
EntailmentBank B-DatasetName
for O
additional O
details O
. O
We O
report O
results O
produced O
by O
their O
official O
evaluation O
code O
. O
6 O
RuleTaker B-DatasetName
. O
To O
demonstrate O
the O
broad O
applicability O
of O
NLProofS B-MethodName
to O
different O
reasoning O
datasets O
, O
we O
also O
evaluate O
on O
RuleTaker B-DatasetName
. O
In O
RuleTaker B-DatasetName
, O
h O
can O
be O
either O
proved O
, O
disproved O
, O
or O
neither O
. O
The O
model O
has O
to O
do O
two O
things O
: O
( O
1 O
) O
predict O
the O
answer O
as O
one O
of O
those O
three O
categories O
and O
( O
2 O
) O
generate O
a O
proof O
when O
h O
can O
be O
proved O
or O
The O
predicted O
answer O
is O
evaluated O
using O
accuracy B-MetricName
, O
whereas O
proofs O
are O
evaluated O
using O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
but O
ignoring O
the O
intermediate O
conclusions O
. O
7 O

Proof B-TaskName
Generation I-TaskName
on O
EntailmentBank B-DatasetName

Table O
2 O
shows O
test O
results O
on O
EntailmentBank B-DatasetName
. O
We O
compare O
with O
EntailmentWriter B-MethodName
and O
MetGen B-MethodName
( O
Hong O
et O
al O
. O
, O
2022 O
) O
: O
two O
prior O
state O
- O
of O
- O
the O
- O
art O
methods O
that O
also O
finetune O
a O
T5 B-MethodName
model O
to O
generate O
proofs O
. O
EntailmentWriter B-MethodName
generates O
the O
entire O
proof O
in O
a O
single O
shot O
, O
whereas O
MetGen B-MethodName
generates O
the O
proof O
stepwise O
. O
En B-MethodName
- I-MethodName
tailmentWriter I-MethodName
has O
two O
versions O
, O
one O
with O
T5 B-MethodName
- I-MethodName
large I-MethodName
( O
737 B-HyperparameterValue
million I-HyperparameterValue
parameters O
) O
and O
the O
other O
with O
T5 B-MethodName
- I-MethodName
11B I-MethodName
( O
11 B-HyperparameterValue
billion I-HyperparameterValue
parameters O
) O
. O
All O
other O
methods O
, O
including O
ours O
, O
use O
only O
T5 B-MethodName
- I-MethodName
large I-MethodName
due O
to O
computational O
constraints O
. O

NLProofS B-MethodName
significantly O
outperforms O
Entailmen B-MethodName
- I-MethodName
tWriter I-MethodName
across O
the O
board O
. O
Take O
Task O
2 O
as O
an O
example O
. O
First O
, O
it O
generates O
more O
correct O
proofs O
overall O
, O
improving O
the O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
metric O
from O
20.9 B-MetricValue
% I-MetricValue
to O
33.3 B-MetricValue
% I-MetricValue
. O
Second O
, O
it O
identifies O
relevant O
supporting O
facts O
more O
effectively O
, O
improving O
the O
Leaves B-MetricName
- I-MetricName
AllCorrect I-MetricName
from O
35.6 B-MetricValue
% I-MetricValue
to O
58.8 B-MetricValue
% I-MetricValue
. O
Third O
, O
it O
generates O
more O
accurate O
proof O
steps O
and O
intermediate O
conclusions O
, O
as O
demonstrated O
by O
the O
Steps O
and O
Intermediates O
metrics O
. O
Moreover O
, O
our O
method O
with O
T5 B-MethodName
- I-MethodName
large I-MethodName
even O
outperforms O
EntailmentWriter B-MethodName
with O
T5 B-MethodName
- I-MethodName
11B I-MethodName
by O
a O
large O
margin O
. O

Compared O
to O
MetGen B-MethodName
, O
we O
perform O
competitively O
on O
Task O
1 O
and O
Task O
3 O
but O
much O
better O
on O
Task O
2 O
, O
improving O
the O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
metric O
from O
27.7 B-MetricValue
% I-MetricValue
to O
33.3 B-MetricValue
% I-MetricValue
. O
Note O
that O
our O
model O
is O
trained O
only O
on O
EntailmentBank B-DatasetName
, O
whereas O
MetGen B-MethodName
requires O
much O
more O
data O
annotation O
efforts O
( O
Sec O
. O
4.1.2 O
in O
Hong O
et O
al O
. O
( O
2022 O
) O
) O
. O
First O
, O
the O
MetGen B-MethodName
authors O
manually O
design O
templates O
of O
different O
reasoning O
types O
and O
use O
them O
to O
collect O
additional O
training O
data O
from O
Wikipedia O
. O
Second O
, O
they O
manually O
annotate O
the O
reasoning O
types O
of O
400 O
training O
proof O
steps O
in O
EntailmentBank B-DatasetName
. O
MetGen B-MethodName
needs O
these O
annotations O
since O
the O
model O
takes O
the O
reasoning O
type O
as O
input O
. O

We O
also O
examine O
whether O
the O
proof O
can O
be O
generated O
in O
a O
single O
shot O
by O
very O
large O
language O
models O
such O
as O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
or O
Codex B-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
, O
through O
prompting O
with O
in O
- O
context O
examples O
. O
Results O
in O
Appendix O
H O
show O
that O
in O
- O
context O
prompting O
performs O
substantially O
worse O
than O
our O
method O
. O

Table O
3 O
shows O
two O
examples O
of O
invalid O
steps O
generated O
by O
EntailmentWriter B-MethodName
but O
avoided O
by O
NLProofS B-MethodName
, O
likely O
due O
to O
its O
verifier O
. O
In O
the O
first O
example O
, O
" O
June O
" O
in O
EntailmentWriter B-MethodName
's O
conclusion O
is O
hallucinated O
based O
on O
the O
hypothesis O
, O
as O
the O
word O
does O
not O
appear O
in O
the O
43.2 O
± O
0.6 O
8.2 O
± O
0.7 O
11.2 O
± O
0.6 O
6.9 O
± O
0.7 O
42.9 O
± O
1.0 O
17.3 O
± O
0.5 O
6.9 O
± O
0.7 O
Here O
we O
report O
the O
results O
of O
the O
MetGen B-MethodName
- I-MethodName
prefixed I-MethodName
model O
, O
as O
the O
other O
MetGen B-MethodName
- I-MethodName
separated I-MethodName
model O
performs O
slightly O
better O
but O
is O
5 O
times O
larger O
. O
All O
methods O
are O
based O
on O
T5 O
- O
large O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
unless O
otherwise O
noted O
. O
For O
our O
method O
, O
we O
report O
the O
average O
performance O
and O
the O
standard O
deviation O
for O
5 O
independent O
runs O
. O
Bold O
and O
underlined O
texts O
highlight O
the O
best O
method O
and O
the O
runner O
- O
up O
. O

Hypothesis O
Premises O
Conclusions O
generated O
by O
models O

The O
next O
new O
moon O
will O
occur O
on O
June O
30 O
. O
1 O
. O
A O
new O
moon O
is O
a O
kind O
of O
phase O
of O
the O
moon O
. O
2 O
. O
A O
moon O
phase O
occurs O
28 O
days O
after O
the O
last O
time O
it O
occurs O
. O

EntailmentWriter B-MethodName
: O
The O
next O
new O
moon O
will O
occur O
28 O
days O
after O
June O
2 O
. O

NLProofS B-MethodName
( O
ours O
) O
: O

The O
next O
new O
moon O
will O
occur O
28 O
days O
after O
the O
last O
new O
moon O
. O

Planting O
trees O
prevents O
soil O
from O
washing O
away O
. O
1 O
. O
Planting O
trees O
increases O
the O
amount O
of O
trees O
in O
an O
environment O
. O
2 O
. O
Tree O
roots O
decrease O
/ O
reduce O
soil O
erosion O
. O

EntailmentWriter B-MethodName
: O
Plants O
trees O
increases O
the O
amount O
of O
trees O
in O
an O
environment O
. O

NLProofS B-MethodName
( O
ours O
) O
: O

Planting O
trees O
decreases O
soil O
erosion O
. O

Table O
3 O
: O
Examples O
of O
invalid O
proof O
steps O
generated O
by O
EntailmentWriter O
but O
not O
our O
method O
. O
In O
the O
first O
example O
, O
" O
June O
" O
in O
the O
conclusion O
is O
hallucinated O
rather O
than O
derived O
from O
the O
premises O
. O
In O
the O
second O
example O
, O
EntailmentWriter O
simply O
copies O
one O
of O
the O
premises O
without O
performing O
any O
meaningful O
reasoning O
. O
premises O
. O
The O
second O
example O
is O
a O
typical O
undesirable O
behavior O
also O
observed O
by O
Bostrom O
et O
al O
. O
( O
2022 O
) O
. O
When O
the O
model O
has O
difficulties O
in O
generating O
a O
conclusion O
, O
it O
falls O
back O
into O
copying O
one O
of O
the O
premises O
. O
Our O
method O
generates O
reasonable O
conclusions O
in O
these O
two O
examples O
. O

Generating B-TaskName
Answers I-TaskName
and I-TaskName
Proofs I-TaskName
on O
RuleTaker B-MethodName

Hypotheses O
in O
RuleTaker B-DatasetName
can O
be O
provable O
, O
disprovable O
, O
or O
neither O
. O
To O
benchmark O
on O
RuleTaker B-DatasetName
, O
we O
use O
a O
similar O
scheme O
to O
Bostrom O
et O
al O
. O
( O
2022 O
) O
to O
adapt O
any O
proof B-TaskName
generation I-TaskName
system O
capable O
of O
producing O
proof O
scores O
. O

In O
training O
, O
we O
( O
1 O
) O
discard O
hypotheses O
that O
are O
neither O
provable O
nor O
disprovable O
and O
( O
2 O
) O
convert O
disprovable O
hypotheses O
into O
provable O
ones O
by O
negating O
them O
. O
We O
negate O
sentences O
by O
adding O
an O
" O
I O
do O
n't O
think O
" O
prefix O
. O

In O
testing O
, O
given O
a O
hypothesis O
h O
, O
we O
try O
to O
generate O
proofs O
and O
the O
associated O
scores O
for O
both O
h O
and O
its O
negation O
¬h O
. O
Then O
we O
train O
a O
linear O
classifier O
on O
top O
of O
the O
two O
scores O
to O
predict O
the O
answer O
. O
Depending O
on O
the O
predicted O
answer O
, O
we O
take O
the O
generated O
proof O
to O
be O
the O
proof O
of O
h O
, O
¬h O
, O
or O
neither O
. O

Results O
are O
in O
Table O
4 O
. O
ProofWriter B-MethodName
is O
the O
iterative O
model O
in O
. O
It O
generates O
proofs O
step O
- O
wise O
based O
on O
T5 B-MethodName
. O
Our O
method O
performs O
competitively O
with O
ProofWriter B-MethodName
and O
FaiRR B-MethodName
( O
Sanyal O
et O
al O
. O
, O
2022 O
) O
. O

Analyses O

Ablation O
Studies O

EntailmentBank B-DatasetName
. O
Our O
full O
model O
searches O
for O
stepwise O
proofs O
, O
relying O
on O
both O
the O
verifier O
and O
the O
prover O
for O
producing O
scores O
. O
We O
conduct O
ablation O
studies O
on O
Task O
2 O
of O
EntailmentBank B-DatasetName
to O
better O
understand O
the O
empirical O
gains O
coming O
from O
each O
of O
these O
components O
. O

First O
, O
we O
compare O
the O
full O
model O
with O
the O
stepwise O
prover O
without O
search O
( O
Sec O
. O
4.1 O
) O
. O
Results O
in O
Table O
5 O
show O
that O
the O
full O
model O
significantly O
improves O
upon O
this O
stepwise O
baseline O
across O
the O
board O
, O
demonstrating O
the O
benefits O
of O
searching O
for O
proofs O
at O
inference O
time O
. O

Note O
that O
the O
stepwise O
baseline O
without O
search O
also O
performs O
significantly O
better O
than O
EntailmentWriter B-MethodName
( O
31.8 B-MetricValue
% I-MetricValue
vs. O
20.9 B-MetricValue
% I-MetricValue
in O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
) O
. O
We O
ask O
how O
much O
of O
the O
improvement O
is O
due O
to O
stepwise O
proof B-TaskName
generation I-TaskName
as O
opposed O
to O
implementation O
details O
and O
hyperparameters O
. O
8 O
. O
In O
. O
Models O
are O
trained O
and O
tested O
on O
the O
D0 O
- O
D3 O
subset O
. O
Methods O
with O
† O
are O
reported O
by O
Sanyal O
et O
al O
. O
( O
2022 O
) O
. O
All O
methods O
are O
based O
on O
T5 B-MethodName
- I-MethodName
large I-MethodName
. O
The O
' O
All O
' O
columns O
are O
accuracies O
on O
the O
entire O
testing O
set O
. O
' O
0 O
' O
, O
' O
1 O
' O
, O
' O
2 O
' O
, O
and O
' O
3 O
' O
are O
accuracies O
broken O
down O
by O
the O
length O
of O
testing O
proofs O
. O
' O
N O
/ O
A O
' O
includes O
testing O
examples O
without O
ground O
truth O
proofs O
since O
they O
can O
be O
neither O
proved O
nor O
disproved O
. O
5 O
, O
we O
experiment O
with O
two O
additional O
versions O
of O
NLProofS B-MethodName
: O
one O
without O
the O
prover O
score O
and O
the O
other O
without O
the O
verifier O
score O
. O
Results O
show O
that O
they O
fail O
to O
improve O
upon O
the O
stepwise O
baseline O
without O
search O
, O
demonstrating O
the O
necessity O
of O
combining O
the O
verifier O
and O
the O
prover O
. O

Table O
5 O
also O
includes O
different O
methods O
' O
average O
inference O
time O
per O
test O
example O
, O
measured O
with O
batch B-HyperparameterName
size I-HyperparameterName
1 B-HyperparameterValue
and O
beam B-HyperparameterName
width I-HyperparameterName
10 B-HyperparameterValue
. O
NLProofS B-MethodName
takes O
4.4 O
seconds O
to O
process O
a O
test O
example O
, O
which O
is O
2x O
slower O
than O
the O
stepwise O
baseline O
. O
Longer O
run O
time O
is O
a O
natural O
consequence O
of O
proof O
search O
, O
and O
2x O
is O
a O
modest O
slow O
down O
. O

RuleTaker B-DatasetName
. O
We O
perform O
similar O
ablation O
experiments O
also O
on O
RuleTaker B-DatasetName
. O
Results O
in O
Table O
6 O
show O
similar O
patterns O
as O
the O
ablations O
on O
EntailmentBank B-DatasetName
. O
However O
, O
the O
main O
difference O
is O
that O
proof O
search O
leads O
to O
a O
much O
larger O
improvement O
on O
RuleTaker B-DatasetName
, O
and O
the O
two O
baselines O
without O
search O
perform O
much O
lower O
than O
prior O
methods O
( O
ProofWriter B-MethodName
in O
Table O
4 O
) O
. O
This O
is O
due O
to O
how O
we O
adapt O
proof O
generation O
systems O
to O
the O
task O
of O
RuleTaker B-DatasetName
. O

As O
described O
in O
Sec O
. O
5.3 O
, O
the O
answer O
is O
produced O
by O
a O
linear O
classifier O
over O
proof O
scores O
of O
the O
hypothesis O
h O
and O
its O
negation O
¬h O
. O
To O
perform O
well O
, O
we O
need O
the O
proof O
generation O
system O
to O
( O
1 O
) O
assign O
high O
scores O
to O
valid O
hypotheses O
and O
( O
2 O
) O
assign O
low O
scores O
to O
invalid O
hypotheses O
. O
However O
, O
proof B-TaskName
generation I-TaskName
systems O
without O
tWriter O
since O
the O
training O
code O
has O
not O
been O
released O
. O
verifiers O
- O
such O
as O
the O
two O
baselines O
- O
have O
never O
seen O
invalid O
proof O
steps O
in O
training O
. O
They O
are O
good O
at O
( O
1 O
) O
but O
not O
necessarily O
( O
2 O
) O
; O
this O
is O
sufficient O
for O
Entailment B-DatasetName
- I-DatasetName
Bank I-DatasetName
( O
with O
only O
valid O
hypotheses O
) O
but O
not O
RuleTaker B-DatasetName
. O
In O
contrast O
, O
proof O
generation O
systems O
with O
verifierssuch O
as O
our O
full O
model O
- O
are O
good O
at O
both O
( O
1 O
) O
and O
( O
2 O
) O
. O
In O
other O
words O
, O
NLProofS B-MethodName
can O
generate O
more O
accurate O
proof O
scores O
for O
both O
valid O
and O
invalid O
hypotheses O
. O

In O
addition O
, O
Table O
6 O
shows O
that O
the O
verifier O
score O
alone O
is O
sufficient O
for O
RuleTaker B-DatasetName
; O
adding O
the O
prover O
score O
does O
not O
make O
much O
difference O
. O
This O
is O
because O
verifiers O
trained O
on O
RuleTaker B-DatasetName
are O
highly O
accurate O
, O
and O
they O
do O
not O
need O
to O
be O
supplemented O
by O
the O
prover O
. O

NLProofS B-MethodName
with O
Oracles O

The O
stepwise O
prover O
and O
the O
verifier O
are O
two O
major O
components O
in O
NLProofS. B-MethodName
To O
analyze O
which O
one O
is O
the O
bottleneck O
, O
we O
construct O
" O
oracle O
" O
versions O
of O
them O
, O
both O
of O
which O
have O
access O
to O
ground O
- O
truth O
information O
for O
better O
predictions O
. O
Given O
a O
partial O
proof O
, O
the O
oracle O
prover O
generates O
multiple O
potential O
proof O
steps O
just O
like O
a O
regular O
prover O
. O
But O
it O
additionally O
includes O
all O
ground O
truth O
steps O
that O
are O
valid O
for O
the O
partial O
proof O
, O
i.e. O
, O
steps O
whose O
premises O
have O
been O
satisfied O
by O
the O
partial O
proof O
. O
The O
oracle O
verifier O
also O
builds O
on O
a O
regular O
verifier O
but O
always O
assigns O
the O
highest O
score O
( O
1.0 O
) O
to O
proof O
steps O
in O
the O
ground O
truth O
. O
Note O
that O
we O
call O
them O
" O
oracles O
" O
, O
but O
neither O
of O
them O
is O
perfect O
. O
For O
example O
, O
the O
oracle O
verifier O
can O
not O
reliably O
tell O
whether O
a O
proof O
step O
is O
valid O
if O
the O
step O
deviates O
even O
slightly O
from O
the O
ground O
truth O
. O

Table O
7 O
shows O
the O
validation O
results O
on O
Task O
2 O
of O
En B-DatasetName
- I-DatasetName
tailmentBank I-DatasetName
. O
The O
oracle O
prover O
alone O
improves O
the O
performance O
significantly O
( O
e.g. O
, O
a O
boost O
of O
29.2 B-MetricValue
in O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
) O
, O
demonstrating O
that O
the O
prover O
is O
a O
major O
bottleneck O
. O
In O
contrast O
, O
the O
oracle O
verifier O
alone O
does O
not O
help O
much O
, O
improving O
only O
0.8 B-MetricValue
in O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
. O
NLProofS B-MethodName
( I-MethodName
no I-MethodName
oracle I-MethodName
) I-MethodName
89.4 B-MetricValue
± I-MetricValue
0.8 I-MetricValue
56.0 B-MetricValue
± I-MetricValue
0.7 I-MetricValue
50.4 B-MetricValue
± I-MetricValue
1.9 I-MetricValue
38.4 B-MetricValue
± I-MetricValue
1.3 I-MetricValue
71.9 B-MetricValue
± I-MetricValue
1.4 I-MetricValue
41.3 B-MetricValue
± I-MetricValue
1.4 I-MetricValue
37.1 O
± O
1.5 O
oracle B-MethodName
verifier I-MethodName
90.0 O
± O
0.5 O
56.9 O
± O
1.6 O
51.3 O
± O
2.2 O
39.1 O
± O
2.1 O
72.9 O
± O
1.5 O
42.0 O
± O
2.1 O
37.9 O
± O
2.2 O
oracle B-MethodName
prover I-MethodName
94.6 O
± O
0.3 O
76.2 O
± O
1.9 O
75.8 O
± O
0.9 O
67.0 O
± O
1.8 O
85.3 O
± O
0.5 O
67.5 O
± O
1.6 O
66.3 O
± O
1.9 O
oracle B-MethodName
prover I-MethodName
+ I-MethodName
verifier I-MethodName
95.7 O
± O
0.7 O
82.7 O
± O
2.7 O
83.0 O
± O
3.1 O
75.4 O
± O
3.9 O
88.1 O
± O
1.8 O
75.7 O
± O
4.1 O
75.2 O
± O
3.8 O

Table O
7 O
: O
Validation O
results O
on O
EntailmentBank B-DatasetName
( O
Task O
2 O
) O
of O
replacing O
the O
prover O
/ O
verifier O
with O
oracles O
. O
However O
, O
0.8 B-MetricValue
might O
underestimate O
the O
importance O
of O
the O
verifier O
, O
as O
the O
oracle O
verifier O
is O
not O
useful O
if O
the O
prover O
fails O
to O
generate O
the O
ground O
truth O
proof O
step O
in O
the O
first O
place O
. O
Actually O
, O
adding O
the O
oracle O
verifier O
to O
the O
oracle O
prover O
improves O
Overall B-MetricName
- I-MetricName
AllCorrect I-MetricName
by O
8.9 B-MetricValue
, O
demonstrating O
that O
the O
verifier O
also O
bears O
room O
for O
improvement O
. O

Impact O
of O
Proof B-HyperparameterName
Length I-HyperparameterName

Prior O
work O
has O
demonstrated O
that O
proof B-TaskName
generation I-TaskName
models O
struggle O
with O
long O
proofs O
. O
In O
Fig O
. O
3 O
, O
we O
break O
down O
the O
test O
performance O
on O
Task O
2 O
of O
EntailmentBank B-DatasetName
by O
proof B-HyperparameterName
length I-HyperparameterName
, O
i.e. O
, O
the O
number O
of O
steps O
in O
the O
ground O
truth O
proof O
. O
Here O
we O
show O
only O
the O
Leaves B-MetricName
- I-MetricName
AllCorrect I-MetricName
metric O
. O
Leaves O
metrics O
are O
relatively O
robust O
against O
the O
issue O
of O
multiple O
valid O
proof O
trees O
per O
example O
. O
Appendix O
H O
include O
figures O
of O
other O
metrics O
. O
But O
they O
may O
exaggerate O
the O
difficulty O
with O
long O
proofs O
, O
as O
the O
issue O
of O
multiple O
valid O
proofs O
is O
particularly O
prominent O
for O
long O
proofs O
. O
Nevertheless O
, O
we O
still O
see O
a O
significant O
performance O
drop O
in O
Fig O
. O
3 O
when O
the O
proof B-HyperparameterName
length I-HyperparameterName
exceeds O
1 B-HyperparameterValue
- O
2 B-HyperparameterValue
, O
suggesting O
that O
generating O
long O
proofs O
remains O
a O
challenge O
. O
However O
, O
we O
also O
see O
the O
benefits O
of O
proof O
search O
since O
its O
improvements O
over O
the O
stepwise O
baseline O
are O
more O
evident O
for O
long O
proofs O
. O

In O
addition O
, O
NLProofS B-MethodName
tends O
to O
generate O
longer O
proofs O
compared O
to O
the O
baselines O
. O
On O
the O
validation O
set O
of O
Task O
2 O
, O
the O
ground O
truth O
proofs O
have O
an O
average O
length B-HyperparameterName
of O
3.2 B-HyperparameterValue
steps O
, O
whereas O
the O
average O
lengths O
of O
the O
generated O
proofs O
are O
2.6 O
, O
2.7 O
, O
and O
2.9 O
for O
the O
single O
- O
shot O
baseline O
, O
the O
stepwise O
baseline O
, O
and O
NLProofS B-MethodName
. O

Reduced O
Hallucination O

The O
verifier O
in O
NLProofS B-MethodName
aims O
to O
prevent O
the O
model O
from O
hallucinating O
invalid O
proof O
steps O
. O
However O
, O
it O
is O
difficult O
to O
evaluate O
hallucination O
automatically O
: O
when O
the O
model O
generation O
deviates O
from O
ground O
truth O
, O
it O
is O
difficult O
to O
evaluate O
whether O
it O
is O
a O
valid O
proof O
step O
. O
Therefore O
, O
besides O
qualitative O
examples O
in O
Table O
3 O
, O
we O
also O
perform O
a O
human O
evaluation O
similar O
to O
Bostrom O
et O
al O
. O
( O
2022 O
) O
. O

We O
compare O
three O
models O
: O
EntailmentWriter B-MethodName
, O
NL B-MethodName
- I-MethodName
ProofS I-MethodName
w O
/ O
o O
search O
( O
our O
model O
without O
the O
verifierguided O
search O
) O
, O
and O
NLProofS B-MethodName
( O
our O
full O
model O
) O
. O
For O
each O
model O
, O
we O
sample O
100 B-HyperparameterValue
generated O
proof O
steps O
and O
manually O
annotate O
them O
as O
valid O
/ O
invalid O
. O
The O
percentage O
of O
valid O
steps O
is O
43 O
% O
, O
65 O
% O
, O
and O
77 O
% O
, O
demonstrating O
the O
effectiveness O
of O
NLProofS B-MethodName
in O
mitigating O
hallucination O
. O

Conclusion O

We O
have O
introduced O
NLProofS B-MethodName
for O
stepwise O
proof O
generation O
in O
natural O
language O
. O
It O
learns O
to O
generate O
relevant O
proof O
steps O
conditioning O
on O
the O
hypothesis O
. O
To O
prevent O
hallucination O
, O
NLProofS B-MethodName
searches O
for O
proofs O
that O
maximize O
a O
validity O
score O
judged O
by O
a O
verifier O
. O
Our O
method O
has O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
En B-DatasetName
- I-DatasetName
tailmentBank I-DatasetName
and O
RuleTaker B-DatasetName
, O
demonstrating O
the O
promise O
of O
stepwise O
proof O
generation O
for O
human O
- O
authored O
proofs O
. O
In O
the O
future O
, O
we O
hope O
to O
see O
increasing O
applications O
of O
verifiers O
and O
proof O
search O
in O
various O
reasoning O
tasks O
. O

Limitations O

Despite O
the O
strong O
performance O
on O
two O
benchmarks O
, O
our O
method O
still O
has O
substantial O
room O
for O
future O
improvement O
. O
Currently O
, O
the O
prover O
( O
Sec O
. O
4.1 O
) O
uses O
beam O
search O
as O
the O
decoding O
algorithm O
, O
which O
has O
two O
problems O
: O
First O
, O
it O
generates O
equivalent O
proof O
steps O
such O
as O
" O
sent1 O
& O
sent2 O
- O
> O
hypothesis O
" O
and O
" O
sent2 O
& O
sent1 O
- O
> O
hypothesis O
" O
. O
It O
would O
be O
more O
efficient O
if O
we O
make O
the O
decoding O
invariant O
to O
the O
permutation O
of O
premises O
. O
Second O
, O
the O
generated O
proof O
steps O
lack O
diversity O
. O
Since O
the O
verifier O
can O
filter O
out O
invalid O
proof O
steps O
, O
it O
is O
more O
important O
for O
the O
prover O
to O
have O
coverage O
and O
diversity O
than O
precision O
. O
It O
would O
be O
interesting O
to O
try O
more O
advanced O
decoding O
algorithms O
such O
as O
Diverse O
Beam O
Search O
( O
Vijayakumar O
et O
al O
. O
, O
2018 O
) O
. O
Like O
prior O
work O
, O
our O
prover O
concatenates O
all O
supporting O
facts O
into O
a O
long O
text O
sequence O
and O
applies O
a O
Transformer O
encoder O
to O
it O
. O
This O
could O
be O
an O
inefficient O
use O
of O
computation O
and O
may O
have O
problems O
scaling O
to O
longer O
sentences O
or O
a O
larger O
number O
of O
supporting O
facts O
. O
Solutions O
like O
Fusion O
- O
in O
- O
Decoder O
( O
Izacard O
and O
Grave O
, O
2021 O
) O
may O
help O
solve O
this O
problem O
. O

Ethical O
Considerations O

Machine O
learning O
and O
NLP O
are O
moving O
from O
lab O
curiosity O
into O
real O
- O
world O
systems O
that O
make O
critical O
decisions O
in O
areas O
such O
as O
hiring O
, O
loan O
approval O
, O
and O
college O
admission O
. O
It O
is O
imperative O
that O
these O
decisions O
are O
interpretable O
to O
humans O
. O
Proof B-TaskName
generation I-TaskName
enhances O
interpretability O
by O
requiring O
the O
model O
to O
produce O
not O
only O
the O
final O
decision O
but O
also O
an O
explicit O
proof O
. O
However O
, O
the O
interpretability O
is O
jeopardized O
if O
the O
model O
learns O
to O
hallucinate O
invalid O
proof O
steps O
, O
like O
a O
person O
trying O
to O
find O
unfaithful O
excuses O
to O
justify O
a O
decision O
. O
Our O
method O
uses O
an O
independently O
trained O
verifier O
to O
check O
the O
validity O
of O
proof O
steps O
, O
which O
effectively O
reduces O
hallucination O
and O
enables O
the O
generated O
proof O
to O
explain O
the O
decision O
more O
faithfully O
. O

C O
Pseudo O
- O
negative O
Examples O
for O

Training O
the O
Verifier O

As O
mentioned O
in O
Sec O
. O
4.2 O
, O
the O
negative O
examples O
used O
for O
training O
the O
verifier O
are O
constructed O
automatically O
using O
the O
procedure O
below O
: O

• O
As O
in O
Fig O
. O

A O
, O
for O
each O
positive O
example O
consisting O
of O
premises O
and O
a O
conclusion O
, O
we O
either O
remove O
some O
premises O
or O
replacing O
one O
premise O
with O
a O
distractor O
retrieved O
from O
C O
using O
BM25 B-MetricName
( O
Robertson O
et O
al O
. O
, O
2009 O
) O
. O

• O
For O
EntailmentBank B-DatasetName
, O
as O
in O
Fig O
. O
B O
, O
we O
generate O
additional O
pseudo O
- O
negatives O
by O
copying O
one O
of O
the O
premises O
as O
the O
conclusion O
. O

• O
For O
RuleTaker B-DatasetName
, O
as O
in O
Fig O
. O
C O
, O
we O
generate O
additional O
pseudo O
- O
negatives O
by O
negating O
the O
conclusion O
. O

D O
Proof O
Graphs O
are O
Loopless O

We O
prove O
that O
the O
proof O
graph O
in O
Algorithm O
1 O
is O
loopless O
. O
Intuitively O
, O
for O
a O
proof O
graph O
( O
C O
, O
I O
, O
S O
, O
h O
) O
, O
as O
we O
traverse O
along O
any O
path O
, O
the O
node O
scores O
in O
C O
∪ O
I O
∪ O
{ O
h O
} O
are O
non O
- O
increasing O
due O
to O
Eqn O
. O
1 O
, O
which O
prevents O
loops O
. O
Theorem O
D.3 O
. O
In O
Algorithm O
1 O
, O
if O
the O
proof O
graph O
is O
loopless O
after O
initialization O
, O
then O
it O
will O
remain O
loopless O
. O

Proof O
. O
We O
just O
need O
to O
prove O
that O
it O
is O
impossible O
to O
introduce O
a O
loop O
during O
any O
iteration O
in O
Algorithm O
1 O
. O
We O
prove O
it O
by O
contradiction O
, O
assuming O
we O
could O
introduce O
a O
loop O
in O
an O
iteration O
, O
as O
in O

v O
→ O
• O
• O
• O
→ O
u O
→ O
s O
→ O
v O
. O

Apply O
Lemma O
D.2 O
to O
the O
path O
from O
v O
to O
u O
, O
and O
we O
have O
scr O
n O
( O
u O
) O
≤ O
scr O
n O
( O
v O
) O
before O
introducing O
the O
loop O
. O

Method O

Leaves O

Steps B-MetricName
Intermediates B-MetricName
Overall B-MetricName
F1 I-MetricName
AllCorrect B-MetricName
F1 I-MetricName
AllCorrect B-MetricName
F1 I-MetricName
AllCorrect B-MetricName
AllCorrect B-MetricName
Our O
format O
86.9 O
± O
0.6 O
45.6 O
± O
1.5 O
42.6 O
± O
1.6 O
29.7 O
± O
1.3 O
64.6 O
± O
1.4 O
32.2 O
± O
2.1 O
27.1 O
± O
1.5 O
EntailmentWriter B-MethodName
format O
87.6 O
± O
0.5 O
47.1 O
± O
2.1 O
42.2 O
± O
1.1 O
29.7 O
± O
1.6 O
64.5 O
± O
0.6 O
32.3 O
± O
1.7 O
27.5 O
± O
1.8 O
( O
Task O
2 O
) O
with O
different O
input O
formats O
. O
All O
methods O
are O
based O
on O
T5 O
- O
large O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

the O
sun O
rising O
/ O
setting O
occurs O
once O
per O
day O
the O
sun O
rising O
is O
a O
kind O
of O
event O
the O
sun O
rising O
and O
setting O
is O
the O
event O
that O
occurs O
once O
per O
day O
the O
sun O
setting O
is O
a O
kind O
of O
event O
the O
sun O
rising O
/ O
setting O
occurs O
once O
per O
day O
the O
sun O
rising O
is O
a O
kind O
of O
event O
the O
sun O
rising O
and O
setting O
is O
the O
event O
that O
occurs O
once O
per O
day O
the O
sun O
rising O
/ O
setting O
occurs O
once O
per O
day O
rising O
means O
moving O
upward O
the O
sun O
rising O
and O
setting O
is O
the O
event O
that O
occurs O
once O
per O
day O
the O
sun O
setting O
is O
a O
kind O
of O
event O
Remember O
how O
the O
step O
s O
is O
executed O
( O
Sec O
. O
4.3 O
) O
, O
the O
tentative O
score O
scr O
n O
( O
v O
) O
= O
min O
scr O
s O
( O
s O
) O
, O
scr O
n O
( O
u O
) O
, O
. O
. O
. O
≤ O
scr O
n O
( O
u O
) O
≤ O
scr O
n O
( O
v O
) O
. O
The O
tentative O
score O
is O
not O
greater O
than O
the O
original O
score O
of O
v. O
So O
the O
step O
is O
a O
no O
- O
op O
that O
should O
not O
be O
executed O
. O
Therefore O
, O
it O
is O
impossible O
to O
introduce O
loops O
. O

E O
Procedure O
for O
Sampling O
Partial O
Proofs O

The O
sample_new O
function O
in O
Algorithm O
1 O
samples O
a O
partial O
proof O
tree O
from O
the O
proof O
graph O
. O
First O
, O
the O
graph O
is O
a O
DAG O
, O
so O
we O
can O
visit O
nodes O
in O
the O
order O
of O
a O
topological O
sort O
- O
successors O
before O
predecessors O
. O
Second O
, O
when O
visiting O
a O
node O
, O
if O
it O
is O
not O
already O
a O
part O
of O
the O
partial O
proof O
, O
we O
add O
it O
with O
a O
probability O
of O
0.5 O
. O
Third O
, O
whenever O
we O
add O
a O
node O
, O
we O
also O
add O
all O
of O
its O
predecessors O
. O
This O
ensures O
the O
result O
is O
a O
valid O
partial O
proof O
. O

F O
Training O
and O
inference O
details O

We O
use O
T5 B-MethodName
- I-MethodName
large I-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
for O
the O
prover O
and O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
for O
the O
verifier O
. O
All O
experiments O
are O
run O
on O
machines O
with O
2 O
CPUs O
, O
16 O
GB O
memory O
, O
and O
one O
NVIDIA O
A6000 O
GPU O
. O
Models O
are O
optimized O
using O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
. O
The O
learning B-HyperparameterName
rate I-HyperparameterName
warms O
up O
linearly O
from O
0 B-HyperparameterValue
to O
a O
maximum O
value O
and O
then O
decays O
following O
the O
cosine O
schedule O
. O
Hyperparameters O
are O
tuned O
on O
the O
validation O
data O
separately O
for O
each O
task O
/ O
method O
. O
We O
report O
test O
results O
of O
models O
trained O
on O
the O
training O
set O
alone O
, O
excluding O
the O
validation O
set O
. O
We O
report O
the O
average O
performance O
and O
the O
standard O
deviation O
for O
5 B-HyperparameterValue
independent O
runs B-HyperparameterName
. O
Our O
results O
on O
EntailmentBank B-DatasetName
are O
produced O
by O
the O
official O
evaluation O
code O
. O
10 O
The O
code O
had O
a O
bug O
fix O
in O
May O
2022 O
that O
impacted O
the O
Intermediate B-MetricName
- I-MetricName
AllCorrect I-MetricName
metric O
of O
methods O
evaluated O
earlier O
, O
including O
IRGR O
( O
Ribeiro O
et O
al O
. O
, O
2022 O
) O
and O
arXiv O
versions O
v1 O
, O
v2 O
of O
Entailmen B-MethodName
- I-MethodName
tWriter I-MethodName
. O
We O
evaluate O
NLProofS B-MethodName
using O
the O
evaluation O
code O
after O
the O
bug O
fix O
. O
And O
we O
report O
the O
EntailmentBank B-DatasetName
numbers O
based O
on O
their O
fixed O
arXiv O
version O
v3 O
that O
was O
released O
on O
May O
28 O
, O
2022 O
. O
The O
numbers O
in O
the O
IRGR O
paper O
have O
not O
been O
updated O
yet O
, O
so O
we O
report O
its O
Intermediates B-MetricName
- I-MetricName
AllCorrect I-MetricName
metric O
based O
on O
private O
correspondence O
with O
the O
authors O
. O

Method O

Leaves B-MetricName

Steps O
We O
also O
evaluate O
on O
distinguishing O
valid O
/ O
invalid O
hypotheses O
introduced O
by O
Bostrom O
et O
al O
. O
( O
2022 O
) O
. O
In O
this O
task O
, O
the O
model O
is O
given O
a O
hypothesis O
h O
and O
supporting O
facts O
C. O
But O
unlike O
in O
proof B-TaskName
generation I-TaskName
, O
here O
h O
can O
be O
either O
valid O
or O
invalid O
w.r O
. O
constructed O
by O
pairing O
the O
supporting O
facts O
in O
one O
example O
with O
the O
hypothesis O
in O
another O
random O
example O
. O

NLProofS B-MethodName
is O
developed O
for O
proof B-TaskName
generation I-TaskName
, O
and O
it O
has O
seen O
only O
valid O
hypotheses O
in O
training O
. O
So O
we O
follow O
Bostrom O
et O
al O
. O
( O
2022 O
) O
to O
adapt O
proof B-TaskName
generation I-TaskName
systems O
to O
this O
new O
task O
: O
( O
1 O
) O
Train O
the O
system O
to O
generate O
proofs O
for O
valid O
hypotheses O
. O
( O
2 O
) O
Apply O
the O
system O
to O
generate O
proof O
scores O
for O
both O
valid O
and O
valid O
hypotheses O
. O
( O
3 O
) O
Train O
a O
linear O
classifier O
on O
top O
of O
the O
scores O
to O
predict O
the O
validity O
of O
hypotheses O
. O
It O
requires O
the O
system O
to O
be O
able O
to O
produce O
proof O
scores O
. O
For O
our O
method O
, O
we O
use O
scr O
n O
( O
h O
) O
defined O
in O
Eqn O
1 O
as O
the O
proof O
score O
. O

Results O
in O
Table O
D O
show O
that O
our O
method O
compares O
favorably O
with O
SCSearch B-MethodName
, O
whereas O
EntailmentWriter B-MethodName
falls O
behind O
. O
The O
results O
suggest O
that O
proof O
scores O
generated O
by O
us O
are O
more O
well O
- O
calibrated O
: O
they O
are O
high O
for O
valid O
hypotheses O
and O
low O
for O
invalid O
ones O
. O
This O
is O
largely O
attributed O
to O
our O
verifier O
, O
which O
prevents O
the O
model O
from O
hallucinating O
invalid O
proofs O
with O
confidence O
. O

However O
, O
results O
on O
this O
task O
should O
be O
interpreted O
with O
caution O
. O
First O
, O
they O
do O
not O
reflect O
the O
performance O
on O
proof B-TaskName
generation I-TaskName
, O
and O
SCSearch B-MethodName
has O
not O
been O
evaluated O
on O
proof B-TaskName
generation I-TaskName
. O
Second O
, O
none O
of O
the O
methods O
are O
explicitly O
optimized O
for O
this O
task O
. O
They O
see O
only O
valid O
hypotheses O
during O
training O
but O
are O
asked O
to O
distinguish O
valid O
/ O
invalid O
hypotheses O
during O
inference O
. O
Third O
, O
the O
particular O
dataset O
constructed O
by O
Bostrom O
et O
al O
. O
( O
2022 O
) O
is O
too O
easy O
. O
An O
invalid O
hypothesis O
has O
very O
little O
lexical O
overlap O
with O
the O
supporting O
facts O
, O
which O
can O
be O
used O
as O
a O
cue O
for O
classifying O
hypotheses O
accurately O
. O
As O
a O
result O
, O
a O
simple O
RoBERTa B-MethodName
baseline O
directly O
optimized O
for O
classifying O
the O
hypothesis O
can O
solve O
this O
task O
to O
almost O
100 O
% O
. O

H O
Additional O
Experimental O
Results O

Validation O
results O
. O
Table O
B O
shows O
our O
proof O
generation O
results O
on O
the O
validation O
set O
of O
EntailmentBank B-DatasetName
( O
Task O
2 O
) O
, O
corresponding O
to O
Table O
2 O
. O
Table O
C O
shows O
the O
validation O
results O
on O
RuleTaker B-DatasetName
( O
OWA O
) O
, O
corresponding O
to O
Table O
4 O
. O
Few O
- O
shot O
prompting O
with O
GPT-3 B-MethodName
or O
Codex B-MethodName
. O
We O
investigate O
whether O
proof O
generation O
can O
be O
solved O
out O
of O
the O
box O
by O
prompting O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
or O
Codex B-MethodName
( O
Chen O
et O
al O
. O
, O
2021 O
) O
with O
few O
- O
shot O
examples O
. O
Fig O
. O
K O
shows O
an O
example O
prompt O
consisting O
of O
7 O
incontext O
examples O
randomly O
sampled O
from O
the O
training O
set O
of O
EntailmentBank B-DatasetName
( O
Task O
2 O
) O
, O
as O
well O
as O
a O
validation O
example O
for O
which O
we O
want O
to O
make O
predictions O
. O

Table O
B O
includes O
the O
results O
on O
the O
full O
validation O
set O
. O
They O
were O
obtained O
on O
October O
20 O
, O
2022 O
using O
the O
model O
text O
- O
davinci-002 O
for O
GPT-3 B-MethodName
and O
code O
- O
davinci-002 O
for O
Codex B-MethodName
. O
We O
report O
the O
mean O
and O
standard O
deviation O
from O
3 O
independent O
runes O
with O
different O
in O
- O
context O
examples O
in O
the O
prompt O
. O
GPT-3 B-MethodName
and O
Codex B-MethodName
perform O
substantially O
worse O
than O
other O
methods O
, O
demonstrating O
that O
we O
can O
not O
easily O
solve O
proof O
generation O
through O
few O
- O
shot O
prompting O
. O
In O
addition O
, O
Codex B-MethodName
performs O
better O
than O
GPT-3 B-MethodName
, O
which O
is O
consistent O
with O
the O
observations O
in O
Madaan O
et O
al O
. O
( O
2022 O
) O
though O
we O
do O
not O
format O
the O
output O
as O
Python O
programs O
. O

Acknowledgements O

This O
work O
is O
partially O
supported O
by O
the O
Office O
of O
Naval O
Research O
under O
Grant O
N00014 O
- O
20 O
- O
1 O
- O
2634 O
. O
We O
gratefully O
acknowledge O
financial O
support O
from O
the O
Schmidt O
DataX O
Fund O
at O
Princeton O
University O
made O
possible O
through O
a O
major O
gift O
from O
the O
Schmidt O
Futures O
Foundation O
. O
We O
also O
thank O
Darby O
Haller O
, O
Jane O
Pan O
, O
Shunyu O
Yao O
, O
and O
the O
members O
of O
the O
Princeton O
NLP O
group O
for O
helpful O
discussion O
and O
valuable O
feedback O
. O

A O
Evaluation O
Metrics O
on O
EntailmentBank B-DatasetName

We O
evaluate O
on O
EntailmentBank B-DatasetName
using O
their O
official O
evaluation O
metrics O
calculated O
by O
their O
evaluation O
code O
. O
Below O
is O
a O
summary O
; O
please O
refer O
to O
the O
EntailmentBank B-DatasetName
paper O
for O
further O
details O
. O

LetT O
be O
a O
generated O
proof O
tree O
, O
with O
T O
being O
the O
ground O
truth O
. O
First O
, O
nodes O
inT O
are O
aligned O
with O
nodes O
in O
T O
using O
a O
tree O
alignment O
algorithm O
based O
on O
the O
" O
sent O
* O
" O
labels O
. O
Once O
aligned O
, O
it O
is O
scored O
using O
4 O
types O
of O
metrics O
- O
Leaves B-MetricName
, O
Steps B-MetricName
, O
Intermediates B-MetricName
, O
and O
Overall B-MetricName
. O

• O
Leaves B-MetricName
( O
F1 B-MetricName
, O
AllCorrect B-MetricName
) O
: O
The O
Leaves B-MetricName
metrics O
compare O
the O
leaf O
nodes O
ofT O
and O
T O
to O
calculate O
an O
F1 B-MetricName
score I-MetricName
and O
an O
" O
AllCorrect B-MetricName
" O
score O
, O
which O
means O
all O
predicted O
nodes O
are O
correct O
. O
In O
other O
words O
, O
AllCorrect B-MetricName
= O
1 B-MetricValue
if O
F1 B-MetricName
= O
1 B-MetricValue
, O
and O
AllCorrect B-MetricName
= O
0 B-MetricValue
if O
F1 B-MetricName
< O
1 B-MetricValue
. O
• O
Steps O
( O
F1 B-MetricName
, O
AllCorrect B-MetricName
) O
: O
The O
Steps B-MetricName
metrics O
measure O
whether O
predicted O
proof O
steps O
are O
structurally O
correct O
. O

A O
predicted O
step O
corresponds O
to O
an O
internal O
node O
u O
∈ O
T O
( O
aligned O
to O
v O
∈ O
T O
) O
. O
It O
is O
structurally O
correct O
if O
the O
children O
of O
u O
and O
v O
are O
also O
perfectly O
aligned O
. O
Since O
there O
are O
multiple O
steps O
inT O
and O
T O
, O
we O
can O
calculate O
F1 B-MetricName
and O
AllCorrect B-MetricName
. O
• O
Intermediates B-MetricName
( O
F1 B-MetricName
, O
AllCorrect B-MetricName
) O
: O
An O
intermediate O
conclusion O
u O
∈T O
( O
aligned O
to O
v O
∈ O
T O
) O
is O
correct O
if O
the O
BLEURT B-MetricName
( O
Sellam O
et O
al O
. O
, O
2020 O
) O
9 O
score O
between O
u O
and O
v O
is O
greater O
than O
0.28 B-MetricValue
. O
We O
calculate O
F1 B-MetricName
and O
AllCorrect B-MetricName
from O
all O
intermediate O
conclusions O
inT O
and O
T O
. O
• O
Overall B-MetricName
( O
AllCorrect B-MetricName
) O
: O
The O
Overall B-MetricName
metric O
evaluates O
whether O
the O
leaves B-MetricName
, O
steps B-MetricName
, O
and O
intermediates B-MetricName
are O
all O
correct O
, O
i.e. O
, O
AllCorrect B-MetricName
= O
1 B-MetricValue
if O
and O
only O
ifT O
matches O
completely O
with O
T O
. O

B O
Different O
Input O
Formats O

We O
use O
a O
slightly O
different O
input O
format O
from O
Entailmen B-MethodName
- I-MethodName
tWriter I-MethodName
, O
as O
their O
format O
had O
not O
been O
released O
when O
we O
developed O
our O
method O
. O
Consider O
the O
example O
in O
Fig O
. O
1 O
. O
The O
input O
to O
our O
single O
- O
shot O
baseline O
( O
NLProofS B-MethodName
w I-MethodName
/ I-MethodName
o I-MethodName
search I-MethodName
w I-MethodName
/ I-MethodName
o I-MethodName
stepwise I-MethodName
in O
Table O
5 O
) O
is O
" O
$ O
hypothesis O
$ O
= O
solar O
is O
a O
kind O
of O
renewable O
energy O
for O
heating O
homes O
; O
$ O
context O
$ O
= O
sent1 O
: O
homes O
are O
buildings O
sent2 O
: O
solar O
is O
renewable O
. O
. O
. O
; O
" O
, O
whereas O
the O
their O
input O
is O
" O
$ O
proof O
$ O
; O
$ O
question O
$ O
= O
As O
a O
kind O
of O
renewable O
energy O
, O
what O
can O
solar O
be O
used O
for O
? O
; O
$ O
answer O
$ O
= O
heating O
homes O
; O
$ O
hypothesis O
$ O
= O
solar O
is O
a O
kind O
of O
renewable O
energy O
for O
heating O
homes O
; O
$ O
context O
$ O
= O
sent1 O
: O
homes O
are O
buildings O
sent2 O
: O
solar O
is O
renewable O
. O
. O
. O
; O
" O
, O
which O
includes O
more O
information O
( O
$ O
question O
$ O
and O
$ O
answer O
$ O
) O
than O
ours O
. O

We O
experiment O
with O
single O
- O
shot O
methods O
implemented O
in O
our O
codebase O
using O
their O
input O
format O
. O
Results O
in O
Table O
A O
indicate O
no O
significant O
difference O
. O

Hypothesis O
: O
if O
a O
fossil O
of O
a O
bird O
can O
not O
be O
identified O
then O
that O
kind O
of O
bird O
is O
probably O
extinct O
Context O
: O
sent1 O
: O
identifying O
is O
similar O
to O
determining O
sent2 O
: O
if O
a O
fossil O
is O
of O
an O
organism O
that O
can O
not O
be O
identified O
then O
that O
organism O
is O
probably O
extinct O
... O
sent25 O
: O
fossils O
can O
be O
used O
to O
study O
the O
history O
of O
organisms O
and O
environments O
on O
earth O
Proof O
: O
sent13 O
& O
sent24 O
- O
> O
int1 O
: O
a O
bird O
is O
a O
kind O
of O
organism O
; O
int1 O
& O
sent2 O
- O
> O
hypothesis O
; O

Hypothesis O
: O
an O
animal O
requires O
water O
and O
air O
and O
food O
for O
survival O
Context O
: O
sent1 O
: O
breathing O
in O
is O
when O
animals O
inhale O
air O
into O
their O
lungs O
sent2 O
: O
animals O
/ O
living O
things O
require O
water O
for O
survival O
... O
sent25 O
: O
the O
amount O
of O
something O
is O
similar O
to O
the O
availability O
of O
something O
Proof O
: O
sent12 O
& O
sent8 O
- O
> O
int1 O
: O
an O
animal O
requires O
air O
for O
survival O
; O
int1 O
& O
sent2 O
- O
> O
int2 O
: O
an O
animal O
requires O
water O
and O
air O
for O
suvival O
; O
sent13 O
& O
sent21 O
- O
> O
int3 O
: O
an O
animal O
requires O
food O
for O
survival O
; O
int2 O
& O
int3 O
- O
> O
hypothesis O
; O

Hypothesis O
: O
stars O
that O
are O
blue O
in O
color O
are O
hottest O
in O
temperature O
Context O
: O
sent1 O
: O
a O
hot O
substance O
is O
a O
source O
of O
heat O
sent2 O
: O
the O
surface O
of O
the O
sun O
is O
extremely O
hot O
in O
temperature O
with O
values O
as O
high O
as O
20 O
000 O
000 O
c O
... O
sent25 O
: O
surface O
type O
is O
a O
kind O
of O
characteristic O
Proof O
: O
sent19 O
& O
sent5 O
- O
> O
hypothesis O
; O

Hypothesis O
: O
as O
mileage O
per O
gallon O
of O
oil O
increases O
, O
the O
amount O
of O
time O
that O
oil O
is O
available O
will O
be O
extended O
Context O
: O
sent1 O
: O
performing O
a O
task O
in O
less O
time O
/ O
more O
quickly O
/ O
faster O
has O
a O
positive O
impact O
on O
a O
person O
's O
life O
sent2 O
: O
a O
measure O
of O
time O
is O
a O
length O
of O
time O
... O
sent25 O
: O
to O
provide O
means O
to O
supply O
Proof O
: O
sent14 O
& O
sent6 O
- O
> O
int1 O
: O
gasoline O
is O
a O
kind O
of O
resource O
; O
int1 O
& O
sent12 O
- O
> O
int2 O
: O
as O
the O
use O
of O
gasoline O
decreases O
, O
the O
length O
of O
time O
that O
gasoline O
is O
available O
will O
increase O
; O
int2 O
& O
sent21 O
- O
> O
int3 O
: O
as O
mileage O
per O
gallon O
of O
gasoline O
increases O
, O
the O
length O
of O
time O
that O
gasoline O
is O
available O
will O
increase O
; O
int3 O
& O
sent8 O
- O
> O
int4 O
: O
as O
mileage O
per O
gallon O
of O
oil O
increases O
, O
the O
amount O
of O
time O
that O
oil O
is O
available O
will O
increase O
; O
int4 O
& O
sent3 O
- O
> O
hypothesis O
; O

Hypothesis O
: O
the O
firecracker O
stores O
chemical O
energy O
as O
its O
original O
energy O
Context O
: O
sent1 O
: O
if O
something O
emits O
something O
else O
then O
that O
something O
increases O
the O
amount O
of O
that O
something O
else O
sent2 O
: O
phase O
means O
state O
... O
sent25 O
: O
heat O
energy O
is O
synonymous O
with O
thermal O
energy O
Proof O
: O
sent11 O
& O
sent21 O
- O
> O
hypothesis O
; O
Hypothesis O
: O
humans O
throwing O
garbage O
into O
a O
stream O
causes O
harm O
to O
the O
stream O
Context O
: O
sent1 O
: O
absorbing O
something O
harmful O
has O
a O
negative O
impact O
on O
a O
thing O
sent2 O
: O
objects O
in O
an O
environment O
are O
a O
part O
of O
that O
environment O
... O
sent25 O
: O
waste O
must O
be O
removed O
Proof O
: O
sent12 O
& O
sent3 O
& O
sent8 O
- O
> O
int1 O
: O
humans O
throwing O
garbage O
in O
an O
environment O
causes O
harm O
to O
that O
environment O
; O
sent14 O
& O
sent7 O
- O
> O
int2 O
: O
a O
stream O
is O
a O
kind O
of O
environment O
; O
int1 O
& O
int2 O
- O
> O
hypothesis O
; O
Hypothesis O
: O
the O
sun O
will O
appear O
larger O
than O
other O
stars O
because O
it O
is O
the O
closest O
star O
to O
earth O
Context O
: O
sent1 O
: O
to O
move O
away O
means O
to O
increase O
distance O
sent2 O
: O
size O
is O
a O
property O
of O
objects O
and O
includes O
ordered O
values O
of O
microscopic O
/ O
tiny O
/ O
small O
/ O
medium O
/ O
large O
... O
sent25 O
: O
distance O
is O
a O
property O
of O
space O
and O
includes O
ordered O
values O
of O
close O
/ O
far O
Proof O
: O
sent22 O
& O
sent23 O
- O
> O
int1 O
: O
earth O
is O
a O
kind O
of O
celestial O
object O
; O
int1 O
& O
sent19 O
& O
sent3 O
- O
> O
int2 O
: O
as O
the O
distance O
from O
a O
star O
to O
earth O
decreases O
, O
the O
star O
will O
appear O
larger O
; O
int2 O
& O
sent15 O
- O
> O
hypothesis O
; O
Hypothesis O
: O
the O
sun O
rising O
and O
setting O
is O
the O
event O
that O
occurs O
once O
per O
day O
Context O
: O
sent1 O
: O
increasing O
is O
a O
kind O
of O
order O
sent2 O
: O
the O
sunlight O
occurs O
during O
the O
day O
... O
sent25 O
: O
all O
the O
time O
means O
at O
day O
and O
at O
night O
Proof O
: O
Improving O
the O
retriever O
. O
For O
Task O
3 O
of O
Entailment O
- O
Bank O
, O
all O
methods O
in O
Table O
2 O
use O
the O
same O
retrieved O
supporting O
facts O
in O
and O
focus O
solely O
on O
proof O
generation O
. O
An O
orthogonal O
direction O
is O
improving O
the O
retriever O
. O
IRGR O
( O
Ribeiro O
et O
al O
. O
, O
2022 O
) O
designs O
a O
multi O
- O
step O
retriever O
, O
which O
obtains O
significant O
improvements O
on O
Task O
3 O
( O
11.8 O
% O
on O
the O
Overall O
- O
AllCorrect O
metric O
) O
but O
worse O
results O
on O
Task O
1 O
and O
Task O
2 O
compared O
to O
the O
EntailmentWriter O
baseline O
. O
We O
do O
not O
compare O
with O
IRGR O
, O
since O
improving O
the O
retriever O
is O
orthogonal O
to O
our O
contributions O
. O

Introduction O

Related O
Work O

Method O

SKILL B-MethodName
: O
Structured B-MethodName
Knowledge I-MethodName
Infusion I-MethodName
for I-MethodName
Large I-MethodName
Language I-MethodName
Models I-MethodName

Large O
language O
models O
( O
LLMs O
) O
have O
demonstrated O
human O
- O
level O
performance O
on O
a O
vast O
spectrum O
of O
natural O
language O
tasks O
. O
However O
, O
it O
is O
largely O
unexplored O
whether O
they O
can O
better O
internalize O
knowledge O
from O
a O
structured O
data O
, O
such O
as O
a O
knowledge O
graph O
, O
or O
from O
text O
. O
In O
this O
work O
, O
we O
propose O
a O
method O
to O
infuse B-TaskName
structured I-TaskName
knowledge I-TaskName
into I-TaskName
LLMs I-TaskName
, O
by O
directly O
training O
T5 B-MethodName
models O
on O
factual O
triples O
of O
knowledge O
graphs O
( O
KGs O
) O
. O
We O
show O
that O
models O
pre O
- O
trained O
on O
Wikidata B-DatasetName
KG I-DatasetName
with O
our O
method O
outperform O
the O
T5 B-MethodName
baselines O
on O
FreebaseQA B-DatasetName
and O
WikiHop B-DatasetName
, O
as O
well O
as O
the O
Wikidata B-DatasetName
- I-DatasetName
answerable I-DatasetName
subset O
of O
Triv B-DatasetName
- I-DatasetName
iaQA I-DatasetName
and O
NaturalQuestions B-DatasetName
. O
The O
models O
pretrained O
on O
factual O
triples O
compare O
competitively O
with O
the O
ones O
on O
natural O
language O
sentences O
that O
contain O
the O
same O
knowledge O
. O
Trained O
on O
a O
smaller O
size O
KG O
, O
WikiMovies O
, O
we O
saw O
3× O
improvement O
of O
exact O
match O
score O
on O
MetaQA B-DatasetName
task O
compared O
to O
T5 B-MethodName
baseline O
. O
The O
proposed O
method O
has O
an O
advantage O
that O
no O
alignment O
between O
the O
knowledge O
graph O
and O
text O
corpus O
is O
required O
in O
curating O
training O
data O
. O
This O
makes O
our O
method O
particularly O
useful O
when O
working O
with O
industry O
- O
scale O
knowledge O
graphs O
. O

Introduction O

Large O
pre O
- O
trained O
language O
models O
, O
such O
as O
BERT B-MethodName
, O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
, O
REALM B-MethodName
( O
Guu O
et O
al O
. O
, O
2020 O
) O
and O
ERNIE B-MethodName
have O
become O
the O
state O
- O
of O
- O
the O
- O
art O
technology O
for O
many O
tasks O
. O
They O
are O
commonly O
pre O
- O
trained O
using O
unstructured O
text O
corpora O
, O
on O
tasks O
such O
as O
next O
word O
prediction O
, O
next O
sentence O
prediction O
( O
NSP O
) O
or O
masked O
language O
modelling O
( O
MLM O
) O
. O
Especially O
for O
T5 B-MethodName
, O
selfsupervised O
learning O
on O
unlabelled O
text O
corpus O
with O
MLM O
has O
been O
a O
common O
pre O
- O
training O
recipe O
( O
Roberts O
et O
al O
. O
, O
2020 O
) O
. O
This O
is O
normally O
followed O
by O
a O
fine O
- O
tuning O
step O
on O
the O
task O
of O
interest O
( O
Ruder O
et O
al O
. O
, O
2019 O
) O
, O
although O
large O
language O
models O
have O
also O
proved O
useful O
without O
this O
task O
- O
specific O
finetuning O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O

Beyond O
the O
capacity O
of O
contextual O
understanding O
, O
human O
- O
level O
language O
understanding O
pivots O
on O
the O
knowledge O
about O
the O
world O
. O
The O
world O
knowledge O
is O
often O
expressed O
as O
factual O
triples O
( O
c.f O
. O
Ji O
et O
al O
. O
, O
2020 O
) O
, O
in O
the O
form O
of O
( O
subject O
entity O
, O
relation O
, O
object O
entity O
) O
. O
A O
knowledge O
graph O
( O
KG O
) O
defined O
by O
a O
set O
of O
factual O
triples O
consists O
of O
the O
subjects O
and O
objects O
as O
vertices O
/ O
nodes O
, O
and O
the O
relations O
forming O
the O
edges O
connecting O
them O
. O
Most O
of O
the O
large O
scale O
KGs O
( O
e.g. O
Wikidata O
, O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
are O
stored O
in O
triple O
format O
. O

LLMs O
demonstrate O
some O
capacity O
of O
learning O
world O
knowledge O
from O
the O
natural O
text O
corpus O
( O
Roberts O
et O
al O
. O
, O
2020 O
) O
, O
but O
it O
is O
unclear O
to O
what O
degree O
they O
are O
also O
able O
to O
learn O
and O
memorize O
new O
knowledge O
directly O
from O
structured O
KG O
triples O
, O
or O
from O
text O
describing O
them O
explicitly O
. O

In O
order O
to O
infuse O
knowledge O
into O
a O
LLM O
, O
one O
option O
is O
to O
generate O
a O
textual O
version O
of O
the O
knowledge O
base O
, O
and O
apply O
the O
standard O
training O
objectives O
, O
e.g. O
MLM O
. O
This O
is O
unfortunately O
highly O
nontrivial O
. O
One O
can O
either O
align O
sentences O
with O
KG O
triples O
, O
as O
done O
in O
ERNIE B-MethodName
, O
or O
generate O
sentences O
from O
triples O
, O
as O
done O
in O
KELM B-DatasetName
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
. O
These O
approaches O
are O
unfortunately O
hard O
to O
port O
to O
knowledge O
graphs O
with O
different O
schemas O
. O
These O
processes O
are O
also O
lossy O
in O
that O
not O
every O
triple O
can O
be O
aligned O
or O
produce O
a O
valid O
sentence O
, O
and O
there O
is O
not O
a O
good O
understanding O
whether O
this O
can O
introduce O
unnecessary O
selection O
biases O
on O
top O
of O
biases O
existing O
in O
the O
original O
KG O
. O

In O
this O
work O
, O
we O
propose O
a O
method O
of O
Knowledge B-TaskName
Infusion I-TaskName
for I-TaskName
Large I-TaskName
Language I-TaskName
Models I-TaskName
( O
SKILL B-MethodName
) O
, O
where O
LLMs O
directly O
learns O
from O
knowledge O
triples O
. O
Experiment O
results O
shows O
the O
checkpoints O
trained O
with O
proposed O
method O
on O
Wikidata B-DatasetName
KG I-DatasetName
outperform O
the O
T5 B-MethodName
baselines O
on O
four O
standard O
closed O
- O
book O
question O
- O
answering O
( O
QA O
) O
tasks O
. O
With O
a O
smaller O
KG O
, O
WikiMovies B-DatasetName
, O
the O
proposed O
method O
gain O
3× O
exact O
match O
score O
performance O
improvement O
on O
MetaQA B-DatasetName
task O
. O
The O
models O
learning O
directly O
from O
knowledge O
triples O
performs O
competitively O
with O
the O
ones O
with O
the O
aligned O
natural O
sentences O
that O
contain O
the O
same O
amount O
of O
knowledge O
. O
Being O
able O
to O
learn O
directly O
from O
knowledge O
triples O
enables O
easy O
addition O
of O
structured O
knowledge O
into O
language O
modeling O
pre O
- O
training O
. O

Related O
work O

Previous O
works O
that O
use O
knowledge O
graphs O
to O
enhance O
the O
quality O
of O
knowledge O
- O
intensive O
downstream O
tasks O
can O
be O
divided O
into O
two O
groups O
: O
using O
knowledge O
graphs O
at O
the O
inference O
time O
, O
and O
infusing O
knowledge O
into O
the O
model O
weights O
at O
the O
pre O
- O
training O
time O
. O
The O
proposed O
method O
falls O
in O
the O
latter O
group O
. O

Explicit O
usage O
of O
knowledge O
graphs O
. O

A O
retrieval O
- O
augmented O
model O
is O
commonly O
used O
, O
in O
order O
to O
retrieve O
and O
apply O
the O
knowledge O
from O
external O
memories O
or O
sources O
. O
FILM B-MethodName
( O
Verga O
et O
al O
. O
, O
2021 O
) O
and O
EaE B-MethodName
( O
Févry O
et O
al O
. O
, O
2020 O
) O
extend O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
models O
with O
external O
entity O
( O
both O
FILM B-MethodName
and O
EaE B-MethodName
) O
and O
fact O
( O
FILM B-MethodName
) O
memories O
. O
REALM B-MethodName
( O
Guu O
et O
al O
. O
, O
2020 O
) O
is O
pre O
- O
trained O
to O
perform O
reasoning O
over O
a O
large O
textual O
knowledge O
corpus O
on O
- O
the O
- O
fly O
during O
inference O
. O
UniK B-MethodName
- I-MethodName
QA I-MethodName
( O
Oguz O
et O
al O
. O
, O
2020 O
) O
combines O
the O
structured O
and O
unstructured O
information O
to O
improve O
the O
open O
- O
domain O
QA O
tasks O
with O
a O
retriever O
- O
reader O
framework O
. O
The O
main O
difference O
between O
the O
proposed O
method O
, O
SKILL B-MethodName
, O
and O
retrieval O
- O
augmented O
models O
is O
that O
SKILL O
does O
n't O
introduce O
retrieval O
system O
or O
external O
memories O
to O
the O
model O
, O
but O
it O
directly O
embeds O
knowledge O
into O
the O
model O
parameters O
, O
which O
introduces O
no O
extra O
cost O
at O
inference O
time O
. O

Knowledge O
infusion O
. O

A O
common O
way O
of O
parameterized O
knowledge O
infusion O
is O
to O
map O
or O
convert O
structured O
knowledges O
into O
natural O
language O
text O
. O
ERNIE B-MethodName
3.0 I-MethodName
trains O
a O
knowledgeenhanced O
model O
on O
a O
corpus O
combining O
triples O
and O
their O
aligned O
sentences O
, O
by O
randomly O
masking O
relation O
in O
a O
triple O
or O
words O
in O
a O
sentence O
. O
On O
the O
contrary O
, O
SKILL B-MethodName
trains O
only O
on O
triples O
. O

KnowBert B-MethodName
incorporates O
knowledge O
from O
Wikipedia B-DatasetName
and O
WordNet B-DatasetName
( O
Miller O
, O
1995 O
) O
into O
a O
BERT O
model O
through O
entity O
embeddings O
with O
knowledge O
- O
attention O
and O
recontextualization O
mechanism O
. O
BERT B-MethodName
- I-MethodName
MK I-MethodName
( O
He O
et O
al O
. O
, O
2020 O
) O
is O
a O
BERT O
- O
based O
model O
that O
integrates O
graph O
contextual O
knowledge O
of O
a O
medical O
KG O
, O
which O
demonstrates O
the O
utility O
of O
graph O
- O
level O
knowledge O
. O
These O
approaches O
requires O
entity O
linking O
and O
sentences O
contextualizing O
the O
knowledge O
graph O
information O
. O

KG B-MethodName
- I-MethodName
FiD I-MethodName
( O
Yu O
et O
al O
. O
, O
2021 O
) O
extends O
the O
Fusion O
- O
in O
- O
Decoder O
model O
( O
Izacard O
and O
Grave O
, O
2021 O
) O
with O
a O
module O
that O
filters O
and O
re O
- O
ranks O
passages O
based O
on O
structural O
connections O
in O
knowledge O
graph O
between O
entities O
described O
in O
those O
passages O
. O
In O
contrast O
to O
the O
SKILL B-MethodName
method O
that O
we O
propose O
, O
it O
requires O
the O
existence O
of O
natural O
text O
passages O
describing O
each O
knowledge O
graph O
entity O
, O
so O
Wikipedia O
corpus O
was O
used O
since O
it O
naturally O
provides O
articles O
that O
describe O
entities O
. O

Heinzerling O
and O
Inui O
( O
2021 O
) O
explored O
the O
ability O
of O
language O
models O
to O
memorize O
and O
understand O
information O
from O
knowledge O
graphs O
, O
but O
used O
natural O
language O
representation O
of O
triples O
based O
on O
predefined O
templates O
instead O
of O
structured O
representation O
. O
Usage O
of O
predefined O
templates O
significantly O
limits O
scalability O
and O
therefore O
only O
relatively O
small O
knowledge O
graphs O
were O
used O
, O
such O
as O
Google B-MethodName
- I-MethodName
RE I-MethodName
1 I-MethodName
. O

In O
contrast O
to O
the O
new O
method O
presented O
in O
this O
paper O
, O
all O
of O
these O
approaches O
require O
an O
explicit O
mapping O
between O
the O
knowledge O
graph O
entities O
or O
facts O
and O
corresponding O
natural O
language O
sentences O
, O
which O
can O
limit O
applications O
to O
industryscale O
knowledge O
graphs O
that O
do O
n't O
have O
such O
a O
mapping O
. O

Different O
goals O
of O
using O
knowledge O
graphs O
. O
Besides O
that O
, O
some O
papers O
embed O
knowledge O
into O
model O
weights O
but O
pursue O
different O
goals O
rather O
than O
improving O
performance O
on O
downstream O
tasks O
. O
COMET B-MethodName
( O
Bosselut O
et O
al O
. O
, O
2019 O
) O
is O
most O
similar O
to O
our O
work O
and O
trains O
a O
commonsense O
- O
aware O
Transformer O
Language O
Model O
by O
learning O
to O
generate O
loosely O
structured O
commonsense O
descriptions O
in O
the O
natural O
language O
given O
the O
structured O
knowledge O
. O
Similar O
to O
us O
, O
it O
also O
uses O
KG O
triples O
in O
surface O
form O
as O
a O
source O
for O
training O
data O
, O
but O
in O
contrast O
to O
our O
research O
, O
the O
final O
goal O
of O
COMET B-MethodName
is O
to O
generate O
new O
knowledge O
instead O
of O
utilizing O
existing O
ones O
. O
Another O
important O
difference O
is O
the O
scale O
: O
COMET B-MethodName
uses O
Atomic B-MethodName
and O
Con B-MethodName
- I-MethodName
ceptNet I-MethodName
( O
Speer O
et O
al O
. O
, O
2017 O
) O
Knowledge O
Graphs O
that O
are O
much O
smaller O
than O
Wikidata B-DatasetName
( O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
. O

KELM B-DatasetName
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
fine O
- O
tunes O
a O
T5 B-MethodName
model O
to O
convert O
KGs O
to O
synthetic O
natural O
language O
sentences O
to O
augment O
existing O
pre O
- O
training O
corpora O
. O
We O
build O
our O
research O
on O
top O
of O
it O
and O
use O
the O
KELM B-DatasetName
dataset O
to O
compare O
structured O
and O
natural O
language O
representations O
of O
knowledge O
. O

Method O

There O
are O
two O
components O
of O
knowledge O
infusion O
for O
LLMs O
( O
SKILL B-MethodName
) O
: O
the O
corpus O
and O
the O
training O
method O
. O
We O
introduce O
the O
method O
based O
on O
Wikidata B-DatasetName
KG I-DatasetName
, O
but O
it O
can O
be O
applied O
to O
any O
other O
KGs O
. O

Training O
corpus O
. O
We O
use O
two O
corpora O
with O
different O
knowledge O
representations O
: O
Wikidata B-DatasetName
KG I-DatasetName
( O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
in O
triple O
format O
, O
and O
KELM O
corpus O
2 O
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
as O
synthetic O
natural O
language O
sentences O
converted O
from O
Wikidata B-DatasetName
KG I-DatasetName
. O
The O
KELM B-DatasetName
corpus O
contains O
15 O
, O
628 O
, O
486 O
synthetic O
sentences O
. O
To O
ensure O
two O
corpora O
share O
the O
same O
knowledge O
, O
we O
take O
the O
snapshot O
of O
the O
Wikidata B-DatasetName
KG I-DatasetName
used O
to O
created O
the O
KELM B-DatasetName
corpus O
, O
which O
contains O
35 O
, O
697 O
, O
715 O
triples O
. O

To O
prevent O
the O
degradation O
of O
model O
performance O
on O
natural O
language O
understanding O
, O
we O
mix O
the O
Wikidata O
corpus O
or O
KELM B-DatasetName
corpus O
with O
natural O
text O
from O
C4 B-DatasetName
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
, O
50 O
: O
50 O
, O
for O
the O
knowledge O
infusion O
training O
data O
. O

Training O
method O
. O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
was O
trained O
through O
masked O
- O
language O
modelling O
with O
random O
span O
corruption O
on O
the O
C4 B-DatasetName
corpus O
. O
Roberts O
et O
al O
. O
( O
2020 O
) O
found O
that O
masking O
salient O
terms O
( O
Guu O
et O
al O
. O
, O
2020 O
) O
in O
pre O
- O
training O
T5 B-MethodName
models O
, O
instead O
of O
masking O
random O
token O
spans O
, O
could O
significantly O
improve O
the O
performance O
on O
downstream O
tasks O
, O
e.g. O
closed O
- O
book O
QA O
. O

We O
apply O
salient O
span O
masking O
for O
unsupervised O
learning O
in O
our O
knowledge O
- O
infusing O
training O
. O
To O
mask O
the O
same O
amount O
of O
information O
is O
for O
both O
corpora O
, O
the O
following O
method O
is O
applied O
. O
For O
a O
knowledge O
triple O
, O
we O
mask O
either O
the O
subject O
or O
object O
entity O
. O
For O
a O
KELM B-DatasetName
sentence O
, O
we O
identify O
the O
aligned O
triple O
, O
with O
details O
in O
Appendix O
A O
, O
and O
mask O
the O
full O
spans O
corresponding O
to O
the O
subject O
or O
object O
in O
the O
triple O
. O
The O
relation O
tokens O
are O
never O
masked O
, O
as O
there O
is O
no O
robust O
way O
to O
map O
the O
abstract O
relation O
in O
knowledge O
triples O
to O
natural O
language O
tokens O
in O
KELM B-DatasetName
sentences O
. O
Examples O
of O
the O
inputs O
for O
both O
corpora O
are O
in O
Table O
1 O
. O

Experiments O

We O
assess O
SKILL B-MethodName
by O
training O
and O
evaluating O
the O
knowledge O
infused O
models O
on O
closed O
- O
book O
QA O
tasks O
, O
where O
questions O
are O
provided O
without O
supporting O
context O
and O
external O
knowledge O
. O

Experiment O
Setup O

SKILL B-MethodName
pre O
- O
training O
. O
We O
apply O
SKILL B-MethodName
on O
three O
T5.1.1 O
pre O
- O
trained O
checkpoints O
3 O
, O
base O
, O
large O
, O
and O
XXL O
, O
with O
sizes O
of O
∼ O
250 B-HyperparameterValue
M I-HyperparameterValue
, O
∼ O
800 B-HyperparameterValue
M I-HyperparameterValue
and O
∼ O
11B B-HyperparameterValue
parameters B-HyperparameterName
, O
respectively O
. O
For O
T5.1.1 B-MethodName
- I-MethodName
base I-MethodName
andlarge O
, O
SKILL B-MethodName
training O
is O
performed O
for O
500 O
K O
steps O
with O
batch O
size O
1024 O
, O
which O
translates O
to O
∼ O
7.17 O
epochs O
on O
Wikidata B-DatasetName
KG I-DatasetName
and O
∼ O
16.38 O
epochs O
in O
KELM B-DatasetName
sentences O
. O
For O
T5.1.1 B-MethodName
- I-MethodName
XXL I-MethodName
, O
the O
model O
is O
trained O
for O
100 O
K O
steps O
to O
finish O
training O
in O
a O
feasible O
time O
. O

As O
baseline O
we O
use O
pre O
- O
trained O
T5 B-MethodName
checkpoints O
of O
the O
same O
size O
. O
To O
make O
sure O
that O
improvements O
come O
from O
knowledge O
infusion O
instead O
of O
from O
longer O
C4 B-MethodName
pre O
- O
training O
, O
we O
use O
a O
second O
baseline O
by O
further O
training O
the O
T5 B-MethodName
checkpoints O
on O
C4 B-MethodName
for O
half O
of O
the O
aforementioned O
steps O
, O
to O
match O
the O
amount O
of O
C4 B-MethodName
pre O
- O
training O
used O
in O
SKILL B-MethodName
. O

All O
the O
model O
variations O
are O
optimized O
by O
AdaFactor B-HyperparameterValue
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
with O
10 B-HyperparameterValue
−3 I-HyperparameterValue
learning B-HyperparameterName
rate I-HyperparameterName
and O
0.1 B-HyperparameterValue
dropout B-HyperparameterName
rate I-HyperparameterName
, O
the O
same O
settings O
that O
were O
used O
for O
T5 B-MethodName
. O

Fine O
- O
tuning O
on O
closed O
- O
book O
QA O
tasks O
. O
We O
evaluate O
the O
checkpoints O
by O
fine O
- O
tuning O
on O
the O
following O
QA O
benchmarks O
: O
FreebaseQA B-DatasetName
( O
Jiang O
et O
al O
. O
, O
2019 O
) O
, O
WikiHop B-DatasetName
( O
Welbl O
et O
al O
. O
, O
2018 O
) O
, O
Triv B-DatasetName
- I-DatasetName
iaQA I-DatasetName
( O
Joshi O
et O
al O
. O
, O
2017 O
) O
and O
NaturalQuestions B-DatasetName
( O
Kwiatkowski O
et O
al O
. O
, O
2019 O
) O
, O
with O
the O
aforementioned O
hyper O
- O
parameters O
for O
optimization O
and O
128 B-HyperparameterValue
batch B-HyperparameterName
size I-HyperparameterName
. O
For O
the O
benchmarks O
without O
a O
test O
split O
, O
we O
use O
the O
dev O
split O
for O
test O
, O
and O
the O
last O
10 O
% O
of O
train O
as O
dev O
split O
. O

The O
Exact B-MetricName
Match I-MetricName
( O
EM B-MetricName
) O
scores O
on O
the O
test O
sets O
are O
calculated O
after O
being O
fine O
- O
tuned O
for O
50 O
K O
steps O
for O
T5.1.1 B-MethodName
- O
base O
and O
-large O
models O
, O
and O
10 O
K O
steps O
for O
-XXL O
models O
. O
All O
models O
converged O
with O
no O
noticeable O
over O
- O
fitting O
according O
to O
the O
EM B-MetricName
scores O
on O
validation O
sets O
. O

Wikidata B-DatasetName
- I-DatasetName
answerable I-DatasetName
QA I-DatasetName
. O
We O
found O
that O
the O
majority O
of O
the O
questions O
in O
FreebaseQA B-DatasetName
and O
Wiki- B-DatasetName
Hop I-DatasetName
can O
be O
answered O
directly O
from O
triples O
in O
Wikidata B-DatasetName
. O
This O
is O
because O
FreebaseQA B-DatasetName
was O
created O
by O
matching O
question O
- O
answer O
pairs O
with O
triples O
in O
Freebase O
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
, O
most O
of O
which O
was O
imported O
into O
Wikidata B-DatasetName
( O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
. O
For O
WikiHop B-DatasetName
, O
the O
questions O
were O
generated O
from O
Wikidata O
triples O
. O
However O
, O
TriviaQA B-DatasetName
and O
NaturalQuestions B-DatasetName
were O
created O
independently O
of O
Wikidata B-DatasetName
, O
and O
not O
every O
question O
can O
be O
answered O
using O
this O
knowledge O
base O
. O
We O
found O
frequent O
freshness O
issues O
, O
e.g. O
the O
golden O
answer O
for O
question O
" O
Who O
is O
the O
largest O
supermarket O
chain O
in O
the O
UK O
? O
" O
is O
" O
Aldi O
" O
, O
while O
today O
it O
would O
be O
" O
Tesco O
" O
. O
Some O
other O
questions O
can O
not O
be O
answered O
by O
WikiData O
, O
e.g. O
" O
Who O
, O
during O
a O
radio O
microphone O
test O
in O
1984 O
said O
, O
' O
I O
just O
signed O
legislation O
which O
outlaws O
Russia O
forever O
. O
The O
bombing O
begins O
in O
five O
minutes O
? O
' O
" O
, O
with O
the O
golden O
answer O
" O
Ronald O
Reagan O
" O
. O

To O
mitigate O
this O
, O
we O
created O
subsets O
of O
TriviaQA B-DatasetName
( O
TQA B-DatasetName
) O
and O
NaturalQuestions B-DatasetName
( O
NQ B-DatasetName
) O
that O
were O
somewhat O
more O
likely O
to O
have O
answers O
in O
Wikidata B-DatasetName
. O
We O
selected O
all O
the O
items O
for O
which O
there O
exist O
a O
triple O
in O
Wikidata B-DatasetName
that O
has O
the O
answer O
either O
as O
subject O
or O
object O
, O
and O
the O
other O
entity O
in O
the O
triple O
is O
mentioned O
in O
the O
question O
. O
We O
match O
the O
entities O
by O
entity O
name O
, O
case O
- O
insensitive O
. O
We O
name O
the O
Wikidataaligned B-DatasetName
version O
of O
TQA B-DatasetName
and O
NQ B-DatasetName
as O
TQA B-DatasetName
- I-DatasetName
matched I-DatasetName
and O
NQ B-DatasetName
- I-DatasetName
matched I-DatasetName
, O
respectively O
. O
The O
dataset O
sizes O
of O
all O
QA O
tasks O
are O
summarized O
in O
Appendix O
B O
. O

Results O

The O
results O
for O
closed O
- O
book O
QA O
tasks O
are O
summarized O
in O
Table O
2 O
. O
SKILL B-MethodName
pre O
- O
trained O
models O
show O
improvements O
on O
FreebaseQA B-DatasetName
, O
WikiHop B-DatasetName
, O
and O
Wikidata B-DatasetName
- O
answerable O
versions O
of O
TriviaQA B-DatasetName
and O
Nat B-DatasetName
- I-DatasetName
uralQuestions I-DatasetName
, O
but O
no O
significant O
improvement O
on O
original O
TriviaQA B-DatasetName
and O
NaturalQuestions B-DatasetName
. O
As O
discussed O
in O
previous O
section O
, O
we O
believe O
this O
is O
due O
to O
the O
misalignment O
between O
the O
datasets O
and O
Wikidata B-DatasetName
. O

Models O
pre O
- O
trained O
on O
Wikidata B-DatasetName
KG I-DatasetName
gives O
competitive O
results O
with O
ones O
on O
KELM B-DatasetName
sentences O
. O
It O
shows O
that O
the O
triple O
representation O
is O
as O
good O
as O
natural O
language O
representation O
, O
while O
being O
much O
easier O
to O
scale O
up O
for O
larger O
KG O
. O

For O
T5.1.1 B-MethodName
- I-MethodName
base I-MethodName
and O
-large O
, O
additional O
pretraining O
on O
C4 B-DatasetName
boosts O
performance O
in O
comparison O
to O
the O
original O
baseline O
. O
For O
T5.1.1 B-MethodName
- I-MethodName
XXL I-MethodName
, O
this O
additional O
pre O
- O
training O
leads O
to O
a O
performance O
regress O
. O
In O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
, O
it O
is O
mentioned O
that O
training O
on O
C4 B-DatasetName
for O
multiple O
times O
may O
reduce O
the O
performance O
of O
a O
T5 B-MethodName
model O
. O
Impact O
of O
model O
size O
. O
As O
shown O
in O
Figure O
1 O
, O
SKILL B-MethodName
pre O
- O
training O
introduces O
bigger O
improvements O
when O
applied O
on O
larger O
models O
. O
With O
more O
than O
35 O
M O
triples O
in O
Wikidata B-DatasetName
KG I-DatasetName
, O
it O
is O
harder O
for O
As O
1 O
- O
hop O
questions O
are O
supported O
by O
single O
triples O
in O
the O
WikiMovies B-DatasetName
KG O
, O
a O
3× O
improvement O
on O
EM B-MetricName
score O
is O
observed O
for O
the O
sub O
- O
task O
. O
In O
order O
to O
answer O
2 O
/ O
3 O
- O
hop O
questions O
it O
is O
not O
enough O
to O
memorize O
the O
triples O
, O
the O
model O
needs O
to O
be O
able O
to O
reason O
with O
them O
. O
This O
requires O
a O
better O
understanding O
of O
the O
graph O
structure O
. O
Training O
with O
single O
triples O
may O
not O
be O
enough O
, O
and O
the O
observed O
improvement O
is O
notably O
smaller O
. O
The O
performance O
could O
be O
further O
improved O
by O
representing O
more O
explicitly O
the O
graph O
structure O
in O
the O
training O
data O
, O
which O
we O
leave O
for O
future O
work O
. O

Conclusion O

We O
proposed O
a O
method O
to O
directly O
infuse O
knowledge O
from O
knowledge O
graphs O
into O
T5 B-MethodName
models O
through O
pre O
- O
training O
. O
Empirical O
results O
show O
that O
T5 B-MethodName
can O
learn O
directly O
from O
structured O
data O
and O
apply O
the O
learned O
knowledge O
to O
improve O
closed O
- O
book O
QA O
results O
. O
We O
also O
demonstrated O
that O
the O
models O
pre O
- O
trained O
on O
factual O
triples O
perform O
competitively O
with O
the O
ones O
on O
natural O
language O
sentences O
that O
contain O
the O
same O
knowledge O
. O
By O
enabling O
knowledge O
infusion O
directly O
from O
triples O
, O
this O
method O
can O
be O
very O
easily O
applied O
to O
industry O
- O
scale O
KGs O
. O

Ethical O
and O
Broader O
Impact O

In O
this O
work O
, O
we O
are O
introducing O
a O
new O
method O
to O
pre O
- O
train O
a O
well O
known O
natural O
language O
understanding O
model O
, O
T5 B-MethodName
, O
on O
the O
full O
corpora O
of O
public O
knowledge O
graphs O
. O
To O
the O
best O
of O
our O
knowledge O
, O
the O
method O
will O
not O
introduce O
extra O
bias O
to O
either O
the O
model O
or O
the O
dataset O
beyond O
the O
one O
potentially O
inherited O
from O
Wikidata B-DatasetName
( O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
and O
WikiMovies B-DatasetName
( O
Miller O
et O
al O
. O
, O
2016 O
) O
knowledge O
graphs O
. O
On O
the O
other O
hand O
, O
through O
knowledge O
fusion O
pre O
- O
training O
introduced O
in O
this O
work O
, O
a O
language O
model O
will O
be O
able O
to O
learn O
factual O
information O
to O
improve O
the O
quality O
of O
parameterized O
knowledge O
embedded O
in O
the O
model O
, O
which O
is O
demonstrated O
by O
improvements O
on O
various O
closedbook O
question O
- O
answering O
tasks O
. O
The O
proposed O
method O
and O
recipe O
will O
provide O
positive O
impact O
to O
the O
natural O
language O
processing O
community O
and O
help O
to O
improve O
the O
factualness O
in O
pre O
- O
trained O
large O
language O
model O
checkpoints O
. O

Limitations O
. O

A O
factual O
triple O
is O
the O
basic O
ingredient O
of O
a O
knowledge O
graph O
. O
However O
, O
as O
a O
semantic O
network O
, O
the O
graph O
structure O
of O
a O
knowledge O
graph O
describes O
how O
the O
factual O
triples O
are O
connected O
. O
This O
information O
is O
not O
easy O
to O
directly O
represent O
by O
random O
set O
of O
triples O
. O
We O
leave O
the O
exploration O
of O
how O
to O
infuse O
the O
information O
implied O
by O
the O
graph O
structure O
for O
future O
work O
. O
We O
expect O
that O
this O
will O
further O
improve O
the O
results O
, O
especially O
for O
multi O
- O
hop O
question O
- O
answering O
tasks O
. O
for O
each O
e O
∈ O
entities O
( O
t O
) O
do O
for O
each O
∃s O
⊂ O
k O
: O
e O
= O
s+ O
" O
( O
* O
) O
" O
do O
23 O
: O

spans.insert O
( O
s O
) O

B O
Dataset O

Wikidata B-DatasetName
( O
Vrandečić O
and O
Krötzsch O
, O
2014 O
) O
was O
released O
under O
the O
Creative O
Commons O
CC0 O
License O
. O
KELM B-DatasetName
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
was O
released O
under O
the O
Creative O
Commons O
CC O
BY O
- O
SA O
2.0 O
License O
. O
NaturalQuestions B-DatasetName
( O
Kwiatkowski O
et O
al O
. O
, O
2019 O
) O
and O
WikiHop O
( O
Welbl O
et O
al O
. O
, O
2018 O
) O
were O
released O
under O
Creative O
Commons O
CC O
BY O
- O
SA O
3.0 O
License O
. O
MetaQA B-DatasetName
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
was O
released O
under O
Creative O
Commons O
CC O
BY O
- O
ND O
3.0 O
License O
. O
C4 O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
and O
TriviaQA B-DatasetName
( O
Joshi O
et O
al O
. O
, O
2017 O
) O
were O
released O
under O
Apache-2.0 O
License O
. O
WikiMovies B-DatasetName
( O
Miller O
et O
al O
. O
, O
2016 O
) O
was O
released O
under O
MIT O
License O
. O
FreebaseQA B-DatasetName
( O
Jiang O
et O
al O
. O
, O
2019 O
) O

A O
Matching O
of O
entities O
in O
KELM B-DatasetName
sentences O

To O
find O
Wikidata B-DatasetName
KG I-DatasetName
entities O
in O
corresponding O
KELM B-DatasetName
sentences O
, O
we O
use O
Algorithm O
1 O
. O
Additional O
cycle O
on O
line O
22 O
is O
needed O
because O
some O
entities O
have O
an O
information O
in O
brackets O
that O
should O
not O
be O
in O
a O
sentence O
, O
for O
example O
John O
Doe O
( O
born O
1990 O
) O
. O
This O
algorithm O
matched O
at O
least O
one O
entity O
to O
15 O
, O
383 O
, O
248 O
out O
of O
15 O
, O
628 O
, O
486 O
KELM B-DatasetName
sentences O
. O

We O
do O
n't O
try O
to O
match O
relation O
part O
of O
triples O
, O
because O
it O
could O
be O
represented O
in O
many O
different O
forms O
. O
For O
example O
, O
the O
triple O
( O
Pulp O
Fiction O
, O
cast O
member O
, O
John O
Travolta O
) O
could O
be O
represented O
as O
" O
John O
Travolta O
was O
an O
actor O
in O
Pulp O
Fiction O
" O
, O
" O
John O
Travolta O
starred O
in O
Pulp O
Fiction O
" O
, O
" O
John O
Travolta O
played O
Vincent O
Vega O
in O
Pulp O
Fiction O
" O
, O
etc O
. O
, O
and O
there O
is O
no O
way O
to O
robustly O
align O
a O
relation O
to O
all O
possible O
surface O
forms O
. O

UniEX B-MethodName
: O
An O
Effective O
and O
Efficient O
Framework O
for O
Unified O
Information B-TaskName
Extraction I-TaskName
via O
a O
Span O
- O
extractive O
Perspective O

We O
propose O
a O
new O
paradigm O
for O
universal B-TaskName
information I-TaskName
extraction I-TaskName
( O
IE B-TaskName
) O
that O
is O
compatible O
with O
any O
schema O
format O
and O
applicable O
to O
a O
list O
of O
IE O
tasks O
, O
such O
as O
named B-TaskName
entity I-TaskName
recognition I-TaskName
, O
relation B-TaskName
extraction I-TaskName
, O
event B-TaskName
extraction I-TaskName
and O
sentiment B-TaskName
analysis I-TaskName
. O
Our O
approach O
converts O
the O
text B-TaskName
- I-TaskName
based I-TaskName
IE I-TaskName
tasks O
as O
the O
token O
- O
pair O
problem O
, O
which O
uniformly O
disassembles O
all O
extraction O
targets O
into O
joint O
span O
detection O
, O
classification O
and O
association O
problems O
with O
a O
unified B-MethodName
extractive I-MethodName
framework I-MethodName
, O
namely O
UniEX B-MethodName
. O
UniEX B-MethodName
can O
synchronously O
encode O
schema O
- O
based O
prompt O
and O
textual O
information O
, O
and O
collaboratively O
learn O
the O
generalized O
knowledge O
from O
pre O
- O
defined O
information O
using O
the O
auto O
- O
encoder O
language O
models O
. O
We O
develop O
a O
traffine O
attention O
mechanism O
to O
integrate O
heterogeneous O
factors O
including O
tasks O
, O
labels O
and O
inside O
tokens O
, O
and O
obtain O
the O
extraction O
target O
via O
a O
scoring O
matrix O
. O
Experiment O
results O
show O
that O
UniEX B-MethodName
can O
outperform O
generative O
universal O
IE O
models O
in O
terms O
of O
performance B-MetricName
and O
inference B-MetricName
- I-MetricName
speed I-MetricName
on O
14 O
benchmarks O
IE O
datasets O
with O
the O
supervised O
setting O
. O
The O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
low O
- O
resource O
scenarios O
also O
verifies O
the O
transferability O
and O
effectiveness O
of O
UniEX B-MethodName
. O

Introduction O

Information B-TaskName
extraction I-TaskName
( O
IE B-TaskName
) O
aims O
at O
automatically O
extracting O
structured O
information O
from O
unstructured O
textual O
sources O
, O
covering O
a O
wide O
range O
of O
subtasks O
such O
as O
named B-TaskName
entity I-TaskName
recognition I-TaskName
, O
relation B-TaskName
extraction I-TaskName
, O
semantic B-TaskName
role I-TaskName
labeling I-TaskName
, O
and O
sentiment B-TaskName
analysis I-TaskName
( O
Muslea O
et O
al O
. O
, O
1999 O
; O
Grishman O
, O
2019 O
) O
. O
However O
, O
the O
variety O
of O
subtasks O
build O
the O
isolation O
zones O
between O
each O
other O
and O
form O
their O
own O
dedicated O
models O
. O
Fig O
1 O
( O
a O
) O
presents O
that O
the O
popular O
IE O
approaches O
handle O
structured O
extraction O
by O
the O
addition O
of O
task O
- O
specific O
layers O
on O
top O
of O
pretrained O
language O
models O
( O
LMs O
) O
and O
a O
subsequent O
fine O
- O
tuning O
of O
the O
conjoined O
model O
( O
Lample O
et O
al O
. O
, O
2016 O
; O
Luo O
et O
al O
. O
, O
2020 O
; O
Ye O
et O
al O
. O
, O
2022 O
) O
. O
The O
isolated O
architectures O
and O
chaotic O
situation O
prevents O
enhancements O
from O
one O
task O
from O
being O
applied O
to O
another O
, O
which O
hinders O
the O
effective O
latent O
semantics O
sharing O
such O
as O
label O
names O
, O
and O
suffer O
from O
inductive O
bias O
in O
transfer O
learning O
( O
Paolini O
et O
al O
. O
, O
2020 O
) O
. O

With O
powerful O
capabilities O
in O
knowledge O
sharing O
and O
semantic O
generalization O
, O
large O
- O
scale O
LMs O
bring O
the O
opportunity O
to O
handle O
multiple O
IE O
tasks O
using O
a O
single O
framework O
. O
As O
shown O
in O
Fig O
1 O
( O
b O
) O
, O
by O
developing O
sophisticated O
schema O
- O
based O
prompt O
and O
structural O
generation O
specification O
, O
the O
IE B-TaskName
tasks O
can O
be O
transformed O
into O
text O
- O
to O
- O
text O
and O
text O
- O
to O
- O
structure O
formats O
via O
large O
- O
scale O
generative O
LMs O
( O
Dong O
et O
al O
. O
, O
2019 O
; O
Paolini O
et O
al O
. O
, O
2020 O
; O
Lu O
et O
al O
. O
, O
2022 O
) O
such O
as O
T5 O
( O
Raffel O
et O
al O
. O
, O
2020a O
) O
. O
Moreover O
, O
the O
universal O
IE O
frameworks O
can O
learn O
general O
knowledge O
from O
multi O
- O
source O
prompts O
, O
which O
is O
beneficial O
for O
perceiving O
unseen O
content O
in O
lowresource O
scenarios O
. O
Despite O
their O
success O
, O
these O
generative O
frameworks O
suffer O
from O
their O
inherent O
problems O
, O
which O
limit O
their O
potential O
and O
performance O
in O
universal O
modeling O
. O
Firstly O
, O
the O
schemabased O
prompt O
and O
contextual O
information O
are O
synthetically O
encoded O
for O
generating O
the O
target O
structure O
, O
which O
is O
not O
conducive O
to O
directly O
leveraging O
the O
position O
information O
among O
different O
tokens O
. O
Secondly O
, O
the O
generative O
architecture O
utilizes O
the O
token O
- O
wise O
decoder O
to O
obtain O
the O
target O
structure O
, O
which O
is O
extremely O
time O
- O
consuming O
. O

The O
aforementioned O
issues O
prompt O
us O
to O
rethink O
the O
foundation O
of O
IE B-TaskName
tasks O
. O
Fundamentally O
, O
we O
discover O
that O
the O
extraction O
targets O
of O
different O
IE B-TaskName
tasks O
involve O
the O
determination O
of O
semantic O
roles O
and O
semantic O
types O
, O
both O
of O
which O
can O
be O
converted O
into O
span O
formats O
by O
the O
correlation O
of O
the O
inside O
tokens O
in O
the O
passage O
. O
For O
instance O
, O
an O
entity O
type O
is O
the O
boundary O
detection O
and O
label O
classification O
of O
a O
semantic O
role O
, O
while O
a O
relation O
type O
can O
be O
regarded O
as O
the O
semantic O
association O
between O
specific O
semantic O
roles O
. O
From O
this O
perspective O
, O
the O
IE B-TaskName
tasks O
can O
be O
decoded O
using O
a O
span O
- O
extractive O
framework O
, O
which O
can O
be O
uniformly O
decomposed O
as O
several O
atomic O
operations O
: O
i O
) O
Span B-TaskName
Detection I-TaskName
, O
which O
locates O
the O
boundaries O
of O
the O
mentioned O
semantic O
roles O
; O
ii O
) O
Span B-TaskName
Classification I-TaskName
, O
which O
recognizes O
the O
semantic O
types O
of O
the O
semantic O
roles O
; O
iii O
) O
Span B-TaskName
Association I-TaskName
, O
which O
establishes O
and O
measures O
the O
correlation O
between O
semantic O
roles O
to O
determine O
semantic O
types O
. O
According O
to O
the O
above O
observation O
, O
we O
propose O
a O
new O
paradigm O
for O
universal B-TaskName
IE I-TaskName
, O
called O
Unified B-MethodName
Extraction I-MethodName
model I-MethodName
( O
UniEX B-MethodName
) O
as O
Figure O
1 O
( O
c O
) O
. O
Specifically O
, O
we O
first O
introduce O
a O
rule O
- O
based O
transformation O
to O
bridge O
various O
extraction O
targets O
and O
unified O
input O
formats O
, O
which O
leverages O
task O
- O
specific O
labels O
with O
identifiers O
as O
the O
schema O
- O
based O
prompt O
to O
learn O
general O
IE B-TaskName
knowledge O
. O
Then O
, O
recent O
works O
( O
Liu O
et O
al O
. O
, O
2019a O
; O
Yang O
et O
al O
. O
, O
2022 O
) O
state O
that O
the O
auto O
- O
encoder O
LMs O
with O
bidirectional O
context O
representations O
are O
more O
suitable O
for O
natural O
language O
understanding O
. O
Therefore O
, O
We O
employ O
BERT O
- O
like O
LMs O
to O
construct O
an O
extractive O
architecture O
for O
underlying O
semantic O
encoding O
. O
Finally O
, O
inspired O
by O
the O
successful O
application O
of O
span O
- O
decoder O
and O
biaffine O
network O
to O
decode O
entity O
and O
relation O
with O
a O
scoring O
matrix O
( O
Yu O
et O
al O
. O
, O
2020b O
; O
Yuan O
et O
al O
. O
, O
2022 O
) O
, O
we O
introduce O
a O
triaffine O
attention O
mechanism O
for O
structural O
decoding O
, O
which O
jointly O
considers O
high O
- O
order O
interactions O
among O
multiple O
factors O
, O
including O
tasks O
, O
labels O
and O
inside O
tokens O
. O
Each O
triaffine O
scoring O
matrix O
is O
assigned O
to O
a O
demand O
- O
specific O
prompt O
for O
obtaining O
span O
- O
extractive O
objectives O
. O

Through O
extensive O
experiments O
on O
several O
challenging O
benchmarks O
of O
4 O
main O
IE B-TaskName
tasks O
( O
entity B-TaskName
/ I-TaskName
relation I-TaskName
/ I-TaskName
event I-TaskName
/ I-TaskName
sentiment I-TaskName
extraction I-TaskName
) O
, O
we O
demonstrate O
that O
compared O
with O
the O
state O
- O
of O
- O
the O
- O
art O
universal O
IE O
models O
and O
task O
- O
specific O
low O
- O
resource O
approaches O
, O
our O
UniEX B-MethodName
achieves O
a O
substantial O
improvement O
in O
performance O
and O
efficiency O
with O
supervised O
, O
few O
- O
shot O
and O
zero O
- O
shot O
settings O
. O

Our O
main O
contributions O
are O
summarized O
as O
: O

• O
We O
develop O
an O
efficient O
and O
effective O
universal O
IE B-TaskName
paradigm O
by O
converting O
all O
IE B-TaskName
tasks O
into O
joint O
span B-TaskName
classification I-TaskName
, O
detection B-TaskName
and O
association B-TaskName
problem O
. O

• O
We O
introduce O
UniEX B-MethodName
, O
a O
new O
unified O
extractive O
framework O
that O
utilizes O
the O
extractive O
structures O
to O
encode O
the O
underlying O
information O
and O
control O
the O
schema O
- O
based O
span O
decoding O
via O
the O
triaffine O
attention O
mechanism O
. O

• O
We O
apply O
our O
approach O
in O
low O
- O
resource O
scenarios O
, O
and O
significant O
performance O
improvements O
suggest O
that O
our O
approach O
is O
potential O
for O
attaching O
label O
information O
to O
generalized O
objects O
and O
transfer O
learning O
. O
Our O
code O
will O
be O
made O
publicly O
available O
. O

Related O
Work O

Unified O
NLP O
Task O
Formats O
Since O
the O
prompttuning O
can O
improve O
the O
ability O
of O
language O
models O
to O
learn O
common O
knowledge O
and O
fix O
the O
gap O
across O
different O
NLP O
tasks O
, O
recent O
studies O
show O
the O
necessity O
of O
unifying O
all O
NLP O
tasks O
in O
the O
format O
of O
a O
natural O
language O
response O
to O
natural O
language O
input O
( O
Raffel O
et O
al O
. O
, O
2020b O
; O
Sanh O
et O
al O
. O
, O
2022 O
; O
Wei O
et O
al O
. O
, O
2021 O
) O
. O
Previous O
unified O
frameworks O
usually O
cast O
parts O
of O
text O
problems O
as O
question O
answering O
( O
McCann O
et O
al O
. O
, O
2018 O
) O
or O
span B-TaskName
extraction I-TaskName
( O
Keskar O
et O
al O
. O
, O
2019 O
) O
tasks O
. O
TANL B-MethodName
( O
Paolini O
et O
al O
. O
, O
2020 O
) O
frames O
the O
structured O
prediction O
tasks O
as O
a O
translation O
task O
between O
augmented O
natural O
languages O
. O
By O
developing O
a O
text O
- O
to O
- O
text O
architecture O
, O
T5 O
( O
Raffel O
et O
al O
. O
, O
2020b O
) O
makes O
prompts O
to O
effectively O
distinguish O
different O
tasks O
and O
provide O
prior O
knowledge O
for O
multitask O
learning O
. O
UIE B-MethodName
( O
Lu O
et O
al O
. O
, O
2022 O
) O
uniformly O
models O
IE B-TaskName
tasks O
with O
a O
textto O
- O
structure O
framework O
, O
which O
encodes O
different O
extraction O
structures O
via O
a O
structured O
extraction O
language O
, O
adaptively O
generates O
varying O
targets O
via O
a O
structural O
schema O
instructor O
. O
Although O
effective O
, O
such O
methods O
focus O
on O
generative O
styles O
and O
thus O
can O
not O
be O
adapted O
to O
the O
knowledge O
selection O
for O
vast O
label O
- O
based O
models O
. O
It O
motivates O
us O
to O
design O
an O
efficient O
and O
effective O
universal O
IE O
method O
, O
where O
we O
develop O
unified O
Extraction O
( O
EX O
) O
formats O
and O
triaffine O
attention O
mechanism O
. O

Label O
Information O
Label O
semantics O
is O
an O
important O
information O
source O
, O
which O
carries O
out O
the O
related O
meaning O
induced O
from O
the O
data O
( O
Hou O
et O
al O
. O
, O
2020 O
; O
Ma O
et O
al O
. O
, O
2022a O
; O
Mueller O
et O
al O
. O
, O
2022 O
) O
. O
The O
L B-MethodName
- I-MethodName
TapNet I-MethodName
( O
Hou O
et O
al O
. O
, O
2020 O
) O
introduces O
the O
collapsed O
dependency O
transfer O
mechanism O
to O
leverage O
the O
semantics O
of O
label O
names O
for O
few O
- O
shot O
tagging O
tasks O
. O
LSAP O
( O
Mueller O
et O
al O
. O
, O
2022 O
) O
improves O
the O
generalization O
and O
data O
efficiency O
of O
few O
- O
shot O
text O
classification O
by O
incorporating O
label O
semantics O
into O
the O
pre O
- O
training O
and O
fine O
- O
tuning O
phases O
of O
generative O
LMs O
. O
Together O
, O
these O
successful O
employments O
of O
label O
knowledge O
in O
low O
- O
resource O
setting O
motivates O
us O
to O
introduce O
label O
semantics O
into O
our O
unified O
inputs O
to O
handle O
few O
- O
shot O
and O
zero O
- O
shot O
scenarios O
. O

Approaches O

Generally O
, O
there O
are O
two O
main O
challenges O
in O
universally O
modeling O
different O
IE B-TaskName
tasks O
via O
the O
extractive O
architecture O
. O
Firstly O
, O
IE B-TaskName
tasks O
are O
usually O
demand O
- O
driven O
, O
indicating O
that O
each O
pre O
- O
defined O
schema O
should O
correspond O
to O
the O
extraction O
of O
specific O
structural O
information O
. O
Secondly O
, O
due O
to O
the O
diversity O
of O
IE B-TaskName
tasks O
, O
we O
need O
to O
resolve O
appropriate O
structural O
formats O
from O
the O
output O
sequence O
to O
accommodate O
different O
target O
structures O
, O
such O
as O
entity O
, O
relation O
and O
event O
. O
In O
this O
section O
, O
we O
outline O
how O
the O
UniEX B-MethodName
exploits O
a O
shared O
underlying O
semantic O
encoder O
to O
learn O
the O
prompt O
and O
text O
knowledge O
jointly O
, O
and O
conduct O
various O
IE B-TaskName
tasks O
in O
a O
unified O
text O
- O
to O
- O
structure O
architecture O
via O
the O
triaffine O
attention O
mechanism O
. O

The O
UniEX B-MethodName
Framework O

Unified O
Input O

Formally O
, O
given O
the O
task O
- O
specific O
pre O
- O
defined O
schema O
and O
texts O
, O
the O
universal O
IE O
model O
needs O
to O
adaptively O
capture O
the O
corresponding O
structural O
information O
from O
the O
text O
indicated O
by O
the O
taskrelevant O
information O
. O
[ O
SEP O
] O
x O
[ O
SEP O
] O
. O

xinp O
= O
[ O
D O
- O
TOK O
] O
i O
s O
i O
d O
N O
sd O
i=1 O
[ O
C O
- O
TOK O
] O
i O
s O
i O
c O
Nsc O
i=1 O
[ O
A O
- O
TOK O
] O
i O
s O
i O
a O
Nsa O
i=1 O

( O
1 O
) O

In O
our O
UniEX B-MethodName
framework O
, O
we O
employ O
the O
BERTlike O
LMs O
as O
the O
extractive O
backbone O
, O
such O
as O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019b O
) O
and O
ALBERT O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
to O
integrate O
the O
bidirectional O
modeled O
input O
x O
inp O
. O
Note O
that O
the O
unified O
input O
contains O
multiple O
labels O
, O
resulting O
in O
undesired O
mutual O
influence O
across O
different O
labels O
and O
leading O
to O
a O
misunderstanding O
of O
the O
correspondence O
between O
the O
label O
and O
its O
structural O
format O
during O
the O
decoding O
phase O
. O
Meanwhile O
, O
in O
some O
tasks O
, O
the O
large O
number O
of O
labels O
allows O
schemas O
to O
take O
up O
excessive O
locations O
, O
squeezing O
the O
space O
for O
text O
. O
Referring O
to O
the O
embedding O
methods O
in O
the O
UniMC O
( O
Yang O
et O
al O
. O
, O
2022 O
) O
, O
we O
address O
these O
issues O
from O
several O
perspectives O
, O
including O
position O
i O
d O
and O
attention O
mask O
. O
Firstly O
, O
to O
avoid O
the O
information O
interference O
caused O
by O
the O
mutual O
interaction O
within O
label O
- O
based O
schemas O
, O
we O
constantly O
update O
the O
position O
i O
d O
pos O
to O
tell O
apart O
intra O
- O
information O
in O
the O
label O
. O
In O
this O
way O
, O
the O
position O
information O
of O
label O
- O
relevant O
tokens O
is O
coequally O
treated O
based O
on O
their O
position O
embedding O
, O
and O
the O
refreshed O
location O
information O
for O
the O
first O
token O
of O
each O
label O
- O
based O
schema O
avoids O
the O
natural O
increase O
of O
the O
location O
i O
d. O
Then O
, O
as O
shown O
in O
Figure O
3 O
, O
due O
to O
the O
detailed O
correlation O
among O
schema O
- O
based O
prompts O
in O
the O
IE B-TaskName
tasks O
, O
we O
further O
( O
Roth O
and O
Yih O
, O
2004 O
) O
. O

x O
/ O
H O
e O
x O
) O
for O
the O
start O
/ O
end O
positions O
of O
the O
inside O
tokens O
. O
To O
further O
interact O
such O
multiple O
heterogeneous O
factors O
simultaneously O
, O
we O
define O
the O
deep O
triaffine O
transformation O
with O
weighted O
matrix O
W O
∈ O
R O
d×d×d O
, O
which O
apply O
the O
triaffine O
attention O
to O
aggregate O
the O
schema O
- O
wise O
span O
representations O
by O
considering O
schema O
as O
queries O
as O
well O
as O
start O
/ O
end O
of O
the O
inside O
tokens O
as O
keys O
and O
values O
. O
In O
this O
process O
, O
the O
triaffine O
transformation O
injects O
each O
schema O
information O
into O
the O
span O
representations O
and O
resolves O
the O
corresponding O
extraction O
targets O
. O
It O
creates O
a O
N O
s O
× O
N O
x O
× O
N O
x O
scoring O
tensor O
S O
by O
calculating O
continuous O
matrix O
multiplication O
as O
following O
: O

H O
s O
x O
= O
FFNs O
( O
Hx O
) O
, O
H O
e O
x O
= O
FFNe O
( O
Hx O
) O
, O
S O
= O
σ O
( O
W O
×1 O
Hs O
×2 O
H O
s O
x O
×3 O
H O
e O
x O
) O
, O
( O
3 O
) O

where O
× O
k O
is O
the O
matrix O
multiplication O
between O
input O
tensor O
and O
dimension O
- O
k O
of O
W. O
σ O
( O
* O
) O
denotes O
the O
Sigmoid O
activation O
function O
. O
At O
this O
point O
, O
the O
tensor O
S O
provides O
a O
mapping O
score O
from O
the O
schema O
to O
internal O
spans O
of O
the O
text O
, O
where O
each O
rank-2 O
scoring O
matrix O
corresponding O
to O
a O
specific O
schema O
is O
the O
structural O
table O
. O
For O
the O
r O
- O
th O
structural O
table O
, O
the O
affine O
score O
of O
each O
span O
( O
p O
, O
q O
) O
that O
starts O
with O
p O
and O
ends O
with O
q O
can O
be O
denoted O
as O
S O
r O
, O
p O
, O
q O
∈ O
[ O
0 O
, O
1 O
] O
, O
while O
the O
affine O
score O
of O
a O
valid O
span O
in O
the O
structural O
table O
is O
the O
spotting O
designator O
. O
We O
divide O
all O
N O
s O
structural O
tables O
into O
three O
parts O
according O
to O
the O
distribution O
of O
the O
schemas O
, O
among O
them O
, O
N O
sd O
for O
span O
detection O
, O
N O
sc O
for O
span O
classification O
, O
and O
N O
sa O
for O
span O
association O
. O
For O
different O
schemas O
, O
we O
develop O
their O
spotting O
designators O
by O
following O
strategies O
: O
Span B-TaskName
Detection I-TaskName
: O
In O
particular O
, O
we O
usually O
use O
the O
structural O
table O
derived O
from O
the O
task O
- O
based O
schema O
representation O
for O
span B-TaskName
detection I-TaskName
, O
which O
can O
be O
obtained O
from O
the O
hidden O
state O
of O
the O
special O
token O
[ O
CLS O
] O
. O
Since O
the O
[ O
CLS O
] O
token O
is O
mutually O
visible O
to O
other O
schemas O
, O
the O
task O
- O
based O
schema O
representation O
can O
capture O
the O
span O
- O
related O
semantic O
information O
of O
the O
semantic O
roles O
from O
the O
task O
and O
label O
names O
. O
The O
spotting O
designators O
identify O
the O
start O
and O
end O
indices O
of O
the O
i O
- O
th O
semantic O
roles O
as O
( O
s O
i O
, O
e O
i O
) O
using O
the O
axes O
. O
Span B-TaskName
Classification I-TaskName
: O
The O
label O
- O
based O
schema O
representations O
for O
entity O
/ O
argument O
/ O
trigger O
/ O
event O
types O
are O
used O
for O
span B-TaskName
classification I-TaskName
. O
The O
spotting O
designators O
are O
identical O
with O
the O
span O
positions O
of O
the O
semantic O
roles O
, O
indicating O
that O
the O
semantic O
type O
of O
the O
i O
- O
th O
span O
can O
be O
identified O
by O
attaching O
to O
the O
( O
s O
i O
, O
e O
i O
) O
position O
in O
the O
corresponding O
structural O
table O
. O
Span B-TaskName
Association I-TaskName
: O
The O
label O
- O
based O
schema O
representations O
for O
relation O
/ O
sentiment O
types O
are O
used O
for O
span B-TaskName
association I-TaskName
. O
In O
this O
process O
, O
we O
model O
the O
potentially O
related O
semantic O
roles O
and O
correlate O
them O
to O
corresponding O
semantic O
types O
. O
The O
spotting O
designators O
locate O
at O
two O
interleaved O
positions O
associated O
with O
the O
semantic O
roles O
of O
the O
semantic O
type O
, O
that O
is O
, O
for O
the O
i O
- O
th O
and O
j O
- O
th O
spans O
, O
the O
extraction O
target O
is O
transformed O
to O
the O
identification O
of O
the O
( O
s O
i O
, O
s O
j O
) O
and O
( O
e O
i O
, O
e O
j O
) O
positions O
in O
the O
corresponding O
structural O
table O
. O
Note O
that O
all O
span O
values O
in O
the O
structural O
table O
for O
label O
- O
based O
schemas O
are O
masked O
except O
for O
the O
spotting O
designators O
, O
because O
we O
only O
need O
to O
observe O
the O
semantic O
types O
and O
semantic O
association O
among O
the O
detected O
spans O
. O
Specifically O
, O
the O
spotting O
designators O
for O
span O
detection O
are O
the O
spans O
with O
q O
≥ O
p O
, O
and O
the O
spotting O
designators O
for O
span O
classification O
and O
association O
are O
defined O
by O
the O
position O
consistency O
and O
interleaving O
of O
valid O
spans O
with O
S O
r O
, O
p O
, O
q O
= O
1 O
in O
span B-TaskName
detection I-TaskName
. O

EX O
Training O
Procedure O

Given O
the O
input O
sentence O
x O
inp O
, O
We O
uniformly O
reformat O
different O
output O
targets O
as O
a O
rank-3 O
matrix O
Y O
, O
sharing O
the O
same O
spotting O
designators O
as O
the O
triaffine O
scoring O
matrix O
. O
Similarly O
, O
we O
denote O
the O
value O
of O
each O
valid O
span O
as O
Y O
r O
, O
p O
, O
q O
∈ O
{ O
0 O
, O
1 O
} O
, O
with O
Y O
r O
, O
p O
, O
q O
= O
1 O
denoting O
the O
desirable O
span O
for O
a O
groundtruth O
and O
Y O
r O
, O
p O
, O
q O
= O
0 O
denoting O
the O
meaningless O
span O
for O
semantic O
role O
or O
semantic O
type O
. O
Hence O
it O
is O
a O
binary B-TaskName
classification I-TaskName
problem O
and O
we O
optimize O
our O
models O
with O
binary O
cross O
- O
entropy O
: O

BCE O
( O
y O
, O
ŷ O
) O
= O
− O
( O
y O
• O
log O
( O
ŷ O
) O
+ O
( O
1 O
− O
y O
) O
• O
log O
( O
1 O
−ŷ O
) O
) O
, O
( O
4 O
) O
L O
= O
Ns O
r=1 O
Nx O
p=1 O
Nx O
q=1 O
BCE O
( O
Yr O
, O
p O
, O
q O
, O
Sr O
, O
p O
, O
q O
) O
. O
( O
5 O
) O

Experiments O

To O
verify O
the O
effectiveness O
of O
our O
UniEX B-MethodName
, O
we O
conduct O
extensive O
experiments O
on O
different O
IE B-TaskName
tasks O
with O
supervised O
( O
high O
- O
resource O
) O
, O
few O
- O
shot O
and O
zeroshot O
( O
low O
- O
resource O
) O
scenarios O
. O

Experimental O
Setup O

For O
the O
supervised O
setting O
, O
we O
follow O
the O
preparation O
in O
TANL B-MethodName
( O
Paolini O
et O
al O
. O
, O
2020 O
) O
and O
UIE B-MethodName
( O
Lu O
et O
al O
. O
, O
2022 O
) O
to O
collect O
14 O
publicly O
available O
IE O
benchmark O
datasets O
and O
cluster O
the O
wellrepresentative O
IE B-TaskName
tasks O
into O
4 O
groups O
, O
including O
entity B-TaskName
, I-TaskName
relation I-TaskName
, I-TaskName
event I-TaskName
and I-TaskName
structured I-TaskName
sentiment I-TaskName
extraction I-TaskName
. O
In O
particular O
, O
for O
each O
group O
, O
we O
design O
a O
corresponding O
conversion O
regulation O
to O
translate O
raw O
data O
into O
the O
unified O
EX O
format O
. O

Then O
, O
for O
the O
few O
- O
shot O
setting O
, O
we O
adopt O
the O
popular O
datasets O
FewNERD B-DatasetName
( O
Ding O
et O
al O
. O
, O
2021 O
) O
and O
Cross B-DatasetName
- I-DatasetName
Dataset I-DatasetName
( O
Hou O
et O
al O
. O
, O
2020 O
) O
in O
few B-TaskName
- I-TaskName
shot I-TaskName
entity I-TaskName
extraction I-TaskName
and O
domain B-TaskName
partition I-TaskName
as O
( O
Ma O
et O
al O
. O
, O
2022b O
) O
. O
For O
the O
zero O
- O
shot O
setting O
, O
we O
use O
the O
common O
zero B-TaskName
- I-TaskName
shot I-TaskName
relation I-TaskName
extraction I-TaskName
datasets O
Wiki B-DatasetName
- I-DatasetName
ZSL I-DatasetName
( O
Chen O
and O
Li O
, O
2021 O
) O
and O
FewRel B-DatasetName
( O
Han O
et O
al O
. O
, O
2018 O
) O
and O
follow O
the O
same O
process O
of O
data O
and O
label O
splitting O
as O
( O
Chia O
et O
al O
. O
, O
2022 O
) O
. O
Following O
the O
same O
evaluation O
metrics O
as O
all O
previous O
methods O
, O
we O
use O
span O
- O
based O
offset O
Micro B-MetricName
- I-MetricName
F1 I-MetricName
with O
strict O
match O
criteria O
as O
the O
primary O
metric O
for O
performance O
comparison O
. O
Please O
refer O
to O
Appendix O
A O
for O
more O
details O
on O
dataset O
descriptions O
, O
unified O
EX O
input O
formats O
, O
metrics O
and O
training O
implementation O
. O

Experiments O
on O
Supervised O
Settings O

In O
our O
experiment O
, O
under O
the O
high O
- O
resource O
scenario O
, O
we O
compare O
our O
approach O
with O
the O
state O
- O
ofthe O
- O
art O
generative O
universal O
IE O
architectures O
that O
provide O
a O
universal O
backbone O
for O
IE B-TaskName
tasks O
based O
on O
T5 O
( O
Raffel O
et O
al O
. O
, O
2020a O
) O
, O
including O
TANL B-MethodName
( O
Paolini O
et O
al O
. O
, O
2020 O
) O
and O
UIE B-MethodName
( O
Lu O
et O
al O
. O
, O
2022 O
) O
. O
For O
a O
fair O
comparison O
, O
We O
only O
consider O
results O
without O
exploiting O
large O
- O
scale O
contexts O
and O
external O
knowledge O
beyond O
the O
dataset O
- O
specific O
information O
, O
and O
present O
the O
average O
outcomes O
if O
the O
baseline O
is O
conducted O
in O
multiple O
runs O
. O
The O
main O
results O
of O
UniEX B-MethodName
and O
other O
baselines O
on O
14 O
IE B-TaskName
datasets O
are O
shown O
in O
Table O
1 O
. O
We O
can O
observe O
that O
: O
1 O
) O
By O
modeling O
IE B-TaskName
as O
joint O
span B-TaskName
detection I-TaskName
, O
classification B-TaskName
and O
association B-TaskName
, O
and O
encoding O
the O
schema O
- O
based O
prompt O
and O
input O
texts O
with O
the O
triaffine O
attention O
mechanism O
, O
UniEX B-MethodName
provides O
an O
effective O
universal O
extractive O
backbone O
for O
all O
IE B-TaskName
tasks O
. O
The O
UniEX B-MethodName
outperforms O
the O
universal O
IE O
models O
with O
approximate O
backbone O
sizes O
, O
achieving O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
almost O
all O
tasks O
and O
datasets O
. O
2 O
) O
The O
introduction O
of O
label O
- O
based O
schema O
facilitates O
the O
model O
learning O
task O
- O
relevant O
knowledge O
, O
while O
the O
triaffine O
scoring O
matrix O
establishes O
the O
correspondence O
between O
each O
schema O
and O
extraction O
targets O
. O
Obviously O
, O
the O
UniEX B-MethodName
can O
better O
capture O
and O
share O
label O
semantics O
than O
using O
generative O
structures O
to O
encode O
underlying O
information O
. O
Meanwhile O
, O
triaffine O
transformation O
is O
a O
unified O
and O
cross O
- O
task O
adaptive O
operation O
, O
precisely O
controlling O
where O
to O
detect O
and O
which O
to O
associate O
in O
all O
IE B-TaskName
tasks O
. O
Compared O
with O
the O
TANL B-MethodName
and O
UIE B-MethodName
, O
our O
approach O
achieves O
significant O
performance O
improvement O
on O
most O
datasets O
, O
with O
nearly O
1.36 B-MetricValue
% I-MetricValue
and O
1.52 B-MetricValue
% I-MetricValue
F1 B-MetricName
on O
average O
, O
respectively O
. O

Experiments O
on O
Low O
- O
resource O
Scenarios O

To O
verify O
the O
generalization O
and O
transferability O
of O
UniEX B-MethodName
in O
low O
- O
resource O
scenarios O
, O
we O
evaluate O
models O
under O
few O
- O
shot O
and O
zero O
- O
shot O
settings O
, O
respectively O
. O
In O
order O
to O
reduce O
the O
influence O
of O
noise O
caused O
by O
random O
sampling O
on O
the O
experiment O
results O
, O
we O
repeat O
the O
data O
/ O
label O
selection O
processes O
for O
five O
different O
random O
seeds O
and O
report O
the O
averaged O
experiment O
results O
as O
previous O
works O
( O
Hou O
et O
al O
. O
, O
2020 O
; O
Chia O
et O
al O
. O
, O
2022 O
) O
. O
We O
use O
the O
BERTbase B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
the O
UniEX B-MethodName
backbone O
to O
align O
with O
other O
low O
- O
resource O
results O
. O

Firstly O
, O
we O
compare O
the O
UniEX B-MethodName
with O
the O
competitive O
few O
- O
shot O
entity O
extraction O
models O
. O
For O
FewN B-DatasetName
- I-DatasetName
ERD I-DatasetName
, O
we O
compare O
the O
proposed O
approach O
to O
De- B-MethodName
comMeta I-MethodName
( O
Ma O
et O
al O
. O
, O
2022b O
) O
, O
ESD B-MethodName
, O
and O
methods O
from O
( O
Ding O
et O
al O
. O
, O
2021 O
) O
, O
e.g. O
, O
ProtoBERT B-MethodName
, O
NNShot B-MethodName
. O
For O
Cross B-DatasetName
- I-DatasetName
Dataset I-DatasetName
, O
we O
compare O
the O
UniEX B-MethodName
to O
DecomMeta B-MethodName
( O
Ma O
et O
al O
. O
, O
2022b O
) O
and O
baselines O
reported O
by O
( O
Hou O
et O
al O
. O
, O
2020 O
) O
, O
e.g. O
, O
TransferBERT B-MethodName
, O
Matching B-MethodName
Network I-MethodName
, O
ProtoBERT B-MethodName
and O
L B-MethodName
- I-MethodName
TapNet+CDT I-MethodName
. O

Table O
2 O
and O
3 O
illustrates O
the O
main O
results O
on O
FewNERD B-DatasetName
and O
Cross B-DatasetName
- I-DatasetName
Dataset I-DatasetName
of O
our O
approach O
alongside O
those O
reported O
by O
previous O
methods O
. O
It O
can O
be O
seen O
that O
UniEX B-MethodName
achieves O
the O
best O
performance O
under O
different O
type O
granularity O
and O
domain O
divisions O
, O
and O
outperforms O
the O
prior O
methods O
with O
a O
large O
margin O
. O
Compare O
with O
DecomMeta B-MethodName
on O
Cross B-DatasetName
- I-DatasetName
Dataset I-DatasetName
, O
UniEX B-MethodName
achieves O
a O
performance O
improvement O
up O
to O
6.94 B-MetricValue
% I-MetricValue
and O
5.63 B-MetricValue
% I-MetricValue
F1 B-MetricName
scores O
on O
average O
in O
1 O
- O
shot O
and O
5 O
- O
shot O
, O
which O
demonstrates O
the O
effectiveness O
of O
our O
approach O
in O
learning O
general O
IE B-TaskName
knowledge O
. O
It O
indicates O
that O
even O
without O
pre O
- O
training O
on O
large O
- O
scale O
corpus O
, O
our O
approach O
can O
still O
sufficiently O
excavate O
the O
semantic O
information O
related O
with O
objective O
entities O
from O
label O
names O
, O
which O
enhances O
the O
understanding O
of O
task O
- O
specific O
information O
when O
data O
is O
extremely O
scarce O
. O

Secondly O
, O
we O
compare O
UniEX B-MethodName
with O
the O
latest O
baselines O
TableSequence B-MethodName
( O
Wang O
and O
Lu O
, O
2020 O
) O
and O
RelationPrompt B-MethodName
( O
Chia O
et O
al O
. O
, O
2022 O
) O
on O
zero B-TaskName
- I-TaskName
shot I-TaskName
relation I-TaskName
triplet I-TaskName
extraction I-TaskName
task O
for O
Wiki B-DatasetName
- I-DatasetName
ZSL I-DatasetName
and O
Few B-DatasetName
- I-DatasetName
Rel I-DatasetName
datasets O
in O
Table O
4 O
. O
In O
both O
single O
- O
triplet O
and O
multi O
- O
triplet O
evaluation O
, O
UniEX B-MethodName
consistently O
outperforms O
the O
baseline O
models O
in O
terms O
of O
Accuracy B-MetricName
and O
overall O
F1 B-MetricName
score O
respectively O
, O
which O
demonstrates O
the O
ability O
of O
our O
approach O
to O
handle O
unseen O
labels O
. O
Although O
we O
observe O
a O
lack O
of O
advantage O
in O
recall O
score O
for O
multi O
- O
triplet O
evaluation O
, O
the O
significant O
improvement O
in O
precision O
allowed O
our O
approach O
to O
achieve O
a O
balanced O
precision B-MetricName
- I-MetricName
recall I-MetricName
ratio I-MetricName
. O
The O
reason O
for O
such O
difference O
is O
probably O
because O
the O
directional O
matching O
in O
the O
triaffine O
transformation O
will O
tend O
to O
guide O
the O
model O
to O
predict O
more O
credible O
targets O
. O

Ablation O
Study O

In O
this O
section O
, O
we O
intend O
to O
verify O
the O
necessity O
of O
key O
components O
of O
the O
UniEX B-MethodName
, O
including O
the O
flow O
controlling O
and O
triaffine O
transformation O
. O
tasks O
, O
which O
demonstrates O
the O
importance O
of O
eliminating O
intra O
- O
information O
of O
labels O
. O
AMM B-MethodName
makes O
the O
labels O
unreachable O
to O
each O
other O
, O
effectively O
avoiding O
the O
mutual O
interference O
of O
label O
semantics O
. O
W B-MethodName
/ I-MethodName
O I-MethodName
TriA I-MethodName
: O
replacing O
the O
triaffine O
transformation O
with O
the O
multi O
- O
head O
selection O
network O
, O
which O
multiplies O
the O
schema O
and O
the O
head O
- O
to O
- O
tail O
span O
of O
the O
text O
respectively O
, O
and O
then O
replicates O
and O
adds O
them O
to O
get O
the O
scoring O
matrix O
. O
The O
significant O
performance O
decline O
demonstrates O
the O
important O
role O
of O
triaffine O
attention O
mechanism O
in O
establishing O
dense O
correspondence O
between O
schemas O
and O
text O
spans O
. O
W B-MethodName
/ I-MethodName
O I-MethodName
Label I-MethodName
: O
replacing O
the O
label O
names O
with O
the O
special O
token O
[ O
unused O
n O
] O
, O
which O
eliminates O
label O
semantics O
while O
allowing O
the O
model O
to O
still O
distinguish O
between O
different O
labels O
. O
We O
find O
a O
slight O
degradation O
of O
model O
performance O
in O
small O
datasets O
CoNLL03 B-DatasetName
and O
16 B-DatasetName
- I-DatasetName
res I-DatasetName
, O
indicating O
that O
the O
prior O
knowledge O
provided O
by O
label O
names O
can O
effectively O
compensate O
for O
the O
deficiency O
of O
training O
data O
. O
As O
the O
correspondence O
between O
schema O
and O
extraction O
targets O
is O
not O
affected O
, O
model O
performance O
in O
large O
datasets O
tends O
to O
stabilize O
. O

Efficiency O
Analysis O

To O
verify O
the O
computation O
efficiency O
of O
our O
approach O
on O
universal O
IE B-TaskName
, O
we O
compare O
inferencespeed B-MetricName
with O
UIE B-MethodName
( O
Lu O
et O
al O
. O
, O
2022 O
) O
on O
the O
four O
standard O
datasets O
mentioned O
in O
section O
4.4 O
. O
As O
shown O
in O
Table O
6 O
, O
we O
can O
find O
that O
since O
generating O
the O
target O
structure O
is O
a O
token O
- O
wise O
process O
, O
the O
inferencespeed B-MetricName
of O
UIE B-MethodName
is O
slow O
and O
limited O
by O
the O
length O
of O
the O
target O
structure O
. O
On O
the O
contrary O
, O
UniEX B-MethodName
can O
decode O
all O
the O
target O
structures O
at O
once O
from O
the O
scoring O
matrices O
obtained O
by O
triaffine O
transformation O
, O
with O
an O
average B-MetricName
speedup I-MetricName
ratio I-MetricName
of O
13.3 B-MetricValue
to O
UIE B-MethodName
. O

Conclusion O

In O
this O
paper O
, O
we O
introduce O
a O
new O
paradigm O
for O
universal O
IE B-TaskName
by O
converting O
all O
IE B-TaskName
tasks O
into O
joint O
span B-TaskName
detection I-TaskName
, O
classification B-TaskName
and O
association B-TaskName
problems O
with O
a O
unified O
extractive O
framework O
. O
UniEX B-MethodName
collaboratively O
learns O
the O
generalized O
knowledge O
from O
schema O
- O
based O
prompts O
and O
controls O
the O
correspondence O
between O
schema O
and O
extraction O
targets O
via O
the O
triaffine O
attention O
mechanism O
. O
Experiments O
on O
both O
supervised O
setting O
and O
low O
- O
resource O
scenarios O
verify O
the O
transferability O
and O
effectiveness O
of O
our O
approaches O
. O

Limitations O

In O
this O
paper O
, O
our O
main O
contribution O
is O
an O
effective O
and O
efficient O
framework O
for O
universal B-TaskName
IE I-TaskName
. O
We O
aim O
to O
introduce O
a O
new O
unified O
IE O
paradigm O
with O
extractive O
structures O
and O
triaffine O
attention O
mechanism O
, O
which O
can O
achieve O
better O
performance O
in O
a O
variety O
of O
tasks O
and O
scenarios O
with O
more O
efficient O
inferencespeed O
. O
However O
, O
it O
is O
non O
- O
trivial O
to O
decide O
whether O
a O
sophisticated O
and O
artificial O
prompt O
is O
required O
for O
complex O
datasets O
and O
large O
label O
sets O
. O
In O
addition O
, O
we O
only O
compare O
with O
limited O
baselines O
with O
specific O
datasets O
configurations O
when O
analyzing O
the O
performance O
of O
the O
UniEX B-MethodName
in O
supervised O
, O
few O
- O
shot O
and O
zero O
- O
shot O
settings O
. O
In O
experiments O
, O
we O
implement O
only O
a O
few O
comparative O
experiments O
between O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
due O
to O
the O
limit O
of O
computational O
resources O
. O

Ethical O
Considerations O

As O
an O
important O
domain O
of O
natural O
language O
processing O
, O
information B-TaskName
extraction I-TaskName
is O
a O
common O
technology O
in O
our O
society O
. O
It O
is O
necessary O
to O
discuss O
the O
ethical O
influence O
when O
using O
the O
extraction O
models O
( O
Leidner O
and O
Plachouras O
, O
2017 O
) O
. O
In O
this O
work O
, O
We O
develop O
a O
new O
universal O
IE O
framework O
, O
which O
enhances O
the O
generalization O
ability O
in O
various O
scenarios O
. O
As O
discussed O
( O
Schramowski O
et O
al O
. O
, O
2019 O
( O
Schramowski O
et O
al O
. O
, O
, O
2022Blodgett O
et O
al O
. O
, O
2020 O
) O
, O
pre O
- O
trained O
LMs O
might O
contain O
human O
- O
made O
biases O
, O
which O
might O
be O
embedded O
in O
both O
the O
parameters O
and O
outputs O
of O
the O
open O
- O
source O
models O
. O
In O
addition O
, O
we O
note O
the O
potential O
abuse O
of O
universal O
IE O
models O
, O
as O
these O
models O
achieve O
excellent O
performance O
in O
various O
domains O
and O
settings O
after O
adapting O
to O
pre O
- O
training O
on O
largescale O
IE B-TaskName
datasets O
, O
which O
allows O
the O
models O
to O
be O
integrated O
into O
applications O
often O
without O
justification O
. O
We O
encourage O
open O
debating O
on O
its O
utilization O
, O
such O
as O
the O
task O
selection O
and O
the O
deployment O
, O
hoping O
to O
reduce O
the O
chance O
of O
any O
misconduct O
. O

A O
Experiment O
Details O

This O
section O
describes O
the O
details O
of O
experiments O
, O
including O
the O
dataset O
descriptions O
, O
unified O
EX O
input O
formats O
, O
metrics O
and O
training O
implementation O
. O

We O
use O
span O
- O
based O
offset O
Micro B-MetricName
- I-MetricName
F1 I-MetricName
as O
the O
primary O
metric O
to O
evaluate O
the O
model O
as O
( O
Lu O
et O
al O
. O
, O
2022 O
) O
• O
Entity O
: O
an O
entity O
mention O
is O
correct O
if O
its O
offsets O
and O
type O
match O
a O
reference O
entity O
. O
• O
Sentiment O
Triplet O
: O
a O
correct O
triplet O
requires O
the O
offsets O
boundary O
of O
the O
target O
, O
the O
offsets O
boundary O
of O
the O
opinion O
span O
, O
and O
the O
target O
sentiment O
polarity O
to O
be O
all O
correct O
at O
the O
same O
time O
. O

To O
make O
a O
fair O
comparison O
, O
we O
first O
initialize O
UniEX B-MethodName
- I-MethodName
base I-MethodName
and O
UniEX B-MethodName
- I-MethodName
large I-MethodName
with O
RoBERTa B-MethodName
- I-MethodName
base I-MethodName
and O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
checkpoints O
( O
Liu O
et O
al O
. O
, O
2019b O
) O
for O
the O
supervised O
setting O
, O
and O
use O
the O
BERT B-MethodName
- I-MethodName
base I-MethodName
checkpoint O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
the O
backbone O
for O
the O
few O
- O
shot O
and O
zero O
- O
shot O
settings O
. O
The O
model O
architectures O
are O
shown O
in O
Table O
9 O
. O
We O
employ O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
as O
the O
optimizer B-HyperparameterName
with O
1e-8 B-HyperparameterValue
weight B-HyperparameterName
decay I-HyperparameterName
. O
Table O
8 O
shows O
the O
detailed O
hyper O
- O
parameters O
for O
downstream O
tasks O
. O
We O
truncate O
the O
concatenated O
overall O
length O
of O
schemabased O
prompt O
s O
and O
raw O
text O
x O
to O
512 B-HyperparameterValue
during O
training O
. O

Introduction O

Overall O
, O
we O
find O
that O
LMs O
can O
capture O
aspects O
of O
symbolic O
knowledge O
, O
with O
the O
newer O
, O
larger O
models O
significantly O
outperform O
their O
previous O
iterations O
. O
Surprisingly O
, O
advanced O
LMs O
performed O
better O
on O
conventional O
symbolism O
( O
more O
idiomatic O
) O
than O
symbolism O
in O
ads O
( O
more O
semantically O
related O
) O
, O
where O
they O
fared O
significantly O
worse O
than O
Word2Vec B-MethodName
. O
This O
reveals O
the O
negative O
impact O
of O
the O
hypothesized O
bias O
in O
pre O
- O
training O
corpora O
. O
We O
demonstrate O
that O
the O
proposed O
debiasing O
method O
improves O
performance O
; O
the O
increase O
is O
the O
most O
dramatic O
for O
situated O
ads O
symbols O
( O
e.g. O
, O
RoBERTa B-MethodName
improved O
by O
260 B-MetricValue
% I-MetricValue
) O
. O
After O
reranking O
, O
GPT B-MethodName
- I-MethodName
J I-MethodName
and O
RoBERTa B-MethodName
achieve O
performances O
comparable O
to O
human B-MethodName
on O
the O
multiple B-TaskName
choice I-TaskName
task O
. O
Further O
analyses O
suggest O
LMs O
perform O
better O
on O
explicit O
relationships O
such O
as O
UsedFor O
than O
implicit O
ones O
, O
and O
the O
debiased O
models O
are O
sufficiently O
robust O
with O
respect O
to O
the O
probing O
prompts O
. O
3 O
2 O
Both O
datasets O
are O
predominantly O
expressed O
in O
English O
so O
that O
our O
studies O
focus O
on O
the O
interpretation O
of O
symbolism O
from O
a O
Western O
cultural O
perspective O
( O
Jones O
, O
1918 O
) O
; O
this O
Englishdominant O
scope O
matches O
the O
pre O
- O
training O
corpora O
of O
LMs O
. O

A O
Implementation O
Details O
T2D B-MethodName
Parameters O
In O
practice O
, O
we O
initialize O
all O
weights O
of O
our O
proposed O
baseline O
method O
from O
T5base B-MethodName
18 O
. O
In O
training O
, O
we O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
, O
ϵ B-HyperparameterName
= O
1e B-HyperparameterValue
− I-HyperparameterValue
08 I-HyperparameterValue
to O
update O
the O
model O
parameters O
. O
We O
fine O
- O
tune O
our O
model O
on O
3 O
RTX O
8000 O
GPUs O
with O
batch B-HyperparameterName
size I-HyperparameterName
12 B-HyperparameterValue
and O
learning B-HyperparameterName
rate I-HyperparameterName
5e B-HyperparameterValue
− I-HyperparameterValue
4 I-HyperparameterValue
for O
20 B-HyperparameterValue
epochs B-HyperparameterName
. O
Balcony O
2 O
is O
located O
in O
the O
most O
southern O
point O
of O
the O
floorplan O
, O
just O
east O
of O
the O
master O
room O
. O
It O
is O
roughly O
12 O
feet O
in O
length O
and O
five O
feet O
in O
width O
. O
Access O
points O
include O
the O
livingroom O
and O
master O
room O
. O

Restricted O

Restricted O

Introduction O

Experiments O

Results O

Conclusion O

i O
, O
|s O
i O
t O
| O
t O

B O
Experimental O
Settings O

B.1 O
Dataset O

B.2 O
Victim O
Models O

B.3 O
Evaluation O
Metrics O

Introduction O

The O
tower O
was O
open O
to O
the O
public O
in O
October O
2018 O
. O

Philadelphia O
, O
known O
colloquially O
as O
Philly O
, O
is O
the O
largest O
city O
in O
the O
U.S. O
state O
and O
Commonwealth O
of O
Pennsylvania O
and O
the O
sixth O
- O
most O
populous O
U.S. O
city O
with O
a O
2018 O
census O
- O
estimated O
population O
of O
1,584,138 O
... O

The O
Aon O
Center O
( O
200 O
East O
Randolph O
Street O
, O
formerly O
Amoco O
Building O
) O
is O
a O
modern O
supertall O
skyscraper O
just O
east O
of O
the O
Chicago O
Loop O
, O
Chicago O
, O
Illinois O
, O
United O
States O
, O
designed O
by O
architect O
... O
. O
Chicago O
( O
/ O
ʃɪˈkɑːɡoʊ O
/ O
( O
listen O
) O
, O
locally O
also O
/ O
ʃɪˈkɔːɡoʊ O
/ O
) O
, O
officially O
the O
City O
of O
Chicago O
, O
... O
With O
an O
estimated O
population O
of O
2 O
, O
705,994 O
( O
2018 O
) O
, O
it O
is O
also O
the O
most O
populous O
city O
in O
the O
Midwestern O
United O
States O
. O
... O
. O
New O
York O
City O
( O
NYC O
) O
, O
also O
known O
as O
the O
City O
of O
New O
York O
or O
simply O
New O
York O
( O
NY O
) O
, O
is O
the O
most O
populous O
city O
in O
the O
United O
States O
. O
With O
an O
estimated O
2018 O
population O
of O
8,398,748 O
distributed O
over O
a O
land O
area O
of O
... O
with O
an O
estimated O
19,979,477 O
people O
in O
its O
2018 O
metropolitan O
statistical O
area O
and O
22,679,948 O
residents O
in O
its O
... O

Related O
Work O

Task O
Definition O

System O
Overview O

Experiments O

Datasets O

Conclusion O

Limitations O

B O
More O
Details O
on O
Experiments O

B.1 O
Datasets O

B.2 O
Baselines O
and O
Competing O
Methods O

B.3 O
Implementation O
Details O

MITQA B-MethodName
is O
implemented O
using O
Pytorch O
version O
1.8 O
and O
Huggingface O
's O
transformers O
3 O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
library O
. O
We O
train O
our O
models O
using O
two O
NVIDIA O
A100 O
GPUs O
. O
We O
train O
the O
row O
retriever O
and O
answer O
extractor O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
and O
select O
the O
best O
model O
based O
on O
dev O
fold O
performance O
. O
We O
optimize O
the O
model O
parameters O
using O
AdamW O
algorithm O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5×10 B-HyperparameterValue
−5 I-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
24 B-HyperparameterValue
. O
We O
set O
per O
- O
GPU O
train O
batch B-MetricName
size I-MetricName
to O
16 B-HyperparameterValue
while O
training O
the O
answer O
extractor O
. O
We O
evaluate O
final O
answers O
using O
EM B-MetricName
( O
exact B-MetricName
match I-MetricName
) O
and O
F1 B-MetricName
metrics O
. O

Hyperparameter O
Details O
: O
We O
tune O
hyperparameters O
based O
on O
loss O
on O
validation O
set O
. O
We O
use O
the O
following O
range O
of O
values O
for O
selecting O
the O
best O
hyper O
- O
parameter O
• O
Batch B-HyperparameterName
Size I-HyperparameterName
: O
8 B-HyperparameterValue
, O
16 B-HyperparameterValue
, O
32 B-HyperparameterValue
• O
Learning B-HyperparameterName
Rate I-HyperparameterName
: O
1e-3 B-HyperparameterValue
, O
1e-4 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
, O
1e-6 B-HyperparameterValue
, O
3e-3 B-HyperparameterValue
, O
3e-4 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
3e-6 B-HyperparameterValue
, O
5e-3 B-HyperparameterValue
, O
5e-4 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
, O
5e-6 B-HyperparameterValue

Introduction O

Related O
Work O

where O
S O
( O
T O
) O
i O
, O
j O
is O
the O
importance O
score O
corresponding O
to O
W O
( O
T O
) O
i O
, O
j O
after O
T O
steps O
update O
, O
L O
and O
α B-HyperparameterName
w I-HyperparameterName
are O
learning O
objective O
and O
learning B-HyperparameterName
rate I-HyperparameterName
of O
W O
i O
, O
j O
. O
Magnitude O
pruning O
selects O
weights O
with O
high O
absolute O
values O
during O
fine O
- O
tuning O
. O

where O
α B-HyperparameterName
s O
is O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
S. O
Compared O
to O
magnitude O
pruning O
, O
movement O
pruning O
selects O
weights O
that O
are O
increasing O
their O
absolute O
value O
. O

Experiments O

Datasets O

To O
show O
the O
effectiveness O
of O
our O
method O
, O
we O
use O
three O
common O
benchmarks O
: O
nature B-TaskName
language I-TaskName
inference I-TaskName
( O
MNLI B-DatasetName
) O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
question B-TaskName
similarity I-TaskName
( O
QQP B-DatasetName
) O
( O
Aghaebrahimian O
, O
2017 O
) O
and O
question B-TaskName
answering I-TaskName
( O
SQuAD B-DatasetName
) O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
following O
Sanh O
et O
al O
. O
Moreover O
, O
we O
also O
use O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
to O
validate O
the O
performance O
of O
our O
method O
at O
low O
sparsity O
. O

Following O
previous O
pruning O
methods O
, O
we O
use O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
to O
perform O
task B-TaskName
- I-TaskName
specific I-TaskName
pruning I-TaskName
and O
report O
the O
ratio O
of O
remaining O
weight O
in O
the O
encode O
. O
For O
the O
task O
- O
specific O
head O
, O
we O
initial O
it O
according O
to O
the O
label O
words O
of O
each O
task O
following O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
. O
For O
SQuAD B-DatasetName
, O
we O
use O
" O
yes O
" O
and O
" O
no O
" O
token O
embeddings O
as O
the O
weights O
for O
starting O
and O
ending O
the O
classification O
of O
answers O
. O
We O
freeze O
all O
weights O
of O
BERT B-MethodName
including O
the O
task O
- O
specific O
head O
and O
only O
fine O
- O
tuning O
mask O
. O
The O
optimizer B-HyperparameterName
is O
Adam O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-2 B-HyperparameterValue
. O
The O
hyperparameter O
λ B-HyperparameterName
R I-HyperparameterName
of O
the O
regularization O
term O
is O
400 B-HyperparameterValue
. O
We O
set O
12 B-HyperparameterValue
epochs B-HyperparameterName
for O
MNLI B-DatasetName
and O
QQP B-DatasetName
, O
and O
10 B-HyperparameterValue
epochs B-HyperparameterName
for O
SQuAD B-DatasetName
with O
bath B-HyperparameterName
size I-HyperparameterName
64 B-HyperparameterValue
. O
For O
tasks O
at O
low O
sparsity O
( O
more O
than O
70 O
% O
remaining O
weights O
) O
, O
we O
set O
N B-HyperparameterName
in O
cubic O
sparsity O
scheduling O
to O
7 B-HyperparameterValue
epochs B-HyperparameterName
. O
For O
tasks O
at O
high O
sparsity O
, O
we O
set O
N B-HyperparameterName
to O
3500 B-HyperparameterValue
steps B-HyperparameterName
. O

We O
also O
report O
the O
performance O
of O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
and O
roberta B-MethodName
- I-MethodName
base I-MethodName
with O
80 O
% O
remaining O
weights O
for O
all O
tasks O
on O
GLUE B-DatasetName
with O
the O
same O
batch B-HyperparameterName
size I-HyperparameterName
and O
learning B-HyperparameterName
rate I-HyperparameterName
as O
above O
. O
For O
sparsity O
scheduling O
, O
we O
use O
the O
same O
scheduling O
for O
bert B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
and O
a O
linear O
scheduling O
for O
roberta B-MethodName
- I-MethodName
base I-MethodName
. O
N B-HyperparameterName
in O
sparsity O
scheduling O
is O
3500 B-HyperparameterValue
. O
For O
the O
large O
tasks O
: O
MNLI B-TaskName
, O
QQP B-TaskName
, O
SST2 B-TaskName
and O
QNLI B-TaskName
, O
we O
use O
12 B-HyperparameterValue
epochs B-HyperparameterName
. O
For O
the O
small O
tasks O
: O
MRPC B-TaskName
, O
RTE B-TaskName
, O
STS B-TaskName
- I-TaskName
B I-TaskName
and O
COLA B-TaskName
, O
we O
use O
60 B-HyperparameterValue
epochs B-HyperparameterName
. O
Note O
that O
the O
above O
epochs B-HyperparameterName
have O
included O
pruning O
steps O
. O
For O
example O
, O
we O
use O
around O
43 B-HyperparameterValue
epochs B-HyperparameterName
to O
achieve O
target O
sparsity O
in O
MRPC B-TaskName
. O
We O
search O
the O
pruning O
structure O
from O
local O
and O
global O
. O

Baseline O

Experimental O
Results O

Following O
previous O
works O
, O
we O
also O
report O
the O
results O
with O
knowledge O
distillation O
in O
Table O
1 O
. O
The O
improvement O
brought O
by O
knowledge O
distillation O
is O
also O
evident O
in O
SMP B-MethodName
- I-MethodName
L I-MethodName
and O
SMP B-MethodName
- I-MethodName
S. I-MethodName
For O
example O
, O
it O
improves O
the O
F1 B-MetricName
of O
SQuAD B-DatasetName
by O
3 B-MetricValue
. I-MetricValue
3 I-MetricValue
and O
4 B-MetricName
( O
Sanh O
et O
al O
. O
, O
2020 O
) O
, O
the O
trainable O
parameters O
of O
first O
- O
order O
methods O
are O
the O
sum O
of O
BERT O
encoder O
( O
85 O
M O
) O
, O
importance O
scores O
S O
( O
85 O
M O
) O
and O
task O
- O
specific O
head O
( O
less O
than O
0.01 O
M O
) O
. O
For O
zero O
- O
order O
pruning O
methods O
like O
magnitude O
pruning O
, O
the O
trainable O
parameters O
are O
85 O
M O
, O
excluding O
S. O
Our O
results O
are O
averaged O
from O
five O
random O
seeds O
. O

Analysis O

For O
training O
from O
scratch O
, O
we O
randomly O
initial O
the O
head O
and O
fine O
- O
tune O
it O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
3e-5 B-HyperparameterValue
following O
previous O
pruning O
methods O
. O
Results O
show O
our O
method O
achieves O
better O
performance O
with O
task O
- O
specific O
heads O
frozen O
. O

Conclusion O

No O
response O
. O

Acknowledgments O

A O
Standard O
Deviation O
of O
Tasks O

Approaches O

w O
′ O
= O
{ O
w O
′ O
1 O
, O
• O
• O
• O
, O
w O
′ O
m O
} O

a O
= O
{ O
a O
1 O
, O
a O
2 O
, O
• O
• O
• O
, O
a O
l O
} O
and O
o O
= O
{ O
o O
1 O
, O
o O
2 O
, O
• O
• O
• O
, O
o O
l O
} O

{ O
r O
′ O
1 O
, O
• O
• O
• O
, O
r O
′ O
n O
} O
. O

Experiments O

We O
conduct O
experiments O
on O
two O
MNER B-TaskName
datasets O
. O

Settings O

Results O

Comparison O
with O
Other O
Variants O

A.3 O
Case O
Study O

A.4 O
Discussion O

Acknowledgements O

A O
Appendix O

Introduction O

Experiment O

Experimental O
Setup O

Conclusion O
and O
Future O
Works O

Acknowledgement O

Introduction O

Datasets O

Models O
and O
experimental O
setting O

Results O

Related O
Work O

Discussion O

C O
Training O

Acknowledgements O

Introduction O

Methodology O

m O
l O
i O
, O
j O
∼ O
B O
( O
σ O
( O
g O
l O
i O
, O
j O
) O
) O
( O
1 O
) O

Training O
and O
Inference O

Related O
Work O

Conclusion O

Acknowledgement O

x−y O
≤ O
1 O
4 O
. O
Proof O
: O

Hyper O
- O
Parameters O
. O
Following O
Hasan O
et O
al O
. O
( O
2021 O
) O
, O
we O
use O
the O
base O
5 O
model O
of O
mT5 O
( O
Xue O
et O
al O
. O
, O
2021 O
) O
, O
in O
which O
L B-HyperparameterName
= O
12 B-HyperparameterValue
for O
both O
encoder O
and O
decoder O
. O
For O
the O
vision O
- O
related O
hyper O
- O
parameters O
mentioned O
in O
§ O
2.2 O
, O
we O
follow O
Yu O
et O
al O
. O
( O
2021a O
) O
for O
a O
fair O
comparison O
. O
Specifically O
, O
we O
use O
a O
4 O
- O
layer O
encoder O
( O
i.e. O
, O
H B-HyperparameterName
= O
4 B-HyperparameterValue
) O
with O
8 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
and O
a O
2048 B-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
dimension I-HyperparameterName
. O
For O
all O
models O
, O
the O
dropout B-HyperparameterName
is O
set O
to O
0.1 B-HyperparameterValue
and O
the O
label B-HyperparameterName
smoothing I-HyperparameterName
is O
set O
to O
0.1 B-HyperparameterValue
. O
The O
d B-HyperparameterName
, O
d B-HyperparameterName
c I-HyperparameterName
, O
and O
d B-HyperparameterName
v I-HyperparameterName
are O
768 B-HyperparameterValue
, O
256 B-HyperparameterValue
, O
and O
2048 B-HyperparameterValue
, O
respectively O
. O
The O
balancing B-HyperparameterName
factor I-HyperparameterName
α B-HyperparameterName
and O
β B-HyperparameterName
in O
Eq O
. O
5 O
are O
set O
to O
1.0 B-HyperparameterValue
, O
which O
are O
not O
tuned O
. O
The O
K B-HyperparameterName
of O
Eq O
. O
6 O
is O
29 B-HyperparameterValue
, O
which O
is O
the O
sum O
of O
the O
number O
of O
mid O
- O
highand O
low O
- O
resource O
languages O
. O
During O
the O
monolingual O
training O
, O
we O
train O
all O
models O
on O
each O
language O
separately O
for O
6 B-HyperparameterValue
- I-HyperparameterValue
20 I-HyperparameterValue
epochs B-HyperparameterName
( O
since O
the O
total O
training O
samples O
were O
limited O
, O
we O
had O
to O
be O
careful O
to O
prevent O
overfitting O
) O
on O
an O
NVIDIA O
Tesla O
V100 O
GPU O
5 O
https O
: O
/ O
/ O
huggingface.co O
/ O
google O
/ O
mt5-base O
/ O
tree O
/ O
main O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
. O
The O
models O
are O
optimized O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.998 B-HyperparameterValue
. O
We O
train O
all O
model O
weights O
with O
a O
slanted O
learning B-HyperparameterName
rate I-HyperparameterName
schedule O
( O
learning B-HyperparameterName
rate I-HyperparameterName
to O
5e-4 B-HyperparameterValue
) O
. O
During O
the O
multilingual O
training O
, O
following O
a O
similar O
training O
strategy O
( O
Conneau O
and O
Lample O
, O
2019 O
; O
Hasan O
et O
al O
. O
, O
2021 O
) O
, O
we O
sample O
each O
batch O
from O
a O
single O
language O
containing O
256 B-HyperparameterValue
samples O
and O
use O
a O
smoothing B-HyperparameterName
factor I-HyperparameterName
( O
0.5 B-HyperparameterValue
) O
so O
that O
batches O
of O
low O
- O
resource O
languages O
would O
be O
sampled O
at O
a O
higher O
rate O
, O
increasing O
their O
frequency O
during O
training O
. O
We O
set O
the O
training B-HyperparameterName
step I-HyperparameterName
to O
35,000 B-HyperparameterValue
steps O
on O
a O
distributed O
cluster O
of O
8 O
NVIDIA O
Tesla O
V100 O
GPUs O
and O
trained O
about O
5 O
days O
. O
We O
use O
the O
Adafactor O
optimizer B-HyperparameterName
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
with O
a O
linear O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
of O
5,000 B-HyperparameterValue
steps O
and O
the O
" O
inverse O
square O
root O
" O
learning B-HyperparameterName
rate I-HyperparameterName
schedule O
. O

We O
employ O
the O
POINTER B-MethodName
- I-MethodName
GEN I-MethodName
implemented O
by O
OpenNMT O
( O
Klein O
et O
al O
. O
, O
2017 O
) O
. O
POINTER B-MethodName
- I-MethodName
GEN I-MethodName
is O
built O
based O
on O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O
We O
set O
the O
layers B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
encoder I-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
as O
2 B-HyperparameterValue
and O
1 B-HyperparameterValue
, O
respectively O
. O
And O
we O
set O
the O
embedding B-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
hidden I-HyperparameterName
size I-HyperparameterName
as O
512 B-HyperparameterValue
. O
T5based B-MethodName
methods O
are O
implemented O
using O
Hugging O
- O
Face O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
and O
inintilized O
by O
T5 B-MethodName
base I-MethodName
3 O
. O
And O
the O
hidden B-HyperparameterName
size I-HyperparameterName
of O
the O
GAT O
layer O
in O
the O
Local O
Node O
Encoder O
is O
set O
to O
512 B-HyperparameterValue
. O
For O
T5 B-MethodName
- I-MethodName
based I-MethodName
methods O
, O
we O
set O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
0.1 B-HyperparameterValue
, O
use O
AdamW O
optimizer B-HyperparameterName
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
and O
employ O
a O
linear O
learning B-HyperparameterName
rate I-HyperparameterName
decay O
schedule O
without O
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
. O
We O
use O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
for O
the O
early O
stopping O
criterion O
. O
Moreover O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
3e-5 B-HyperparameterValue
and O
batch B-HyperparameterName
size I-HyperparameterName
is O
4 B-HyperparameterValue
for O
all O
experiments O
. O
During O
decoding O
, O
we O
employ O
beam O
search O
with O
a O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
. O
All O
experiments O
are O
trained O
on O
Nvidia O
Tesla O
V100 O
32 O
GB O
GPUs O
. O

A O
Appendix O

A.3 O
Implementation O
Details O

A.5 O
Queries O

Introduction O

In O
doing O
so O
, O
we O
argue O
that O
our O
approach O
to O
pretraining O
implicitly O
incentivizes O
the O
model O
to O
discover O
and O
encode O
discrete O
character O
classes O
in O
its O
internal O
representations O
, O
while O
ignoring O
style O
differences O
occurring O
in O
lines O
using O
different O
fonts O
or O
languages O
, O
or O
authored O
by O
other O
scribes O
. O

Related O
Work O

Approach O

Model O

Datasets O

Trove B-DatasetName
A O
dataset O
of O
historic O
Australian O
newspapers O
( O
c. O
1803 O
- O
1954 O
) O
from O
the O
National O
Library O
of O
Australia O
( O
Holley O
, O
2010 O
) O
. O
We O
use O
the O
manually O
transcribed O
version O
totaling O
450 O
lines O
( O
Berg O
- O
Kirkpatrick O
et O
al O
. O
, O
2013 O
) O
. O

Old O
Bailey O
A O
manually O
transcribed O
set O
of O
20 O
documents O
printed O
1716 O
- O
1906 O
, O
consisting O
of O
30 O
lines O
per O
document O
, O
taken O
from O
Berg O
- O
Kirkpatrick O
and O
Klein O
( O
2014 O
) O
. O
Shoemaker O
( O
2005 O
) O
compiled O
the O
documents O
, O
which O
describe O
proceedings O
of O
London O
's O
Old O
Bailey O
Courthouse O
. O

Experiments O

Experimental O
Details O

Results O

Conclusion O

In O
this O
paper O
, O
we O
proposed O
a O
two O
- O
phase O
pretrain O
/ O
fine O
- O
tune O
approach O
for O
document O
transcrip O
- O
tion O
and O
applied O
it O
to O
historical O
documents O
in O
low O
- O
resource O
settings O
. O
Our O
pre O
- O
training O
strategy O
, O
inspired O
by O
reconstructing O
missing O
information O
, O
or O
lacuna O
, O
in O
documents O
uses O
hundreds O
of O
thousands O
of O
unlabeled O
line O
images O
to O
learn O
rich O
visual O
language O
representations O
. O
After O
supervised O
fine O
- O
tuning O
on O
tens O
of O
transcribed O
line O
images O
, O
we O
showed O
large O
character O
error O
rate O
reduction O
on O
Islamicate O
manuscripts O
exhibiting O
major O
script O
and O
style O
variation O
and O
we O
improved O
over O
several O
stateof O
- O
the O
- O
art O
OCR B-TaskName
systems O
on O
early O
modern O
English O
printed O
works O
. O
We O
estimate O
that O
our O
approach O
could O
save O
human O
annotators O
significant O
amounts O
of O
time O
and O
enable O
more O
distant O
readings O
of O
library O
collections O
. O

Ethical O
Considerations O

Acknowledgements O

Introduction O

Related O
Work O

Absorbing O
ideas O
from O
paradigm O
1 O
and O
2 O
, O
we O

Data O
preprocessing O

Methodology O

Task O
Definition O
and O
Model O
Overview O

x O
m O
= O
( O
x O
m O
1 O
, O
x O
m O
2 O
, O
• O
• O
• O
, O
x O
m O
T O
) O
. O

r O
= O
1 O
n O
n O
i=1 O
r O
i O
( O
7 O

where O
b B-HyperparameterName
is O
the O
size O
of O
mini O
- O
batch O
. O

Experiments O

Datasets O

Implementation O

We O
set O
λ B-HyperparameterName
1 I-HyperparameterName
/ O
λ B-HyperparameterName
2 I-HyperparameterName
/ O
λ B-HyperparameterName
3 I-HyperparameterName
in O
Equation O
1 O
to O
1 B-HyperparameterValue
/ O
1 B-HyperparameterValue
/ O
1 B-HyperparameterValue
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
4 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
5e-5 B-HyperparameterValue
, O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
of O
the O
encoder O
and O
decoder O
to O
512 B-HyperparameterValue
for O
both O
generation O
stages O
in O
the O
Chinese O
experiments O
. O
And O
the O
hyper O
- O
parameters O
for O
English O
experiments O
are O
the O
same O
except O
that O
λ B-HyperparameterName
1 I-HyperparameterName
/ O
λ B-HyperparameterName
2 I-HyperparameterName
/ O
λ B-HyperparameterName
3 I-HyperparameterName
are O
set O
to O
0.5 B-HyperparameterValue
/ O
0.5 B-HyperparameterValue
/ O
0.5 B-HyperparameterValue
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
2.5e-5 B-HyperparameterValue
. O
More O
implementation O
details O
are O
presented O
in O
Appendix O
D O
. O

Baselines O

Automatic O
Evaluation O

Results O
on O
the O
Chinese O
Dataset O

Case O
Study O

Conclusion O

Limitations O

Ethics O
Statement O

A O
Style O
- O
Specific O
Contents O

B O
Data O
Pre O
- O
Processing O

C O
Different O
Style O
Samples O

D O
More O
Implementation O
Details O

E O
More O
Ablation O
Study O
Results O

H O
More O
Case O
Studies O

Acknowledgments O

This O
work O
was O
supported O
by O
the O
NSFC O
projects O
( O
Key O
project O
with O
No O
. O
61936010 O
) O
. O
This O
work O
was O
also O
supported O
by O
the O
Guoqiang O
Institute O
of O
Tsinghua O
University O
, O
with O
Grant O
No O
. O
2020GQG0005 O
. O

D5 O
. O
Did O
you O
report O
the O
basic O
demographic O
and O
geographic O
characteristics O
of O
the O
annotator O
population O
that O
is O
the O
source O
of O
the O
data O
? O
4.5 O
and O
Appendix O
F O

