Balancing Multi - Domain Corpora Learning for Open - Domain Response Generation
Open - domain conversational systems are assumed to generate equally good responses on multiple domains . Previous work achieved good performance on the single corpus , but training and evaluating on multiple corpora from different domains are less studied . This paper explores methods of generating relevant responses for each of multiple multi - domain corpora . We first examine interleaved learning which intermingles multiple corpora as the baseline . We then investigate two multidomain learning methods , labeled learning and multi - task labeled learning , which encode each corpus through a unique corpus embedding . Furthermore , we propose Domainspecific Frequency ( DF ) , a novel word - level importance weight that measures the relative importance of a word for a specific corpus compared to other corpora . Based on DF , we propose weighted learning , a method that integrates DF to the loss function . We also adopt DF as a new evaluation metric . Extensive experiments show that our methods gain significant improvements on both automatic and human evaluation . We share our code and data for reproducibility . 1 * This work was done prior to the author joining Amazon . 1 https : / / github.com / yujie-xing / Balancing_Multi_Domain_Corpus_Learning _ for_Open_Domain_Response_GenerationWhat are you going to do on the remote system exactly ? PersonaChat I am going to be a pilot . I am going to fly planes . 4 corpora ( concatenated ) I am going to go to the beach .
Introduction
Recent work has achieved improvements in general performance for open - domain response generation ( Vinyals and Le , 2015 ; Serban et al . , 2017 ; Li et al . , 2016 ; Xu et al . , 2018 ) . However , most studies are restricted to single - corpus training and evaluating , while there lacks studies for training and evaluating with multiple corpora from different domains . Single - corpus training has intrinsic limitations . For example , a corpus of everyday chats , e.g. , the PersonaChat corpus ( Dinan et al . , 2019 ) , does not cover technical topics discussed in Ubuntu chatlogs ( Lowe et al . , 2015 ) . A conversational system that learns only from PersonaChat or from multiple corpora without an appropriate technique is not likely to generate relevant responses for certain topics ( see Table 1 ) . Therefore , it is necessary for an open - domain conversational system to learn from multiple corpora , and to learn with good techniques . Furthermore , the case of using a single smallscale open - domain corpus has apparent weaknesses . A common way of dealing with a smallscale corpus is through fine - tuning ( Li et al . , 2016 ; Akama et al . , 2017 ; Chu et al . , 2017 ) . Fine - tuning on a single corpus tends to make the model overfit on that specific corpus while performing worse on other corpora . Table 2 shows the result of a GPT-2 model gaining good performance on PersonaChat while performing poorly on other corpora .
This paper explores how to train and evaluate on multiple corpora from different domains for the open - domain response generation task . We propose several methods to make a model generate relevant responses for each of the multiple corpora .
Since simply training multiple corpora one by one does not solve the imbalanced performance ( as shown in Table 1 and 2 ) , we first investigate interleaved learning , a method that intermingles the training data instead of simply concatenating , which ensures a model learns from all corpora evenly . We use this method as a baseline . Additionally , we explore two multi - domain learning methods : labeled learning and multi - task labeled learning . Labeled learning comes from a control technique in response generation ( Li et al . , 2016 ; Johnson et al . , 2017 ; . Previous works focus on controlling persona and style , while our method controls corpus 's information with the corpus embedding . Multi - task labeled learning is inspired by works of domain adaption ( Luan et al . , 2017 ; Niu and Bansal , 2018 ; Chu and Wang , 2018 ) , where multiple losses from both the corpus classifier and response generator are minimized . To the best of our knowledge , this paper is the first that uses corpus embeddings on the open - domain response generation task for multiple corpora .
Furthermore , we propose a novel weighted learning with Domain - specific Frequency ( DF ) . DF is a word - level importance weight ( Leopold and Kindermann , 2002 ) that assigns different weights ( importance ) to the same words from different corpora . In the training process , we weight the loss of a model with DF , so that the model focuses on the most important words for a specific corpus .
For automatic evaluation metrics , we eliminate the stop words and use ROUGE-1 ( precision , recall , F1 ) ( Lin , 2004 ) to measure the relevance of the generated responses . In addition , we adopt DF to see how relevant the generated response of a model is to a specific corpus . We will explain DF as an evaluation metric in Section 4.4 . Results show that for overall performance , the best method ( weighted learning ) improves 27.4 % on precision , 45.5 % on recall , and 34.1 % on F1 . Further , it has at least 20.0 % higher DF , stating that it uses more important words from the " correct " corpus . We also conduct an extensive human evaluation on 2400 generated responses . The human evaluation shows a highly significant ( p < 0.001 ) improvement on all of our proposed methods , especially the weighted learning method .
We summarize our work as follows :
• We explore the problem of training and eval - uating on multiple corpora from different domains for open - domain response generation . The task is to make the conversational models generate relevant responses for each corpus .
• We examine several multi - domain corpora learning methods for their ability to solve the proposed task .
• We propose Domain - specific Frequency ( DF ) as in weighted learning and as an evaluation metric . DF distinguishes important words for each corpus and helps a model to focus on these important words in the training process .
Related Work
Open - Domain Response Generation Recent work of open - domain response generation generally follows the work of Ritter et al . ( 2011 ) where the task is treated as a machine translation task , and many of them use a Seq2Seq structure ( Sutskever et al . , 2014 ) following previous work ( Vinyals and Le , 2015 ; Shang et al . , 2015 ; Sordoni et al . , 2015 ) .
In recent years , substantial improvements have been made ( Serban et al . , 2017 ; Li et al . , 2016 ; Wolf et al . , 2019 ) , and embeddings are used to control response generation on extra information such as persona ( Li et al . , 2016 ) , profiles , coherence ( Xu et al . , 2018 ) , emotions ( Huang et al . , 2018 ) , and dialogue attributes like response - relatedness ( See et al . , 2019 ) . However , there is a lack of work that uses embeddings to control response generation over multiple corpora . Our work follows the common models of opendomain conversational systems , while we study the problem of multiple corpora of different domains .
Multi - Domain Learning and Domain Adaption
Multi - domain learning aims at making a conversational model learn from multiple domains to prevent the performance from degrading due to domain differences ( Ben - David et al . , 2007 ) . There are two categories of solutions for multidomain learning ( Joshi et al . , 2012 ) : ( i ) capturing domain - specific characteristics in the parameters ( Daumé III , 2007 ) ; ( ii ) capturing the relationship among different domains ( Saha et al . , 2011 ) . Some work of natural language generation and machine translation is related to multi - domain learning . Luan et al . ( 2017 ) and Niu and Bansal ( 2018 ) use multi - task learning for domain adaption respectively on speaker - role and politeness . Wen et al . ( 2016 ) and Akama et al . ( 2017 ) utilizes finetuning as a common way of domain adaption for language generator and style transferer . For machine translation , in order to deal with the mixeddomain parallel corpus , Zeng et al . ( 2018 ) adjust the weights of target words in the training objective based on their relevance to different domains . We differ in that we propose DF and we deal with the response generation task . Chu et al . ( 2017 ) propose mixed fine - tuning , which adds the out - ofdomain pre - training data to the fine - tuning dataset , and they observe an improvement of performance . In this paper , we also mix small - scale fine - tuning datasets with out - of - domain training data , while the data we add is not necessarily used during pretraining . Shi et al . ( 2015 ) state that fine - tuning can be done by placing the corpus to be fine - tuned at the end of the entire corpus , which is an extension of curriculum learning proposed by Bengio et al . ( 2009 ) . We also explore how the order of multiple corpora influences the result , but our focus is on balancing performance . Recently , Smith et al . ( 2020 ) investigated blending conversational skills with knowledge and empathy skills , where they mix 3 corpora . They focus on selecting appropriate skills and they propose a blended corpus with labels , while we focus on generating responses that are most relevant to a specific corpus .
Base Models
We use two base models : an LSTM Seq2Seq model with attention ( Hochreiter and Schmidhuber , 1997 ; Sutskever et al . , 2014 ; Bahdanau et al . , 2015 ) and a pre - trained GPT-2 model ( Radford et al . , 2019 ) . The LSTM Seq2Seq model with attention is a common model for conversational systems ( Li et al . , 2016 ; See et al . , 2019 ) , and the GPT2 model is a state - of - the - art model for the response generation task ( Zhang et al . , 2020 ; Zhao et al . , 2020 ) .
The basic task of response generation is to predict the next word given the past and current words of the context and response , and to make the generated response as similar to the original response as possible . The task can be described as follows . Probability of response Y given context X is predicted as :
P ( Y |X ) = n t=1 P ( y t |y 1 , . . . , y t−1 , X ) , ( 1
where X = x 1 , . . . , x m and Y = y 1 , . . . , y n is a context - response pair .
LSTM Seq2Seq Model with Attention
We simplify an LSTM with attention unit as LSTM * since it is well introduced in previous work ( Li et al . , 2016 ) . We calculate the hidden vector h t at step t as :
h t = LSTM * ( h t−1 , E ( z t ) ) , ( 2 )
where h t−1 ∈ R dim is the hidden vector at step t − 1 , dim is the dimension of hidden vectors , and E ( z t ) is the word embedding for word z t ∈ ( x 1 , . . . , x m , y 1 , . . . , y n−1 ) . We apply dot multiple in the attention mechanism when calculating the context vector c t :
c t = H • ( sof tmax ( H • h t ) )
where H ∈ R d×m is the concatenation of hidden vectors from the encoder . c t is input to the next step t + 1 in the decoder . Each token 's hidden vector h t in the decoder is combined with c t through a linear layer and an activation to predict the next token .
GPT-2
As for GPT-2 , we follow the adaption of Wolf et al . ( 2019 ) . The transformer block of GPT-2 captures the relation of multiple words in one sentence , which largely follows Vaswani et al . ( 2017 ) . The hidden vector to be input to the transformer block is calculated as :
h 0 [ t ] = E ( X , Y [ 1 : t ] ) + ( E 0 , E 1 ) + W p , ( 3 )
where Y [ 1 : t ] is ( y 1 , . . . , y t ) , E ( X , Y [ 1 : t ] ) is the subword embedding for context X and response Y [ 1 : t ] . E 0 and E 1 are dialogue - state embeddings , which tutor the model to distinguish between contexts and responses . W p is a pre - trained position embedding . The probability of the subword to generate is then calculated as :
h [ t ] = transformer _ block ( h 0 [ t ] ) ( 4 ) P ( y ) t+1 = softmax ( E ( h [ t ] ) ) , ( 5 )
where y ∈ V , and V stands for the sub - word vocabulary . We simplify the structure of transformer block as transformer _ block . In the block , a mask is filled in the attention matrix , which bans past words from attending to future words . This ensures that the model follows the traditional language modeling . The hidden vector of t th sub - word is used to generate the probability distribution for the vocabulary ( P ( y ) , y ∈ V ) for ( t + 1 ) th subword . E means that the model uses the sub - word embeddings in calculating sub - word probabilities for generation ( Press and Wolf , 2017 ) .
4 Proposed Methods
Interleaved Learning
Interleaving is a concept in cognitive psychology proven to be efficient for learning ( Kornell and Bjork , 2008 ) : intermingling learning material of different topics helps students to gain better learning results than learning the material topic by topic . Previous work from machine learning also shows that training order greatly influences the performance ( Bengio et al . , 2009 ) . When the training is conducted on a simple concatenation of multiple corpora , the model tends to concentrate on the last corpus ( Shi et al . , 2015 ) . To address this issue , we propose interleaved learning as an alternative : each time we collect one context - response pair from each of the corpora , and we randomly shuffle them . For example , if there are 3 corpora ( a 1 , a 2 , ... ) , ( b 1 , b 2 , ... ) , ( c 1 , c 2 , ... ) where a i , b i and c i are context - response pairs , the resulting mixed corpus might be
( b 1 , a 1 , c 1 , c 2 , b 2 , a 2 , ... ) .
Interleaved learning guarantees that the combined corpus is evenly distributed , which helps the model learn from multiple corpora evenly .
Labeled Learning
We propose our labeled learning as follows : each corpus is assigned a randomly initialized unique embedding , and the conversational model learns these embeddings together with conversations during the training period . We denote these embeddings as " corpus embedding " , or E c . A model captures each corpus 's characteristics through the corpus embedding and uses it to control the generated responses . To know which corpus embedding to use , each context is labeled with which corpus it comes from , and these labels are provided to the model both in the training and generation period . We propose an approach for each of our base models for encoding corpus embeddings .
For the LSTM model , following Li et al . ( 2016 ) , we input the corpus embedding E c into the first layer of the decoder LSTM at every step , together with the response words . Calculation of a hidden vector h t in the decoder LSTM is then adapted to :
h t = LSTM * ( h t−1 , E ( y t ) , E c ) . ( 6
The structure is illustrated in the dashed red rectangle of Figure 1a .
For the GPT-2 model , our method is based on Wolf et al . ( 2019 ) . Instead of two kinds of dialogue - state embeddings ( context embedding E 0 and response embedding E 1 ) , we replace the response embedding with corpus embeddings E c . As a result , the model is aware of which corpus the response belongs . Calculation of a hidden vector to be input to the transformer block is adapted to :
h 0 [ t ] = E ( X , Y [ 1 : t ] ) + ( E 0 , E c ) + W p . ( 7
The structure is illustrated in Figure 1b .
Multi - Task Labeled Learning
Labeled learning needs corpus labels for both training and generation processes . To avoid providing labels in the generation process , we combine multitask learning with labeled learning on multiple corpora . Here , the conversational model has to predict by itself which corpus a context belongs to , which is expected to result in worse performance , but less information is required . In the encoder , we have a classifier layer that uses the sum of hidden vectors from the encoder ( H ) to predict the corpus of a context . The loss of the classifier is calculated as :
L c = −log softmax H • W [ c ] , ( 8
where W [ c ] ∈ R dim is the part from the classifier layer for target corpus c. L c is summed up with the loss from the response generator . The predicted corpus embedding is input into the decoder like labeled learning ( see Section 4.2 ) . The simplified structure is illustrated in Figure 1a .
Document - specific Frequency ( DF )
We propose Domain - specific Frequency ( DF ) to measure how important a word is with respect to a different corpus under a collection of corpora . DF is used for weighted learning and evaluation . It is calculated as follows :
f ( w ) d = freq ( w ) d − min v { freq ( v ) d } ( 9 ) df ( w ) d = 0 f ( w ) d = 0 f ( w ) d d∈D f ( w ) d f ( w ) d = 0 ( 10 ) DF ( w ) d = df ( w ) d max v { df ( v ) d } , ( 11
where freq ( w ) d is the relative frequency of a word w in a corpus d , and D represents the set of all corpora . It is easy to see from Equation 10that DF ( w ) d represents the importance of word w for corpus d compared to other corpora . For a word w that frequently appears in corpus d but seldom A word that frequently appears in all corpora ( e.g. , " I " , " you " ) is punished , resulting in a lower DF ( w ) d . A word that seldom appears in corpus d but frequently appears in other corpora ( e.g. , " music " seldom appears in Ubuntu corpus , but is common in other corpora ) has the lowest DF ( w ) d . Words that appear minimal times ( e.g. , once ) in a corpus are ignored with Equation 9 . Words that appear few times ( e.g. , twice or three times ) are not dealt with , yet they are not of great influence in our experiments . We apply a normalization in the final step ( Equation 11 ) to make DF ( w ) d of each corpus d range from 0 to 1 . We show DF ( w ) Ubuntu and DF ( w ) PersonaChat of some words in Table 3 . We also show the results of TF - IDF ( log normalization variant ) , a commonly used word importance weight , as a comparison . As expected , for the corpus Ubuntu and PersonaChat , most unique words w have very different DF ( w ) Ubuntu and DF ( w ) PersonaChat . Unique words of each corpus get the highest values for the corresponding corpus , like " upgrade " for the Ubuntu corpus and " music " for the PersonaChat corpus ; these words receive the lowest values for incorrect corpora , like " upgrade " for PersonaChat and " music " for Ubuntu . The stress on unique words makes DF more suitable for our task .
Weighted Learning with DF Weighted learning weights the loss of the predication y for each target word w using DF ( w ) d .
L weighted = DF ( w ) d • −log softmax ( y w ) , ( 12 )
where y w represents the model 's predicted score for the target word w. With the weighted loss , the model concentrates on words that are important to the corpus of the current context , and focuses less on frequent words or words that are not important to the current corpus . The structure is illustrated in Figure 1c .
Evaluation with DF For the generated responses to be relevant to a specific corpus , they have to be similar to that corpus , which includes using important words of that corpus ( e.g. , responses generated for the Ubuntu corpus should have more technical words than other corpora ) . Thus , we propose DF as an evaluation metric that shows to what extent the generated responses use important words of the corresponding corpus . We want to decrease the influence of common words like " i " , " to " , etc . , and thus address the important words . So we adopt exponential DF with α as the base ( αDF ) :
αDF ( w ) d = 0 DF ( w ) d = 0 α DF ( w ) d DF ( w ) d = 0 , ( 13
where α is a constant . αDF ( w ) d rescales DF ( w ) d by exponent with α as a base . In our experiments , we set α to be 100 , which transforms the range of the metric from ( 0 , 1 ) to ( 0 , 100 ) . This makes the difference between high and low αDF more significant than DF and gives a 100 - scale score . We show αDF ( w ) Ubuntu and αDF ( w ) PersonaChat ( calculated purely on test set ) in Table 3 . As expected , αDF has a more significant difference between important words and common words .
Is DF a Legal Evaluation Metric ? Although DF is used for both weighted learning and evaluation , we see DF as a suitable evaluation metric for our task and not biased in favor of weighted learning due to : 1 ) A word receives multiple DF values in the training process given the corpus that a context belongs to ; 2 ) in the generation process , DF is never used . 3 ) In the evaluation process , DF can be calculated purely on the test sets . Note that since a word receives multiple DF values in the training step , it is equivalently likely for the model trained with weighted learning to be influenced by DF weights of incorrect corpus . Above all , in the evaluation step , if the trained model is influenced more by DF weights from the correct corpus , it already means that the model is good at distinguishing which corpus a given context is from , thus is suitable for our task .
Experiment Setup
Datasets Data Collection
We collected 4 commonly used English corpora of different domains from the Par - lAI platform ( Miller et al . , 2017 ) : OpenSubtitles corpus ( OSDB ) 2 ( Lison et al . , 2018 ) , Twitter corpus 3 ( Miller et al . , 2017 ) , Ubuntu chatlogs corpus ( Lowe et al . , 2015 ) 4 , and PersonaChat corpus ( Zhang et al . , 2018 ) from the NeurIPS 2018 ConvAI2 Challenge ( Dinan et al . , 2019 ) . Each corpus contains 250 K context - response pairs , as much as the size of the original PersonaChat used in ConvAI2 competition . This gives us 1 M contextresponse pairs in total . The corpus for training is a combination of these 4 corpora . For comparison , we have a single corpus - PersonaChat - trained on both base models . For testing , each of the 4 corpora has a test set of 30 K context - response pairs , which is the same size of the test set of PersonaChat .
The OpenSubtitles corpus ( OSDB ) is a noisy dataset of film subtitles . We removed films that belonged to genres that usually had few conversations , such as musical and documentary films . We regarded two neighboring sentences as a contextresponse pair following Vinyals and Le ( 2015 ) . The Twitter corpus contains one - turn dialogues extracted from Twitter . The original author has already cleaned it , so we only removed special symbols such as hashtags , Emojis , and @ . The Ubuntu corpus contains dialogues about solving technical problems of Ubuntu . The PersonaChat corpus contains dialogues between two workers acting as specific personas ; we focused on the dialogue part and ignored the persona part . This corpus allows us to compare our base models with state - of - the - art performance . These 4 corpora have very different characteristics , confirmed by the imbalanced performance of GPT-2 fine - tuned on a single corpus ( see Table 2 ) .
Training and Decoding
We used Pytorch ( Paszke et al . , 2017 ) to implement the LSTM Seq2Seq model with attention and the pre - trained GPT-2 models . For GPT-2 , we adapted ( 2019 ) : the batch size was 32 , learning rate was 6 × 10 −5 , β 1 = 0.9 , β 2 = 0.999 , L2 weight decay set to 0.01 , learning rate linearly decreased to zero at the end . We followed these hyper - parameters to ensure state - of - the - art performance for the base models . We use the same hyper - parameters for both base models and models with our proposed methods , so the proposed methods work slightly 5 https : / / huggingface.co / .
( but not much ) worse than it should be . This is to avoid the extra improvement caused by hyperparameters . We pre - trained the LSTM model on 3 large - scale corpora ( OSDB , Twitter and Ubuntu ) with interleaved learning until converging . GPT-2 is already pre - trained , so we directly used it for finetuning ( details about pre - training convergence can be found in Section B ) . For decoding , we adopted greedy decoding for all the models to ensure an equal condition .
Evaluation
For automatic metrics , to measure the relevance of the generated responses , we eliminated punctuation and stop words , and adopted Rouge-1 6 ( precision , recall , F1 ) as multi - grams become meaningless without stop words . However , Rouge-1 compares the generated responses with the golden ones , while there is never a standard response for any context , so in addition to Rouge , we use αDF score that shows to what extent the generated responses use important words of the corresponding corpus , as stated in Section 4.4 . Due to the limitation of automatic evaluation methods ( Liu et al . , 2016 ) , we also conduct an extensive human evaluation on the relevance of generated responses to contexts ( see Section 6.1 for details ) .
Results
Our base models achieve perplexity scores of 28.9 ( LSTM model ) and 19.6 ( GPT-2 ) on the test set of the PersonaChat dataset from the ConvAI2 competition when fine - tuned with the single PersonaChat corpus ( more details can be found in Section C ) . These results would likely advance the models to the second round in the competition . Table 4 shows that models trained with our proposed methods gain better performance on Rouge than baselines . Baselines concentrate on the last trained corpus ( PersonaChat ) , while with the proposed methods , performance is more balanced on multiple corpora . Weighted learning has the best overall performance on all metrics , and it performs especially well on the Ubuntu corpus , indicating that it might be good at distinguishing the unique technical words from the Ubuntu corpus . Labeled learning is the second best with stable improvement from interleaved learning , indicating that the corpus embeddings function as expected . Multi - task labeled learning has slightly worse performance than interleaved learning , indicating that predicting the corpus of a contexts is not easy , and wrong predictions result in worse performance .
Table 5 shows αDF d scores for generated responses of each corpus . Full results can be found in Section E. We use both αDF d calculated purely on the train set ( train - set - αDF ) and αDF d calculated purely on the test set ( test - set - αDF ) . The black scores are scores for the corresponding corpus ( we expect high scores for these parts ) , while the grey scores are scores for non - related corpus - PersonaChat ( we expect low scores for these parts ) . Note that scores for different corpora are in different scales . From the table , we can see that train - set - DF scores and test - set - DF scores are similar , and weighted learning always has the highest score , indicating that weighted learning distinguishes well which corpus a context comes from . Labeled learning is the second best , indicating that the learned corpus embeddings help the model to use more important words of the corresponding corpus . Compared to the concatenated corpus , the improvement is at least 20 % , while the decrease in PersonaChat is just 9 % at most .
Human Evaluation
We conducted a human evaluation on all GPT-2 models : base models and models adapted with our proposed methods . We randomly picked 2400 responses : 400 different contexts evenly from 4 corpora with 6 responses generated by each of our models . 3 judges 7 are asked to pick the most and the least relevant response ( s ) for the given context . The most relevant response ( s ) are given score 3 , the least relevant response ( s ) are given score 1 , and the other ( s ) are given score 2 . Table 6 shows the overall scores of all GPT-2 based models . Table 7 shows the p - value for the t - test conducted between every two models . The overall scores of our proposed methods are all highly significantly ( p < 0.001 ) higher than the concatenated models , especially the weighted learning method .
Response Examples
The generated responses from better methods are more relevant to the corresponding corpus , while worse methods can not distinguish contexts from different corpora ( e.g. , they may answer any questions in a " PersonaChat " way ) . To show an intuition of the difference among our proposed methods , we present some response examples generated by GPT-2 in Section G .
Possible Limitations
Our proposed methods are meant to be able to work in most models , which is why we choose the most common conversational models as our base models . However , there are many variants of conversational models focusing on different aspects , such as integrating knowledge , avoiding dull responses , keeping the speech style , etc . We can not ensure that our methods work for all of these variant models . Also , dialogues are always multi - turn , while we focus on a simpler task : single - turn response generation . Furthermore , the methods are trained and evaluated on English corpora . There can be a limitation on applying the methods to other languages .
Conclusions
We have experimented with 4 methods - interleaved learning ( baseline ) , labeled learning , multi - task labeled learning , and weighted learning - to help common open - domain conversational systems generate relevant responses for multiple corpora of different domains . We adopted Rouge ( precision , recall , F1 ) for auto evaluation . In addition , we used DF to evaluate how well a model uses relevant words for a corresponding corpus . We also did an extensive human evaluation . Our results show significant improvement in performance for our proposed methods , especially weighted learning . Future work of multi - turn response generation is potential . We have focused on one - turn response generation , while dialogue is naturally multi - turn so further research is needed . Example words are divided into five blocks . The first block has frequent words in all corpora , the second block has unique words from OSDB , the third block has unique words from Twitter , the fourth block has unique words from Ubuntu , and the fifth block has unique words from PersonaChat . The values of the corresponding corpus are marked with different colors .
From this table , it is clear that the commonly used word importance weight , TF - IDF , is not suitable for our task . This is due to the vast range of frequency , which leads to a relatively small penalty for IDF ( Inversed Document Frequency ) over words with too large TF ( Term Frequency ) .
Acknowledgements
This paper is funded by the collaborative project of DNB ASA and Norwegian University of Science and Technology ( NTNU ) . We also received assist on computing resources from the IDUN cluster of NTNU ( Själander et al . , 2019 ) . We would like to thank Aria Rahmati , Zhirong Yang ( Norwegian Research Council , 287284 ) and Özlem Özgöbek for their helpful comments .
In the pre - training period , it takes 21 epochs for the concatenated corpus to converge on the base LSTM model , while only 12 epochs with interleaved learning , which is 43 % shorter . When trained on the concatenated corpus in the order of OSDB → Twitter → Ubuntu , it takes 20 epochs for the perplexity on OSDB and Ubuntu to be balanced , while with interleaved learning , it takes less than one epoch . For concatenated corpus , the performance of the Ubuntu corpus is sacrificed in order to balance the performance of the two corpora , which results in worse overall performance . GPT-2 PersonaChat ( single ) 478.8 4.9 6.7 159.6 5.5 6.7 264.7 5.1 7.7 19.6 14.1 16.2 44.7 7 . Models of labeled , multi - task labeled and weighted learning do not have the best hyper - parameters , but the same hyper - parameters as the base models . Their perplexity is slightly worse than it should be .
The results of the single corpus PersonaChat trained with the LSTM model confirm our concern on a small fine - tuning corpus . The LSTM model is pre - trained on OSDB , Twitter and Ubuntu ; however , the performance for the 3 corpora greatly decreases after fine - tuning .
The automatic evaluation with stop words is not good for measuring relevance , since stop words are taken too much into account . See BLEU and F1 scores of PersonChat ( single ) and weighted learning as an example . Models trained on PersonaChat ( single ) can not answer Ubuntu technical questions at all , yet they receive better scores than weighted learning . But once the stop words are removed , the scores of weighted learning surplus PersonaChat ( single ) a lot .
D Additional Results of automatic evaluation without stop words
McQueen : a Benchmark for Multimodal Conversational Query Rewrite
The task of query rewrite aims to convert an in - context query to its fully - specified version where ellipsis and coreference are completed and referred - back according to the history context . Although much progress has been made , less efforts have been paid to real scenario conversations that involve drawing information from more than one modalities . In this paper , we propose the task of multimodal conversational query rewrite ( McQR ) , which performs query rewrite under the multimodal visual conversation setting . We collect a largescale dataset named McQueen based on manual annotation , which contains 15k visual conversations and over 80k queries where each one is associated with a fully - specified rewrite version . In addition , for entities appearing in the rewrite , we provide the corresponding image box annotation . We then use the McQueen dataset to benchmark a state - of - the - art method for effectively tackling the McQR task , which is based on a multimodal pre - trained model with pointer generator . Extensive experiments are performed to demonstrate the effectiveness of our model on this task 1 .
Introduction
Recent years have witnessed an increasing attention in conversational - related tasks , such as conversational question answering ( Choi et al . , 2018 ; Reddy et al . , 2019 ) , visual conversation modeling ( Das et al . , 2017 ) , etc . One main challenge in multi - turn conversation modeling is that information from context history is easy to be abbreviated or omitted in the follow - up queries , causing the so - called coreference and ellipsis . To address this concern , the task of query rewrite ( Elgohary et al . , 2019 ; Pan et al . , 2019 ; Su et al . , 2019 ) aims to reconstruct the original query to a fully specified form based on its history context . The rewrite eliminates the coreference and ellipsis in the original query without changing its semantic information , thus helping turn the more challenging multi - turn conversation modeling problem to a single - turn version .
Following this line , several attempts have been made in the query rewrite task which achieve decent performance on the natural language level . Nevertheless , conversations in real scenario tend to involve knowledge from more than one modalities , such as vision , text , speech , etc . Information from different modalities is not handled in isolation , but often integrated together to improve the quality of perception and understanding . For example , as shown in Figure 1 , in the first turn of the visual conversation , with the lack of context , the user directly uses the pronoun " it " to refer to the dog in the image . In the third turn , for ellipsis that does not appear in the context history , in order to perform ellipsis completion , one also needs to find clues from the corresponding image . Rewriting the query under the circumstance where grounding outside the text information is needed poses more challenges to traditional query rewrite models that based only on textual features .
In this paper , we propose the task of multimodal conversational query rewrite ( McQR ) , which aims to perform query rewrite under the multimodal visual conversation setting . To achieve this goal , we collect a large - scale dataset called McQueen . Specifically , for each visual conversation consisting of an image and the corresponding history questionanswer context , we provide manual rewrite for the query , where the coreference resolution and ellipsis completion are performed respectively . Furthermore , in order to assist downstream tasks such as coreference entity detection , for all the entities appearing in the rewrite , we annotate the image boxes for representing their corresponding image area .
We then use the McQueen dataset to benchmark a state - of - the - art method for effectively tackling the McQR task . Inspired by the big success of pre - trained models such as BERT ( Devlin et al . , 2019 ) , our model is based on a multimodal pretrained model where interactions between different modalities can be better captured . Furthermore , we enhance the model with a pointer generator specially designed for the multimodal Transformer blocks ( Vaswani et al . , 2017 ) , so that the rewritten query is either generated from scratch or copied from certain contextual parts with high attention weights . Extensive experiments are conducted to compare our method with several state - of - the - art methods . Our model outperforms all the methods in both the McQR task and two subtasks . We further perform analysis to investigate the role of different modalities in this task and demonstrate that the introduction of image information provides extra guidance for the concerned query rewrite task .
In summary , the contribution of our paper lies in three folds :
• We formally define the task of multimodal conversational query rewrite ( McQR ) , which aims to generate a fully - specified rewrite query based on both the context history and the visual image .
• We propose a large - scale dataset McQueen , containing 15k visual conversations and over 80k rewrites . For the entities appearing in the rewrites , we also annotate the image boxes for representing their corresponding image area .
• We benchmark a multimodal Transformer - based model with pointer mechanism for effectively tackling the McQR task . Extensive analysis shows the role of different modalities in our model .
Related Work
Query Rewrite
The task of query rewrite provides reconstructed queries based on abbreviated in - context queries without changing their semantic meaning . First introduced by ( Elgohary et al . , 2019 ; Su et al . , 2019 ; Pan et al . , 2019 ) , most works formulate it as a standard generation task , which can be solved via a Sequence - to - Sequence model ( Quan et al . , 2019 ; . Some attempts have been made to introduce a multitask learning setup in order to enhance the training process ( Rastogi et al . , 2019 ; Zhang et al . , 2020 ) . Some works seek to focus on query rewrite under the low - resource scenario Voskarides et al . , 2020 ; .
For modeling the linguistic knowledge in conversational context more effectively , prior knowledge is leveraged such as using semantic role labeling to provide extra guidance ( Xu et al . , 2020 ) , and reducing the generation search space via sequence tagging . Although these works have achieved great performance on their corresponding task , query rewrite under the multimodal setting has not been explored .
Visual Coreference Resolution
Visual dialog entails answering a set of questions grounded by an image ( Das et al . , 2017 ) . Based on that , visual coreference resolution involves linking the words in the text ( usually nouns and pronouns ) to a certain area in the image ( Kong et al . , 2014 ; Kottur et al . , 2018 ) . Following this line , Li et al . ( 2021 ) restrict coreference resolution to pronouns and resolve coreferences in visual dialog in an unsupervised way . Yu et al . ( 2019 )
The McQueen Dataset
Dataset Overview
Our dataset is based on a visual dialog dataset called VisDial ( Das et al . , 2017 ) . The original VisDial dataset consists of over 133k dialogs , each associated with an image and 10 rounds of questionanswer pairs . All question - answer pairs are conducted in a conversational format and revolve around the content of the picture .
Our dataset randomly selects 15k conversations from the VisDial dataset with the total of over 80k rewrite utterances . For each query in a visual conversation , we conduct manual annotation to resolve the information omission . The query is reconstructed based on the image as well as the history context so that the coreference and ellipsis are referred - back or completed . For negative queries that do not contain any information omission , the rewrite stays the same as the original query . In addition , for all the entities appearing in the coreference and ellipsis , we annotate the image boxes for representing their corresponding image areas .
Dataset Construction
Text Rewrite Annotation
For manual annotation , we hire 16 annotators in total . Before the annotation starts , we provide 100 examples for all the annotators to refer to . We also provide a guideline and some tutorials by listing some typical coreference and ellipsis cases so that the bias and language style shift between individuals are minimized as possible . After that , the annotators start working on a small portion of data where query rewrite is performed . After all the results are returned and the data quality is checked , the main annotation phase begins and the rest of data is labeled . On average , each annotator is in charge with the rewrite of 5059 queries . The rewrite annotation interface can be seen in Appendix A .
Image Box Annotation
Besides the rewrite annotation , we also provide image annotation to assist downstream or related tasks ( e.g. coreference entity detection ) . The image box annotation begins right after the rewrite annotation . The overall procedure also follows the ( 1 ) tutorial ( 2 ) trial phase ( 3 ) main phase pipeline . Specifically , the annotators have to extract the entities in the ellipsis and coreference part and draw the bounding boxes of them in the image . Each annotator is in charge of the image annotation of the rewrites written by him / herself . The image annotation interface can be seen in Appendix B .
Quality Control
After the all the annotation is finished , we re - group and shuffle the annotators to perform cross quality inspection . Each group is asked to check the annotation results of other groups . In addition , two new annotators who do not take part in the annotation phase are recruited to check the quality of all the annotation results . The annotators have to answer three questions for each query : ( 1 ) Is the rewrite result correct or not ? ( 2 ) Are all the coreference and ellipsis resolved in the rewrite ? ( 3 ) Are the entities in the coreference and ellipsis correctly annotated in the image ? All the conversation rewrites must get the all " yes " result from all the annotators before official acceptance , otherwise they are collected to be revised and re - checked ( the questionnaire interface is shown in Appendix C ) . The whole check - revise process lasts for three iterations . Considering chance agreement , we measured the Inter - Annotator Agreement ( IAA ) in terms of Cohen 's κ ( Cohen , 1960 ) . The final κ score is 0.82 , reaching the " almost perfect " level 2 . Besides , after each quality check iteration , we randomly sample 100 conversations from the dataset and manually evaluate the utterance - level precision and recall rate , where precision denotes the rate of the retrieved rewrites being correct , while the recall rate records the portion that the coreference and ellipsis being handled . The precision and recall rate in the 1st / 2nd / 3rd iteration are ( 89.0 % , 87.1 % ) / ( 95.5 % , 94.2 % ) / ( 98.3 % , 98.2 % ) , respectively .
Annotation Cost and Duration
The overall phase including the annotation and quality check spanned for 10 weeks ( from March to May 2022 ) , where the annotation guidance lasts for 2 weeks , data annotation lasts for 5 weeks , quality check lasts for 3 weeks . All the annotators are English native speakers recruited from a professional data management company Appen 3 . The annotation costs $ 5942 US dollars in total , with $ 0.31 per utterance rewrite , $ 0.03 per image box annotation . occurs in the query . Table 3 lists the statistics of our dataset , where each visual conversation contains 5.40 Q - A turns with 2.02 image boxes on average . Table 4 lists the number of rewrites in our dataset under different history context lengths , where most of the data contains context from 3 - 4 turns , and over 16k data contains context over 9 turns . Futhermore , we compare our dataset with existing datasets from related works . According to Table 1 , our dataset is : ( 1 ) more complete -we provide manual annotation on both image and text levels , where the rewrite query and entity boxes are both provided ; ( 2 ) more diverse -since our dataset is designed not only for all the coreference cases , but also performs ellipsis completion in the rewrite ;
Dataset Statistics
According to
( 3 ) much larger -compared with existing VCR dataset , the dataset has a larger scale . ( 1 )
Methods
Model Overview
Figure 2 depicts the overall structure of our proposed model . Our model is based on a multimodal pre - trained model VL - T5 ( Cho et al . , 2021 ) , where a Transformer - based encoder and decoder are jointly trained to perform generation . Besides , we add a pointer generator to the multimodal Transformer decoder so that the rewrite is either generated from scratch or copied from history context .
Image and Text Embeddings Text Embedding
The text input of our model includes two parts : the task prefix and the history context . The task prefix is a short prompt which aims to differentiate the concerned McQR task from other tasks in the pre - training phase ( e.g. visual grounding ) . Specifically , we use " query rewrite : " as the task prefix . Following it , the history context contains all the utterances including all the queries and answers in previous turns . The input text is then tokenized and embedded before passed into the encoder . Following the setting in T5 ( Raffel et al . , 2020 ) , we also add a relative position bias to each self - attention layer . As a result , the text input is represented as e t .
Image Embedding To extract image features , we first detect several object regions from the image , denoted as Region of Interest ( ROI ) . Following previous works ( Cho et al . , 2021 ; Lu et al . , 2019 ) , we also use Faster R- CNN ( Ren et al . , 2015 ) pretrained on the Visual Genome dataset ( Krishna et al . , 2016 ) to extract ROI features in our task . For each image I , we extract n = 36 ROIs from it .
The final visual vector can be represented as e v .
Encoder - Decoder Structure
Encoder . We use a multimodal Transformer encoder - decoder structure to encode the image text features and generate the target rewrite . The multimodal encoder is a stack of L Transformers which take the image and text representations as input
h 0 = [ e t 1 , .. , e t m , e v 1 , ... , e v n ] ( 2 )
h i = F N N ( M ultiHead ( h i−1 , h i−1 , h i−1 ) ) ( 3 ) Decoder .
Similarly , the decoder is also a stack of L Transformers . Given the decoder input x t , the decoding step consists three phases , where the first sub - layer is a self - attention layer which can be represented as
d i = M ultiHead ( d i−1 , d i−1 , d i−1 ) ( 4 )
where d 0 = x t . After that , the second sub - layer calculates the cross attention between the encoder outputs and the decoder self - attention result
s i = M ultiHead ( d i , h L , h L ) ( 5 )
The third sub - layer is a position - wise fully connected feed - forward neural network , followed by a softmax layer to output the final probability
P vocab ( y t ) = i : w i ∈V β t , i = Sof tmax ( F N N ( s i ) ) ( 6 )
where β is the softmax score over the whole vocabulary V . Multimodal Transformer with Pointer Generator . Additionally , following the motivation that most part of the rewrite sentence can be related to certain parts of the input context , we add a pointer generator ( See et al . , 2017 ) to the multimodal Transformer such that the rewrite is either generated from scratch or directly copied from the input . Specifically , the cross attention in the last decoder layer can be naturally taken as the context vector
c i = M ultiHead ( d i , h U L , h U L ) ( 7 )
where h U L is the textual part of the encoding result , which is the embedding of tokens before the " [ SEP ] " token .
α t , i = sof tmax ( ( W s s t ) T W h h i √ d ) ( 8
P copy ( y t ) = i : w i ∈H α t , i ( 9 )
where α is the copy distribution over the input H , P copy determines where to copy at time step t. By incorporating the pointer generator , the final probability of generating the target word y t at time step t is represented as
P ( y t ) = λP vocab ( y t ) + ( 1 − λ ) P copy ( y t ) ( 10 ) λ = sigmoid ( w T d c t + w T l s t + w T a x t ) ( 11 )
where w d , w l , w a are the weights to learn .
Training
We adopt the standard generation loss when finetuning the pre - trained model parameterized θ on our McQR task . At each time step t , the decoder output token is determined based on the generated rewrites of previous time steps denoted as y < t , the input image and text encoding vector e v and e t . We minimize the negative log - likelihood of generating
5 Experiments
Compared Methods
We compare our methods with several baselines .
• Original ( Elgohary et al . , 2019 ) is the method where the rewrite is set to be the same as the input query .
• AllenNLP Coref ( Gardner et al . , 2018 ) is a deep learning based NLP tool . We utilize its coreference resolution model to generate the rewrite .
• ( L / T ) -Gen ( Su et al . , 2019 ) denotes the LSTM / Transformer based encoder - decoder generation model . For Transformer - based models , we report the performance of two variants : Early Fusion which utilizes the same encoder to encode image and text features , and Late Fusion which first embeds image and text into vector spaces separately and then performs fusion into a joint embedding .
• ( L / T ) -Ptr ( See et al . , 2017 ) adds a pointer generator to the ( L / T ) -Gen model .
• VL- ( Bart / T5 ) ( Cho et al . , 2021 ) is the mulitimodal implementation of large pre - trained language model Bart / T5 .
• VL- ( Bart / T5 ) -Ptr is the model depicted in Section 4.2 that adds a pointer generator to the pre - trained model .
Experimental Settings
Our code is implemented based on Pytorch and Huggingface Transformers ( Wolf et al . , 2019 ) . We use the base version of the pre - trained VL - Bart / T5 in all our experiments . Our dataset is split into the training , testing , and validation dataset following the portion of 60 % , 20 % , 20 % , resulting in 48566 queries for the training set , 16189 queries for the testing and validation set . By default , we set the batch size as 32 and the learning rate to be 5e-5 , the model is fine - tuned for 20 epochs with the random seed of 42 . In the testing stage , all models decode words by beam search with beam size set to 5 . We employ several evaluation metrics to evaluate the quality of the generated rewrite . We first report the BLEU ( Papineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) , and METEOR ( Denkowski and Lavie , 2014 ) rate . In addition , we report the Exact Match ( EM ) of both positive samples that involves some changes in the rewrite and negative samples that the rewrite is the same as the query . On two subtasks , we report the precision , recall , and F1 score .
Experiment Results
We first report the overall performance on the query rewrite task . Then we perform experiments on two subtasks including coreference resolution and ellipsis completion , respectively .
Table 5 demonstrates the overall performance of different models on the McQR task . We have the following observations : first of all , compared with LSTM based models ( e.g. L - Gen ) , Transformer based models have a stronger generation ability ( e.g. T- ( E ) Gen ) , where the BLEU-2 score increases from 77.14 to 78.03 . In addition , with the help of the pointer generator , certain parts from the original query and the history context are able to be preserved in the rewrite , where the negative EM Table 6 shows the results of different models on the visual coreference resolution ( VCR ) subtask . We further utilize one state - of - the - art VCR method named MBERT to compare ( Yu et al . , 2022 ) . According to the table , traditional text - only coreference resolution methods such as AllenNLP Coref may not have a good performance in the VCR task . The reason is that , in the conversations , many pronouns refer to entities that never appear in the history context , but can be found clues in the image . Furthermore , it can be concluded that pointer network also has a positive influence on the results since many coreferred entities can be directly copied from previous turns .
Besides VCR , we also perform tests on the visual ellipsis completion task , as shown in Table 7 . The overall condition is similar to results in Table 6 , while there encounters a performance degradation in this task . This may result from the fact that recovering the omitted information may not be that apparent ( we will use some examples to illustrate in Section 6.3 ) . In conclusion , our model shows superior performance on both two tasks , demonstrating the effectiveness of our model in resolving the information omission of the conversations .
6 Extensive Analysis
Image Role Analysis
In order to further investigate the role of image information in the query rewrite task , we remove the image features and compare the performance with the original model under different length of context respectively . Specifically , we divide the overall 16189 testing data into five categories by the history context length , ending up with 3136 / 3380 / 3180 / 3185 / 3308 data records with the context turn length from 0 - 1 / 2 - 3 / 4 - 5 / 6 - 7 / 8 + , as shown in Figure 3 . According to the figure , the performance gets improved with the help of image information . Compared with Transformer generator , our model is less sensitive to the turn length , where the model still has a decent performance when the conversation goes deep . We can also observe that , in our model , when there is no or short textual context that the information in conversation history is limited , the gap between monomodal and multimodal performance reaches to the largest , showing that the model is more dependant to the image information in this case . While when the conversation is deep , the gap decreases , showing that the model learns to copy information from the rich history context when rewriting the query .
Multimodal Attention Visualization
We utilize the self - attention weight heat map in the Transformer block of our model to visualize how the model learns the cross - modal relationship of the conversation . In the example shown in Figure 5 ( the full conversation information can be found in Appendix D ) , the first three image segments correspond to ROI " man " , " boy " , " shorts " , respectively . According to the map , the pronoun " they " is correctly related to the " man " and " boy " ROI segments and also has a high attention weight to the text " the man " in the history context 4 . The learned relationship between visual and text representations can serve as the reason that our model shows superior performance concerning the multimodal feature incorporation in the McQR task .
Case Study
We provide several example rewrites generated by different methods to vividly demonstrate the quality of rewrites , according to Figure 4 . In the first case where the query is the first utterance where no textual context is provided , non pre - trained models ( e.g. T ( L ) -Ptr ) have difficulty resolving the pronoun " he " in the query . The results imply that with the help of prior knowledge , multimodal pretrained models have a superior ability in incorporating image and text features , mitigating the gap between representations of multimodal heterogeneous space . The second case shows that traditional Transformer pointer network tends to copy incorrect context segments whose rewrite results may not make sense to human readers . For example , the abbreviated entity after " except for " should apparently be " the bear " instead of " the people " , while our model has a better ability of understanding the whole context and outputing more accurate results . Furthermore , when it comes to complicated cases where coreference and ellipsis are both observed , our model is both capable of copying the entities from history to solve the coreference ( e.g. the red part in the third case ) and generate parts that require reasoning ( e.g. the blue part in the third case ) from scratch .
Error Case Analysis
As shown in Figure 6 , we demonstrate the most common error cases of our model , which can be classified into several cases including bad sentence structure , wrong coreference result , inaccurate omission completion , etc . According to the figure , we can see that pre - trained language models can generate sentences that have a proper format , but still have some problems in understanding the deep semantic correlation in the visual conversations . For example , in the top two cases , although the coreference seems simple for human readers , it poses challenges for machines other than directly making a copy from context . Specifically , for com- Original Query : Anything else to note ? GT : Is there anything else to note except for the train , the track , the grass , the water , and the mountain ? VLBart : is there anything else to note except for the train , the water , the mountains , VLBart - p : Is there anything else to note except for the train , the tracks , the water , the water ? Original Query : do the people in the group appear related to each other in some way or are they just a random group of people crossing the road at the same time GT : do the people in the group appear related to each other in some way or are they just a random group of people crossing the road at the same time VLBart : do the people in the group appear related to each other VLBart - p : do the people in the group appear related to each other in some way plicated cases such as when coreference and ellipsis appear at the same time , when the conversation is long , and when the omitted information requires reasoning between history entities ( e.g. the bottom cases ) , there still remains much space to explore .
Conclusion and Future Work
We propose the task of multimodal conversational query rewrite ( McQR ) , which aims to perform query rewrite under a multi - turn visual conversation . To facilitate the research , we benchmark a large - scale dataset with manual annotation which covers 15k visual conversations with more than 80k rewrites . We also provide image box annotation of entities appearing in the rewrites . Accordingly , we propose a model based on an existing multimodal pre - trained model and further enhance it with a pointer generator . Extensive experiments are performed to show the effectiveness of our model .
Limitations
Although our model achieves over 90 % BLEU-2 rate on our dataset , there is still room for improvements in the future work , since the exact match for positive rewrite queries can be further lifted . In addition , in the future work , we will also continue to explore how the dataset and model can further benefit downstream research under this scenario . Furthermore , fine - grained image features such as the image box annotation can be leveraged for improving the performance .
Ethics Statement
All data records of our dataset are originally from the Visual Dialog ( Das et al . , 2017 ) dataset , where all the images are collected from the COCO dataset . When annotating the data , we make sure that the annotators do not have any other rights except for the conversation information . Upon data publication , we will strictly follow the user terms of the Visual Dialog dataset .
A Rewrite Annotation Interface
Figure 7 shows the annotation interface when preparing the rewrite . We assign one text box for each query in the visual conversation so that the annotators can rewrite the query according to the image and history context .
B Image Box Annotation
Figure 8 shows the image box annotation interface , where the annotators extract entities from the rewrites and mark them on the image in the format of box .
Annotation area Content
Object List C Quality Control Questionnaire
We design the quality control questionnaire as shown in Figure 9 . All quality checkers have to click and answer the three questions .
Figure 9 : The questionnaire interface in the quality control phase .
D Dataset Examples
We list some examples of our dataset to vividly demonstrate the characteristic of the task . All the examples are listed in Figure 10 .
Movie101 : A New Movie Understanding Benchmark
To help the visually impaired enjoy movies , automatic movie narrating systems are expected to narrate accurate , coherent , and role - aware plots when there are no speaking lines of actors . Existing works benchmark this challenge as a normal video captioning task via some simplifications , such as removing role names and evaluating narrations with ngram - based metrics , which makes it difficult for automatic systems to meet the needs of real application scenarios . To narrow this gap , we construct a large - scale Chinese movie benchmark , named Movie101 . Closer to real scenarios , the Movie Clip Narrating ( MCN ) task in our benchmark asks models to generate role - aware narration paragraphs for complete movie clips where no actors are speaking . External knowledge , such as role information and movie genres , is also provided for better movie understanding . Besides , we propose a new metric called Movie Narration Score ( MNScore ) for movie narrating evaluation , which achieves the best correlation with human evaluation . Our benchmark also supports the Temporal Narration Grounding ( TNG ) task to investigate clip localization given text descriptions . For both two tasks , our proposed methods well leverage external knowledge and outperform carefully designed baselines . The dataset and codes are released at https : / / github.com / yuezih / Movie101 .
Introduction
The estimated number of visually impaired people worldwide was about 285 million by 2020 , according to reports ( He et al . , 2020 ) . While regulations are in place to ensure increased access for these audiences to experience the culturally dominant movies and TV shows on popular media platforms , technologies that provide them with genuine experience are becoming increasingly important . Audio description ( AD , also known as video description ) * * Corresponding Author . is a form of such technology intended for visually impaired audiences to experience the movie or TV show by hearing what is happening on - screen . However , producing movie narration scripts is not trivial , often requiring a professional writer to oversee the original movie . The high cost of narration generation ( Lakritz and Salway , 2006 ) greatly hinders the production of movies with AD and thus limits the opportunities for visually impaired users to experience movies .
To address this issue , attempts have been carried out to automate AD production . Datasets of movies with ADs are constructed to support the research on automatic AD generation , including the MPII - MD dataset ( Rohrbach et al . , 2015 ) and M - VAD dataset ( Torabi et al . , 2015 ) , with shotlevel ADs or scripts aligned to the visual contents of movie . Consequently , different solutions for automatic movie narrating have been proposed based on these datasets .
However , existing benchmarks suffer from several limitations . Firstly , there is a gap between the designed tasks and the actual movie narration scenario . These tasks mainly focus on generating single - sentence narrations for shots of a few seconds . They can not support the generation of coherent narrations for longer plots , which is critical for the visually impaired to better understand the movie , and the timestamps of these shots are carefully annotated , which are difficult to obtain for new movies in real application . Meanwhile , these tasks treat the very distinctive movie narrating task as a normal video captioning task through some simplifications such as replacing role names with SOMEONE , resulting in the inability to connect roles to plots . Secondly , these benchmarks evaluate the generated narrations with ngram - based metrics , which can over - penalize a semantically correct but textually inconsistent narration , especially when there is only one reference available . In addition , these existing datasets are all in English . However ,
Introduction
At the wedding of his first love QiuYa , XiaLuo pretended to be rich and made a fool of himself , and was exposed by his wife MaDongmei ... about one - fifth of the world 's population speaks Chinese as their mother tongue , of whom more than 17 million are visually impaired ( Yu and Bu , 2021 ) . Therefore , building a Chinese movie narration benchmark is necessary .
Intending to address the limitations of the existing narrating benchmarks , in this work , we propose a new benchmark with 101 Chinese movies for movie understanding , named Movie101 . We collect the movies from the barrier - free channel on Xigua Video platform 1 , where normal movies are remastered with ADs . Through automatic process and manual correction , we obtain the ADs and actor lines from the raw videos . We crawl rich metainformation relevant to the movies as well . Finally , Movie101 contains 30,174 narration clips totaling 92 hours , with data samples as shown in Fig . 1 . As our investigation shows that narrations mostly occur at those times when no actors are speaking ( see Appendix A ) , to achieve realistic movie narrating , we propose the Movie Clip Narrating ( MCN ) task that requires a model to narrate where there are no lines . It brings a potential benefit for identifying where to narrate in an unlabeled new movie , since the timestamps of the actor lines are easily accessible 2 . Meanwhile , in order for the audience to accurately comprehend the role - related plots , concrete role names should be contained in the generated narration . For the MCN task , we reorganize the Movie101 dataset , merging the narration clips between two actor dialogues into a longer clip , to simulate real - scenario movie narrating . We thus obtain 14,109 long clips of variable length for narration generation . Moreover , to better evaluate the quality of model - generated narrations , we conduct 1 https : / / www.ixigua.com / channel / barrier_free 2 The timestamps of the lines can be obtained from the movie script or by automatic methods such as OCR and ASR .
human evaluations and design a new metric specific to movie narrating , namely Movie Narration Score ( MNScore ) , which well aligns with human evaluation . In addition to the MCN task , our dataset also supports the Temporal Narration Grounding ( TNG ) task , which asks a model to locate target clips in the movie according to some text descriptions . For both tasks , we benchmark the performance of existing methods , and further propose our improved models by incorporating auxiliary external knowledge . In addition to MCN and TNG tasks , Movie101 can also potentially support other movie understanding tasks such as visual question answering and action recognition , etc .
The main contributions of this paper are as follows : 1 ) We propose a new benchmark for movie understanding , Movie101 , with a large number of video - aligned text descriptions in Chinese . 2 ) We propose two primary tasks , MCN and TNG , and a new narrating evaluation metric MNScore , where MCN is more in line with the needs of actual movie narrating , while MNScore is more consistent with human evaluation . 3 ) We benchmark state - of - theart models and propose improved models enhanced by external knowledge for MCN and TNG , respectively . We expect our proposed Movie101 benchmark can inspire more explorations on narrating and understanding a whole movie .
Related Works
Datasets . Existing datasets to support the automatic narration generation task include M - VAD ( Torabi et al . , 2015 ) and MPII - MD ( Rohrbach et al . , 2015 ) , which are merged into LSMDC . M - VAD , which is collected based on an automatic AD segmentation and alignment method , contains 47 K videos from 92 DVDs , with an average length of 6.2s , each with an aligned narration . MPII - MD contains 68 K videos from 94 movies with an average duration of 3.9s , about half of which come with paired scripts and the other half with paired ADs . In addition to movies , TV shows are also good data sources for automatic narration generation . Lei et al . ( 2020 ) propose TV Show Caption ( TVC ) , a variant of TV Show Retrieval ( TVR ) . It contains 11 K short videos averaging 9.1s in length , and 26 K captions describing the visual content , dialogues , and subtitles . All the existing datasets are in English .
Video Captioning . As a classic vision and language task , the video captioning task requires a model to generate natural language descriptions for given videos . Solutions for normal video captioning go through stages from pre - designed templates ( Kojima et al . , 2002 ; Guadarrama et al . , 2013 ) to sequence - to - sequence generation with deep neural networks ( Pasunuru and Bansal , 2017 ) . A challenging variant for this task is dense video captioning ( Krishna et al . , 2017 ) , which requires the generation of multi - sentence descriptions for long multievent videos . The two - stage generation approach , which firstly performs proposal detection on the video and then generates descriptions for each proposal separately , has been the dominant approach ( Krishna et al . , 2017 ; Park et al . , 2019 ; Rohrbach et al . , 2014 ; . Recently , some works avoid event detection and generate paragraph descriptions directly based on the video , such as the one - stage paragraphing model ( OVP ) ( Song et al . , 2021 ) , obtaining competitive performance compared to previous works , inspired by which we propose our knowledge - enhanced movie narrating model . Identity - aware video description that distinguishes different persons is more practical in real applications . Park et al . ( 2020 ) attempt to achieve role - aware movie narrating by distinguishing different people using labels such as PERSON1 , PERSON2 , etc . However , it fails to generate concrete role names and falls short in terms of practicality .
Temporal Sentence Grounding . The temporal sentence grounding ( TSG ) task aims to localize the moment in a video based on a natural language query ( Gao et al . , 2017 ) . A two - step pipeline has been the mainstream approach , which first produces a large number of moment candidates via sliding windows , then ranks them with their similarity to the query sentence . The following works try to improve the grounding performance by enhancing interaction between video and query modalities or introducing novel detection heads ( Lei et al . , 2021 ; Zhang et al . , 2020a ) . Specifically , for interaction methods , adopt an Iterative Alignment Network ( IA - Net ) to iteratively interact interand intra - modal features within multiple steps . explicitly decompose video and query into multiple structured hierarchies and learn finegrained semantic alignment among them . In this work , we propose to incorporate external knowledge based on the IA - Net model structure .
Dataset
Data Collection
Movie Acquisition . To the best of our knowledge , there are only a handful of platforms that provide accessible movies in Chinese . The barrierfree channel of Xigua Video is one such platform that provides over 100 accessible movies online , and new movies are still being released that can support further expansion of our dataset . From Xigua Video , we collect all 101 movies available to date and crawl as much meta information as possible for each movie , including title , introduction , genres , directors , actors , etc . We emphasize actors in particular , including actor names , role names , actor portraits , role rankings , and other information about important roles . We expect such information can benefit the movie narrating task and general movie understanding tasks . Narrations and Lines Extraction . As the movie lines and narrations are only available in the subtitle and audio format respectively from the platform , we therefore leverage OCR and automatic speech recognition ( ASR ) tools for transcription . For lines , we extract text from subtitles by open - source OCR toolkit PaddleOCR 3 at 2.4 FPS , and manually remove the irrelevant subtitles from the beginning and the end of each movie . For narrations , we extract the audio track from the movie and utilize the ASR service provided by iFlyTek 4 , which detects the speech in the audio and transcribes it into text . In addition , the service supports identifying different speakers , which helps discriminate the narrator from the actors . However , the ASR service is not perfect , and its outputs contain errors such as wrong characters , unreasonable sentence breaking , and misidentification of narrations as movie dialogues , etc . Therefore , we recruit human annotators to further correct the ASR transcription errors and remove non - narration texts manually to improve the data quality . We also delete the irrelevant fragments at the beginning ( e.g. , movie synopsis , cast introductions ) and the summary narration at the end . For coherency , we further organize the narration fragments at the clip level . We merge every two fragments if their temporal gap is less than 1 second . we also apply a paragraph - length threshold of 100 characters to limit over - merging to avoid excessively long clips . We take punctuation into account as well , for example , a period in Chinese is likely to mean the end of a narrative paragraph . Further detailed descriptions of data quality can be found in Appendix B. Movie101 - N and Movie101 - G. For real - life movie narrating , models are expected to narrate in the breaks between different actor dialogues . Thus , we reorganize Movie101 to fit this task format . Concretely , we first merge the independent lines in Movie101 into dialogues , where two lines with a temporal gap shorter than 5 seconds are considered to belong to one dialogue . Then , we merge all the narration clips between two adjacent dialogues into a long paragraph . In this way , we obtain Movie101 - N with narration paragraphs separated by dialogues , which well simulates the practical narrating challenge . Meanwhile , with rich videotext pairs in Movie101 , we create another variant dataset to support the temporal grounding tasks , named Movie101 - G , where narrations are taken as queries and aligned videos serve as targets . For validation and testing , we carefully select 10 movies of different genres for each respectively .
Dataset Statistics
Movie Properties . Movie101 contains 101 movies , involving 41 genres ( a movie can belong to up to 4 genres ) and 645 roles in total . Fig . 2 shows the numbers of movies in the top 10 most popular genres , with comedy , romance , and action in the top 3 . Clip Properties . Movie101 contains a total of 1 shows that Movie101 - N contains much longer video clips and text descriptions than existing movie narrating datasets , while the length distribution in Fig . 4 indicates that the clip length varies a lot . Movie101 - G contains 30,174 clips to be located from 101 movies . The average video length of 6,144 seconds also greatly exceeds existing TSG datasets .
Movie Clip Narrating
Task Description
In order to help the visually impaired keep up with the plot in the movie , we first propose a Movie Clip Narrating ( MCN ) task , which aims to generate a plot - related paragraph description given a clip in Movie101 - N. Besides , the narration styles may vary across different genres of movies . The role portraits are important external knowledge for a model to accurately describe the subject of actions . Thus , we also provide this information in Movie101 - N to support the MCN task .
Proposed Method
For the MCN task , with multimodal inputs including video , movie genres , role names , and actor portraits , we propose a Transformer - based ( Vaswani et al . , 2017 ) model with an encoderdecoder framework , namely Role - pointed Movie Narrator ( RMN ) , where the encoder mainly encodes video clips and the decoder generates narrations , as shown in Fig . 5 ( a ) .
On the encoder side , taking into account the frame - level visual information , the video clip is embedded into a sequence of frame - level features . To emphasize the roles , we extract face features from each frame and concatenate them to the corresponding frame feature sequentially based on the confidence scores of face detection . With learnable genre embeddings , genres are also represented as a sequence of genre features . After video and genre representation , we apply a Transformer encoder to perform cross - encoding . Then , we follow the One - stage Video Paragraphing model ( OVP ) ( Song et al . , 2021 ) to use a dynamic memory bank to refine the video - part representations , which updates at each decoding step .
On the decoder side , in addition to the Transformer decoder , we enable the model to directly choose a complete role name from the movie cast according to context during token - by - token generation via a pointer network ( Gu et al . , 2016 ) . At the decoding step t , with the decoder hidden state h t , we first calculate the token scores y voc t among normal vocabulary . Then we design a Role Selector module to get the name scores among external
y t = f ( [ y voc t ; λy role t ] ) ( 1 )
where [ ; ] means concatenation , λ is a gate computed from h t , f ( ) is the softmax function .
Evaluation
Existing movie narration benchmarks directly adopt ngram - based metrics including CIDEr , BLEU , and METEOR as in normal video captioning . However , there are pitfalls for these metrics , such as underestimating semantically correct but textually inconsistent phrases , which have been widely reported ( Zhang et al . , 2020b ; Shi et al . , 2022 ) . For movie narrating , a movie clip can be narrated in multiple expressions , while there is only one reference . Thus , text matching is inadequate to measure the quality of a narration paragraph .
To better evaluate the generated narrations in the MCN task , we conduct a manual evaluation to investigate how humans assess different narrations . We randomly select 30 movie clips , each with 5 candidate narrations , of which 3 are derived from the predictions of different models and 2 are obtained by disturbing the ground truth narrations . Next , we recruit 10 annotators to individually rank the candidates for each video in terms of accuracy , informativeness , and textual quality . Accuracy defines how the narration accurately describes the video , especially roles , actions , and objects ; informativeness defines how richly the narration reveals the video content ; textual quality is determined by the narration fluency and grammatical correctness .
With the human evaluation results , we investigate a wide range of objective metrics as follows : ( 1 ) State - of - the - art video captioning metrics based on deep neural networks including CLIP - Score ( Hessel et al . , 2021 ) , BERTScore ( Zhang et al . , 2020b ) and EMScore ( Shi et al . , 2022 ) , which are reported outperforming ngram - based metrics in video captioning evaluation ; ( 2 ) Textual quality metrics including n - grams diversity ( Shetty et al . , 2017 ) ( DIV ) and causal language model perplexity ( PPL ) ; ( 3 ) F1 score of role name generation ( RoleF1 ) . For every two candidate narrations of a video , we use human ranking as a reference to determine whether these metrics correctly judge which of the two candidates is better or worse , and the accuracy is used for evaluating metrics ' correlation with human judgment . Finally , we settle on a new metric Movie Narration Score ( MNScore ) as follows :
mns = 1 • ems + 4 • berts + 1 • rf 1 6 × 100 ( 2 )
where mns , ems , berts and rf 1 refer to MNScore , EMScore , BERTScore and RoleF1 , respectively . As shown in Table 2 , BERTScore outperforms ngram - based metrics in narration evaluation accuracy , while our new proposed MNScore achieves the best alignment with human evaluation . More details about the implementation of the candidate narrations and the above metrics are presented in Appendix C .
Experiments
Implementation Details . In our proposed method , models are trained with next - token language modeling by the maximum likelihood estimation ( MLE ) objective . For videos , we use CLIP ( Radford et al . , 2021 ) pre - trained on large - scale image - text pairs and MIL - NCE ( Miech et al . , 2020 ) pre - trained on HowTo100 M videos ( Miech et al . , 2019 ) to extract frame - level CLIP and S3D features with dimensions of 512 and 1024 , respectively , at 1 FPS , and further concatenate them . For faces in video frames and portraits , we use the Arcface model ( Deng et al . , 2019 ) pre - trained on MS1 M ( Guo et al . , 2016 ) to extract face features . When there are insufficient faces detected within a frame , the 3 , RMN outperforms the baselines by a large margin , especially on RoleF1 . This indicates that our model learns to generate role names from external knowledge with the help of the pointer network . To verify the contribution of the genre and face representations in our RMN model , we also perform an ablation study by progressively adding these representations as input . From the results , face features extracted from video frames bring significant gains in role awareness , which shows that using face features to bridge the video content and external actor portraits is beneficial for generating role - related narrations . Qualitative results can be found in Appendix D .
Temporal Narration Grounding
Task Description
To help people locate clips of interest during movie entertainment , an AI agent should be able to understand users ' intentions and locate the target clips . To achieve this goal , we propose the Temporal Narration Grounding ( TNG ) task . Given a clip narration as the query , TNG aims to predict the starting and ending time of the clip in the whole movie .
Proposed method
Existing temporal sentence grounding models can hardly handle an entire movie input with limited computational resources . Thus , we propose a twostage framework for the TNG task , with global shot retrieval to coarsely locate the target clip in the first Global Shot Retrieval . To find the approximate location of the target , we treat it as a text - video retrieval subtask . We divide a movie into 20s - long shots , and the shot with the highest similarity to the text query will be used as the anchor for further grounding in the second stage . For training such a retrieval system , we construct a temporary dataset Movie101 - GSR ( temp ) . Concretely , after cutting the movie into shots , each shot and each annotated narration in Movie101 are judged with the temporal overlap whether they can be considered as an aligned video - text pair . 6 We build the retrieval model by transferring a Chinese Vision - Language Pre - training ( VLP ) model ChineseCLIP ) ( CNCLIP ) from image - text to video - text . Specifically , the shot frames are separately encoded as image features by the visual encoder of CNCLIP , and the final video feature is obtained by performing mean pooling over the CLS tokens of all frames . We then perform contrastive learning between the video and text features on Movie101 - GSR ( temp ) to fine - tune the modified CNCLIP . Local Temporal Grounding . After obtaining the anchor shot in the first stage , we further lo-
Experiments
Implementation Details . For Global Shot Retrieval , we use average Recall @ n ( n ∈ 1 , 5 , 10 ) to evaluate the retrieval performance on all movies . For Local Temporal Grounding , following previous works ( Zhang et al . , 2020a ) , we use " R @ n , IoU @ m " as metrics , which are defined as the percentage of at least one of top - n proposals having a larger temporal IoU than m with the ground truth . We fine - tune CNCLIP - huge on our Movie101 - GSR ( temp ) for Global Shot Retrieval , and benchmark two code - released state - of - the - art temporal grounding models 2D - TAN ( Zhang et al . , 2020a ) and IA - Net on Movie101 - LTG ( temp ) for Local Temporal Grounding . In our RNL model , the video frame , face , and text feature extractors are pre - trained MIL - NCE , Arcface ( same as in the MCN task ) and BERT - base - Chinese ( Devlin et al . , 2019 ) , respectively .
Results & Analysis . Table 4 and Table 5 show the performance of models on Global Shot Retrieval and Local Temporal Grounding , respectively . Our RNL outperforms baselines by introducing roleaware video and text encoding , indicating that distinguishing actions of different roles is critical for grounding movie narration . Furthermore , we perform an ablation study to verify the effectiveness of role - aware encoding . As shown in Table 5 , adding face features to either video or text representations outperforms our base method IA - Net . RNL with both role - aware video and text encoding achieves the best performance . Table 6 shows the performance of combined inference by Global Shot Retrieval and Local Temporal Grounding . We in addition show the performance of k - way re - ranking , where the top - k shots retrieved in the first stage are respectively used as the anchors in the second stage , and all predictions obtained are re - ranked with their confidence scores . The experimental results show that k - way re - ranking improves Rank @ 5 performance but harms Rank @ 1 performance . Qualitative results can be found in Appendix D .
Conclusion
In this work , we propose Movie101 , a Chinese large - scale video benchmark for movie understanding . To assist visually impaired people in enjoying movies , we propose a more realistic Movie Clip Narrating task to address the automatic movie description issue and design a human - preferencecompatible metric MNScore for narrating evaluation . Movie101 also supports the Temporal Narration Grounding task , which is more challenging than the previous TSG benchmarks . Furthermore , our experiments validate the importance of external knowledge including genres and roles for movie understanding . However , there is still a significant gap between our models and expert annotations . This reveals that further research endeavors are still needed to help visually impaired people enjoy movies by AI .
Limitations
Keeping narration coherent within a movie is crucial for visually impaired people to enjoy the movie .
In this work , we move a step forward for this target by setting the ground - truth texts in the Movie Clip Narrating task as narration paragraphs and providing longer video clips as inputs . However , how to ensure description coherence across different clips within a movie has not been studied in this work . This requires a higher - level comprehending ability of models to process the whole movie and connect different plots . We leave this to our future investigation .
Ethics Statement
We propose Movie101 , a new benchmark to support exploring technologies to benefit the accessibility of the visually impaired . There are two potential ethical issues with our work , regarding data source and crowdsourcing services , respectively . We state each of them as follows : Data Source . The collected movies are publicly available from Xigua Video , and are allowed to be crawled according to the service contract of the website 7 . Considering the copyright issue , we will only release the url list of movies . Besides , our data source does not contain any information that names or uniquely identifiable individuals or offensive content .
Crowdsourcing Services . We recruited 20 Chinese college students ( 12 females and 8 males ) via social media . For ASR outputs cleaning , workers were required to correct errors in the narration text while watching the movie . For each movie , it took about 2 hours with a payment of 50 RMB ( $ 7.40 USD ) . To review corrections , for each movie , it took about 30 minutes with a payment of 25 RMB ( $ 3.70 USD ) . Our payment is fair and reasonable in China , especially since the work is easy and fun . Before the annotation works began , we introduced the future use of the data in the task document to ensure that everyone was informed .
A Narration Distribution
Clips where ' no actors are speaking ' refer to ANY scene wherein no verbal dialogue is being employed by the actors , regardless of whether they are visually present or absent . This definition encompasses , for example , a scene focused solely on a depiction of the sky . We detail the dialogues and narrations in the 101 collected movies . By merging the actor lines , we obtain a total of 15,307 dialogues , constituting 15,206 dialogue gaps with a total duration of 99.4 hours . The 30,174 narration clips we collect fill in 95.3 % of the dialogue gaps in terms of quantity and cover 92.9 % in terms of duration . Therefore , it is reasonable to assume that where there are no lines , there is a need for narration .
B Dataset Quality Description
We adopt a two - stage annotation process to ensure the quality of the narrations . In the first stage , a group of workers is recruited to clean the data according to our guidelines . In the second stage , another group of workers further checks and corrects the annotation data . Our heuristics used to divide the paragraphs are designed based on our observation experience . We further conduct a manual evaluation of the narration quality . Of the randomly sampled 300 paragraphs , ( 1 ) in terms of narration recognition , 96.7 % are textually consistent with original ADs ; ( 2 ) as for the paragraph coherence , 90 % maintain complete and coherent semantics , 7.7 % should be merged with contexts , and 2.3 % should be divided into multiple paragraphs . Thus , the narration is of good quality to support downstream tasks .
C Implementation Details
Candidate Narrations . In Section 4.3 , We provide 5 different candidate narrations for each sampled movie clip for human evaluators to rank . These candidates are created as follows :
1 . generated by the Vanilla Transformer ( Zhou et al . , 2018 ) ; 2 . generated by the OVP model ( Song et al . , 2021 ) ; 3 . generated by our proposed RMN model ; 4 . generated by disturbing the ground truth with role name removal and replacement ; 5 . generated by disturbing the ground truth with nouns and verbs replacement .
Metrics Implementation . For CLIP - based metrics including CLIPScore and EMScore , we finetune ChineseCLIP - huge on our dataset in the same way as in Section 5.2 . For each movie clip and generated narration , CLIP - Score is calculated with the mean pooled feature of 10 uniformly selected frames and the overall text feature , while EMScore is calculated with all selected frame features and textual token features .
For BERTScore , we use the BERT - base - Chinese ( Devlin et al . , 2019 ) model checkpoint to calculate , and rescale the raw BERTScore with baseline 8 . For DIV , we calculate 1 - gram diversity and 2 - gram diversity following Shetty et al . ( 2017 ) , and average them . For PPL , we obtain the perplexity of each narration with the causal Ernie 3.0 model ( Sun et al . , 2021 ) following the calculation of Hugging - Face 9 . For RoleF1 , we extract role names from the ground truth and the generated narration . We measure how the generated narration covers the roles appearing in the movie clip by Recall ; given that these generated role names may also come from the model 's hallucination , for example from a wrong movie , we also take Precision into account . Finally , we calculate the F1 score with Precision and Recall . Hyperparameters and Computation . We detail the key hyperparameters and computational burden for the models training in Table 7 . For each model , the results are derived from a single run .
D.2 Temporal Narration Grounding
Fig . 7 shows the qualitative results of our proposed two - stage method . Through Global Shot Retrieval , we obtain an anchor shot near the target clip from the whole movie , which further helps Local Temporal Grounding to locate the final target . GT : 现场 ⼤ 屏幕上的数字转了起来， ⻩ 达和主持 ⼈ 转身看向 ⼤ 屏幕，数字转动了 ⼀ 会 ⼉ 之后停了下来， ( The numbers on the big screen turn up , and HuangDa and the host turn to look at the big screen , the numbers turns for a while and then stops . )
VT : 他们在台上观察着，时间间的位置上台下的观 ⾳ 室内， ( They watch from the stage , the position between time on the stage in the chamber of the observer , )
OVP : 第 ⼆ 天，三 ⼈ 来到现场，孟云和余 ⻜ ⼀ 起看着屏幕上的选择题，三 ⼈ 离开了。 ( The next day , the three come to the scene , MengYun and YuFei look at the multiple choice questions on the screen together , the three leave . ) RMN : ⻩ 达看着台下的电脑，这时余 ⻜ 和 ⻩ 达拉着 ⼿ 来到台球厅，他们相互打量着这 ⼀ 切， ( HuangDa looks at the computer under the stage , at this time , YuFei and HuangDa come to the billiard room with hands holding , they survey all this each other )
GT : 王多 ⻥ 站在保险公司 ⼤ 厦最顶层，穿着红 ⾊ ⾐ 服绿裤衩， ⼀ 只 ⼿ 背在后 ⾯ ，另 ⼀ 只 ⼿ 扶着巨 ⼤ 的 " 瘦 " 字，双腿交叉带着 ⼀ 脸享受闭上双眼，倚靠在 " 瘦 " 字上 ⾯ 。镜头缓 缓上升拉伸，王多 ⻥ 变得越来越渺 ⼩ ，最后完全看不 ⻅ 了。 ( WangDuoyu stands at the top of the insurance company building , wearing red clothes and green pants , one hand behind the back , the other hand holding the huge " thin " character , legs crossing , with a face of enjoyment , closing eyes , leaning on the " thin " character . The camera slowly rises , WangDuoyu becomes smaller and smaller , and finally completely invisible . )
VT : 在众 ⼈ 的 ⾼ 举 ⾏ 下，张彪也在这 ⼀ 场，下 ⾯ 的 ⾼ 楼下，阿俊也摔倒在地上。 ( In the crowd hold under the high , ZhangBiao is also in this scene , below the high floor , Arjun also falls to the ground . )
OVP : 随着 ⻜ 机的轰鸣声，继续朝下抓捕， ⽽ 在楼梯上的临时，他选择了 ⼀ 个 ⾼ 挑的身 影，这时，焦急的他选择了 ⼀ 个按钮， ⼤ 厦向下 ⻜ 去。 ( With the roar of the plane , continues capture downward , while on the stairs of the temporary , he chooses a tall figure , at this point , anxious he chooses a button , the building flies downward . )
RMN : 随后，王多 ⻥ 在天台上朗着升机，踏上了 ⾏ 程，登基本的装饰演员，王多 ⻥ 独 ⾃ 在空中 ， 美丽在云 ⼤ 楼 ⾥ 摆着各种姿势 ， 王多 ⻥ 顺着绳索向上攀爬 ( Subsequently , WangDuoyu boards on the rooftop … , embarking on a trip , … decorative actors , WangDuoyu is in the air alone , posing in a variety of positions beautifully in the cloud building ; WangDuoyu climbs upward along the rope )
GT : 桃 ⼦ 边说话边拦下 ⼀ 辆出租 ⻋ ，然后坐上 ⻋ 快速离去了。 ⻩ 达 ⼀ 个 ⼈ 愣在原地 。 ( Taozi stops a cab as she talks , then gets in and quickly leaves . HuangDa freezes alone . )
VT : 卢 ⼩ ⻥ 看到了他的眼神，他低头看着他，然后低下头， ( LuXiaoyu sees the look in his eyes , and he looks down at him , then lowers his head , )
OVP : 江丰回过头来看着他，然后叹了 ⼝ ⽓ ， ( JiangFeng looks back at him , then sighs , ) In the narration texts , green and red characters denote the correctly and wrongly generated role names , respectively . In the tables , metrics in green indicate that the ranking of candidates by the metric is consistent with human ranking , while red indicates inconsistency .
Acknowledgements
This work was partially supported by the National Key R & D Program of China ( No.2020AAA0108600 ) and the National Natural Science Foundation of China ( No . 62072462 ) .
Query : 影 ⽚ 开始挂满鲜花的欧式 ⼤ 铁 ⻔ 缓缓打开，铁 ⻔ 内是 ⼀ 座欧式建筑 ， ⼀ 辆 ⼩ 汽 ⻋ 从 ⻔ 外 ⾏ 驶 ⽽ 进，它穿过摆满花束的院 ⼦ 中， ⼀ 名保安在礼堂前 ⼀ ⼿ 拿起路障， ⼀ ⼿ 指挥着汽 ⻋ 向前，这辆 ⻋ 没有停下 。 ( At the beginning of the film , a large European - style iron gate full of flowers slowly opens . Inside the iron gate is a European - style building . A car drives in through the gate , and crosses the courtyard full of flowers . A security guard holds a barricade in front of the auditorium with one hand and directs the car forward with the other ; this car does not stop . )
( Another day , a paper airplane flies through the roof of the school building . QiuYa is standing alone on the roof , when YuanHua slowly walks over . QiuYa takes a look at YuanHua and then frowns , while touching her pigtails , while lowering her head , YuanHua spits out his mouth and asks with tears . ) Section : Appendix C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Section : Appendix
GT
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022 , with the addition of a question on AI writing assistance .
Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts
This work demonstrates that Legal Judgement Prediction systems without expert - informed adjustments can be vulnerable to shallow , distracting surface signals that arise from corpus construction , case distribution , and confounding factors . To mitigate this , we use domain expertise to strategically identify statistically predictive but legally irrelevant information . We adopt adversarial training to prevent the system from relying on it . We evaluate our deconfounded models by employing interpretability techniques and comparing to expert annotations . Quantitative experiments and qualitative analysis show that our deconfounded model consistently aligns better with expert rationales than baselines trained for prediction only . We further contribute a set of reference expert annotations to the validation and testing partitions of an existing benchmark dataset of European Court of Human Rights cases .
Introduction
The task of Legal Judgment Prediction ( LJP ) has recently gained increasing attention in the legal and mainstream NLP communities ( Aletras et al . , 2016 ; Zhong et al . , 2018 ; Medvedeva et al . , 2020 ; Liu et al . , 2019 ; Sert et al . , 2021 ) . Legal cases are resolved through the exchange of arguments in front of a decision body by lawyers who represent litigating parties . This typically involves evidential reasoning , the determination of relevant rules from sources of law ( e.g. , codes , regulations , precedent ) , their application to the case , and the balancing of legal and societal values . In the NLP context , LJP takes the form of classifying the outcome of a case from some textual representation of its specific facts , effectively skipping legal reasoning . This forms a counterpoint to knowledge - focused approaches to outcome prediction ( e.g. , Brüninghaus and Ashley , 2005 ; Branting , 2013 ; Grabmair , Figure 1 : Our deconfounding experiment architecture 2017 ) that connect to a lawyer 's understanding of the domain but also require substantial knowledge engineering .
This carries particular risk in the legal domain , where systems may rely on data elements that are statistically predictive but legally irrelevant , or even forbidden as decision criteria ( e.g. , the race of an accused person ) . This can lead to undesirable consequences , ranging from suboptimal litigation strategy decisions , flawed inference about factors predictive for the outcome , to disparate impact of decisions across groups that are to be treated equally . If legal decisions are to be informed by predictive systems processing textual case descriptions , then such systems must strive to be as closely aligned with legally relevant and permissible parts of the input as possible .
In this work , we focus on LJP for the European Court of Human Rights ( ECtHR ) , which adjudicates complaints by individuals against states about alleged violations of their rights as enshrined in the European Convention of Human Rights . We trained deep neural models on four tasks across two existing , related datasets ( Chalkidis et al . , 2019 ( Chalkidis et al . , , 2022a around predicting such violations alleged by the claimant and decided by the court . We find that the models substantially base their predictions on aspects of the text that correlate with the outcome but either have no legal bearing or are forbidden nationality - related information that stem from the distribution of cases arising at the court .
To improve the alignment of model focus with legal expert understanding , we apply a series of deconfounding measures , including a vocabularybased method which identifies predictive tokens using a simple model . The third author , who is an ECtHR expert , then identifies distractors among them . The distracting signal can subsequently be removed from the encodings via adversarial training . This procedure is an effective way of engaging with domain experts and obtaining information about what the model should be steered away from by means of deconfounding , rather than trying to attract the model towards relevant elements via expensive data collection for supervised training . For simplicity , throughout this paper , we use ' deconfounding ' in an inclusive sense as the mitigation of distracting effects of ( a ) confounders in the statistical sense that influence both the dependent and independent variables , ( b ) reverse causation relationships , and ( c ) other attributes that spuriously correlate with the target variable . See Fig . 1 for an overview of our experiment design .
We evaluate our trained and deconfounded models with regard to an alignment of its explanation rationales with ( 1 ) a dataset of expert passage relevance assessments we collected and will make available to community as a supplement to Chalkidis et al . ( 2019 ) , and ( 2 ) on expert relevance assessments published as part of Chalkidis et al . ( 2021 ) . Our results show that our deconfounding steps succeed in improving the model focus alignment with expert - identified , relevant patterns on both sets of reference annotations .
In sum , we make the following contributions :
• We introduce an expert - informed deconfounding method which identifies distracting effects from confounders and spurious correlations using a simple model , and mitigates them through adversarial training , thus helping to improve the alignment of the model focus with legal expert rationales .
• We empirically evaluate this method on four tasks in legal judgment prediction on ECtHR data and show that our model consistently aligns better with expert rationales than a baseline trained for the prediction target only .
• We release a set of gold rationales annotated by an ECtHR expert as a supplement to an existing dataset to facilitate future work on deriving more useful insight from trained predictive systems in the legal domain . *
Related Work
LJP as an NLP task has been tackled using ngram representations ( e.g. , Aletras et al . , 2016 ; Medvedeva et al . , 2020 ) , word embeddings and domain models ( Branting et al . , 2021 ) , and deep neural networks ( e.g. , Chalkidis et al . , 2019 ; Ma et al . , 2021 ; Xu et al . , 2020 ) . Special attention must be given to the origin of the text from which the prediction is to be made . Medvedeva et al . ( 2021Medvedeva et al . ( , 2022 recharacterize LJP on texts produced before the outcome is known as ' forecasting ' and observes that most current works ' classify ' judgments based on the data compiled after the outcome has been determined . They also find that forecasting is a harder task . This result is consistent with our finding of confounding effects from text production by the ECtHR , resulting in a prediction from fact descriptions that were influenced by the decision . Moverover , the relationship between the information LJP models rely on and legal expert analysis of texts remains underexplored . Bhambhoria et al . ( 2021 ) find that transformer - based models exploit spurious correlations and that simple models , such as XGBoost , can achieve similar performance . Chalkidis et al . ( 2021 ) extract model rationales for alleged violation prediction and observes limited overlap with expert markup . Similarly , a small study in Branting et al . ( 2021 ) finds that users do not perceive case prediction - derived highlighting as useful in making predictions themselves . Our work contributes to this state of the art by using adversarial deconfounding to improve the overlap between what systems predict from with what legal experts consider relevant . Deconfounding A growing number of works have raised awareness that deep neural models may exploit spurious statistical patterns and take erroneous shortcuts ( McCoy et al . , 2019 ; Bender and Koller , 2020 ; Geirhos et al . , 2020 ) . A common method of mitigating this is adversarial learning . Pryzant et al . 2018 use a gradient reversal layer ( Ganin et al . , 2016 ) to deconfound lexicons in text classification . Other domains that adopt adversarial training to eliminate confounders include bioinformatics ( Dincer et al . , 2020 ) and political science ( Roberts et al . , 2020 ) . Many existing works on identifying shortcuts focus on situations where these patterns are known in advance and may require potentially expensive data collection . In fairness - focused legal NLP , Chalkidis et al . ( 2022b ) observe and remedy group disparities in LJP performance on the EC - tHR informed by metadata attributes ( respondent state , applicant gender , applicant age ) . We extend this to explainability in LJP by involving a legal expert in a procedure that allows an efficient , incremental identification of distracting information , as well as its removal via adversarial training . Interpretability We employ interpretability techniques to evaluate model alignment with expert rationales . Danilevsky et al . ( 2020 ) reviews and categorizes the main current interpretability methods . Though initial works ( Ghaeini et al . , 2018 ; Lee et al . , 2017 ) used attention scores as explanation for model decisions , Bastings and Filippova ( 2020 ) ; Serrano and Smith ( 2019 ) point out that saliency methods , such as gradient based methods ( Sundararajan et al . , 2017 ; Li et al . , 2016 ) , propagation based methods ( Bach et al . , 2015 ) , occlusion based methods ( Zeiler and Fergus , 2014 ) , and surrogate model based methods ( Ribeiro et al . , 2016 ) are better suited for explainability analysis . However , the reliability and informativeness of these methods remains an open research problem . Our model uses the currently most commonly used Integrated Gradients ( IG ) ( Sundararajan et al . , 2017 ) , which computes the gradient of the model 's output with respect to its input features .
ECtHR Tasks & Datasets
The ECtHR has been the subject of substantial prior work in LJP . We use two datasets for model training and evaluation : First , for binary violation we use the dataset by Chalkidis et al . ( 2019 ) of approx . 11k case fact statements , where the target is to predict whether the court has found at least one convention article to be violated . To evaluate alignment , we annotate 50 ( 25 each ) expert rationales for cases from both the development and test partitions ( See App . C for the annotation process ) . Second , for article - specific violation , we use the LexGLUE dataset by Chalkidis et al . ( 2022a ) , which consists of 11k case fact statements along with information about which convention articles have been alleged to be violated , and which the court has found to be violated . For alignment , we merge this data with the 50 test set rationales from Chalkidis et al . ( 2021 ) . While both datasets stem from the EC - tHR 's public database , they differ in case facts and outcome distribution as we explain in Sec . 3.1 . The input texts consist of each case 's FACTS section extracted from ECtHR judgments . This section is drafted by court staff over the course of the case proceedings . While it does not contain the outcome explicitly , it is not finalized before the final decision has been determined , potentially creating confounding effects .
We conduct experiments on four LJP tasks : Task J -Binary Violation For our task J , the model is given a fact statement and is asked to predict whether or not any article of the convention has been violated . We train our models on Chalkidis et al . ( 2019 ) and evaluate alignment on the set of expert rationales we collected . Task B -Article Allegation We train and evaluate on LexGLUE 's ECtHR B , * where the fact description is the basis to predict the set of convention articles that the claimant alleges to have been violated . It can be conceptualized as topic classification in that the system needs to identify suitable candidate articles ( e.g. , the right to respect for private and family life ) from fact statements ( e.g. , about government surveillance ) . We test alignment on the expert rationales by Chalkidis et al . ( 2021 ) . Task A -Article Violation We also experiment with LexGLUE 's ECtHR A , which is to predict which of the convention 's articles has been deemed violated by the court from a case 's fact description . Task A is a more difficult version of task B , where both an identification of suitable articles and a prediction of their violation must be performed . For alignment , we again use the expert rationales by Chalkidis et al . ( 2021 ) , which are technically intended for task ECtHR B , but which we consider to also be suitable for an evaluation of task A. * Task A|B -Article Violation given Allegation We further disentangle the LexGLUE tasks and pose ECtHR A|B. Given the facts of a case and the allegedly violated articles , the model should predict which ( if any ) specific articles have been violated . This task reflects the legal process , as the court is aware of allegations made by the applicants when deciding . Providing information about the allegations shifts the nature of the task from topic classification to article - specific violation / non - violation prediction , thus refocusing the model and ideally leading to violation - specific explanations .
Data Distribution & Preprocessing
In order to facilitate model alignment , we worked with our ECtHR expert to identify shallow prediction signals in the fact statements that are unrelated to the legal merits of the complaint .
Length and Respondent State
For the task J dataset of Chalkidis et al . 2019 , we find that the distribution of fact description length ( number of sentences ) and the distribution of respondent states are different between the two classes ( see Appendix A ) . We hence account for the identity of the respondent state and the length of the fact descriptions via our deconfounding procedure for both datasets .
Accounting for Inadmissible Cases
We also observe in the task J dataset that the magnitudes of the running paragraph numbers differ between the classes , and that the single word " represented " strongly correlates with the positive class . This phenomenon arises because 2.6k of the 7k training cases are ' inadmissible ' cases labeled as ' non - violation ' . Legally , inadmissible cases are not necessarily ' non - violation ' as inadmissibility relates to complaints not fulfilling the court 's formal or procedural criteria . * In such cases , the court does not examine the merits of the application . The more interesting non - violation cases are such that are admissible , but in which no violation of the convention has been found . The single negative class contains instances of both inadmissible and admissible - but - no - violation - found cases . As explained above , the input texts of Chalkidis et al . 2019 are extracted from the FACTS section of full ECtHR decisions . In inadmissible cases , the applicant 's background information can typically be found at the beginning of that section . We found that almost all inadmissible case facts start with * For example , the applicants lodge the complaint outside the time limit after the final domestic judicial decision or fail to exhaust required domestic remedies before complaining to the ECtHR , etc . It should be noted that the majority of inadmissible cases are decided by single judges and not available on the public database HUDOC .
the same formulaic sentence stating the applicant 's name , nationality , and legal representation . This specific sentence is absent from the texts of admissible cases ( violation and non - violation ) , where that information is part of a separate PROCEDURE section not included in the dataset . Moreover , due to the PROCEDURE section preceding the FACTS section in admissible cases , the running paragraph numbers appearing in FACTS sections of inadmissible cases are smaller than those of the admissible cases . If not remedied , these phenomena provide a considerable predictive signal for the label and distract the system from legally relevant information . In our experiments , we hence remove paragraph numbers from the input via preprocessing and account for distractor vocabulary via our deconfounding procedure described in Sec . 4 . Still , the nature of task J remains unchanged and requires the system to classify the outcomes of a collection of both admissible and inadmissible cases .
Article - Specific Violation
By contrast , the more recent LexGLUE dataset only contains admissible cases and corresponding information about which articles the claimant has alleged to have been violated ( for task B ) along with those that the court has found to have been violated , if any ( task A ) . The collection covers 10 different convention articles that make up the largest share of ECtHR jurisprudence . Each article has been alleged in a partition of the cases , and has been found to be violated in a subset of these . * For a given article in task B , all cases in which it has been alleged can be considered positive instances while the remaining cases are negatives . We consider task B as akin to topic classification , where the rights enshrined in the convention articles ( e.g. , Art . 6 : right to a fair trial ; Art . 1 Protocol 1 : protection of property , etc . ) may correlate with certain case fact language ( e.g. , related to law enforcement or expropriation , respectively ) . Task A incorporates this step and adds violation prediction per article , which is more difficult in principle . However , we observe that a few articles account for a large portion of the data and the conditional probability of a positive violation label in task A given its allegation labels from task B can be very high ( see App . B ) . This makes an analysis of what trained models focus on more difficult , since they may learn to identify these dominant articles with high conditional violation probability , and be distracted from focusing on information that specifically signals violations of those articles . To remedy this , we propose task A|B that provides models an easy access to the label information of B , facilitating their focus only on determining whether the court finds a violation of given articles . This task is realistic since the allegations by the claimant are known to the court at the time that it decides whether the respondent state has violated the convention in the case .
Expert - Informed Deconfounding
We apply an expert - informed deconfounding method designed to mitigate the distracting effects of confounding elements and spurious correlations . As Pryzant et al . ( 2018 ) observe , accounting for confounders is common practice throughout many data analysis tasks to capture the intended signal and facilitate explainable models . In LJP , we understand confounding elements as such that influence both the observed legal outcome ( convention violations found by the court as coded in the dataset ) and the input text from which this outcome is to be predicted ( here : ECtHR fact statements ) . Already covered examples are the different distribution of information across sections for admissible and inadmissible cases and the length of the fact descriptions ( inadmissible cases tend to require less factual information to be dismissed ) . * An example of a spurious correlation is the identity of the respondent state ( certain article violations will be claimed more often against a small number of governments , leading to a correlation ) . They each should have no bearing on the probability of an outcome in a given case as a judge will not decide against a violation because the facts are short , or because the case is against a particular government .
Confounding effects and spurious information in LJP may not be known ahead of time , especially if the legal decision is not made on the basis of an immutable a priori document , but rather on the basis of text that is technically a part of the eventual judgment . Our expert - informed method is intended to mitigate such situations where spurious correlations are introduced in the text production but may not be known in advance as explicit confounders .
Our method consists of two steps : ( i ) Identification of distracting attributes for deconfounding through a combination of simple model training and minimal expert markup , and ( ii ) mitigation of these effects through adversarial training .
Step 1 : Identification of Distracting Attributes and Tokens
We first identify input attributes and categorize them as either distracting or genuinely legally relevant in an expert consultation . ' Distracting ' attributes are highly correlated with the task label but not relevant in a human expert prediction . Attributes can be either ( i ) explicit in the text ( such as vocabulary tokens ) or ( ii ) implicit ( e.g. , country , text length , etc . ) . Implicit attributes can be derived from available metadata or a corpus analysis .
For textual attributes , we apply depth - limited decision trees on an n - gram representation of the fact statement to predict the case outcome . We extract all tokens that appear in the trees and iterate , successively removing tokens identified as predictive . Compared to extracting tokens from a single larger tree , this process is better suited to remove high - entropy - reducing tokens one typically finds near the root of trees . The list of removed tokens is then presented to a legal expert , who categorizes them into spurious and legally genuine ( see Appendix Sec . F for the list of spurious vocabulary identified by the expert and the rationale behind the choices ) . This requires substantially less effort from the expert compared to other methods , such as data annotation or manual creation of counterfactuals . To prevent trees from picking up very sparse tokens , we filter the extracted terms using local mutual information ( LMI ) ( Schuster et al . , 2019 ) , a re - weighted version of pointwise mutual information ( PMI ) ( Church and Hanks , 1990 ) . We calculate LMI for each pair of token and label as illustrated in Appendix Sec . G .
Step 2 : Mitigation of Distracting Attributes
We assume a neural NLP model M consisting of a feature extractor F and classifier C with parameters θ f and θ c , respectively . For each confounder k , we apply a discriminator D k with parameters θ d K to the feature extractors . We use adversarial training to maximize the feature extractor 's ability to capture information for the main classification target while minimizing its ability to predict the value of distractor attributes . This encourages the model to generate distractor - invariant feature representation for the classifier . We use the following adversarial training objective :
k arg min θ d k L ( D k ( F ( x ) ) , y k ) ( 1 ) arg min θ f , θc [ L ( C ( F ( x ) ) , y c ) − k λ k L ( D k ( F ( X ) ) , y k ) ] ( 2 )
where L represents the loss , λ is a hyperparameter , x is the input , y c is the label , and y k is the distracting attribute k. The above optimization is performed using a gradient reversal layer ( GRL ) ( Ganin and Lempitsky , 2015 ) to jointly optimize all the components instead of alternately updating the components as in GANs ( Goodfellow et al . , 2014 ) . The GRL is inserted between the feature extractor and discriminators . It acts as the identity during the forward pass but , during the backward pass , scales the gradients flowing through by −λ , making the feature extractor receive the opposite gradients from the discriminator . This changes the overall objective function to :
arg min θ f , θc , θ D [ L ( C ( F ( x ) ) , y c ) + k λ k L ( D k ( GRL ( ( F ( X ) ) ) , y k ) ]
( 3 ) We hypothesize that learning distractor - invariant feature representations through adversarial learning will help the model to focus on parts of the input that experts consider relevant .
Experiments & Discussion
In this section we describe our experiments in using our proposed deconfounding methodology to improve the alignment of model focus on the input with expert rationales on our set of LJP tasks .
Models
Baseline : We use the BERT variant of Hierarchical Attention Networks ( Yang et al . , 2016 ) as a baseline model . To segment our very long input texts we resort to a greedy sentence packing strategy in which we pack as many sentences as possible into one packet until it reaches the predefined maximum length ( 512 tokens constrained BERT ) . When a sentence exceeds this maximum , we split it into parts to fit into multiple packets . We encode each packet with LegalBERT ( Chalkidis et al . , 2020 ) to obtain the token level representations . Following Yang et al . , 2016 , we use a token attention layer aggregating the representation of the tokens and form a sentence ( packet ) vector . We pass these sentence vectors through a GRU encoder to obtain contextual representations . These are aggregated at the document level using a sentence attention layer . This model constitutes the feature extractor component F in our architecture . The obtained document representation is passed through dense layers for the final target prediction , constituting our classifier component C. paraRem : Same as the baseline model but trained on data from which the paragraph number artifacts have been removed ( see Sec . 3.1 ) . gradCou : paraRem model extended with a multiclass discriminator with a cross - entropy loss predicting the identity of the respondent government , and a corresponding deconfounding GRL . gradLen : paraRem model extended with a length discriminator predicting the length ( number of sentences ) of the document via a set of bins and a cross - entropy loss , and a corresponding GRL to predict the bin value . gradVocab : paraRem model extended with a vocabulary discriminator to predict the presence of identified spurious tokens , and associated GRL . As there can be multiple spurious tokens in a document , we employ binary entropy loss per token as it is a multi - label classification . We refer to the above three deconfounded models collectively as singleGrad models . gradAll : paraRem model extended with all country , length , and vocabulary discriminators in parallel , and associated GRLs . Please refer to Appendix Sec . H for details on model configuration and training .
Quantitative Evaluation & Discussion
Expert Alignment Evaluation
Our main objective is to evaluate the alignment of the model 's focus on the input text with legal expert rationales ( i.e. , selected subsets of relevant segments of the input ) . Following Chalkidis et al . 2021 , we measure the model 's ability to identify the correct rationales at the paragraph level , which is the natural granularity of ECtHR fact sections . To extract the importance score for each paragraph , we rely on an interpretability technique which quantifies the impact of a particular input token towards the final prediction of the model .
We use integrated gradients ( Sundararajan et al . , 2017 ) to obtain a token - level focus score and ag- gregate paragraph - level scores as the squared L2norm of token scores in the paragraph divided by the square root of its number of tokens to account for length variation . We compute precision @ k conditioned on some fixed k between the top - k paragraphs based on paragraph scores and golden paragraph rationales . The number of relevant paragraphs in gold rationales varies considerably , so a predefined k is inadequate . Thus , we compute precision @ Oracle following Chalkidis et al . , 2021 , where Oracle is the number of relevant paragraphs in the gold rationales . For tasks J , A , and A|B , the negative label ( i.e. , non - violation ) is of similar interest as the positive label . In task B , however , the negative label merely indicates that a specific article has not been alleged , which is legally largely uninteresting . Hence , we reduce negative IG scores of tokens ( indicating a negative contribution to the prediction ) to zero .
Prediction Performance Evaluation
We also report the models ' performance on the main four LJP tasks . For Task J , we report the macro F1 - score for binary violation prediction . For Task A and B , following ( Chalkidis et al . , 2022a ) , we report micro - F1 ( µ-F1 ) and macro - F1 ( m - F1 ) scores . For Task A|B , we also report micro - F1 and macro - F1 scores . In computing the above metrics for tasks A and A|B , we consider the cases in which a particular article has been deemed violated as pos - itive instances and the rest of the instances as negatives . We also introduce hard - macro - F1 ( hm - F1 ) for both Task A and A|B , in which F1 is computed for each article using only those instances as negatives where an article has been alleged as violated but not found so by the court .
Quantitative Evaluation Results
Table 1 and Table 2 show the performance of different models on expert alignment and outcome prediction , respectively . paraRem vs. baseline : We observe that paraRem outperforms the baseline model in expert alignment across all tasks with a minimal drop in prediction performance . Task J stands out in that removing distracting signals via paragraph number removal even leads to a marginal improvement . Notably , we separately confirm the vulnerability of the baseline model by applying it to the test set with paragraph numbers removed and evaluate it on a test set without paragraph numbers , resulting in macro - F1 of 51.16 ( i.e. , a nearly 30 points drop ) . gradCou , gradLen , gradVocab vs. paraRem : In all tasks , we observe that all singleGrad models improve in expert alignment performance over paraRem by a small but consistent margin . This demonstrates the ability of our deconfounding component to help the model better identify legally relevant parts of the input . Notably , gradVocab shows the most improvement in alignment over paraRem in all tasks except Task B ( alleged article prediction ) , where gradLen performs best . During development on task B , we observed that the decisiontree based removal of predictive words led to only a marginal falloff in tree model accuracy , even after multiple iterations , since there was simply a lot of topical words ( e.g. , for police misconduct , legal proceedings , etc . ) to take over as some of them were removed . This in part reflects the different nature of the tasks and shows a limitation of our tree - training - based method for identifying spurious tokens . Similar to paraRem , the gradLen model ( in case of Task A , B , and A|B ) also shows improvement in prediction performance compared to the baseline model . This suggests that deconfounding can potentially prevent the model getting stuck in distractor - related local optima . Alignment : All singleGrad models outperform the baseline with regard to expert alignment . We observe that gradAll achieves the highest score , which establishes some degree of complementarity among the three singleGrad models and the distracting signals they remedy . A paired t - test ( gradAll vs. paraRem ) reveals p - values above typical significance levels for the validation partition of task J , along with a considerable divergence in the general score level for the two tasks . We conjecture that this is the result of our small rationale sample size ( 50 from each partition ) and differences in distribution between the task J data partitions , which have been split along the timeline rather than random . We also see a higher p - value for task A|B , which is intuitive since it is the most difficult . Its dataset lacks easily identifiable inadmissible cases ( as in task J ) and it has access to B 's labels as concurrent , non - textual input . To gain some more insight into A|B , we report on a qualitative error analysis of the model rationales below .
Qualitative Evaluation & Discussion
Expert Scores : We sample 40 cases from task A|B validation and test sets ( see App . Sec . D ) . We provide the expert with randomized visualizations of IG scores at the token level derived from our paraRem and gradAll models . Following ( Jayaram and Allaway , 2021 ) , the expert was asked to rate these on a five - point Likert scale ( range -2 to 2 ) on two metrics : ( i ) Sufficiency : Is a sufficiently large set of tokens focused on to arrive at the prediction ? ; and ( ii ) Irrelevance : How many irrelevant tokens does the model focus on ? We phrased the scale such that , for both parameters , a higher rating signals a better alignment between the model focus and the expert 's assessment . Table 3 presents averages of the raw scale scores . We observe that the deconfounded gradAll model scores higher ( See App . I for an example pair of IG visualizations ) . Manual IG Inspection : For the paraRem model , we notice that high scoring IG tokens are sparse , whereas in gradAll , focus is densely distributed . There , contiguous spans of tokens tend to receive higher scores . This phenomenon is likely due to paraRem being drawn to single word distractors . Deconfounding helps the gradAll model to spread its focus across larger segments of the text . Our ECtHR expert further observed that gradAll highlighted words that , in conjunction , were indicative of the outcome , even if those were a considerable distance apart . At the same time , however , it seemed that two words hinting at opposite outcomes in a single sentence forced the system to focus only on one of the two , leaving the other one unhighlighted . We conjecture that these long - and short - distance phenomena are a result of the hierarchical model architecture necessitated by the long documents and leave their further exploration for future work .
An inspection of high scored tokens in paraRem reveals that many of them are highly discriminative in our decision tree models , showing that complex neural models can easily fall for distractors at the expense of missing equally predictive but semantically more complex signals . This reinforces our paradigm to identify discriminative tokens using a simpler model and subject them to expert scrutiny . In particular , we found that the word " represented " forms a natural decoy and , when injected into a violation - outcome fact statement , flips the predicted label of trained deep neural models . This led us to believe those models rely more on individual words than one might expect , and motivated us to explore how this can be exploited with information derived from simple models . Figure 2 shows that the performance of decision trees with unigram features ( at iteration 1 without removed tokens ) can even come close to BERT models .
In paraRem , we further observe that tokens at the start of sentences receive higher IG scores . We believe this to be the model counting sentences , which justifies deconfounding for length . For gradAll , we observe that sentence beginnings still receive focus , but less strongly so . This may be due to BERT recognizing sentence boundaries . Further alignment improvement : The overall low precision @ Oracle scores show that considerable differences in alignment with human experts remain . We conjecture that the model is shifting its focus , at least in part , to other spurious attributes which our current setup could not reveal . This calls for further investigation to design effective methods to identify such patterns . However , we expect them to be increasingly subtle and difficult to recognize , potentially even for legal experts . An intuitive upper bound for the system would be the annotation agreement of multiple experts , which to the best of our knowledge remains unexplored in the current state of the art . Expert Pattern Identification : Our results naturally raise the question of how distractors can be identified in ECtHR fact texts by experts . Generally , the patterns we focused on affect the relationship between the argumentation in the judgment and the supportive facts given . There is copious literature on the court 's inconsistent approach to Voeten , 2020 ) . We hence paid attention to specific markers in the fact section and correlated them to existing precedents and argumentation patterns .
A few examples : The court may decide to make use of positive obligations and decide against the state ( violation ) by highlighting failures of national authorities , or may decide to use those same positive obligations under ' the responsible authorities ' doctrine , highlighting the efforts of national authorities to bring domestic legislation in line with the convention , thus deciding that there has been no violation . There are also fact patterns and practices specific to particular state parties to the convention ( e.g. , prison overcrowding , procedural issues in child abduction cases ) . The court may also sometimes highlight specific facts of a case with the view to ' document ' its resemblance to , or divergence from , an existing precedent . A detailed , legally informed case study on predictive patterns is beyond the scope of this work .
Recommendations for LJP Research
In order to produce value for legal practice , we believe that LJP / LJF as an NLP task should strive for a productive combination of expert knowledge with data - derived insight . Based on our results , we formulate the following recommendations : First , as has already been observed in the field , any prediction / classification should happen from suitable source text that does not encode information about the outcome but contains as complete factual information as possible , or at least control for this influence . Second , straightforward predictors ( e.g. , input length and shallow unigram models ) should be used to identify distractors and confounders . Third , claimed performance levels in predicting case outcomes should be contextualized by information about the distribution of the legal issues and respective conditional outcome probabilities in the corpus , as well as against baseline classifiers capable of exploiting known distractors . Fourth , more granular outcome variable information ( e.g. , case declared inadmissible vs. case dismissed on the merits , decomposition into outcomes of individual issues ) will allow the development of more nuanced prediction / classification systems . Taken together , if such models can be explained and integrated into a decision support system for suitable tasks in legal practice , experts will be more likely to perceive them as adding value .
Conclusion
Our results show that our deconfounded LJP models are consistently better aligned with expert rationales than a baseline optimized for the target label only , and in many cases can even achieve better prediction performance . However , the improvement is small and the paragraphs focused on by all our models are still quite different from what an expert has annotated as relevant , as indicated by generally low precision @ Oracle scores ( < 50 % ) . Still , our quantitative results show that expert - informed deconfounding LJP works in principle and can potentially go a long way to train more robust and trustworthy neural LJP models , as well as derive more useful legal insight from them .
Limitations
We present a case study in deconfounding legal judgment prediction on the ECtHR , and all results are to be understood as relative to the ECtHR , its jurisprudence , the used datasets , and the formal tasks . The distracting attributes we identify include confounding effects of the court 's document production , where the decision may be known before the decision text ( including the fact section ) is finalized . A replication of this study in other LJP settings is of course warranted before general applicability can be claimed . Our analysis of task B has further revealed that redundant vocabulary distribution can challenge the system 's ability to point out individual ' smoking gun ' distracting tokens . This aspect is particularly complex in light of differing legal systems and their respective cultures and patterns of drafting texts that may form the basis of predictive or , more generally , assistive systems . Morphologically rich languages , where distracting signal may be spread across multiple tokens , may make this challenge more difficult and require stem - or lemma - based processing as part of the method .
Our deconfounding method is work - intensive and assumes the identifiability of distracting information in text and metadata by an expert . Legal expert agreement about what parts of decisions are relevant remains underexplored , and the division of genuine versus spurious language may also vary in between multiple experts . While we are convinced that further research on effective deconfounding of legal NLP systems is needed if these systems are to become robust and trustworthy , the time - intensive nature of collaboratively developing and qualitatively evaluating such models with legal experts poses a considerable resource challenge .
A technical difficulty in working with legal documents is their length , and the use of packet - based hierarchical models constrains the maximum distance across which tokens can directly attend to one another . The impact of this limitation on model performance in various types of tasks is the subject of ongoing exploratory work ( e.g. , Dai et al . 2022 ) .
Ethics Statement
The research presented here works exclusively with publicly available datasets of ECtHR decisions , which are based on HUDOC * , the public database of the Court . While these decisions are not anonymized and contain the real names of individuals involved , our work does not engage with the data in a way that we consider harmful beyond this availability .
Our models are designed to be used with pretrained language models and hence inherit any bi- * https : / / hudoc.echr.coe.int ases they may contain . This entails an obligation to screen incorporated models and to test any developed system with regard to its performance across groups of cases ( e.g. Chalkidis et al . 2022b ) , and to remedy any disparities before deploying it as a prediction and inference tool . Our experiments are targeted at controlling for legally irrelevant distractors in the input , which is in line with this responsibility .
The task of legal judgment prediction raises ethical concerns , both general as well as specific to the European Court of Human Rights . ( Fikfak , 2021 ) emphasizes focal issues with regard to the court considering the use predictive technology to tackle its caseload , including system bias and the challenges of designing the interaction between judges and predictive systems . The latter is of course especially sensitive given experiences made with recidivism risk prediction ( Collins 2018 ) and possible disparate effects of how judges interact with scores ( Albright 2019 ) . Our research group is committed to research on LJP as a means to derive insight from legal decision data towards increasing accountability , fairness , and transparency in the use of technology in legal systems . The premise of this work is that the behavior of legal outcome prediction systems is to be scrutinized with great care . This paper does not advocate for the practical use of such systems , but rather empirically explores difficulties that arise in their development and recommends a closer connection between technical research and legal expertise ( see Sec . 5.4 ) .
All models of this project were developed and trained on Google Colab . Our models adapted pretrained language models and we did not engage in any training of such large models from scratch . We did not track computation hours .
A Dataset Statistics
Table 5 demonstrates the artefacts from corpus construction and admissibility - related confounding information in the training set of Task J. Figure 3 and 4 display the distribution of text length and the respondent state in the Task J train set . Figure 5 and 6 show the statistics of text length and respondent state in the Task B train set .
B LexGlue Dataset Characteristics
Table 4 describes the conditional probability of a violation finding by the court given the allegation of a particular article as well as the probability of a violation finding regarding a particular article even though it was not alleged .
C Rational annotation Process for Task J
We sampled 50 cases ( 25 each ) from the validation and test split . In each split , we sample two cases for each of the ten violated articles , one containing the token ' represented ' and one without , along with five inadmissible cases . While the article information is available in the task J dataset , we do not use it as it was introduced as a binary violation classification task . The rationale annotation process was done using the GLOSS annotation tool . The third author of this paper , who is an ECtHR expert , read the case fact statements and highlighted paragraphs which she considered indicative of an eventual finding of a violation for any convention article by the court . Despite our sampling involving randomness , the expert was already familiar with a considerable portion of the decisions . Given this , we abstained from producing a human expert outcome prediction baseline .
D Case Sampling for Qualitative Evaluation
For the qualitative evaluation of Task A|B , we sample 40 cases ( 20 each ) from validation and test split .
In each split , we sample two cases for each of the ten allegedly violated articles , one with a finding of a convention violation and with a non - violation finding .
E Decision Tree Performance
Figure 2 shows the performance of our decision tree model across iterations for different tasks . After each iteration , we remove the informative to - kens from previous iterations . In case of task J , we notice a steep fall after iteration 5 . Tasks A and B exhibit less dramatic falloff of macro - averaged F1 , owing to the different nature of the tasks as article - specific violations . Performance on Task A|B even shows small increases , albeit with a low absolute score . The large standard deviations bands computed across all articles show considerable variation .
F Spurious Vocabulary identified by Expert
Following is the spurious vocabulary we obtained with respect to each task .
• Task J : represented , national , mr , summarised , practising , lawyer , agent , paragraph The words were chosen as relevant or irrelevant by using the daily vocabulary of a human rights lawyer working at the ECtHR as a reference . A word was considered legally relevant if , taken individually , it could be introduced into legal reasoning . For instance , the word " religious " was spurious because taken individually it says nothing about the content of a norm . One may talk about religious freedom , but the legally relevant word there is freedom . Article 9 mentions religion , but restrictions related to religion may also be present under Article 8 , 3 , 2 , 5 , etc . Under the same Article 9 for instance , the court decides whether there has been a violation depending on criteria such as tolerance , pluralism , etc . It is those criteria that are relevant whereas " religion " is not by itself relevant as a part of the legal reasoning .
G LMI Calculation
We calculate LMI for each pair of token t and label y as follows :
LM I ( t , y ) = p ( t , y ) × P M I ( t , y ) ( 4 )
p ( t , y ) = count ( t , y )
|D|
( 5 )
P M I ( t , y ) = log p ( t | y ) p ( t ) ( 6 )
where count ( t , y ) denotes the co - occurrence of t and label y , and |D| is the number of unique words in the training set .
In the case of binary classification ( task J ) and one - vs - one multi - label classification ( task A|B ) , we calculate the LMI score for a token as the absolute difference between LMI scores for both positive and negative labels , as both the labels represent a particular class . In one - vs - rest ( tasks A , B ) , we simply take the difference between LMI scores for both positive and negative labels ( rather than absolute difference ) as the negative label does not specifically represent a particular class . Finally , we calculate the z - score statistic of the effective LMI score for each token to identify significant tokens .
H Model configuration & Training
Spurious token identification : We train a series of decision trees of depth 3 to assemble lists of predictive tokens for expert filtering . The feature vector consists of whitespace - tokenized unigrams reduced by the LMI filtering explained above . For task J , this means training trees that predict the binary violation label . For task A and B we employ a one - vs - rest classification to produce one decision tree series per article . For task A|B we provided the task B labels ( allegedly violated articles ) in onevs - one fashion per article , with positive instances being facts where that particular article was deemed violated , and negatives where that particular article was merely alleged but not deemed violated . LJP models : Our models compute BERT - based word embeddings of size 768 . Our word level attention context vector size is 300 . The sentence level GRU encoder dimension is 200 , thus giving a bidirectional embedding of size 400 , and a sentence level attention vector dimension of 200 . The final dense classifier for all tasks has 100 hidden units . The output dimension is 1 for task J and 10 for the other tasks ( i.e. one per convention article ) . For task A|B , we concatenate a multi - hot 10 - element feature vector containing the task B labels to the output of the feature extractor before it is passed to the classifier . All discriminators ( country , length , and vocabulary ) are built as analogous classifiers with a hidden dimension of 100 and output layer dimensions as required by each of them . We use mini batches size of 8 in case of Task J and 16 for all other tasks . The model is optimized endto - end using Adam ( Kingma and Ba , 2015 ) . The dropout rate ( Srivastava et al . , 2014 ) in all layers is 0.1 . We determine the best learning rate using a grid search on the development set and use early stopping based on the development set F1 score .
I Visualization of IG score
Figure 7 exhibits screenshot excerpts of a sample case text provided to the legal expert for qualitative evaluation . The yellow background highlight was not in the original visualization and has been supplied here as a reference . We add it here as an example of focus patterns shifting incurred by our deconfounding method .
Acknowledgments
We are grateful to Jaromir Savelka for the ability to use the Gloss annotation tool and for providing feedback on the draft . We also thank the anonymous reviewers for valuable comments .
Entity Cloze By Date : What LMs Know About Unseen Entities
Language models ( LMs ) are typically trained once on a large - scale corpus and used for years without being updated . However , in a dynamic world , new entities constantly arise . We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained . We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles , from which we can find sentences about each entity . We evaluate LMs ' perplexity on masked spans within these sentences . We show that models more informed about the entities , such as those with access to a textual definition of them , achieve lower perplexity on this benchmark . Our experimental results demonstrate that making inferences about new entities remains difficult for LMs . Given its wide coverage on entity knowledge and temporal indexing , our dataset can be used to evaluate LMs and techniques designed to modify or extend their knowledge . Our automatic data collection pipeline can be easily used to continually update our benchmark .
Introduction
New entities arise every day : new movies , TV shows , and products are created , new events occur , and new people come into the spotlight . Whatever the capabilities of language models ( LMs ) to represent entity knowledge , these new entities can not possibly be included in the language models ' parametric knowledge ( i.e. , knowledge acquired during pretraining ) , as they did not exist when LMs were trained . Since this temporal mismatch between LMs and real - world knowledge affects model performance on downstream tasks Dhingra et al . , 2021 ; Lazaridou et al . , 2021 ) , understanding what LMs know about real - world entities is an important task .
The existing literature provides various benchmarks to measure LMs ' knowledge about entities ( Petroni et al . , 2019 ( Petroni et al . , , 2021Dhingra et al . , Figure 1 : Our framework ( ECBD ) collects entities indexed by the year when they were first introduced in Wikipedia and their cloze sentences , unlike existing cloze datasets ( LAMA ( Petroni et al . , 2019 ) ) which broadly cover entities introduced prior to 2019 .
2021 ) . Those benchmarks are typically formulated as cloze - style tasks covering a limited set of relations bounded by knowledge bases : LAMA uses around 40 Wikidata relations and entities collected in 2017 . Newer cloze benchmarks ( Dhingra et al . , 2021 ; Jang et al . , 2021 ) integrate temporal aspects to identify a time period when a cloze sentence is valid , but do not differentiate new and existing entities . These knowledge probing datasets fail to test broad knowledge about real - world entities or evaluate how LMs ' knowledge differs on entities that are seen or unseen during pre - training .
To fill this gap , we propose a framework to evaluate LMs ' knowledge about entities classified by their origination date . We extract a set of Origination Date Indexed Entities ( ODIE ) based on metadata from Wikidata . We then construct cloze statements by masking sentences in those entities ' Wikipedia articles . Unlike past knowledge probing datasets , these cloze sentences test the ability of a model to make a wide range of inferences related to entities , without being resticted to a pre - defined set of KB relations . We choose masked spans near these entities that likely contain information related to the entities , which we evaluate based on the perplexity gap between the raw sentence and the sentence with the entity replaced .
We release the Entity Cloze by Date ( ECBD ) dataset of 35k masked sentences that contain mentions of 2.1 K ODIE entities , 1 split by year covering a time period from 2017 to 2021 , together with 8k masked sentences of popular entities from any time period . In our experiments , we evaluate three pre - trained language models in terms of perplexity . We establish that injecting additional information such as a text definition can meaningfully teach the model to make better guesses about masked spans , highlighting this dataset 's utility for benchmarking methods of knowledge injection .
Entity Cloze by Date
We aim to test language models ' 1 ) broader entity knowledge and 2 ) ability to reason about completely unseen entities ( i.e. , unseen during pretraining ) . Thus , we want to have the following properties in our entity cloze sentences . ( 1 ) Date indexing . If each cloze example is associated with an entity and indexed by the origination date of that entity , we can understand whether a model may have seen it in its pre - training corpus or not . ( 2 ) Diverse sentences . When going beyond KB triples , entity knowledge can take many forms : actions that an entity can take , other entities that action can effect , typical ways in which an entity is described , and more . Thus , we want include diverse sentences and masked spans that cover rich relations and various syntactic categories ( e.g. , POS and nonterminal categories , span length ) .
Task Definition
Each entity e is paired with e i , its origination year . Given a sentence s containing an entity mention span m e and a masked query span m q , a language model is asked to predict the gold masked span m y . See the following example : We evaluate language models by perplexity on the masked span m q ( see Appendix D for a discussion of recall as another metric ) .
Sentence Collection
Entities with Origination Date
Sentences with Entity Mentions
Masked Sentences
Entity Mining
Span Selection
Figure 2 : Overview of the data collection process .
Data Collection
Our data collection protocol consists of three stages : entity mining , sentence collection and span selection . We use English Wikipedia ( the September 1 , 2021 dump ) and Wikidata as knowledge sources .
ODIE Mining
We begin by gathering all entities on Wikidata that have an associated start time , announcement date , time of discovery or invention , inception date , point in time , or date it was introduced on . For such entities , we take the first of these dates to create our temporal splits , assuming that this is the earliest date the entity could have appeared in any pretraining corpus .
To compare with ODIE which covers relatively new entities originated in 2017 at the earliest , we use a set of POPULAR entities ranked by article contributor numbers and incoming links from prior work ( Onoe et al . , 2021 ; Geva et al . , 2021 ) .
Entity Sentence Collection
Once we obtain a list of entities , we look up their English Wikipedia articles . To enrich the candidate sentence pool and exclude trivial sentences from stub articles , we filter entities if their corresponding articles contain less than 500 words . From each article , we exclude the first paragraph of the article , to be used as an entity definition , and sample sentences from the rest of the paragraphs . We sample sentences that include the entity name or one of their Wikidata aliases . We do not accept entity mention spans located in quotes since they are often in nested named entities such as book titles . We also filter out any sentences with less than five words .
Span Selection Next , we determine spans m q to be masked on a sentence , s ; we can have multiple masked spans per sentence , masked separately . All spans must be : ( a ) not overlapping with the entity mention span , m e , ( b ) located after the entity mention span , m e , and ( c ) starting no more than ten words away from the mention span , to improve relatedness to the entity . We select spans after the entity mention so left - to - right language models will condition on the entity at test time .
We extract two types of spans : NP spans are selected from any suitable noun phrases in the sentence using spaCy ( Honnibal and Montani , 2017 ) . These spans primarily represent relational knowledge about the entity , analogous to the object in a KB triple . Random spans are arbitrary sequences of words sampled from the sentence . This broader set of spans may cover other types of entity knowledge ( e.g. , probable actions an entity can take ) . We uniformly sample span length between 1 and 5 and then randomly select the starting location of the span within the sentence . We only accept valid spans not overlapping with the entity mention . We extract at most 100 spans per entity to limit any one entity 's contribution to the final dataset .
Span sensitivity to entity knowledge
To see if our design choices are effective , we perform a test that measures the performance drop in perplexity using T5 when we replace the entity mention with a generic reference to " the entity . " We use entities from our POPULAR set to ensure that the LM has seen them during pre - training . If a masked span is related to the entity , the perplexity of that span should increase when the entity mention is omitted .
We see that the median perplexity of a span increases by 32.2 % when the entity is removed , indicating that these spans are indeed related to the entity . Moreover , removing the distance - based criterion for span selection decreases the perplexity change to 25.9 % . These results indicate that our selected spans are correlated with the entity . This gap test was performed only for analysis and we do not use any model - based data filtering . set of entities , ranging from events , products to organizations . One notably missing entity category is people ; it is hard to pin down an origination year because of the significant gap between birth year and the year someone became prominent . Table 2 reports statistics on our cloze task data and existing probe dataset ( Petroni et al . , 2019 ) . While containing fewer entities , our dataset exhibits much richer vocabulary ( 19 K vs. 2 K ) , demonstrating diverse knowledge it covers . We split this data into dev and test sets by entities ( i.e. , no shared entities between dev and test ) . To balance out the data sizes across the groups , we sample 4k examples for each year group , yielding 35k examples in total ( approx . 20k for dev and 20k for test ) . Earlier dates contain a larger set of entities ( 599 entities for 2017 compared to 158 entities for 2021 ) as entities are continuously updated in Wikidata . In other words , many entities originated in 2021 have not been yet added to Wikidata . We sample the same number of NP spans and random spans . Within the NP spans , 35 % of them are proper noun phrases .
Dataset Statistics
Experiments
Setup We evaluate T5 - large ( Raffel et al . , 2020 ) , BART - large ( Lewis et al . , 2020 ) , and GPT - Neo ( Black et al . , 2021 ) on our dataset in the zero - shot setting where the model parameters are fixed . In addition to the original masked sentence ( ORIGINAL ) , we feed three modified masked sentences . NO ENT replaces the entity mention span with a generic string " the entity . " RANDOM DEF . prepends a definition sentence of a randomly selected entity . DEFINITION prepends the first sentence of the entity 's Wikipedia article to the cloze sentence .
We evaluate these models on the subsets split by year as well as a set of popular entities . Note that the entities in the 2020 and 2021 subsets are unseen for T5 and BART . Most entities in the 2020 and 2021 subsets are unseen to GPT - Neo , but its training data ( the Pile ( Gao et al . , 2020 ) ) does include the March 2020 English Wikipedia dump . In our experiments , we group the 2020 and 2021 subsets together as they consist of " unseen " entities . Similarly , we group the 2017 , 2018 , and 2019 subsets whose entities are " seen " during pre - training . See Appendix B for perplexity per year .
Evaluation Metric We compute tokennormalized perplexity over the span as a proxy for entity knowledge stored in LMs . Each subset has different distribution of entity types ( e.g. , 2020 contains many COVID related entities and a lot less sports events compared to other years ) , and some frequent entities might contribute to perplexity excessively . To mitigate biases from particular entities , we first average negative log - likelihood ( token normalized ) over entities then average over examples . We follow the target sequence format used in LMs ' pre - training tasks ( see Figure 3 ) .
Figure 3 shows the perplexity computation . For left - to - right language models like GPT - Neo , we compute the perplexity of the span given the left context only . T5 and BART , as seq2seq models , are able to also condition on the right context in their input ; this makes perplexity values between these model classes not directly comparable ( in addi- tion to differences in tokenization and pre - training tasks ) . For T5 and BART , we condition on the input with a single mask . At decoding , for BART we initialize the decoder with the left context of the span and compute perplexity on the true span filler following this left context . For T5 , we compute perplexity on the output span between the special tokens < extra_id_0 > and < extra_id_1 > .
Results Table 3 reports perplexity ( lower is better ) on the test set that is split into three subsets : POPULAR , 2017POPULAR , -2019POPULAR , , and 2020POPULAR , -2021 . Note that absolute perplexity across years is sensitive to factors such as distribution of sentences or entity types ; we thus focus on relative performance . In all subsets , we observe two consistent trends across three LMs . ( 1 ) NO ENT always degrades performance compared to ORIGINAL . This result confirms that our masked spans are sensitive to the content of the entity span , although it is not conclusive proof of entity knowledge being required , as changing to " the entity " modifies other latent stylistic attributes that the LMs may be sensitive to .
( 2 ) DEFINITION always boosts performance over ORIGINAL , indicating that providing more information about entities helps to retrieve information distributed over LMs ' parameters . RANDOM DEF . distracts BART and GPT - Neo but slightly improves T5 performance even though the additional information is taken from a random entity . This could be due to the model using different positional encodings as a result of having a definition , or LMs may select information if it is useful in some cases , leading the small gains .
Performance on unseen entities Recall that we consider 2020 - 2021 as unseen entities , and 2017 - 2019 and POPULAR as seen entities . All three LMs give higher perplexity on unseen entities , showing that the spans in 2020 - 2021 are relatively unexpected to the LMs .
We further investigate the performance delta between ORIGINAL and DEFINITION per subset . For all three LMs , we see that the performace delta is relatively larger on 2020 - 2021 , indicating definition sentences are more useful on unseen entities .
Also , the performance delta on the popular entity set is notably smaller than 2020 - 2021 ( compare T5 numbers : 13.02 → 11.04 for POPULAR versus 19.43 → 13.60 for 2020 - 2021 ) . This implies that LMs contain some prior knowledge about common entities they have observed before , and can use additional information about new entities or less frequent entities . How to inject knowledge requires further investigation .
Use Cases
We envision this dataset as being useful for general knowledge probing , as the real - world knowledge covered by the existing benchmarks is gradually outdated . With our framework , we can easily update datasets using the most recent knowledge sources with a controlled manner . Since the entity knowledge in our dataset is time - indexed , this is suitable for evaluating knowledge editing approaches ( Sinitsin et al . , 2020 ; Zhu et al . , 2020 ; Mitchell et al . , 2021 ; Meng et al . , 2022 ) and also continual knowledge learning approaches ( Jang et al . , 2021 ) . Crucially , ex - isting work studies whether these approaches can inject single facts , but not whether they can enable models to robustly support a broad range of new inferences about entities , like our dataset allows .
Related Work
Temporal mismatch / misalignment between large pre - trained LMs and real - world knowledge is an emerging research direction . Lazaridou et al . ( 2021 ) show that the corpus - level perplexity on documents from beyond LMs ' training period becomes increasingly poor over time . Dhingra et al . ( 2021 ) propose TEMPORALLAMA , which is based on time - dependent knowledge base triples ( i.e. , valid subject , relation , and object combinations given time ) . SITUATEDQA ( Zhang and Choi , 2021 ) includes time - dependent QA examples . While these datasets primarily test temporal information about entities in the pre - training data , ECBD focuses on new entities which did not exist during pretraining . TemporalWiki ( Jang et al . , 2022 ) annotates new facts / entities based on the differences between Wikidata / English Wikipedia dumps , but does not necessarily reflect real - world changes during the time period ( e.g. , an ancient queen can be added to Wikidata in 2022 ) . ECBD selects entities based on their origination date to align them with the real - world timeline .
Another line of work has looked at diachronic embeddings : ( Wijaya and Yeniterzi , 2011 ; Kim et al . , 2014 ; Hamilton et al . , 2016 ; Bamler and Mandt , 2017 ) , which can model changing meanings of words over time . Our setting focuses on introducing new concepts rather than rewriting existing ones , but data similar to ECBD could be collected for new usages of existing words .
Although our dataset follows the widely - used cloze format , our focus is orthogonal to datasets like the Children 's Book Test ( Hill et al . , 2016 ) and LAMBADA ( Paperno et al . , 2016 ) , which come from fiction and do not cover real - world entities .
Conclusion
In this paper , we present a dataset to understand language models ' broad inferences about entities across time . We collect 43k cloze - style sentences associated with a time - indexed set of entities . We also perform analysis on our data set and show that handling completely unseen entities remains challenging for the current LMs .
A Examples of ECBD Sentences
See Table 4 for examples of masked sentences in the ECBD data .
B Perplexity per year
See Table 5 for a more fine - grained view of the results in Table 3 .
C Perplexity per span type
See Table 6 for a breakdown of the perplexity that T5 achieves on different types of spans , showing that random spans are generally higher perplexity than NP spans but that adding definitions can help both .
D Recall @ 10
LMs can be evaluated on recall @ 10 , i.e. , a binary score indicating if model 's top ten predictions contains the gold masked span m y . For T5 , we first generate sequences using beam search ( we choose beam size = 100 in our experiments ) . Then we take the top ten unique sequences and extract the text spans between < extra_id_0 > and < extra_id_1 > as predictions . Table 7 reports recall @ 10 on each subset . Table 8 list recall @ 10 per span type for each subset .
We only explore recall on T5 , since it is not obvious how to compute it for the other two models . For BART , we can extract the predicted span by aligning the model 's prediction with the gold context , assuming that it starts to copy from the input right context at some point . However , in some cases , we found that the generated right context does not match with the gold right context ; it 's unclear how to be handle this . For GPT - Neo , since it is a leftto - right LM , extracting the predicted span would require conditioning on the span length , which is information that T5 does not have access to . As a result , we do not report recall @ 10 for these models .
E Data Licensing
The Wikipedia text we used is licensed under CC BY - SA . Our use of Wikipedia , constructing a dataset which we will make publicly available under the same license , is consistent with the terms of the license .
F Computational Resources
All experiments were conducted using an NVIDIA Quadro RTX 8000 . We only evaluate existing mod-
Acknowledgments
This work was partially supported by NSF Grant IIS-1814522 and by the Air Force Research Laboratory ( AFRL ) , Google Research Award , DARPA for the KAIROS program under agreement number FA8750 - 19 - 2 - 1003 . The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies , either expressed or implied , of DARPA , or the U.S. Government . The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein .
els on our datasets and did not do any finetuning . One evaluation experiment typically takes 15 minutes to complete . For T5 experiments , we use Hugging Face 's Transformer package ( Wolf et al . , 2020 ) .
Robust Representation Learning with Reliable Pseudo - labels Generation via Self - Adaptive Optimal Transport for Short Text Clustering
Short text clustering is challenging since it takes imbalanced and noisy data as inputs . Existing approaches can not solve this problem well , since ( 1 ) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets , and ( 2 ) they are vulnerable to noises . To tackle the above issues , we propose a Robust Short Text Clustering ( RSTC ) model to improve robustness against imbalanced and noisy data . RSTC includes two modules , i.e. , pseudo - label generation module and robust representation learning module . The former generates pseudo - labels to provide supervision for the later , which contributes to more robust representations and correctly separated clusters . To provide robustness against the imbalance in data , we propose self - adaptive optimal transport in the pseudo - label generation module . To improve robustness against the noise in data , we further introduce both class - wise and instance - wise contrastive learning in the robust representation learning module . Our empirical studies on eight short text clustering datasets demonstrate that RSTC significantly outperforms the state - of - the - art models . The code is
Introduction
Text clustering , one of the most fundamental tasks in text mining , aims to group text instances into clusters in an unsupervised manner . It has been proven to be beneficial in many applications , such as , recommendation system ( Liu et al . , 2021 ( Liu et al . , , 2022a , opinion mining ( Stieglitz et al . , 2018 ) , stance detection , etc . With the advent of digital era , more and more people enjoy sharing and discovering various of contents on the web , where short text is an import form of information carrier . Therefore , it is helpful to utilize short text clustering for mining valuable insights on the web .
However , short text clustering is not a trivial task . On the one hand , short text has many categories and the category distributions are diversifying , where the heavy imbalanced data is common . The heavy imbalanced data is prone to lead to degenerate solutions where the tail clusters ( i.e. , the clusters with a small proportion of instances ) disappear . Specifically , the recent deep joint clustering methods for short text clustering , ( Hadifar et al . , 2019 ) and ( Zhang et al . , 2021 ) , adopt the clustering objective proposed in ( Xie et al . , 2016 ) , which may obtain a trivial solution where all the text instances fall into the same cluster ( Yang et al . , 2017 ; Ji et al . , 2019 ) . ( Zhang et al . , 2021 ) introduces instancewise contrastive learning to train discriminative representations , which avoids the trivial solution to some extent . However , ( Zhang et al . , 2021 ) still tends to generate degenerate solutions , especially on the heavy imbalanced datasets .
On the other hand , short text is typically characterized by noises , which may lead to meaningless or vague representations and thus hurt clustering accuracy and stability . Existing short text clustering methods cope with the noise problem in three ways , i.e. , ( 1 ) text preprocessing , ( 2 ) outliers postprocessing , and ( 3 ) model robustness . Specifically , earlier methods ( Xu et al . , 2017 ; Hadifar et al . , 2019 ) apply preprocessing procedures on the text ( HaCohen - Kerner et al . , 2020 ) for reducing the negative impact of noises . The recent method ( Rakib et al . , 2020 ) proposes to postprocess outliers by repeatedly reassigning outliers to clusters for enhancing the clustering performance . However , both preprocessing and postprocessing methods do not provide model robustness against the noise in data . The more recently short text clustering method SCCL ( Zhang et al . , 2021 ) proposes to utilize the instance - wise contrastive learning to support clustering , which is useful for dealing with the noises in the perspective of model robustness . However , the learned representations of SCCL lack discrim - inability due to the lack of supervision information , causing insufficiently robust representations .
In summary , there are two main challenges , i.e. , CH1 : How to provide model robustness to the imbalance in data , and avoid the clustering degeneracy ? CH2 : How to improve model robustness against the noise in data , and enhance the clustering performance ?
To address the aforementioned issues , in this paper , we propose RSTC , an end - to - end model for short text clustering . In order to improve model robustness to the imbalance in data ( solving CH1 ) and the noise in data ( solving CH2 ) , we utilize two modules in RSTC , i.e. , pseudo - label generation module and robust representation learning module . The pseudo - label generation module generates pseudo - labels for the original texts . The robust representation learning module uses the generated pseudo - labels as supervision to facilitate intra - cluster compactness and inter - cluster separability , thus attaining more robust representations and more correctly separated clusters . The better cluster predictions in turn can be conductive to generate more reliable pseudo - labels . The iterative training process forms a virtuous circle , that is , the learned representations and cluster predictions will constantly boost each other , as more reliable pseudo - labels are discovered during iterations .
The key idea to solve CH1 is to enforce a constraint on pseudo - labels , i.e. , the distribution of the generated pseudo - labels should match the estimated class distribution . The estimated class distribution is dynamically updated and expected to get closer to the ground truth progressively . Meanwhile , the estimated class distribution are encouraged to be a uniform distribution for avoiding clustering degeneracy . We formalize the idea as a new paradigm of optimal transport ( Peyré et al . , 2019 ) and the optimization objective can be tractably solved by the Sinkhorn - Knopp ( Cuturi , 2013 ) style algorithm , which needs only a few computational overheads . For addressing CH2 , we further introduce class - wise contrastive learning and instancewise contrastive learning in the robust representation learning module . The class - wise contrastive learning aims to use the pseudo - labels as supervision for achieving smaller intra - cluster distance and larger inter - cluster distance . While the instancewise contrastive learning tends to disperse the representations of different instances apart for the separation of overlapped clusters . These two modules cooperate with each other to provide better short text clustering performance .
We summarize our main contributions as follows : ( 1 ) We propose an end - to - end model , i.e. , RSTC , for short text clustering , the key idea is to discover the pseudo - labels to provide supervision for robust representation learning , hence enhancing the clustering performance . ( 2 ) To our best knowledge , we are the first to propose self - adaptive optimal transport for discovering the pseudo - label information , which provides robustness against the imbalance in data .
( 3 ) We propose the combination of class - wise contrastive learning and instance - wise contrastive learning for robustness against the noise in data . ( 4 ( Scott and Matwin , 1998 ; Salton and McGill , 1983 ) often obtain very sparse representations that lack discriminations . The deep learning method ( Xu et al . , 2017 ) leverages pre - trained word embeddings ( Mikolov et al . , 2013 ) and deep neural network to enrich the representations . However , the learned representations may not appropriate for clustering . The deep joint clustering methods Hadifar et al . ( 2019 ) ; Zhang et al . ( 2021 ) integrate clustering with deep representation learning to learn the representations that are appropriate for clustering . Moreover , Zhang et al . ( 2021 ) utilizes the pre - trained SBERT ( Reimers and Gurevych , 2019 ) and contrastive learning to learn discriminative representations , which is conductive to deal with the noises . However , the adopted clustering objectives are prone to obtain degenerate solutions ( Yang et al . , 2017 ; Ji et al . , 2019 ) , especially on heavy imbalance data .
Among the above methods , only Zhang et al . ( 2021 ) provides model robustness to the noise in data . However , its robustness is still insufficient due to the lack of supervision information . Besides , Zhang et al . ( 2021 ) can not deal with various imbalanced data due to the degeneracy problem . As a contrast , in this work , we adopt pseudo - label technology to provide reliable supervision to learn robust representations for coping with imbalanced and noisy data .
Pseudo - labels for Unsupervised Learning
Pseudo - labels can be helpful to learn more discriminative representations in unsupervised learning ( Hu et al . , 2021 ) . Caron et al . ( 2018 ) shows that k - means clustering can be utilized to generate pseudo - labels for learning visual representations . However , it does not have a unified , well - defined objective to optimize ( i.e. , there are two objectives : k - means loss minimization and cross - entropy loss minimization ) , which means that it is difficult to characterize its convergence properties . Asano et al . ( 2020 ) proposes SeLa to optimize the same objective ( i.e. , cross - entropy loss minimization ) for both pseudo - label generation and representation learning , which can guarantee its convergence . Besides , SeLa transforms pseudo - label generation problem into an optimal transport problem . Caron et al . ( 2020 ) proposes SwAV which combines SeLa with contrastive learning to learn visual representations in an online fashion . However , both SeLa and SwAV add the constraint that the distribution of generated pseudo - labels should match the uniform distribution , to avoid clustering degeneracy . With the constraint , it is hard for them to cope with imbalanced data . As a contrast , in this work , we propose self - adaptive optimal transport to simultaneously estimate the real class distribution and generate pseudo - labels . Our method enforce the distribution of the generated pseudo - labels to match the estimated class distribution , and thus can avoid clustering degeneracy and adapt to various imbalanced data .
Methodology
An Overview of RSTC
The goal of RSTC is to discover and utilize the pseudo - labels to provide supervision for robust representation learning . RSTC consists of pseudolabel generation module and robust representation learning module , as illustrated in Fig . 1 . The pseudo - label generation module aims to generate reliable pseudo - labels for the robust representation learning module . To achieve this aim , we first obtain cluster predictions by the cluster assignment step , then we excavate pseudo - label information from the predictions by the self - adaptive optimal transport ( SAOT ) step . The robust representation learning module aims to use the generated pseudolabels as supervision to train robust representations . To achieve this goal , we introduce class - wise and instance - wise contrastive learning . In this way , RSTC can provide robustness to imbalanced and noisy data , thus enhancing the clustering performance .
Pseudo - label Generation Module
We first introduce the pseudo - label generation module . Although the deep joint clustering methods ( Xie et al . , 2016 ; Hadifar et al . , 2019 ; Zhang et al . , 2021 ) are popular these days , their clustering performance is limited due to the following reasons . Firstly , lacking supervision information prevents the deep joint clustering methods from learning more discriminative representations ( Hu et al . , 2021 ) . Secondly , they are prone to obtain degenerate solutions ( Yang et al . , 2017 ; Ji et al . , 2019 ) , especially on heavy imbalanced datasets . Therefore , to provide reliable supervision information for various imbalanced data , we propose SAOT in the pseudo - label generation module to generate pseudo - labels for the robust representation learning module . The overview of pseudo - label generation module is shown in Fig . 1 ( a ) , which mainly has two steps : Step 1 : cluster assignment , and Step 2 : SAOT .
Step 1 : cluster assignment . Cluster assignment aims to obtain cluster predictions of the original texts . Specifically , we adopt SBERT ( Reimers and Gurevych , 2019 ) as the encoding network Φ to encode the original text X as Φ ( X ) = E ∈ R N ×D 1 where N denotes batch size and D 1 is the dimension of the representations . We utilize the fully connected layers as the clustering network G p to predict the cluster assignment probability ( predictions ) , i.e. , G p ( E ) = P ∈ R N ×C , where C is the category number . The encoding network and the clustering network are fixed in this module .
Step 2 : SAOT . SAOT aims to exploit the cluster predictions to discover reliable pseudo - label . Asano et al . ( 2020 ) extends standard cross - entropy minimization to an optimal transport ( OT ) problem to generate pseudo - labels for learning image representations . This OT problem can be regarded as seeking the solution of transporting the sample distribution to the class distribution . However , the class distribution is unknown . Although Asano et al . ( 2020 ) sets it to a uniform distribution to avoid degenerate solutions , the mismatched class distribution will lead to unreliable pseudo - labels . Therefore , it is essential to estimate real class distribution for addressing this issue . The recent research studies the class distribution estimation , but it tends to cause clustering degeneracy on heavy imbalanced data , which we will further discuss in Appendix A. Hence , to discover reliable pseudo - labels on various imbalanced data , we propose SAOT . We will provide the details of SAOT below . We expect to minimize the cross entropy loss to generate the pseudo - labels by solving a discrete OT problem . Specifically , we denote the pseudo - labels as Q ∈ R N ×C . Let π = 1 N Q be the transport matrix between samples and classes , M = − log P be the cost matrix to move probability mass from samples to classes . The reason that we use 1 N between π and Q is the transport matrix should be a joint probability ( Cuturi , 2013 ) , i.e. , the sun of all values in the π should be 1 , while the sum of each raw in Q is 1 . We have ,
Weights Sharing Weights Sharing Projecting ( ! ) " , # " , # Augmented Pairs ( " ) , ( # ) Clustering ( & ) Encoding ( Φ ) Original Texts Encoding ( Φ ) Clustering ( & ) ℒ & ℒ ' " , # Representations
Q * = argmin Q ⟨Q , − log P ⟩ = N argmin π ⟨π , M ⟩. Thus , the OT problem is as fol- lows : min π ⟨π , M ⟩ + ϵH ( π ) s.t . π1 = a , π T 1 = b , π ≥ 0 , ( 1
where ϵ is a balance hyper parameter , H ( π ) = ⟨π , log π − 1⟩ is the entropy regularization ( Cuturi , 2013 ) , a = 1 N 1 is the sample distribution , and b is an unknown class distribution . To avoid clustering degeneracy and obtain reliable transport matrix with randomly initialized b , we introduce a penalty function about b to the OT objective and update b during the process of solving the transport matrix . We formulate the SAOT optimization problem as :
min π , b ⟨π , M ⟩ + ϵ 1 H ( π ) + ϵ 2 ( Ψ ( b ) ) T 1 s.t . π1 = a , π T 1 = b , π ≥ 0 , b T 1 = 1 , ( 2 )
where ϵ 1 and ϵ 2 are balance hyper - parameters ,
Ψ ( b ) = − log b−log ( 1−b )
is the penalty function about b. The penalty function not only limits b j ( a value of b ) ranges from 0 to 1 , but also avoids clustering degeneracy by encouraging b to be a uniform distribution . The encouragement is achieved by increasing the punishment for b j that is close to 0 or 1 . Besides , the level of the encouragement can be adjusted by ϵ 2 . Specifically , there are two critical terms in Equation ( 2 ) for exploring b , i.e. , ( 1 ) the cost matrix M and ( 2 ) the penalty function Ψ ( b ) , and we use ϵ 2 to balance these two terms . For balanced data , both M and Ψ ( b ) encourage b to be a uniform distribution . For imbalanced data , M encourages the head clusters ( i.e. , the clusters with a large proportion of instances ) to have larger b j and the tail clusters ( i.e. , the clusters with a small proportion of instances ) to have smaller b j . When b j of a tail cluster approaches 0 , this tail cluster tends to disappear ( clustering degeneracy ) . Whereas Ψ ( b ) still encourages b to be a uniform distribution for avoiding the degeneracy . With a decent trade - off parameter ϵ 2 , SAOT can explore appropriate b and obtain reliable π for various imbalanced data . We provide the optimization details in Appendix B. After obtaining π , we can get pseudo - labels by argmax operation , i.e ,
Q ij =    1 , if j = argmax j ′ π ij ′ 0 , otherwise . ( 3 )
It should be noted that , for convenience , we let π = 1 N Q before . However , π is essentially a join probability matrix and π ij can be decimals , while each row of Q is a one - hot vector .
Through the steps of cluster assignment and selfadaptive optimal transport , we can generate reliable pseudo - labels on various imbalanced data for the robust representation learning module .
Robust Representation Learning module
We then introduce the robust representation learning module . To begin with , motivated by ( Wenzel et al . , 2022 ) , we propose to adopt instance augmentations to improve the model robustness against various noises . Furthermore , inspired by ( Chen et al . , 2020 ) , ( Zhang et al . , 2021 ) and ( Dong et al . , 2022 ) , we adopt both class - wise and instance - wise contrastive learning to utilize the pseudo - labels and the augmented instance pairs for robust representation learning , as shown in Fig . 1 ( b ) . The class - wise contrastive learning uses pseudo - labels as the supervision to pull the representations from the same cluster together and push away different clusters . While the instance - wise contrastive learning disperses different instances apart , which is supposed to separate the overlapped clusters .
Next , we provide the details of the robust representation learning module . We utilize contextual augmenter ( Kobayashi , 2018 ; Ma , 2019 ) to generate augmented pairs of the original texts as X ( 1 ) and X ( 2 ) . Like the cluster assignment step in the pseudo - labels generation module , we can obtain the representations of augmented pairs X ( 1 ) and
X ( 2 ) as E ( 1 ) ∈ R N ×D 1 and E ( 2 ) ∈ R N ×D 1 , re- spectively .
We can obtain the predictions of them as P ( 1 ) ∈ R N ×C and P ( 2 ) ∈ R N ×C , respectively . We use the fully connected layers as the projecting network G z to map the representations to the space where instance - wise contrastive loss is applied , i.e. , G z ( E ( 1 ) ) = Z ( 1 ) ∈ R N ×D 2 and G z ( E ( 2 ) ) = Z ( 2 ) ∈ R N ×D 2 , where D 2 is the dimension of the projected representations . The encoding network and the clustering network share weights with the pseudo - label generation module .
The class - wise contrastive learning enforces consistency between cluster predictions of positive pairs . Specifically , the two augmentations from the same original text are regarded as a positive pair and the contrastive task is defined on pairs of augmented texts . Moreover , the pseudo - label of an original text is considered as the target of corresponding two augmented texts . We use the augmented texts with the targets as supervised data for cross - entropy minimization to achieve the consistency . The class - wise contrastive loss is defined as below :
L C = 1 N ⟨Q , − log P ( 1 ) ⟩ + 1 N ⟨Q , − log P ( 2 ) ⟩ .
( 4 ) The instance - wise contrastive learning enforces consistency between projected representations of positive pairs while maximizing the distance between negative pairs . Specifically , for a batch , there are 2N augmented texts , their projected representations are Z = [ Z ( 1 ) , Z ( 2 ) ] T , given a positive pair with two texts which are augmented from the same original text , the other 2 ( N − 1 ) augmented texts are treated as negative samples . The loss for a positive pair ( i , j ) is defined as :
l ( i , j ) = − log exp ( sim ( Z i , Z j ) / τ ) 2N k=1 1 k̸ = i exp ( sim ( Z i , Z k ) / τ ) , ( 5 )
where sim ( u , v ) denotes cosine similarity between u and v , τ denotes the temperature parameter , and 1 is an indicator . The instance - wise contrastive loss is computed across all positive pairs in a batch , including both ( i , j ) and ( j , i ) . That is ,
L I = 1 2N N i=1 ( l ( i , 2i ) + l ( 2i , i ) ) . ( 6 )
By combining the pseudo - supervised class - wise contrastive learning and the instance - wise contrastive learning , we can obtain robust representations and correctly separated clusters .
Putting Together
The total loss of RSTC could be obtained by combining the pseudo - supervised class - wise contrastive loss and the instance - wise contrastive loss . That is , the loss of RSTC is given as :
L = L C + λ I L I , ( 7 )
where λ I is a hyper - parameter to balance the two losses . By doing this , RSTC not only provides robustness to the imbalance in data , but also improve robustness against the noise in data .
The whole model with two modules forms a closed loop and self evolution , which indicates that the learned representations ( more robust ) and cluster predictions ( more accurate ) elevate each other progressively , as more reliable pseudo - labels are discovered during the iterations . Specifically , we firstly initialize the pseudo - labels Q by performing k - means on text representations . Next , we train the robust representation learning module by batch with the supervision of pseudo - labels . Meanwhile , we update Q throughout the whole training process in a logarithmic distribution , following ( Asano et al . , 2020 ) . Finally , we can obtain the cluster assignments by the column index of the largest entry in each row of P . The training stops if the change of cluster assignments between two consecutive updates for P is less than a threshold δ or the maximum number of iterations is reached .
Experiment
In this section , we conduct experiments on several real - world datasets to answer the following questions : ( 1 ) RQ1 : How does our approach perform compared with the state - of - the - art short text clustering methods ? ( 2 ) RQ2 : How do the SAOT , and the two contrastive losses contribute to the performance improvement ? ( 3 ) RQ3 : How does the performance of RSTC vary with different values of the hyper - parameters ?
Datasets
We conduct extensive experiments on eight popularly used real - world datasets , i.e. , AgNews , StackOverflow , Biomedical , SearchSnippets , GoogleNews - TS , GoogleNews - T , GoogleNews - S and Tweet . Among them , AgNews , Stack - Overflow and Biomedical are balanced datasets , SearchSnippets is a light imbalanced dataset , GoogleNews , GoogleNews - T , GoogleNews - S and Tweet are heavy imbalanced datasets . Following ( Zhang et al . , 2021 ) , we take unpreprocessed data as input to demonstrate that our model is robust to noise , for a fair comparison . More details about the datasets are shown in Appendix C.1 .
Experiment Settings
We build our model with PyTorch ( Paszke et al . , 2019 ) and train it using the Adam optimizer ( Kingma and Ba , 2015 ) . We study the effect of hyper - parameters ϵ 1 and ϵ 2 on SAOT by varying them in { 0.05 , 0.1 , 0.2 , 0.5 } and { 0 , 0.001 , 0.01 , 0.1 , 1 } , respectively . Besides , we study the effect of the hyper - parameter λ I by varying it in { 0 , 1 , 5 , 10 , 20 , 50 , 100 } . The more details are provided in Appendix C.2 . Following previous work ( Xu et al . , 2017 ; Hadifar et al . , 2019 ; Rakib et al . , 2020 ; Zhang et al . , 2021 ) , we set the cluster numbers to the ground - truth category numbers , and we adopt Accuracy ( ACC ) and Normalized Mutual Information ( NMI ) to evaluate different approaches . The specific definitions of the evaluation methods are shown in Appendix C.3 . For all the experiments , we repeat five times and report the average results .
Baselines
We compare our proposed approach with the following short text clustering methods . BOW ( Scott and Matwin , 1998 ) & TF - IDF ( Salton and McGill , 1983 ) applies k - means on the TF - IDF representations and BOW representations respectively . STC 2 -LPI ( Xu et al . , 2017 ) first uses word2vec to train word embeddings on the in - domain corpus , and then uses a convolutional neural network to obtain the text representations where k - means is applied for clustering . Self - Train ( Hadifar et al . , 2019 ) follows ( Xie et al . , 2016 ) uses an autoencoder to get the representations , and finetunes the encoding network with the same clustering objective . The difference are that it uses the word embeddings provided by ( Xu et al . , 2017 ) with SIF ( Arora et al . , 2017 ) to enhance the pretrained word embeddings , and obtains the final cluster assignments via k - means . K - means_IC ( Rakib et al . , 2020 ) first applies k - means on the TF - IDF representations and then enhances clustering by the iterative classification algorithm . SCCL ( Zhang et al . , 2021 ) is the more recent short text clustering model which utilizes SBERT ( Reimers and Gurevych , 2019 ) as the backbone and introduces instance - wise contrastive learning to support clustering . Besides , SCCL uses the clustering objective proposed in ( Xie et al . , 2016 ) for deep joint clustering and obtains the final cluster assignments by k - means .
Clustering Performance ( RQ1 )
Results and discussion The comparison results on eight datasets are shown in Table 1 . SBERT ( kmeans ) denotes the pre - trained SBERT model with k - means clustering , which is the initial state of our RSTC .
From the results , we can find that : ( 1 ) Only adopting traditional text representations ( BOW and ( Yang et al . , 2017 ; Ji et al . , 2019 1 . From it , we can observe that they all can not achieve satisfactory results due to their limitations . Specifically , ( 1 ) RSTC - OT will be guided by the mismatched distribution constraint to generate unreliable pseudo - labels . ( 2 ) RSTC - C is good at aggregating instances , but it has difficulties to address the situation when different categories are overlapped with each other in the representation space at the beginning of the learning progress , which may lead to a false division . shows that choosing the proper hyper - parameters for different imbalance levels of datasets is important , especially on the heavy imbalanced dataset GoogleNews - T. Empirically , we choose ϵ 1 = 0.1 on all datasets , ϵ 2 = 0.1 on the balanced datasets , ϵ 2 = 0.01 on the light imbalanced datasets , and ϵ 2 = 0.001 on the heavy imbalanced datasets . Then we perform experiments by varying λ I in { 0 , 1 , 5 , 10 , 20 , 50 , 100 } . The results on three datasets are shown in Fig . 4 . From them , we can see that the performance improves when λ I increases , then keeps a relatively stable level after λ I reaches 1 and finally decreases when λ I becomes too large . We can conclude that when λ I is too small , the ability of instance - wise contrastive learning can not be fully exploited . When λ I is too large , the ability of class - wise contrastive learning will be suppressed , which also reduces the clustering performance . Empirically , we choose λ I = 10 for all datasets .
Conclusion
In this paper , we propose a robust short text clustering ( RSTC ) model , which includes pseudo - label generation module and robust representation learning module . The former generates pseudo - labels as the supervision for the latter . We innovatively propose SAOT in the pseudo - label generation mod - ule to provide robustness against the imbalance in data . We further propose to combine classwise contrastive learning with instance - wise contrastive learning in the robust representation learning module to provide robustness against the noise in data . Extensive experiments conducted on eight real - world datasets demonstrate the superior performance of our proposed RSTC .
Limitations
Like existing short text clustering methods , we assume the real cluster number is known . In the future , we would like to explore a short text clustering method with an unknown number of clusters . Moreover , the time complexity of self - adaptive optimal transport is O ( n 2 ) , we are going to seek a new computation to reduce the complexity .
Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , NAACL - HLT 2021 , Online , June 6 - 11 , 2021 , pages 5419 - 5430 . Association for Computational Linguistics .
Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 . Character - level convolutional networks for text classification . Advances in neural information processing systems , 28 .
A Different Class Distribution Estimation Methods
We have tried three class distribution estimation methods , including : ( 1 This method replaces the penalty function in our method with the common entropy regularization Ψ ( b ) = KL ( b ∥b ) , whereb is the last updated b , and the current b can be updated the same way our method does . Note that , the parameters of M2 are following , the parameters of M3 are the same as M1 ( ours ) .
For comprehensive comparison , we conduct the experiments on one imbalanced dataset GoogleNews - T and one balanced dataset Stack - Overflow with randomly initialized b for visualizing how the accuracy and the number of predicted clusters are changing over iterations . Moreover , except the update of b , everything else about the experiments is the same for three methods . The results are shown in Fig . 5 ( a ) - ( d ) . From them , we can find that : ( 1 ) For the imbalanced dataset , M1 ( ours ) achieves the best accuracy and converges to the real category number , while other methods have clustering degeneracy problem . ( 2 ) For the balanced dataset , M2 achieves best accuracy more quickly while M1 ( ours ) catches up in the end , and all methods obtain real category number . Although M3 can obtain good accuracy on the imbalanced dataset , it has the worst accuracy on the balanced dataset . In addition , although M2 achieves good accuracy on the balanced dataset , it has the worst accuracy on the imbalanced dataset . Only M1 ( ours ) achieves Figure 5 : The accuracy and the number of predicted clusters at different iterations on GoogleNews - T ( first row ) and StackOverflow ( second row ) . Note that because the samples in GoogleNews - T are too short , which makes it difficult to generate relatively reliable pseudolabels , we pre - train the representations with L I for three methods in the first 600 steps . Due to the same pretraining process , we omit the curves in the first 600 steps on GoogleNews - T .
fairly good performance on both datasets , which indicates that our method are robust to various imbalance levels of datasets . The experiments prove the effectiveness of our class distribution estimation method .
B SAOT
As mentioned in Section 3.2 , the SAOT problem is formulated as :
min π , b ⟨π , M ⟩ + ϵ 1 H ( π ) + ϵ 2 ( Ψ ( b ) ) T 1 , s.t . π1 = a , π T 1 = b , π ≥ 0 , b T 1 = 1 . ( 8 )
where ϵ 1 and ϵ 2 are balance hyper - parameters ,
Ψ ( b ) = − log b − log ( 1 − b )
is the penalty function about b. We adopt the Lagrangian multiplier algorithm to optimize the problem :
min π , b ⟨π , M ⟩ + ϵ 1 H ( π ) + ϵ 2 ( Ψ ( b ) ) T 1 − f T ( π1 − a ) − g T ( π T 1 − b ) − h ( b T 1 − 1 ) , ( 9 )
where f , g , and h are all Lagrangian multipliers . Taking the differentiation of Equation ( 9 ) on the variable π , we can obtain :
π ij = exp ( f i + g j − M ij ϵ 1 ) > 0 . ( 10
We first fix b , due to the fact that π1 = a and π T 1 = b , we can get :
exp ( f i ϵ 1 ) = a i C j exp ( g j −M ij ϵ 1 ) , ( 11 )
exp ( g j ϵ 1 ) = b j N i exp ( f i −M ij ϵ 1 ) . ( 12 )
Then we fix f and g , and update b by :
min b ϵ 2 ( Ψ ( b ) ) T 1 + g T b − h ( b T 1 − 1 ) . ( 13
Taking the differentiation of Equation ( 13 ) on the variable b , we can obtain :
( g j − h ) b 2 j − ( ( g j − h ) + 2ϵ 2 ) b j + ϵ 2 = 0 . ( 14 )
It is easy to get the discriminant of Equation ( 14 )
∆ j = ( g j − h ) 2 + 4ϵ 2 2 > 0 , b j ( h ) = ( g j − h + 2ϵ 2 ) ± ∆ j 2 ( g j − h ) . ( 15 )
Note that ,
b j ( h ) = ( ( g j − h ) + 2ϵ 2 ) + ∆ j 2 ( g j − h ) ≥ 1 . ( 16 )
Thus , we choose the following b j ( h ) :
b j ( h ) = ( ( g j − h ) + 2ϵ 2 ) − ∆ j 2 ( g j − h ) . ( 17 )
Taking Equation ( 17 ) back to the original constraint b T 1 = 1 , the formula is defined as below :
( b ( h ) ) T 1 − 1 = 0 , ( 18
where h is the root of Equation ( 18 ) , and we can use Newton 's method to work out it . Specifically , we first define that f ( h ) = ( b ( h ) ) T 1 − 1 , then h can be updated by :
h ← h − f ( h ) f ′ ( h ) , ( 19 )
where the iteration number is set to 10 . Then we can obtain b by Equation ( 17 ) . In short , through iteratively updating Equation ( 11 ) , ( 12 ) , ( 19 ) , and ( 17 ) , we can obtain the transport matrix π on Equation ( 10 ) . We show the iteration optimization scheme of SAOT in Algorithm 1 .
Algorithm 1 The optimization scheme of SAOT Input : The cost distance matrix : M . Output : The transport matrix : π . Procedure :
1 : Initialize f and g randomly ; 2 : Initialize b randomly and perform normalization so that b T 1 = 1 ; 3 : Initialize h = 1 . 4 : for i = 1 to T do
C Experiment
C.1 Datasets
We conduct extensive experiments on eight popularly used real - world datasets . The details of each dataset are as follows .
AgNews ( Rakib et al . , 2020 ) is a subset of AG 's news corpus collected by ( Zhang et al . , 2015 ) which consists of 8,000 news titles in 4 topic categories . StackOverflow ( Xu et al . , 2017 ) consists of 20,000 question titles associated with 20 different tags , which is randomly selected from the challenge data published in Kaggle.com 1 . Biomedical ( Xu et al . , 2017 ) is composed of 20,000 paper titles from 20 different topics and it is selected from the challenge data published in BioASQ 's official website 2 . SearchSnippets ( Phan et al . , 2008 ) contains 12,340 snippets from 8 different classes , which is selected from the results of web search transaction . GoogleNews ( Yin and Wang , 2016 ) consists of the titles and snippets of 11,109 news articles about 152 events ( Yin and Wang , 2014 ) which is divided into three datasets : the full dataset is GoogleNews - TS , while GoogleNews - T only contains titles and GoogleNews - S only has snippets . Tweet ( Yin and Wang , 2016 )
C.2 Experiment Settings
We choose distilbert - base - nli - stsb - mean - tokens in Sentence Transformer library ( Reimers and Gurevych , 2019 ) to encode the text , and the maximum input length is set to 32 . The learning rate is set to 5×10 −6 for optimizing the encoding network , and 5 × 10 −4 for optimizing both the projecting network and clustering network . The dimensions of the text representations and the projected representations are set to D 1 = 768 and D 2 = 128 , respectively . The batch size is set to N = 200 . The temperature parameter is set to τ = 1 . The threshold δ is set to 0.01 . The datasets specific tuning is avoided as much as possible . For BOW and TF - IDF , we achieved the code with scikit - learn ( Pedregosa et al . , 2011 ) . For all the other baselines , i.e. , STC 2 -LPI 4 , Self - Train 5 , K - means_IC 6 , and SCCL 7 ( MIT-0 license ) , we used their released code .
Besides , we substitute the accuracy evaluation code of K - means_IC with the evaluation method described in our paper .
In addition , as STC 2 -LPI and Self - Train use the word embeddings pre - trained with in - domain corpus , and there are only three datasets ' pre - trained word embeddings provided , therefore we do not report the results of other five datasets for them .
C.3 Evaluation Metrics
We report two widely used evaluation metrics of text clustering , i.e. , accuracy ( ACC ) and normalized mutual information ( NMI ) , following ( Xu et al . , 2017 ; Hadifar et al . , 2019 ; Zhang et al . , 2021 ) . Accuracy is defined as :
ACC = N i=1 1 y i = map ( ŷ i ) N , ( 20 )
where y i andŷ i are the ground truth label and the predicted label for a given text x i respectively , map ( ) maps each predicted label to the corresponding target label by Hungarian algorithm ( Papadimitriou and Steiglitz , 1998 ) . Normalized mutual information is defined as :
N M I ( Y , Ŷ ) = I ( Y , Ŷ ) H ( Y ) H ( Ŷ ) , ( 21 )
where Y andŶ are the ground truth labels and the predicted labels respectively , I ( ) is the mutual information , and H ( ) is the entropy .
C.4 Visualization
To better show the clustering degeneracy problem , we visualize how the number of predicted clusters ( we call it clusters later ) are changing over iterations on SCCL and RSTC . The results are shown in Fig . 6 . From it , we verify that SCCL has relatively serious clustering degeneracy problem while RSTC solves it to some extent . Specifically , the clusters of SCCL is much less than the real category number . Moreover , the degeneracy has a negative effect on the final k - means clustering performance because it makes the representations getting worse . Whereas the clusters of RSTC almost convergent to real category number , which assures the high accuracy of RSTC . The visualization results illustrate the validity of our model .
C.5 Computational Budget
The number of parameters in our model is 68M .
Our training for each dataset takes about 10 - 30 minutes , using a GeForce RTX 3090 GPU .
Acknowledgements
This work was supported in part by the Leading Expert of " Ten Thousands Talent Program " of Zhejiang Province ( No.2021R52001 ) and National Natural Science Foundation of China ( No.72192823 ) .
B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ?
We use the datasets the same way as existing work .
B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? The datasets we use only have the text instances and their category IDs .
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Appendix C.1 B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Appendix C.5
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022 , with the addition of a question on AI writing assistance .
Norm - based Noisy Corpora Filtering and Refurbishing in Neural Machine Translation
Recent advances in neural machine translation depend on massive parallel corpora , which are collected from any open source without much guarantee of quality . It stresses the need for noisy corpora filtering , but existing methods are insufficient to solve this issue . They spend much time ensembling multiple scorers trained on clean bitexts , unavailable for low - resource languages in practice . In this paper , we propose a norm - based noisy corpora filtering and refurbishing method with no external data and costly scorers . The noisy and clean samples are separated based on how much information from the source and target sides the model requires to fit the given translation . For the unparallel sentence , the target - side history translation is much more important than the source context , contrary to the parallel ones . The amount of these two information flows can be measured by norms of source- / target - side context vectors . Moreover , we propose to reuse the discovered noisy data by generating pseudo labels via online knowledge distillation . Extensive experiments show that our proposed filtering method performs comparably with state - ofthe - art noisy corpora filtering techniques but is more efficient and easier to operate . Noisy sample refurbishing further enhances the performance by making the most of the given data 1 .
Introduction
Neural machine translation ( NMT ) has achieved significant progress with help from large parallel corpora for training ( Tiedemann , 2012 ; . These data are typically extracted from the web without much control over the quality , which presents misalignment , wrong languages , too many numbers or URLs , etc . In this case , noisy corpora filtering holds a critical research area to prevent noisy bitexts from degrading the generalization performance of NMT ( Khayrallah and Koehn , 2018 ) .
wang yong @ @ zhi [ 37 @ @ 69 30 @ @ 57 18 @ @ 07 ] is the chief designer in china 's manned spaceflight project .
Src :
Tgt : Source - side Our metric Target - side [ 37 @ @ 69 30 @ @ 57 18 @ @ 07 ] in China Figure 1 : An example of the required source- / target - side information when the model fits an unparallel Zh⇒En sentence pair ( words in red are real noisy segments ) . The amount of information on each side is counted by norms of corresponding context vectors , positively correlated to the darkness of color blocks .
Much effort has been devoted to this field with the promotion of a WMT shared task for parallel corpus filtering . However , prior work is difficult to operate in practice due to two drawbacks . ( 1 ) High time and computational cost . Their good performance relies on the ensembling of multiple largescale scorers , which involves costly pre - training and fine - tuning ( Esplà - Gomis et al . , 2020 ; Lu et al . , 2020 ) . ( 2 ) Dependence on clean bitexts . The training of above scorers needs clean parallel sentence pairs as positive samples , which are scarce for lowresource languages in real - world applications .
This paper introduces a norm - based noisy sample filtering and refurbishing method , which avoids extra clean bitexts and heavy scorers . We distinguish unparallel sentence pairs from others based on observed model behaviors during the training of NMT . Generally , the model captures two aspects of information to predict the given translation , the source - side context information from the encoder and the target - side one from history translations in the decoder . For unparallel sentence pairs , provided translations are partially or entirely unrelated to the source sentence . In this case , the NMT model behaves as a language model , which requires excessive target - side information to fit noises . Thus , we use the information ratio of the source to the target side as the criterion to filter noises .
Specifically , we observe that a greater vector norm implies richer context information captured by the model . Thus , we calculate the amount of each information flow by norms of corresponding context vectors . This metric is easy to obtain in the training process and sufficient to model how much information is encoded on each side . We take Figure 1 as an example . When the model predicts the content word " china " , the norm of the source - side context information is more significant than that of the target side . The opposite situation is in generating the function word " in " . However , the quantity of target - side information appears to be exceptionally great for the noisy fraction , which presents a deeper color than others , leading to a lower score than correct translations under our estimation .
We further propose to refurbish discovered noisy samples by generating pseudo labels via online knowledge distillation . By doing this , the source sentence in Figure 1 is regarded as the monolingual data to complement limited clean bitexts . Throughout the whole , we incorporate filtering and refurbishing into the training of NMT rather than separating data filtering and training , thus considerably improving computational efficiency .
We validate the effectiveness of our approaches on Transformer - based NMT ( Vaswani et al . , 2017 ) , including the WMT2020 shared task for parallel corpus filtering ( Km⇒En and Ps⇒En ) and our inhouse web - crawled datasets ( He , I d , Pt , Ko , and Es⇒Zh ) . Empirical results show that our proposed method performs comparably with SOTA noisy corpora filtering approaches . Refurbishing noisy samples further substantially boosts the performance . Detailed analyses show that our metric can reflect the alignment extent at word and sentence levels .
The contributions of this paper are three - fold :
• We propose to use norms of source - and targetside context vectors to represent the amount of information flowing from each side . We find that the model needs excessive target - side information to fit the unparallel sentence pair , which is the basis of the following work .
• We propose a norm - based noisy corpora filtering method by calculating the information ratio from the source to the target side . It is experimentally efficient and effective under the condition of no extra data and costly scorers . The model first sees history translations and then sourceside contexts by attention mechanism and obtains two context vectors t l j and s l j . In the enlarged view of MHA , the arrows in the circles represent the corresponding vectors . The sizes of circles illustrate the values of attention weights or the vectors ' norms .
• We propose to refurbish discovered noisy samples by producing pseudo labels via online knowledge distillation , which makes the most of the corpora and further boosts the performance .
Background
In this section , we first briefly introduce a mainstream NMT framework , Transformer , with a focus on how to capture source - and target - side contexts . We then present how the vector norm serves as an indicator of diverse features , which motivates us to count how much information is encoded in context vectors of each side based on vector norms .
Transformer - based NMT
The Transformer is an encoder - decoder framework which alternately looks over source - and target - side contexts to make prediction . The encoder with L layers transforms an input x = { x 1 , x 2 , ... x n } to a sequence of hidden states h where y < j is a partial translation . c L j denotes the j - th hidden state in the L - th decoder layer . We take the l - th decoder layer as an example in Figure 2 . The model first attends to the history translation c l−1 < j by multi - head attention ( MHA ) and obtains the target context vector t l j .
L = h L 1 , h L 2 , ... h L n ,
t l j = MHA ( c l−1 < j , c l−1 j ) ( 2 )
where MHA enables dynamically selecting relevant tokens by assigning different attention weights . t l j is then transformed to z l j by layer normalization ( Ba et al . , 2016 ) and residual network ( He et al . , 2016 ) . The model later looks at the sourceside context to obtain the source context vector s l j :
s l j = MHA ( h L , z l j ) ( 3 )
Two - side information is mixed up to calculate the next - layer hidden state c l j .
o l j = LayerNorm ( s l j + z l j ) c l j = LayerNorm ( o l j + FFN ( o l j ) ) ( 4 )
where FFN denotes a feedforward neural network .
Norm - based Word Importance Measurement
As the key element in the NMT model , word representations capture rich semantic features . Schakel and Wilson ( 2015 ) report that the L 2 norm of word vectors learned in the word2vec model ( Mikolov et al . , 2013 ) is informative , where words with low frequency or diverse contexts are more likely to be assigned higher norms . Liu et al . ( 2020 ) state that the norm of word embeddings in the NMT model is also a good proxy of word importance .
Here , we extend to hidden states attended by the attention mechanism ( h L produced by the encoder ' HFRGLQJ6WHSj and c L−1 j in the decoder ) . As shown in Figure 3 , norms of h L shift downwards with the frequency increasing . Specifically , norms of content words are relatively higher than function words . It suggests that the rare and informative words obtain a high norm of h L , which stays consistent with results of c L−1 j as given in Appendix A. Thus , norms of hidden states that the attention mechanism looks at can indicate word importance .
As context vectors are formed as a weighted sum of those hidden states via attention mechanism , the derived context vectors ' norms would grow in line with the model 's increasing attention towards informative words with higher norms . These observations motivate us to evaluate how much information is encoded in context vectors from this point .
Methodology
We aim to detect and refurbish noisy sentence pairs by observing how the model predicts each token . A sentence pair is potentially misaligned if the model depends heavily on history predictions rather than the source sentence to fit given translations . To this end , we first introduce a norm - based measurement to count the amount of information extracted from the source and target side ( Section 3.1 ) . Then , we show how to use this metric to filter noisy samples ( Section 3.2 ) , which are further refurbished by producing pseudo labels via online knowledge distillation ( Section 3.3 ) .
Norm - based Source - and Target - side Information Measurement
As shown in Figure 2 , the model repeatedly collects information from the source sentence ( h L ) and history translation ( c l−1 < j ) to calculate context vectors by attention mechanisms . Specifically , it computes a weighted sum of hidden states , the norm of which indicates word importance as stated in section 2.2 . If the model pays more attention to content words with greater norms , the norm of obtained context vector would correspondingly increase , and vice versa . Thus , we can use the norm of the source and target context vector ( ∥s l j ∥ 2 and ∥t l j ∥ 2 ) to count the amount of information extracted from two sides .
However , directly comparing ∥t l j ∥ 2 at different steps is unfair , for more history translations are available for the model in the later steps . In this case , the decoder has access to more content words and gets a high - norm context vector . As shown in Figure 4 , ∥t L j ∥ 2 rapidly increases at first , and then the growth slows down . The overall trend is similar to the function y = 3 √ x. Thus , we normalize the norm of the target context vector with 3 √ j. Here , we extract context vectors from the L - th layer and design the metric as :
γ j = ∥s L j ∥ 2 ∥t L j ∥ 2 / 3 √ j ( 5 )
which is positively related to how much the source sentence is relied on to make predictions . Different values of γ j indicate different cases :
• If γ j is big , the model mainly depends on the source sentence x to predict y j , which may be nouns , verbs , or other content words .
• If γ j is medium , the partial translation has a larger impact on the prediction of y j , which is slightly related to x. Here , y j may be prepositions , determiners , or other function words .
• If γ j is small , the model relies on the language model to produce the unrelated translations , which are exactly our targeted noisy samples .
Norm - based Corpus Filtering
Based on the metric γ j calculated at each step , we measure how much the target sentence y is aligned with the input x as follows :
R ( x , y ) = 1 m m j=1 γ j ( 6 )
When R ( x , y ) is smaller than a threshold k , the target sentence may be desperately inadequate or even wholly unrelated to the source sentence . To eliminate the impact of these noisy samples , we erase their loss during training by the norm - based sentence - level objective :
L = I R ( x , y ) > k • L NLL ( 7 )
where the indicative function I R ( x , y ) > k is equal to 1 if R ( x , y ) > k , else 0 . k is a hyperparameter which is used to adjust the filtering ratio . L NLL is the loss of the NMT calculated by the negative log - likelihood in Equation ( 1 ) . Considering the early NMT model is not welltrained to gather information from the source and target sides , we first warm up the model on the entire dataset for T steps and then filter the noisy sentence pairs based on observed model behaviors . In the middle and later stages , I R ( x , y ) > k would stable at a particular value .
Noisy Label Refurbishing
The detected unparallel sentence pairs hamper the training of the NMT system . But they can split into individual monolingual sentences , which remain to be fully re - utilized . From this perspective , we propose to refurbish these noisy samples by generating pseudo labels via knowledge distillation ( Hinton et al . , 2015 ; Kim and Rush , 2016 ) .
The biggest issue of integrating knowledge distillation in our scenario is how to acquire a strong teacher model , which decides the performance of the student model ( Gou et al . , 2021 ) . However , the absence of a large - scale clean corpus makes it hard to train an offline competitive teacher model . Alternatively , we employ online self - distillation ( Wei et al . , 2019 ) to let the history model generate the translation for noisy samples .
Concretely , we use the checkpoint with the best performance on the validation set as the teacher . Then , the current model learns to match the teacher model 's prediction q ( •|x ) on the noisy data . The word - level self - distillation loss can be defined as :
L SD = − m j=1 |V| i=1 q ( y j = i|y < j , x ; θ T ) × log p ( y j = i|y < j , x ; θ ) ( 8 )
where θ T and θ parameterize the teacher and student model separately . V is the target vocabulary set . In this way , we make full use of the corpus by precisely figuring out the clean data and replacing the remaining noises with pseudo labels :
L = I R ( x , y ) > k • L NLL + I R ( x , y ) ≤k • L SD ( 9 )
Experiments
We conduct experiments on two types of datasets :
( 1 ) WMT 2020 shared task on parallel corpus fil- tering and alignment for low - resource conditions 2 : Khmer⇒English ( Km⇒En ) and Pashto⇒English ( Ps⇒En ) , and ( 2 ) our in - house web crawled corpora : Hebrew ( He ) , Indonesian ( I d ) , Korean ( Ko ) , Portuguese ( Pt ) , and Spanish ( Es ) to Chinese ( Zh ) .
Dataset
WMT20 corpus filtering task asks participants to select different - scale subsets of high - quality sentence pairs from the noisy data . The quality of selected subsets is measured by the performance of an NMT system trained on this data . This task provides three kinds of data : ( 1 ) 4.17 M Km⇒En and 1.02 M Ps⇒En noisy sentence pairs which participants have to score for filtering , and ( 2 ) clean parallel and monolingual data to train quality estimation models that help the filtering task , and ( 3 ) development and test sets used to evaluate translation systems trained on filtered data . Note that we do not use the second part of the data and only experiment with ( 1 ) and ( 3 ) . We strictly follow Koehn et al . ( 2020 ) to preprocess the raw data . For the in - house corpora , we apply sentence pieces on tokenized text . We construct the vocabulary with the size of 20k tokens where the source and target languages are separately encoded .
The raw corpora are firstly filtered by heuristic rules to remove extremely noisy sentence pairs . We implement rule - based filtering as in Lu et al . ( 2020 ) , the details of which are listed in Appendix B .
A cursory review of the above corpora is given in Figure 5 . We categorize unparallel sentence pairs into three types based on the level of misalignment : ( I ) words , ( II ) phrases , and ( III ) the whole sentence . We randomly sample 200 sentence pairs from each preprocessed dataset and manually annotate them with predefined labels based on their noise degrees .
We find a high noise rate in the WMT20 corpora , while noise types in in - house datasets are diverse . These two kinds of datasets pose different challenges for our methods to filter noises accurately .
Settings
We strictly follow model configurations and evaluation settings provided by WMT20 organizers . The evaluation is done on subsets of two predefined sizes , 5 M and 7 M English words . The most striking difference between participants and us is that we simultaneously perform data filtering and model training rather than " filter first and next train " .
For our in - house datasets , we experiment with Transformer Base ( Vaswani et al . , 2017 ) . More details about experimental settings for WMT20 and in - house datasets are given in Appendix C .
The choice of the threshold k is key to our methods . In practice , we rank 200 samples manually labeled with noise extents in Section 4.1 by R ( x , y ) . k is set based on different scenarios . If given the remaining data size , i.e. , WMT20 predefines the size of selected data , we select the corresponding k in 200 annotated samples by the ratio of remaining data . In the WMT20 scenario , k is 2.75 and 2.45 for the 5 M and 7 M words setting in Km⇒En . For Ps⇒En , k is 2.3 and 1.4 for those two settings . In the case of no required size for the data left , we set k based on the noise rate of annotated samples and filter the noisiest samples ranking at the bottom . For various noise rates in in - house datasets , k is 1.8 for He , Pt , Ko⇒Zh , 1.65 for Id⇒Zh , and 1.9 for Es⇒Zh . We find that the model capacity affects the value of R ( x , y ) , making k differ greatly for experiments on WMT20 and in - house datasets ( model parameters 47 M vs. 68 M ) . Besides , it is easy to see that different language pairs have similar ranges of R ( x , y ) under one experimental setting .
Main Results
To thoroughly compare with participants in the WMT20 corpus filtering task , we report the performance of two models ranking the top ( Esplà - Gomis et al . , 2020 ; Lu et al . , 2020 ) and the official baseline LASER ( Artetxe and Schwenk , 2019 ) . The best showings leverage the clean external parallel and monolingual data to score each language pair , whereas we do not use this part of the data .
Table 1 presents the performance of the NMT model trained on participants ' selected subsets with varying scales . Our proposed method yields comparable results with the best results in this competi- We find that further benefits from our methods vary across different datasets , which are minor in He⇒Zh and Es⇒Zh but extremely significant in Id⇒Zh and Pt⇒Zh . There are two main reasons for that : the scale of the dataset and the noise rate . A large - scale dataset in He⇒Zh makes it robust to a high percentage of noises ( Jayanthi and Pratapa , 2021 ) . On the other hand , as seen in Figure 5 , the type III noise , which presents the most misaligned sentence pairs , only accounts for 7 % in Es⇒Zh , which leads to low demand for noisy data filtering .
Notably , our method has a specific scope of applications . We do not suggest using noisy label refurbishing when the noise rate 3 exceeds 30 % , i.e. , WMT20 datasets , for massive noises lead to a weak baseline model . However , our norm - based corpora filtering method still works in these cases . ( Zhang and Zong , 2016 ) . " sample " and " beam search " are two inference ways to get the synthetic data .
Variations of Knowledge Distillation . As aforesaid , we use the best checkpoint on the validation set as the teacher model to distill located noisy samples only . It raises the question whether we have better options for the teacher model or whether we can conduct a wide range of knowledge distillation .
For comparison , we try more variations of knowledge distillation and present results in Table 3 . We find that the best checkpoint is more competent than the last one , which is largely similar to the current model with limited complementary knowledge to the student . For selective distillation , we see that distilling the whole dataset is not a good choice . The amount of teacher knowledge is not " more is better " ( Wang et al . , 2021 ) . It may induce more noise , especially for a weak teacher model . Among them , the bottom 50 % of R ( x , y ) are in a higher demand for distillation . Those samples with
+ H = K 5 [ \ , G = K 2ND\ 7\SH , 7\SH , ,7\SH , , , Figure 7 :
R ( x , y ) of manually annotated samples with varying noise degrees in section 4.1 . Type I , II , and III represent word- , phrase- , and sentence - level misalignment , respectively . Type III and partial type II are our target noisy samples that need to be filtered . The dashed red line is the threshold k we set .
higher R ( x , y ) are likely with clean labels where the teacher model is useless . The results show the necessity of carefully selecting distilled samples in the presence of noise . Furthermore , our method is related to Forward Translation ( FT ) ( Zhang and Zong , 2016 ) for exploiting the monolingual data . They use the earlier trained model as the teacher to translate source sentences to target translation , and the obtained synthetic corpora are fed to the student model trained later . To study the usefulness of FT in our scenario , we regard our proposed noisy data filtering method as the teacher . Then , we split misaligned samples , where R ( x , y ) ≤ k ( 1.01 M in He⇒Zh ) , as monolingual source sentences . The following steps are in line with FT . From the last two lines in Table 3 , the synthetic data is of poor quality if generated by sampling for a weak teacher model . Beam search ensures good translations and performs better but is computationally expensive . Unlike sentence - level distillation , the word - level distillation in this paper allows the transfer of local word distributions . It eliminates the error propagated from the teacher model , which is more suitable in the noise scenario .
Analysis
We conduct extensive analyses to evaluate the ability of R ( x , y ) to pinpoint unparallel sentence pairs . We first examine whether R ( x , y ) can reflect the overall degree of misalignment . From a more finegrained view , we explore the correlation between the score γ j at step j and linguistic properties .
Correlation with the Misalignment Degree
As previous , we filter sentence pairs where R ( x , y ) is lower than the threshold k. To explore whether filtered samples are indeed corrupted , we calculate R ( x , y ) of annotated samples in section 4.1 , which are categorized into four classes based on the degree of misalignment . As shown in Figure 7 , R ( x , y ) is negatively correlated with the extent of noises . Type III and part of type II noisy samples are assigned with relatively lower R ( x , y ) where too much target - side information is required to predict the translation . It indicates that our proposed measurement is reflective of misalignment and sufficient to filter unparallel sentence pairs for NMT .
+ H = K j , G = K 1RXQ $ GM 9HUE 3UHS 3XQF ' HWH
Correlation with Linguistic Properties
As seen in Equation ( 6 ) , the sentence - level R ( x , y ) is averaged over γ j at step j , which depicts whether each target word corresponds to any source words . This section studies the relation between γ j and two properties , syntactic roles and fertility . Chinese sentences are POS tagged by jieba 4 . Fertility reveals how many source tokens a target token is aligned to , which is obtained by fast align ( Dyer et al . , 2013 ) to extract bilingual alignment . Results are reported on the validation set .
As shown in Figure 8 , content words are in great need of the source context , thus leading to a higher γ j . However , content - free words , like punctuation and determiner , mainly rely on the target - side information , where γ j is significantly below average . Furthermore , the value of γ j is positively related to the fertility of the target word . As illustrated in Figure 9 , the prediction of the target token aligning to more source words relies more on the source sentence , thus leading to a higher γ j . These findings fully show the rationality of our proposed metrics . 4 https : / / pypi.org / project / jieba /
Related Work
Many web - crawled data for training the NMT system are so noisy that we should select the highquality subset . In this section , we first review recent advances in noisy corpora filtering for NMT . As we treat the discovered noisy data as unlabeled monolingual data to distill in this paper , another related work is knowledge distillation in NMT .
Parallel Corpus Filtering
There is a rich body of work on filtering out noises in parallel data . Xu and Koehn ( 2017 ) construct the noisy synthetic data ( inadequate and non - fluent translations ) and train a classifier to distinguish good from the bad . Açarçiçek et al . ( 2020 ) follow this idea with a classifier based on a multilingual version of the RoBERTa ( Conneau et al . , 2020 ) . Many other studies employ different bilingual and monolingual language models to score the sentence pairs ( Lu et al . , 2020 ; Esplà - Gomis et al . , 2020 ) , which are data - hungry and time - consuming in practice . Unlike them , our method does not use external data and yields comparable results . Also , we perform data filtering and model training in one stage to reduce time and computation costs .
Knowledge Distillation in NMT
Knowledge distillation transfers the knowledge from the teacher to the student model . It is widely studied in NMT to obtain a lightweight and effective model . Kim and Rush ( 2016 ) use an offline large - capacity NMT system as the fixed teacher model , which is also extended to multilingual NMT ( Tan et al . , 2019 ) . Instead , the same - capacity teacher model is used in Zhang and Zong ( 2016 ) ; Sennrich et al . ( 2016 ) . Such approaches need massive clean data for training an accurate teacher model , which is impractical in the limited noisy corpora . Another line of work applies self - distillation to NMT using the current or history model as the teacher model ( Wei et al . , 2019 ; Hahn and Choi , 2019 ) , updating the distilled knowledge as a better model comes . Here , we focus on a new scenario where self - distillation is employed to relabel the noisy samples in the training of the noisy corpora .
Conclusion
This paper presents a novel norm - based noisy corpora filtering and refurbishing method . We propose to use the information ratio from the source to the target side to distinguish unparallel sentence pairs . The amounts of these two information flows are calculated by norms of context vectors of each side . Unlike parallel sentence pairs , the excessive targetside information is needed for the model to fit unparallel ones , which present relatively lower scores . We incorporate the noisy corpora filtering into the training of NMT without any extra clean data or costly pre - trained scorers . Extensive experiments show that our method performs comparably with SOTA results with significant advantages in time and computational costs . We further refurbish the discovered noisy data by producing pseudo labels via online knowledge distillation , which obtains further performance gains .
Limitations
Our methods have a specific scope of applications due to the methodology design . As highlighted in Section 3 , the basis of our approach is the difference in how the model processes unparallel and parallel sentence pairs . Thus , it can not work well when a large extent of noise makes the NMT model hard to converge . We take Nepali - English in WMT 2019 shared task on parallel corpus filtering and alignment 5 as an example . By sampling inspection , we find that 89.4 % of the dataset is wholly misaligned after pre - processing . It is unstable to directly train an NMT model with the whole dataset , which results in our worse performance than the best showing ( 2.2 BLEU vs. 3.2 BLEU ) . In this case , extra clean data or resources of similar languages are necessary to build a competent scorer .
A Norm - based Word Importance Measurement
We present the relation between the norm of c L−1 i and token frequency in Figure 10 . We can see that the norm of c L−1 i decreases with a high word frequency . Moreover , the distribution of blue dots ( function words ) is lower than that of green dots ( content words ) . It indicates that words with more diverse semantics receive higher norms . The observations in Figure 3 and 10 show that we can infer how much information is encoded in word representations from the perspective of norms .
H H H 7RNHQ ) UHTXHQF\ versus token frequency of English words in the LDC Chinese - to - English vocabulary labeled content words ( green dots ) and function words ( blue dots ) . This produces a downward trendline .
B Rule - based Pre - filtering
Following Lu et al . ( 2020 ) , we apply a series of heuristic rules to filter the low - quality sentence pairs , which includes :
• The length of the sentence . The too short ( ≤ 2 words ) or too long ( > 50 words ) sentences will be removed .
• The length ratio of the source sentence to the target sentence . The ratio is set between 0.2 to 5 for all language pairs .
• The proportion of valid tokens . A valid token should include the letters in the corresponding language . The sentence is dropped if the validtoken ratio is less than 0.2 .
• Language filtering . We detect the language of a sentence by using a language detection tool fasttext . It helps remove the sentence pairs if either the source or the target sentence does not belong to the required language .
• URLs or numbers . We remove the sentence which contains URLs or more than 25 % numerical tokens .
The size of data after ruled - based pre - filtering is given in Table 4 . We find that WMT20 datasets are very noisy , where around 72.66 % of Km - En and 51.96 % of Ps - En are filtered out . The noise rate in He - Zh is relatively lower . As shown in Table 1 , pre - filtering is necessary to relieve stress for the following filtering method .
C Experimental Settings
For WMT20 datasets , we strictly follow the model configuration and evaluation settings provided by the WMT20 organizers ( Koehn et al . , 2020 ) . It includes five stacked encoder layers and five stacked decoder layers . During training , we use Adam optimizer with β 1 = 0.9 , β 2 = 0.998 , an inverse sqrt learning rate of 4,000 warm - up steps , and dropout is 0.4 . All experiments last for 100 epochs with a single GPU , where the batch size is 4000 tokens . We accumulate the gradient of parameters and update every 4 steps . Scores on test sets are reported by case - insensitive Sacrebleu ( Post , 2018 ) .
For our in - house datasets , we experiment with Transformer Base ( Vaswani et al . , 2017 ) . During training , we use label smoothing of value ϵ ls = 0.1 and employ the Adam ( β 1 = 0.9 , β 2 = 0.998 ) for parameter optimization with a scheduled learning rate of 4,000 warm - up steps . The training lasts 150k for He⇒Zh and Id⇒Zh , 100k for the other three . We average the last ten checkpoints and use beam search ( beam size 5 , length penalty 1.2 ) for inference . We measure case - insensitive BLEU calculated by multi-bleu.perl .
Our method introduces two hyper - parameters . The first one is the warm - up step T to ensure the stability of our metric , and the second is the threshold k to determine the amount of the filtered data . For the former , we observe that our proposed metric reaches a stable range after several thousand steps and thus set T = 3k . As aforesaid in Section 4.2 , we determine the threshold k based on the percentage of the remaining data if given the required size of selected subsets . We take Ps - En as an example . The dataset after rule - based filtering contains 7.78 M English words . Thus , we should remove 10 % and 35 % of the data to obtain the 5 M and 7 M words settings . We determine the threshold k as the lower decile in the R ( x , y ) of 200 annotated samples for 10 % removal , similar to 35 % .
Acknowledgements
This work is supported by the Natural Science Foundation of China under Grant 62122088 and U1836221 .
Introduction
Results
Models
Results and Discussion
Related Work
A.7 Annotations
Annotations
Acknowledgements
From the One , Judge of the Whole : Typed Entailment Graph Construction with Predicate Generation
Entailment Graphs ( EGs ) have been constructed based on extracted corpora as a strong and explainable form to indicate contextindependent entailment relations in natural languages . However , EGs built by previous methods often suffer from the severe sparsity issues , due to limited corpora available and the longtail phenomenon of predicate distributions . In this paper , we propose a multi - stage method , Typed Predicate - Entailment Graph Generator ( TP - EGG ) , to tackle this problem . Given several seed predicates , TP - EGG builds the graphs by generating new predicates and detecting entailment relations among them . The generative nature of TP - EGG helps us leverage the recent advances from large pretrained language models ( PLMs ) , while avoiding the reliance on carefully prepared corpora . Experiments on benchmark datasets show that TP - EGG can generate high - quality and scale - controllable entailment graphs , achieving significant in - domain improvement over state - of - the - art EGs and boosting the performance of down - stream inference tasks 1 .
Introduction
The entailment relation between textual predicates plays a critical role in natural language inference and natural language understanding tasks , including question answering ( Pathak et al . , 2021 ; McKenna et al . , 2021 ) and knowledge graph completion ( Yoshikawa et al . , 2019 ; Hosseini et al . , 2019 . To detect entailment relations , previous works pay attention to the Recognizing Textual Entailment ( RTE ) task , which takes a pair of sentences as input and predicts whether one sentence entails the other ( Bowman et al . , 2015 ; He et al . , 2021b ; Pilault et al . , 2020 ) . Current RTE models perform well on RTE benchmarks , but most of them are lacking in explainability , as they make use of the black - box Language Models ( LM ) without providing any explainable clues .
Recent works focus on learning the Entailment Graph ( EG ) structure , which organizes typed predicates in directional graphs with entailment relations as the edges ( Hosseini et al . , 2018 ( Hosseini et al . , , 2019McKenna et al . , 2021 ) , as shown in Figure 1 . With the explicit graph structure containing predicates and their entailment relations , similar to Knowledge Graphs ( KGs ) , using EGs becomes an explainable and context - independent way to represent the knowledge required in natural language inference and other NLP tasks .
Most existing EGs are constructed with the Distributional Inclusion Hypothesis ( DIH ) , which suggests that all typical context features of a predicate v can also occur with another predicate w if v entails w ( Geffet and Dagan , 2005 ) . Constructing EGs with DIH requires distributional cooccurrences of contextual features from large corpora to calculate the semantic similarity between predicates ( Szpektor and Dagan , 2008 ; Schoenmackers et al . , 2010 ) . However , the EGs constructed from large corpora often suffer from two different kinds of sparsity issues : the predicate sparsity and the edge sparsity . Existing corpora used for EG construction are mainly collected from
generative LMs
Figure 2 : An illustration of our TP - EGG . Given three seed predicates , TP - EGG generates a graph with 8 predicates and 15 entailment relations . The circles represents different predicates , while the rounded rectangles is sentences in natural language . Seed predicates is in green , and newly generated predicates is in blue . specific resources ( Zhang and Weld , 2013 ) , such as news articles . As a result , entailment relations could not be learned between those predicates that do not appear in the corpora , which leads to the predicate sparsity issue . Meanwhile , if two predicates scarcely appear around similar contexts in the given corpora , the DIH could not indicate the potential entailment relationship between them . It leads to the edge sparsity of EGs as the corresponding edges may be missing due the limited coverage of the corpora .
To tackle the sparsity issues , previous works pay attention to learning global graph structures to mine latent entailment relations and alleviate the edge sparsity ( Berant et al . , 2011 ( Berant et al . , , 2015Hosseini et al . , 2018 ; Chen et al . , 2022 ) , but predicate sparsity is still holding back the improvement of EGs . Solving predicate sparsity by simply scaling up the distributional feature extraction is impracticable , due to the long - tail phenomenon of predicate distribution ( McKenna and Steedman , 2022 ) .
The shortcomings of extractive methods come in quest for non - extraction way to overcome . Recent progress in deep generative LMs , including GPT-3 ( Brown et al . , 2020 ) and T5 ( Raffel et al . , 2022 ) , makes it possible to produce predicates and entailment relations by generative methods . Inspired by the Commonsense Transformer ( Bosselut et al . , 2019 ) , we propose a novel generative multi - stage EG construction method , called Typed Predicate - Entailment Graph Generator ( TP - EGG ) . As shown in Figure 2 , TP - EGG takes several seed predicates as input of the LM - based predicate generator to depict the domain of predicates and generate more in - domain predicates . With generated predicates , TP - EGG uses a novel transitivity - ensured edge se - lector by representing predicates as spheres in the vector space , to pick out the potential entailment relations among generated predicates . Then TP - EGG calculates the corresponding edge weights by the LM - based edge calculator . Our key insight is that by re - modeling the predicate extraction process as a generation process , we can leverage the underlying knowledge about natural language inference inside the LMs to avoid the data sparsity issues of extractive methods . By choosing appropriate seed predicates and setting the parameters of TP - EGG , one can generate EGs containing knowledge from a specific domain in arbitrary scales to fit the downstream requirement , without limitations from the uncontrollable distribution in domain - independent corpora . Since almost all the EG construction modules in TP - EGG is controlled by pre - trained LMs , the output EGs can be seen as explicit representations of the knowledge in LMs and used in downstream tasks , such as RTE in our experiments .
In a word , our contributions can be summarized as follows : ( 1
Related Work
Previous EG construction methods construct feature representations for typed predicates , weighted by counts or Pointwise Mutual Information ( Berant et al . , 2015 ) , and compute the distribution similarity guided by DIH . For a predicate pair , different similarities are calculated , such as cosine similarity , Lin ( Lin , 1998 ) , Weed ( Weeds and Weir , 2003 ) , and Balanced Inclusion ( Szpektor and Dagan , 2008 ) . Markov chain of predicate - argument transition ( Hosseini et al . , 2019 ) and temporal information from extracted corpora ( Guillou et al . , 2020 ) are also used in EGs construction . These methods independently calculate the entailment relations for each pair , called local methods . Besides , global constraints are used to detect new entailment relations beyond local relations . The transitivity in EGs , which means a entails b and b entails c indicate a entails c for three predicates a , b and c , is the most widely used in previous works as hard constraints ( Berant et al . , 2011 ( Berant et al . , , 2015 or soft loss functions ( Hosseini et al . , 2018 ; Chen et al . , 2022 ) . The weight similarity constraints between different typed EGs and similar predicates are also taken into consideration ( Hosseini et al . , 2018 ) .
As one of the most important areas of NLP , text generation , or Natural Language Generation ( NLG ) , has also been advanced by the surgent development of pre - trained LMs . BART ( Lewis et al . , 2020 ) uses encoder - decoder transformer architecture to re - correct the corrupted data in pre - training phase ; GPT-3 ( Brown et al . , 2020 ) uses transformer decoder to achieve in - context learning with massive multi - task unsupervised data . T5 ( Raffel et al . , 2022 ) unifies different tasks into natural language prefixes and solves them by text generation .
Pre - trained LMs are also applied in recent EG methods . CNCE initializes the contextualized embeddings of entity - relation triplets by BERT ( Devlin et al . , 2019 ) and uses random walk to get the entailment probability ; EGT2 ( Chen et al . , 2022 ) fine - tunes a patternadapted LM on the predicate sentences and recalculates high - quality edge weights for global constraints ; McKenna and Steedman ( 2022 ) applies RoBERTa ( Liu et al . , 2019 ) as predicate encoder and matches missing predicates in EGs with K - Nearest Neighbor algorithm to alleviate the predicate sparsity . As far as we are concerned , our method is the first attempt to use generative LM in EG construction and directly generate EGs without the distributional features from large corpora .
Our Approach
EGs store predicates as nodes and entailment relations between them as edges in graph structures . Following previous EG methods ( Hosseini et al . , 2018 ( Hosseini et al . , , 2019Chen et al . , 2022 ) , we use the neo - Davisonian semantic form of binary relation ( Parsons , 1990 ) to indicate typed predicates , whose types are defined by the combination of argument types . Predicate p connecting two arguments a 1 , a 2 with types t 1 , t 2 can be represented as p = ( w 1 .i 1 , w 2 .i 2 , t 1 , t 2 ) , where w j is the center relation tokens ( and perhaps prepositions ) about a j , and i j is corresponding argument order of a j in w j . For example , the event " The government is elected in 1910 and adored by natives " contains two predicates ( elect.2 , elect.in.2 , government , time ) and ( adore.1 , adore.2 , person , government ) . We denote P as the collection of all typed predicates , T as the collection of all argument types , and τ 1 , τ 2 : P → T as type indicator functions , where τ 1 ( p ) = t 1 and τ 2 ( p ) = t 2 for any predicate p = ( w 1 .i 1 , w 2 .i 2 , t 1 , t 2 ) .
We formally define that a typed entailment graph G ( t 1 , t 2 ) = < P ( t 1 , t 2 ) , E ( t 1 , t 2 ) > includes the collection of typed predicates P ( t 1 , t 2 ) = { p| ( τ 1 ( p ) , τ 2 ( p ) ) ∈ { ( t 1 , t 2 ) , ( t 2 , t 1 ) } } , and the directional weighted edge set E ( t 1 , t 2 ) , which can be represented as an adjacent matrix W ( t 1 , t 2 ) ∈ [ 0 , 1 ] |P ( t 1 , t 2 ) |×|P ( t 1 , t 2 ) | . For those G ( t 1 , t 2 ) whose t 1 ̸ = t 2 , the order of types t1 , t2 is naturally determined . When t 1 = t 2 = t , argument types are ordered such that G ( t , t ) can determine the order of types like " Thing A " and " Thing B " to distinguish predicates like " Thing A eat Thing B " and " Thing B eat Thing A " . This order obviously affect the meaning of predicates , as " Thing A eats Thing B " entails " Thing B is eaten by Thing A " , but " Thing eats Thing " is doubtful to entail " Thing is eaten by Thing " .
Predicate Generation
In order to avoid the predicate sparsity issue in a given corpus , TP - EGG uses a predicate generator G to generate novel in - domain predicates . G takes a set of seed predicates P seed ⊂ P ( t 1 , t 2 ) as input and outputs a set of generated predicates P G , where P seed are expected to contain the domain knowledge of required EGs and P G should be semantically related to P seed in varying degrees .
Our G is designed to be based on generative LMs , thus the input predicates p ∈ P seed should be converted into natural language forms to fit in the LMs . We use Chen et al . ( 2022 ) 's sentence generator S to convert predicate p into its corresponding sentence S ( p ) . For example , p = ( elect.2 , elect.in.2 , government , time ) will be converted into Government A is elected in Time B. With converted sentences , generator G uses a generative LM , T5 - large ( Raffel et al . , 2022 ) in our experiments , to generate new sentences and then re - converts them into generated predicates by a sentence - predicate mapping function S −1 ( details in Appendix C ) . Starting from the seed sentences S 0 = { S ( p ) |p ∈ P seed } , the generative LM outputs sentences S 1 for the next step , and S 1 is used to generate S 2 and so on , while S −1 is used to re - convert S i to P i = S −1 ( S i ) for every step . The generation process continues until the union of seed predicates and generated predicates
P ′ i = P seed ∪ P 1 ... ∪ P i is equal to P ′ i−1 or its size |P ′ i | exceeds a pre - defined scale parameter K p .
To use T5 - large as the generation component , we need to design an input template to generate new sentences . For sentence s ∈ S i , the input template will be constructed like :
s , which entails that t 1 A < extra_id_0 > t 2 B. s , which entails that t 2 B < extra_id_0 > t 1 A. where < extra_id_0 > is the special token representing the generating location of the T5 - large output . The max length of stripped output sequence s ′ is limited to 5 , and the new predicate p ′ is produced by S −1 ( " t 1 A s ′ t 2 B. " ) or S −1 ( " t 2 B s ′ t 1 A. " ) correspondingly . For each s , T5 - large uses beam - search algorithm with beam size K beam to find top - K sent output sequences s ′ with highest probabilities .
To ensure the quality of generated predicates and filter noisy ones , only those predicates which are generated by T5 - large from at least two different predicates in P ′ i−1 could be included in P i . Algorithm 1 depicts how predicate generator G works ( more details and exmaples in Appendix D ) .
Edge Selection
After generating new predicates P ( t 1 , t 2 ) = P G , TP - EGG constructs G ( t 1 , t 2 ) by generating weighted edge set E ( t 1 , t 2 ) . As TP - EGG does not use large corpora to calculate distributional features regarding context coherence , we need to determine which predicate pairs could be potential entailment relations for later calculation . Regarding ALL pairs as candidates is a simple solution , but when P ( t 1 , t 2 ) scales up , calculating all |P | 2 pairs Algorithm 1 The predicate generator G .
Require : A set of seed predicates P seed , sentence generator S , parameter K beam , Ksent , Kp Ensure : A set of generated predicates PG 1 :
Ponce = { } 2 : i = 0 , P0 = P ′ 0 = P seed 3 : while |P ′ i | ≤ Kp do 4 : Si = { S ( p ) |p ∈ Pi } 5 : Pi+1 = { } 6 :
for s ∈ Si do 7 :
S g = T 5 ( s , K beam , Ksent ) 8 :
P g = Set ( S −1 ( s g ) |s g ∈ S g ) 9 :
P g = P g − P ′ i 10 :
Pi+1.update ( P g ∩ Ponce ) 11 :
Ponce = Ponce XOR Pg 12 :
end for 13 :
Pi+1 = Pi+1 − P ′ i 14 :
P ′ i+1 = P ′ i ∪ Pi+1 15 : if P ′ i+1 = P ′ i then 16 : return PG = P ′ i 17 :
end if 18 :
i = i + 1 19 : end while 20 : return PG = P ′ i will be unacceptably expensive as we intend to adopt an LM - based edge weight calculator , which only takes one pair as input at a time . Therefore , we require an effective edge selector M to select potential pairs E ′ ⊂ P ( t 1 , t 2 ) ×P ( t 1 , t 2 ) with acceptable computational overhead , where |E ′ | should be equal to a given parameter K edge .
Calculating embeddings for each predicate and quickly getting similarities between all pairs in P ( t 1 , t 2 ) perform worse than pair - wise LMs with cross attention in general , but are good enough as the edge selector to maintain high - quality pairs in high ranking . Inspired by Ristoski et al . ( 2017 ) , we represent predicate p as a sphere in the vector space . TP - EGG uses BERT - base ( Devlin et al . , 2019 ) to calculate embedding vector v p for every predicate p based on S ( p ) , and represents p as a sphere ⊙ p in a vector space with center c p and radius r p :
v p = BERT ( S ( p ) ) ∈ R dv , c p = f c ( v p ) ∈ R dc , r p = f + ( f r ( v p ) ) ∈ R + . ( 1 )
where f c , f r are two - layer trainable neural networks , d v , d r are corresponding vector dimensions , f + ( x ) ∈ { exp ( x ) , x 2 } ensures the positive radius . By representing p as a sphere , we expect that when p entails q , ⊙ q should enclose ⊙ p , as all points in ⊙ p are also included in ⊙ q . Under such assumption , the transitivity referred in Section 2 is naturally satisfied as ⊙ a ⊂ ⊙ b ⊂ ⊙ c . The overlapping ratio between spheres can be seen as the entail - ment probability P r ( p → q ) , and we simplify the calculation of sphere overlapping to diameter overlapping along the straight line between two centers :
d pq = ||c p − c q || 2 P r ( p → q ) =      0 , r q ≤ d pq − r p 1 , r q ≥ d pq + r p rp+rq−dpq 2rp
, otherwise
( 2 ) Chen et al . ( 2022 ) defines soft transitivity as P r ( a → b ) P r ( b → c ) ≤ P r ( a → c ) for all predicate pairs above a threshold . Similar in spirit , our simplified sphere - based probability holds transitivity in part :
Theorem 1 Given a threshold ϵ ∈ ( 0 , 1 ) , ∀a , b , c where P r ( a → b ) > ϵ and P r ( b → c ) > ϵ , we have
P r ( a → c ) > ϵ − ( 1 − ϵ ) r b
ra . We give its proof in Appendix A. Noted that while ϵ is close to 1 , the right part ϵ − ( 1 − ϵ ) r b ra will be nearly equal to ϵ. As we use this probability in edge selection , higher P r ( a → b ) and P r ( b → c ) will naturally ensure the appearance of ( a , c ) in final entailment relations , without the disturbance from low - confident edges . As P r ( p → q ) is constant when r q ≤ d pq − r p or r q ≥ d pq + r p , its gradient becomes zero which makes it untrainable . Therefore , we smooth it with order - preserving Sigmoid function and interpolation , and finally get the selected edge set for G ( t 1 , t 2 ) :
M ( p , q ) = σ ( 2r q − 2d pq r p ) , E ( t 1 , t 2 ) = { topK edge ( M ( p , q ) ) |p , q ∈ V ( t 1 , t 2 ) } ( 3 )
where σ is Sigmoid function σ ( x ) = 1 / ( 1 + e x ) . A geometrical illustration presenting how the selector M works can be found in Appendix B .
Edge Weight Calculation
With the selected edge set E ( t 1 , t 2 ) ⊂ P ( t 1 , t 2 ) × P ( t 1 , t 2 ) , TP - EGG calculates the edge weight W p , q for each predicate pairs ( p , q ) individually in the adjacent matrix W ( t 1 , t 2 ) . Inspired by Chen et al . ( 2022 ) , as the distributional features of generated predicates are unavailable for TP - EGG , we re - implement their local entailment calculator W to obtain the entailment edge weight W p , q . W is based on DeBERTa ( He et al . , 2020 ( He et al . , , 2021a ) and fine - tuned to adapt to the sentence patterns generated by S. The entailment - oriented LM will produce three scores , corresponding to entailment ( E ) , neutral ( N ) and contradiction ( C ) respectively , for each sentence pair . The score of entailment class is used as the entailment edge weight in our EGs :
W p , q = W ( p , q ) = exp ( LM ( E|p , q ) ) r∈ { E , N , C } exp ( LM ( r|p , q ) ) ( 4 )
where LM ( r|p , q ) is the score of class r. After calculating all predicate pairs ( p , q ) ∈ E ( t 1 , t 2 ) by the LM - based calculator W , TP - EGG completes the adjacent matrix W ( t 1 , t 2 ) , and consequently constructs G ( t 1 , t 2 ) , as shown in Figure 2 .
Experimental Setup
Datasets . Following previous works ( Hosseini et al . , 2018 ( Hosseini et al . , , 2019Chen et al . , 2022 ) 1 . More details can be found in Appendix F .
Metrics . Following previous works , we evaluate TP - EGG on the test datasets by calculating the area under the curves ( AUC ) of Precision - Recall Curve ( PRC ) for precision > 0.5 and traditional ROC curve . 2 The evaluated EGs are used to match the predicate pairs in datasets and return the entailment scores . Noted that our generated predicates might be semantically same with required ones but have different forms , like ( use.2 , use.in.2 , thing , event ) and ( be.1 , be.used.in.2 , thing , event ) are both reasonable for " Thing A is used in Event B " while our S −1 generates the first one . Hence we relax the predicate matching standard in evaluation from exactly matching to sentence matching , i.e. , S ( p ) = S ( p ′ ) rather than p = p ′ . This modification has nearly no effect on previous extraction - based EGs , but can better evaluate generative methods .. Implementation Details . In experiments , TP - EGG uses BERT - base in M and T5 - large in G implemented by the Hugging Face transformer library ( Wolf et al . , 2020 ) 3 , and DeBERTa reimplementation from Chen et al . ( 2022 ) to finetune on MNLI and adapt to sentence pattern in W. Taking both EG performance and computational overhead into account , we set K p = 5 × 10 3 , K edge = 2 × 10 7 , K beam = 50 , K sent = 50 , d r = 16 , d v = 768 . Discussion about K p and K edge can be found in Appendix E .
For EG generation , TP - EGG uses the predicates in validation set of Levy / Holt - r and SherLIiC Dataset respectively as the seed predicate P seed . With different P seed , we also only use corresponding validation set as the training data for all later modules to keep the EGs in - domain , called TP - EGG L / H−r and TP - EGG SherLIiC respectively .
Only positive pairs are used to generate the training inputs and outputs to fine - tune T5 - large in the predicate generator G with learning rate α G = 10 −3 . We use f + ( x ) = exp ( x ) for TP - EGG L / H−r and f + ( x ) = x 2 for TP - EGG SherLIiC . The edge selector M is also trained by the validation predicate pairs , but the positive examples are repeat 5 times ( for Levy / Holt - r ) or 2 times ( for SherLIiC ) to alleviate the label imbalance in training . BERT - base parameters are trained with learning rate α M,1 = 10 −5 , while other parameters , including f c and f r , are trained with learning rate α M,2 = 5 × 10 −4 . The edge weight calculator W is trained by the same method in Chen et al . ( 2022 ) .
All modules are trained by AdamW optimizer ( Loshchilov and Hutter , 2018 ) with cross entropy loss function , and controlled by early - stop mechanism , which stops the training when performances ( loss for G and F 1 for others ) on validation set do not reach the highest in the last 10 epoches . It takes about 5 - 6 hours to train all modules in TP - EGG , and about 2 - 3 hours to generate a typed EG on GeForce RTX 3090 . The three modules , G , M and W , contain 738 M , 109 M and 139 M parameters respectively .
To be comparable with previous works ( Hosseini et al . , 2018 ) , we apply their lemma - based heuristic on all datasets except SherLIiC , and their average backup strategy on all datasets .
Compared Methods We compare TP - EGG with the best local distributional feature , Balanced Inclusion or called BInc ( Szpektor and Dagan , 2008 ) , and existing state - of - the - art local and global EG construction methods , including Hosseini et al . ( 2018Hosseini et al . ( , 2019 , CNCE and EGT2 ( Chen et al . , 2022 ) . Downstream Task . Despite of evaluating on EG construction benchmarks , we adapt an LM - based three - way RTE framework into the EG evaluation testbed . For premise pm and hypothesis h , RTE models take their concatenation [ pm ; h ] as inputs , and return three probability scores of three classes . In order to incorporate the knowledge in EGs into RTE models , we design the following architecture available to any LM - based RTE model : given pm and h , we extract binary predicates from them , and try to match the predicates in our EGs . Each matched predicates a in premise pm will be replaced by its K nbr neighbors b with highest weight W ab . For h , the neighbors b are with highest weight W ba . Replaced sentences pm 1 , ... , pm j and h 1 , ... , h k for pm and h will be concatenated to represent the information from EGs in calculation :
( s E1 , s N1 , s C1 ) = Softmax ( LM 1 ( [ pm ; h ] ) ) ,
( s E2 , s N2 , s C2 ) = Softmax ( LM 2 ( [ pm ; pm 1 ; ... ; pm j ; h ; h 1 ; ... ; h k ] ) ) ,
s i = ( s i1 + s i2 ) / 2 , i ∈ { E , N , C } . ( 5 )
where LM 1 and LM 2 represent two different LMs followed by a linear layer respectively . As the additional calculation unfairly requires more parameters , we also consider the models with equal parameters but do not use the EGs , referred as NO - EG setting , by inputting [ pm ; h ] into LM 2 directly . We use SNLI ( Bowman et al . , 2015 ) and SciTail ( Khot et al . , 2018 ) as our RTE benchmark datasets . We use BERT - base and DeBERTa - base as the backbone , learning rate α RT E = 10 −5 , K nbr = 5 for SNLI and K nbr = 3 for SciTail .
Results and Analysis
Main Results
The performance of different EGs on benchmark datasets are shown in using extracted features from large corpora , TP - EGG achieves significant improvement or at least reaches comparable performance with baselines for in - domain evaluations ( L / H and L / H - r for TP - EGG L / H−r and SherLIiC for TP - EGG SherLIiC ) . Interestingly , TP - EGG always performs better on the AUC of PRC , which indicates the strong ability of our generative methods to maintain impressive recall with high precision as shown in the curves . On Levy / Holt - r , TP - EGG L / H−r significantly outperforms all other extraction - based methods on pre - cision > 0.5 , showing that with higher classification threshold , extraction - based methods fail to detect the entailment relations between rare predicates due to the sparsity issues , while generation - based TP - EGG successfully finds these relations by generating more predicates and correctly assigns high probabilities between them . Noted that our TP - EGG is a local method , although certain global properties are ensured by our edge selector M. We try to apply a state - of - the - art global method , EGT2 - L 1 ( Chen et al . , 2022 ) on our local EGs 4 . As shown in the bottom of Table 2 , the global method further improves the performance of TP - EGG , demonstrating the potential of our local EGs to continuously reducing the data sparsity with global EG learning methods .
Although we have observed the significant improvement of evaluation metrics by TP - EGG , it is not clear enough to determine TP - EGG can alleviate the predicate sparsity to what extent . Therefore , we count the predicate pairs in Levy / Holt testset that exactly appeared as edges in EGs . We find that 6,873 pairs appear in TP - EGG L / H−r , meanwhile 875 in EGT2 - L 3 . The far more appearance of in - domain predicates indicates the alleviation of predicate sparsity . Previous works have claimed that LMs for entailments might be strong in undirectional paraphrasing , but weak in directional entailment recognizing ( Cabezudo et al . , 2020 ; Chen et al . , 2022 True , and therefore symmetric models will have AUC < 0.5 . TP - EGG performs better than baselines on the directional portion , and the AUC far higher than 0.5 indicates its directional entailment ability . Global models perform better here , which is reasonable as global constraints are strongly related to the directional reasoning .
Learning with Multiple Domains
Although TP - EGG performs well on in - domain evaluation , the out - domain scenario is still hard , as the knowledge required for out - domain evaluation is inaccessible in all training and generation steps of TP - EGG . Next , we study the effect of using merged validation sets of Levy / Holt - r and SherLIiC Dataset at different modules . The performance of TP - EGG trained with the merged data , referred as L+S , are shown in Table 5 . While using merged data as P seed and also as training data for other modules ( ❶ ) , TP - EGG reaches impressive performances on both datasets , which is not surprising , as both datasets are in - domain in this situation .
Using merged dataset to train G , M and W boosts out - domain performance with in - domain performance loss ( comparing ❷ and ❹ , ❺ and ❼ ) . However , adding some out - domain predicates into P seed is surprisingly beneficial to the in - domain evaluation while improving out - domain generalization ( comparing ❷ and ❸ , ❺ and ❻ ) . We attribute it to the diversity of generated predicates led by the newly incorporated seed predicates , which might not be generated with the in - domain seed predicates . The out - domain predicates help TP - EGG to find new predicates related to in - domain predicates as Algorithm 1 might tend to generate predicates from at least two predicates across two domains . Therefore , the predicate coverage over evaluation datasets can be increased .
Results on RTE
In downstream task evaluation , we use EGs generated by different methods to enhance LM - based RTE models , and report the results in Table 6 . Compared with CNCE and EGT2 , our TP - EGG achieves better performance on two RTE datasets with both BERT base and DeBERTa base backbones . The performances of TP - EGG on DeBERTa base are significantly better than NO - EG ( p < 0.05 ) . Noted that TP - EGG offers pm j , h k for 4,600 sentences in SNLI testset , which is 5,596 for EGT2 - L 3 . Even with lower coverage over predicates in the dataset , TP - EGG supports RTE models with more highquality entailment relations to generate pm j , h k and improve the performance . On the other hand , the noisy entailment relations in CNCE and EGT2 perhaps misguide RTE models , thus lead to even worse results than NO - EG in some cases .
Ablation Study
We run the ablation experiments which directly use the original version of LMs in G , M and W without fine - tuning on EG benchmark datasets . For M , as non - LM parameters are involved , we replace it with randomly selecting K edge edges . As shown in Table 7 , without fine - tuning G or W , the performance on Levy / Holt - r suffers a significant drop ( about 0.1 ) , indicating the importance of fine - tuned modules for EG generation . The performance on SherLIiC also decreases severely without fine - tuning G , as the fine - tuning step can improve the quality of generated predicates and cover
Limitations
First , as we do not rely on specific corpora and avoid the shortcomings of extractive methods , we also lose their advantages . The typed EGs generated by our TP - EGG is strongly related to the seed predicates and training data of generation modules , while extractive EGs can generate domainindependent EGs from large corpora and do not require supervised training data to a considerable degree . Second , the edge calculator W is time - consuming even we can control the scales of output EGs , as the edge num |E ( t 1 , t 2 ) | will be relatively large for TP - EGG to generate powerful EGs . Furthermore , how to effectively select seed predicates still remains a difficult problem which has not been discussed thoroughly in this work by using the validation datasets . We assume that this problem could be solved by carefully confirming how the seed predicates represent corresponding domain knowledge and we leave it to future work .
Ethics Statement
We re - annotate the Levy / Holt Dataset which is a publicly available dataset for entailment graph evaluation . Annotators receive a competitive pay of about 100 yuan per hour under the agreement of the institute , which is more than 4 times the local minimum wage . The annotation complies with the ACL Code of Ethics . The sentences used in annotation are generated from the original dataset and we do not incorporate external content into the sentences . However , there may still be sentences containing potentially improper content , which do not reflect the views or stances of the authors . The re - annotation results are confirmed by the majority voting of annotators , and may still contain natural errors . Further usage of the re - annotated dataset should be aware of the limitation and the authors are not responsible for any issues in further usage of this dataset . A The Proof of Theorem 1
Theorem 1 Given a threshold ϵ ∈ ( 0 , 1 ) , ∀a , b , c where P r ( a → b ) > ϵ and P r ( b → c ) > ϵ , we have P r ( a → c ) > ϵ − ( 1 − ϵ ) r b ra .
As P r ( p → q ) = rp+rq−dpq 2rp
holds when d pq − r p < r q < d pq + r p , and P r ( p → q ) = 1 ≤ rp+rq−dpq 2rp holds when r q ≥ d pq + r p , we have :
r a + r b − d ab 2r a ≥ P r ( a → b ) > ϵ → d ab < r b + ( 1 − 2ϵ ) r a . ( 6 )
Similarly , for b , c :
d bc < r c + ( 1 − 2ϵ ) r b . ( 7 )
For the case P r ( a → c ) = 1 , obviously the theorem holds for ϵ ∈ ( 0 , 1 ) ;
For the case P r ( a → c ) = 0 or P r ( a → c ) = rp+rq−dpq 2rp
, we have P r ( a → c ) ≥ rp+rq−dpq 2rp as r p + r q − d pq < 0 under P r ( a → c ) = 0 , and therefore :
P r ( a → b ) ≥ r a + r c − d ac 2r a ≥ r a + r c − ( d ab + d bc ) 2r a ( d ac ≤ d ab + d bc ) > r a + r c − ( r b + ( 1 − 2ϵ ) r a + r c + ( 1 − 2ϵ ) r b ) 2r a = ϵr a + ( ϵ − 1 ) r b r a = ϵ + ( ϵ − 1 ) r b r a . ( 8 ) Q.E.D .
B Geometrical Illustration of Edge
Selector M
To understand how the edge selector M works more intuitively , we pick four predicate sentences from Levy / Holt Dataset and visualize their corresponding spheres ⊙ p in Figure 4 : p 0 : Living Thing A is imported from Location B. p 1 : Living Thing A is native to Location B. p 2 : Living Thing A is found in Location B. p 3 : Living Thing A is concentrated in Location B .
The centers c p and radius r p are generated by M from our final TP - EGG model , while the dimension of c p are reduced to maintain the distances between them . Three entailment relations , p 0 → p 1 , p 1 → p 2 and p 3 → p 2 , are annotated in the dataset , and p 0 → p 3 is also plausible . In Figure 4 , the hypothesis spheres obviously enclose premise spheres , and the more generic a predicate is , the bigger its sphere becomes , which is consistent with our expectation about M. With high directional overlapping , all of the four entailment relations will correctly appear in later weight calculation while low - confident inverse edges will be filtered out .
C The Sentence - Predicate Mapping Function S −1
The sentence - predicate mapping function S −1 used in predicate generation is described in Algorithm 2 . Noted that S −1 is a simplified approximation of the reverse function of sentence generator S while different predicates might generate the same sentence by S. Therefore , S −1 does not cover all possible predicates and sentences . appear in final predicate set P ′ 2 .
E Discussion about Graph Scales
As referred in Section 4 , we set the number of predicates K p = 5 × 10 3 and edges K edge = 2 × 10 7 , which determine the final scale of generated EGs . We report the performance of TP - EGG L / H−r on the evaluation datasets with different K p and K edge in Figure 5 . Changing K p from 1 × 10 3 to 2 × 10 4 , the overall performance is the best while K p = 5 × 10 3 . We assume that lower K p might limit the coverage of predicate set , while higher K p makes the EGs more sparse and miss potential entailment relations . Noted that the computational overhead and space occupation is almost proportional to K edge , setting K edge = + ∞ to regard ALL pairs as candidates is impractical ( the largest EG in TP - EGG L / H−r will contain 7 × 10 7 edges ) . We find that K edge = 2 × 10 7 is able to reach the overall performances comparable with K edge = + ∞ under our settings , while further decreasing K edge will significantly cut down the performances . To balance between the overall performance and computational overhead , we finally set K p = 5 × 10 3 and K edge = 2 × 10 7 .
F Details about Datasets
Levy and Dagan ( 2016 ) uses questions and candidate answers with textual predicates to collect the entailment relations , and proposes a widely used EG evaluation dataset which is later re - annotated by Holt ( 2018 ) , called Levy / Holt Dataset . For example , if the annotator figures out that " The government is adored by natives " can be used to answer " Who recognize the government ? " , the dataset will indicate that " adore " entails " recognize " between type person and government . Levy / Holt Dataset contains 18,407 predicate pairs ( 14,491 negative and 3,916 positive ) . We use the 30 % / 70 % splitting of validation / test set as Hosseini et al . ( 2018 ) in our experiments . However , because the QA annotation form incorporates additional information about entities related to the predicates , some consistent predicates pairs are annotated with different labels , and the transitivity is disobeyed between some predicate pairs . The inconsistent pairs are those ( a , b ) which ( a , b , T rue ) and ( a , b , F alse ) both appear in the dataset . The transitivity - disobeying pairs are those ( a , b ) , ( b , c ) and ( a , c ) which ( a , b , T rue ) , ( b , c , T rue ) and ( a , c , F alse ) all appear . We find that there are 89 inconsistent pairs and 159 transitivity - disobeying pairs in Levy / Holt Dataset , and re - annotate these 248 pairs by five annotators with Fleiss ' κ = 0.43 . After re - annotating , we get the final Levy / Holt - r Dataset with 14,490 negative and 3,777 positive pairs . Berant et al . ( 2011 ) proposes an annotated entailment relation dataset , containing 3,427 positive and 35,585 negative examples , called Berant Dataset . Schmitt and Schütze ( 2019 ) extracts verbal relations from ClueWeb09 ( Gabrilovich et al . , 2013 ) based on Freebase ( Bollacker et al . , 2008 ) entities , and splits the extracted relations into typed one based on their most frequent Freebase types , which is naturally compatible to typed EG settings . We use their manually - labeled 1,325 positive and 2,660 negative examples in our EG benchmark , called SherLIiC Dataset . The dataset is split into 25 % ( validation ) and 75 % ( test ) in our experiments . B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ? Section 3 and 4 B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Section 3 , 4 and " Ethics Statement " B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Section 4 B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . Section 4 C Did you run computational experiments ? Section 4 and 5 C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Section 4
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022 , with the addition of a question on AI writing assistance .
Acknowledgements
This work is supported in part by NSFC ( 62161160339 ) . We would like to thank the anonymous reviewers for their helpful comments and suggestions .
Algorithm 2
The mapping function S −1 .
Require : A generated sentence s. Ensure : A predicate p , or N U LL indicating that s is not a valid predicate sentence . 1 : Split the sentences into tokens l and strip t1 A , t2 B 2 : prefix= " " 3 : if |l| = 0 then 4 :
return NULL 5 : end if 6 : if not or n't in l [ 0 ] then 7 :
prefix = N EG / / representing the negation 8 : end if 9 : Remove the modal verbs in l 10 : if l begins with have been or has been then 11 : l = l [ 1 : ] 12 : end if 13 : if |l| > 1 and l [ : 2 ] is have+P.P. then 14 :
l = l [ 1 : ] 15 : end if 16 : if |l| > 2 and the present tense of l [ : 2 ] is have to then 17 : l = l [ 2 : ] 18 : end if 19 : if |l| = 0 then 20 :
return NULL 21 : end if 22 : i head = 0 , i tail = |l| − 1 23 : while i head ≤ i tail and l [ i head ] is not a verb do 24 :
i head = i head + 1 25 : end while 26 : while i head ≤ i tail and l [ i tail ] is not a verb or a preposition do 27 :
i tail = i tail − 1 28 : end while 29 : if i head > i tail then 30 :
return NULL 31 : end if 32 : l ′ = l [ i head : i tail + 1 ] / / cut the token between i head and i tail 33 : if l ′ [ 0 : 2 ] is a verb like be doing then 34 :
is an adjective or a noun , and l ′ [ −1 ] is a preposition then 46 :
D An Example of Generating Predicates from Seed Predicates
We show an example process of generating new predicates by the generator G of TP - EGG in Table 8 .
The predicates repeating in current generation or appearing in previous stages , and sentences that can not be resolved by S −1 are omitted .
Predicates generated from at least two different s are in red , and predicates appeared in generation of previous steps are in blue . According to Algorithm 1 , only seed predicates and colored predicates will
Continual Contrastive Finetuning Improves Low - Resource Relation Extraction
Relation extraction ( RE ) , which has relied on structurally annotated corpora for model training , has been particularly challenging in lowresource scenarios and domains . Recent literature has tackled low - resource RE by selfsupervised learning , where the solution involves pretraining the entity pair embedding by RE - based objective and finetuning on labeled data by classification - based objective . However , a critical challenge to this approach is the gap in objectives , which prevents the RE model from fully utilizing the knowledge in pretrained representations . In this paper , we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning . Since in this kind of representation learning paradigm , one relation may easily form multiple clusters in the representation space , we further propose a multi - center contrastive loss that allows one relation to form multiple clusters to better align with pretraining . Experiments on two document - level RE datasets , BioRED and Re - DocRED , demonstrate the effectiveness of our method . Particularly , when using 1 % end - task training data , our method outperforms PLMbased RE classifier by 10.5 % and 6.1 % on the two datasets , respectively .
Introduction
Relation extraction ( RE ) is a fundamental task in NLP . It aims to identify the relations among entities in a given text from a predefined set of relations . While much effort has been devoted to RE in supervised settings ( Zhang et al . , 2017 ( Zhang et al . , , 2018Nan et al . , 2020 ) , RE is extremely challenging in high - stakes domains such as biology and medicine , where annotated data are comparatively scarce due to overly high annotation costs . Therefore , there is a practical and urgent need for developing low - resource RE models without the reliance on large - scale end - task annotations .
To realize low - resource RE , previous work has focused on pretraining entity pair embedding on large corpora using RE - based pretraining objectives . Particularly , Baldini Soares et al . ( 2019 ) propose a self - supervised matching - theblanks ( MTB ) objective that encourages embeddings of the same entity pairs in different sentences to be similar . Later work ( Peng et al . , 2020 ; Qin et al . , 2021 ) extends this idea with distant supervision ( Mintz et al . , 2009 ) and improves representation learning using contrastive learning ( Hadsell et al . , 2006 ; Oord et al . , 2018 ; . To adapt to training on RE annotations , these works finetune pretrained entity pair embedding on labeled data using classification - based objectives . Although this paradigm produces better results compared to RE models initialized with pretrained language models ( PLMs ) , it creates a significant divergence between pretraining and finetuning objectives , thus preventing the model from fully exploiting knowledge in pretraining .
In this paper , we aim to bridge this gap in RE pretraining and finetuning . Our key idea is to use similar objectives in pretraining and finetuning . First , we propose to continually finetune pretrained embedding by contrastive learning , which encourages the entity pair embeddings corresponding to the same relation to be similar . However , as pretraining and finetuning are conducted on different tasks , entity pairs of the same relation can form multiple different clusters in the pretrained embedding , where standard supervised contrastive loss ( Khosla et al . , 2020 ) may distort the representation because of its underlying onecluster assumption ( Graf et al . , 2021 ) . Therefore , we further propose a multi - center contrastive loss ( MCCL ) , which encourages an entity pair to be similar to only a subset of entity pairs of the same relation , allowing one relation to form multiple clusters . Second , we propose to use classwise k - nearest neighbors ( kNN ; Khandelwal et al . 2020Khandelwal et al . , 2021 in inference , where predictions are made based on most similar instances .
We focus our work on document - level RE ( Jia et al . , 2019 ; Yao et al . , 2019 ) , which consists of both intra - and cross - sentence relations . To the best of our knowledge , this work represents the first effort to explore self - supervised pretraining for document - level RE . Unlike prior studies ( Peng et al . , 2020 ; Qin et al . , 2021 ) , we do not use distant supervision . Instead , we pretrain entity pair embedding with an improved MTB objective on unlabeled corpora , where we use contrastive learning to learn representations that suit downstream RE . We then finetune the pretrained model on labeled data with MCCL . Experiments on two datasets , BioRED ( Luo et al . , 2022 ) in the biomedical domain and Re - DocRED ( Tan et al . , 2022b ) in the general domain , demonstrate that our pretraining and finetuning objectives significantly outperform baseline methods in low - resource settings . Particularly , in the low - resource setting of using 1 % of labeled data , our method outperforms PLM - based classifiers by 10.5 % and 6.1 % on BioRED and Re - DocRED , respectively . Based on our pretrained representations , MCCL outperforms classification - based finetuning by 6.0 % and 4.1 % , respectively . We also find observe that as more data becomes available , the performance gap between MCCL and classification - based finetuning diminishes .
Our technical contributions are three - fold . First , we propose to pretrain the PLMs based on our improved MTB objective and show that it significantly improves PLM performance in lowresource document - level RE . Second , we present a technique that bridges the gap of learning objectives between RE pretraining and finetuning with continual contrastive finetuning and kNNbased inference , helping the RE model leverage pretraining knowledge . Third , we design a novel MCCL finetuning objective , allowing one relation to form multiple different clusters , thus further reducing the distributional gap between pretraining and finetuning .
Related Work
Document - level RE . Existing document - level RE models can be classified into graph - based and sequence - based models . Graph - based models construct document graphs spanning across sentence boundaries and use graph encoders such as the graph convolution network ( GCN ; Kipf and Welling 2017 ) to aggregate information . Particularly , build document graphs using words as nodes with innerand inter - sentence dependencies ( e.g. , syntactic dependencies , coreference , etc . ) as edges . Later work extends this idea by applying different network structures ( Peng et al . , 2017 ; Jia et al . , 2019 ) or introducing other node types and edges ( Christopoulou et al . , 2019 ; Nan et al . , 2020 ; Zeng et al . , 2020 ) . On the other hand , sequencebased methods Zhang et al . , 2021 ; Tan et al . , 2022a ) use PLMs to learn crosssentence dependencies without using graph structures . Particularly , propose to enrich relation mention representation by localized context pooling . Zhang et al . ( 2021 ) propose to model the inter - dependencies between relation mentions by semantic segmentation ( Ronneberger et al . , 2015 ) . In this work , we study a general method of self - supervised RE . Therefore , our method is independent of the model architecture and can be adapted to different RE models .
Low - resource RE . Labeled RE data may be scarce in real - world applications , especially in low - resource and high - stakes domains such as finance and biomedicine . Much effort has been devoted to training RE models in low - resource settings . Some work tackles low - resource RE by indirect supervision , which solves RE by other tasks such as machine reading comprehension ( Levy et al . , 2017 ) , textual entailment ( Sainz et al . , 2021 ) , and abstractive summarization . However , indirect supervision may not be practical in high - stake domains , where annotated data for other tasks are also scarce . Other efforts ( Baldini Soares et al . , 2019 ; Peng et al . , 2020 ; Qin et al . , 2021 ) improve low - resource RE by pretraining on large corpora with RE - based objectives . Specifically , Baldini Soares et al . ( 2019 ) propose an MTB objective that encourages embeddings of the same entity pairs in different sentences to be similar . Peng et al . ( 2020 ) propose to pretrain on distantly labeled corpora , where they make embeddings of entity pairs with the same distant label to be similar . They also introduce a contrastive learning based training objective to improve representation learning . Qin et al . ( 2021 ) further introduce an entity discrimination task and pretrain the RE model on distantly labeled document corpora . In this paper , we study self - supervised pretraining for document - level RE . We study how to reduce the gap between pretraining and finetuning , which is critical to bridge the training signals obtained in these two stages but has been overlooked in prior work .
Method
In this work , we study a self - supervised approach for document - level RE . Given a document d and a set of entities { e i } N i=1 , where each entity e i has one or multiple entity mentions in the document , document - level RE aims at predicting the relations of all entity pairs ( e s , e o ) s , o ∈ { 1 , ... , N } from a predefined set of relationships R ( including an NA class indicating no relation exists ) , where e s and e o are the subject and object entities , respectively . In the self - supervised RE setting , we have a large unlabeled document corpus for pretraining and a labeled RE dataset for finetuning . The document corpus has been annotated with entity mentions and the associated entity types but no relations . Our goal is to train a document - level RE classifier , especially in the low - resource setting .
Our training pipeline consists of two phases : pretraining and finetuning . In pretraining , we use the ( unlabeled ) document corpus to pretrain the entity pair embedding based on our improved matching - the - blanks training objective ( MTB ; Baldini Soares et al . 2019 ) , where the LM learns to decide whether two entity pair embeddings correspond to the entity pairs or not , and the learning of representation is enhanced with contrastive learning . In finetuning , we continue to train the pretrained model on relation - labeled data using a multi - center contrastive loss ( MCCL ) , which achieves better performance than the traditional classifier paradigm due to its better - aligned learning objective with pretraining . After training , we use classwise k - nearest neighbor ( kNN ) inference that suits well the contrastively finetuned model .
The rest of this section is organized as follows : we introduce the model architecture used in both pretraining and finetuning in Section 3.1 , the pretraining process in Section 3.2 , finetuning in Section 3.3 , and inference in Section 3.4 .
Model Architecture
Encoder . Given a document d = [ x 1 , x 2 , ... , x l ] , we first mark the spans of the entity mentions by adding special entity markers [ E ] and [ / E ] to the start and the end of each mention . Then we encode the document with a PLM to get the contextual embedding of textual tokens :
H = [ h 1 , h 2 , ... , h l ] = PLM ( [ x 1 , x 2 , ... , x l ] ) .
We take the contextual embedding of [ E ] at the last layer of the PLM as the embedding of entity mentions . We accumulate the embedding of mentions corresponding to the same entity by Log - SumExp pooling ( Jia et al . , 2019 ) to get the entity embedding h e i . Entity pair embedding . Given an entity pair t = ( e s , e o ) in document d , where e s and e o are the subject and object entities , respectively , we calculate the entity pair embedding by : ( es , eo ) .
z t = W linear h es , h eo , c
Here h es , h eo ∈ R d are embeddings of subject and object entities , c es , eo ∈ R d is the localized context encoding for ( e s , e o ) , W linear ∈ R 3d×d is a linear projector . The localized context encoding is introduced by to derive the context embedding conditioned on an entity pair , which finds the context that both the subject and object entities attend to . Specifically , denote the multi - head attention in the last layer of PLM as A ∈ R m×l×l , where m is the number of attention heads , l is the input length , we first take the attention scores from [ E ] as the attention from each entity mention , then accumulate the attention of this entity mention by mean pooling to get the entitylevel attention A ( e i ) ∈ R m×l . Finally , we compute c ( es , eo ) by : ( es , eo ) .
A ( es , eo ) = A ( es ) ⊙ A ( eo ) , q ( es , eo ) = m i=1 A ( es , eo ) i , a ( es , eo ) = q ( es , eo ) / 1 ⊺ q ( es , eo ) , c ( es , eo ) = H ⊺ a
We introduce in the rest of the section how to pretrain and finetune the RE model based on the entity pair embedding z ( es , eo ) .
Pretraining
We pretrain the LM on the document corpus using the MTB objective . MTB is based on a simple assumption that , in contrast to different entity pairs , it is more frequent for the same entity pair to be connected with the same relation . The MTB objective transforms the similarity learning problem into a pairwise binary classification problem : given two relation - describing utterances where entity mentions are masked , the model classifies whether the entity pairs are the same or not . This pretraining objective has shown effectiveness in several sentence - level RE datasets ( Zhang et al . , 2017 ; Hendrickx et al . , 2010 ; Han et al . , 2018 ) .
However , when it comes to document - level RE , Qin et al . ( 2021 ) have observed no improvement led by the vanilla MTB pretraining . Therefore , we replace the pairwise binary classification with contrastive learning , which is adopted in later RE pretraining works ( Peng et al . , 2020 ; Qin et al . , 2021 ) and can effectively learn from more positive and negative examples . Details of training objectives are elaborated in the rest of the section . We introduce the details of data preprocessing of the pretraining corpus in Appendix A .
Training objective . The overall goal of pretraining is to make the embedding of the same entity pair from different documents more similar than different entity pairs . For clarity , we call two same entity pairs from different documents as a positive pair , and two different entity pairs as a negative pair . We use the InfoNCE loss ( Oord et al . , 2018 ) to model this objective . Given the documents in batch , P as the set of all positive pairs , and N t denote the set of entity pairs different to t , the contrastive MTB loss is 1 :
L rel = − 1 |P| t i , t j ∈P log e sim ( z t i , z t j ) / τ Z t i , ( 1 )
Z t i = e sim ( z t i , z t j ) / τ + t k ∈Nt i e sim ( z t i , z t k ) / τ ,
where sim ( z t i , z t j ) denotes the similarity between the embeddings of t i and t j , and τ is a temperature hyperprameter . Following Chen et al . ( 2020 ) , we use cosine similarity as the similarity metric . Similar to SimCSE , we further add a self - supervised contrastive loss that requires the same entity pair embedding augmented by different dropout masks to be similar , thus encouraging the model to learn more instance - discriminative features that lead to less collapsed representations . Specifically , denote the two entity pair embeddings of t derived by different dropout masks as z t andẑ t , respectively , the set of all entity pairs in the batch as T , and the set of entity pairs in positive pairs as T P , the self - supervised loss is :
L self = − 1 |T P | t i ∈T P log e sim ( z t i , ẑ t i ) / τ Z t i , Z t i = e sim ( z t i , ẑ t i ) / τ + t k ∈T \ { t i } e sim ( z t i , ẑ t k ) / τ .
Finally , we use a masked language model loss L mlm to adapt the LM to the document corpus . The overall pretraining objective is :
L pretrain = L rel + L self + L mlm .
For faster convergence , we initialize our model with a PLM that is pretrained on a larger corpus , and continually pretrain the PLM on the document corpus with our new pretraining objectives . We use BERT ( Devlin et al . , 2019 ) for the general domain and PubmedBERT ( Gu et al . , 2021 ) for the biomedical domain .
Finetuning
After pretraining , we finetune the LM on labeled document - level RE datasets . In previous studies ( Baldini Soares et al . , 2019 ; Peng et al . , 2020 ; Qin et al . , 2021 ) , pretraining and finetuning are conducted in processes with different learning objectives . Specifically , after using the pretrained weights to initialize a RE classifier , the model is finetuned with a classification - based training objective . Based on our model architecture , a straightforward finetuning method is to add a softmax classifier upon the entity pair embedding , for which a cross - entropy loss for a batch of entity pairs T is formulated as :
P t i r = softmax ( W r z t i + b r ) , L ce = − 1 |T | t i ∈T log ( P t i yt i ) ,
where y t is the ground - truth label for entity pair t , W r , b r are the weight and bias of the classifier . Though this approach has shown improvements , it may produce sub - optimal outcomes from MTB pretraining since it implicitly assumes that entity pairs corresponding to the same relation are in the same cluster , while MTB pretraining may learn multiple clusters for a relation . For example , the entity pairs ( Honda Corp. , Japan ) and ( Mount Fuji , Japan ) , although likely to be expressed with Therefore , to accommodate this multi - cluster assumption , we need to finetune the representations with a training objective that suits multiple clusters for each relation . Beside using the softmax classifier with cross - entropy loss , we also consider supervised contrastive loss ( SupCon ; Khosla et al . 2020 ; Gunel et al . 2021 ) . SupCon has a similar loss form to InfoNCE in Eq . ( 1 ) , except that it uses instances of the same / different relations as positive / negative pairs . However , previous work ( Graf et al . , 2021 ) has shown that both softmax and SupCon are minimized when the representations of each class collapse to the vertex of a regular simplex . In our case , this means the entity pair embeddings corresponding to the same relation in pretraining collapses to a single point , which creates a distributional gap between pretraining and finetuning .
Training objective . We thereby propose the MCCL objective . Given entity pairs T and sets of entity pairs grouped by their relations { T r } r∈R , our loss is formulated as :
w ( t i , t j ) r = e sim ( z t i , z t j ) / τ 1 t k ∈Tr\ { t i } e sim ( z t i , z t k ) / τ 1 , s t i r = t j ∈Tr\ { t i } w ( t i , t j ) r
sim ( z t i , z t j ) ,
P t i r = softmax ( ( s t i r + b r ) / τ 2 ) , L mccl = − 1 |T | t i ∈T log ( P t i yt i ) ,
where τ 1 and τ 2 are temperature hyperparameters , b r ∈ R is the classwise bias . The loss calculation can be split into two steps . First , we calculate the similarity between t i and relation r , which is a weighted average of the similarity between t i and t j ∈ T r such that a more similar t j has a larger weight . Next , we use the cross - entropy loss to make the similarity of ground - truth relation larger than others . In this way , MCCL only optimizes t i to be similar to a few closest entity pairs of the ground - truth relation , and thus encourages multiple clusters in entity pair embedding . Note that MCCL can be easily extended to support multilabel classification scenarios , for which details are given in Appendix B .
Proxies . We use batched training for finetuning , where entity pairs in the current batch are used to calculate MCCL . However , it is possible that a subset of relations in R , especially the long - tail relations , are rare or missing in the current batch . When T r \ { t i } is empty , s t i r and MCCL become undefined . To tackle this problem , we propose the use of proxies ( Movshovitz - Attias et al . , 2017 ; Zhu et al . , 2022 ) . We add one proxy vector p r for each relation r , which is a trainable parameter and associated with an embedding z p r . We incorporate the proxies into MCCL by changing T r to T ′ r = T r ∪ { p r } , ensuring that T ′ r \ { t i } is never empty in training and preventing MCCL from becoming undefined . The proxies are randomly initialized and updated during training by backward propagation .
Inference
We use the classwise kNN ( Christobel and Sivaprakasam , 2013 ) for inference , which predicts relations based on similarly represented instances and thus aligns with our contrastive finetuning objective . Given a new entity pair to predict , we first find k most similar instances 2 in the training data of each relation ( including NA ) , then calculate the average cosine similarity of each relation s avg r . Finally , the model returns the relation with the maximum s avg r + b r for single - label prediction , and all relations with higher s avg r + b r than NA for multilabel prediction . We use classwise kNN because it is more suitable for RE datasets , where the label distribution is usually long - tailed ( Zhang et al . , 2019 ) .
Experiments
We evaluate our proposed method with a focus on low - resource RE ( Sections 4.1 - 4.3 ) , and present detailed analyses ( Section 4.4 ) and visualization ( Section 4.5 ) to justify method design choices .
Datasets
We conduct experiments with two documentlevel RE datasets . The BioRED dataset ( Luo et al . , 2022 ) is a manually labeled single - label RE dataset in the biomedical domain . The entity pairs are classified into 9 types ( including an NA type indicating no relation ) . It has a training set consisting of 400 documents , which we use in finetuning . For pretraining , we use the PubTator Central corpus , which annotates the PubMed corpus with entity mentions and their named entity types . The Re - DocRED dataset ( Tan et al . , 2022b ) is a multi - label largescale dataset of the general domain . It is a relabeled version of the DocRED dataset ( Yao et al . , 2019 ) . Re - DocRED addresses the incomplete annotation issue of DocRED , where a large percentage of entity pairs are mislabeled as NA . The entity pairs in Re - DocRED are classified into 97 types ( incl . NA ) . It has a training set consisting of 3,053 documents , which we use in finetuning . For pretraining , we use the distantly labeled training set provided by DocRED , which consists of 101,873 documents . We remove the relation labels and use our improved MTB to pretrain the model .
Experimental Setup
Model configurations . We implement our models using Hugging Face Transformers ( Wolf et al . , 2020 ) . We use AdamW ( Loshchilov and Hutter , 2018 ) in optimization with a weight decay of 0.01 . During pretraining , we use a batch size of 16 , a learning rate of 5e-6 , a temperature of 0.05 , and epochs of 3 and 10 for BioRED and DocRED , respectively . During finetuning , we use a batch size of 32 , a learning rate of 5e-5 , and epochs of 100 and 30 for BioRED and DocRED , respectively . The temperatures in MCCL are set to τ 1 = τ 2 = 0.2 for BioRED and τ 1 = 0.01 , τ 2 = 0.03 for DocRED . We search k from { 1 , 3 , 5 , 10 , 20 } for classwise kNN using the development set 3 . We run experiments with Nvidia V100 GPUs .
Evaluation settings . In this work , in addition to the standard full - shot training , we consider lowresource settings . To create each of the settings , we randomly sample a fixed proportion p % of the entity pairs from the training set as our training data , and use the original test set for evaluation . We use the same evaluation metrics as the original papers . We use micro - F 1 for BioRED , and micro - F 1 and micro - F 1 -Ign for Re - DocRED . The micro - F 1 -Ign removes the relational facts in the test set that have appeared in training .
Compared methods . We experiment with the following finetuning objectives : ( 1 ) Lazy learning , which directly uses the pretrained embedding and training data to perform kNN without finetuning ; ( 2 ) Cross - entropy loss ( CE ) , which adds a softmax classifier on top of PLM and uses crossentropy loss to finetune the model ; ( 3 ) Supervised contrastive loss ( SupCon ) ; and ( 4 ) Multicenter contrastive loss ( MCCL ) . In inference , classwise kNN is used for all methods except for CE . Note that as SupCon does not apply to multilabel scenarios , we only evaluate it on BioRED . For each objective , we also evaluate the PLM before and after MTB pretraining . We use different PLMs as the backbone of the model , namely PubmedBERT BASE for BioRED and BERT BASE for Re - DocRED , which are pretrained on the biomedical and general domains , respectively .
Main Results
The results on the test sets of Re - DocRED and BioRED are shown in Table 2 and Table 3 Considering other training objectives , we observe that lazy learning produces meaningful results . On both datasets , the results of lazy learning based on MTB with 10 % of data are comparable to finetuning with 1 % of data . This shows that the entity pair embedding pretrained on unlabeled corpora contains knowledge that can be transferred to unseen relations . We also observe that SupCon using kNN - based inference underperforms both CE and MCCL on BioRED , showing that its one - cluster assumption hurts the knowledge transfer .
F 1 F 1 -Ign F 1 F 1 -Ign F 1 F 1 -Ign F 1 F 1 -
Ablation Study
Pretraining objectives . We analyze the effectiveness of our proposed pretraining losses in Section 3.2 . To do so , we pretrain the model with one loss removed at a time while keeping the finetuning setup on BioRED fixed with the MCCL .
The results are shown in Table 4 . Overall , we observe that all losses are effective . If we remove all proposed techniques and use the vanilla MTB pretraining objective of binary pairwise classification , the results are only slightly better or even worse . Among the techniques , removing L rel leads to the largest performance drop , showing that MTB - based pretraining is critical to improve low - resource RE . Removing L self also leads to a large performance drop . It is because L self encourages the model to learn more discriminative features that lead to less collapsed representations . Our finding aligns with recent studies in computer vision ( Islam et al . , 2021 ; , showing that reducing collapsed representations with self - supervised contrastive learning improves the transferability to downstream tasks .
Performance w.r.t . different temperatures . We discuss the impact of two temperatures in MCCL .
In MCCL , τ 1 controls the weighting of instances .
With a very small τ 1 , each instance will only form a cluster with its nearest neighbor in the batch , while with very large τ 1 , instances of the same relation will collapse to the same cluster . τ 2 controls the importance of hard instances , which is also used in other contrastive losses ( e.g. , τ in Eq . ( 1 ) ) . Wang and Liu ( 2021 ) observe that small τ 2 makes the model focus more on hard instances , while Khosla et al . ( 2020 ) observe that too small τ 2 leads to numerical instability . We show the results of using different temperatures in Figure 1 , where we keep one temperature fixed and change the other . For τ 1 , we find that using large temperature harms the performance , showing that our multi - cluster assumption improves low - resource RE . For τ 2 , we observe that both small and large values impair the performance , which is aligned with prior observations .
Performance w.r.t . different amount of data .
The main results show that MCCL outperforms CE in the low - resource setting , while slightly underperforming CE when full training data is used . We further evaluate MCCL and CE using different amounts of end - task data . We experiment on BioRED and use the entity pair embedding pretrained with MTB . Results are shown in Figure 2 . We observe that MCCL consistently outperforms CE by a large margin when less than 20 % of training data is used , while it performs similarly or worse than CE after that . It again demonstrates the effectiveness of MCCL in low - resource RE . However , as the pretraining and finetuning are based on different tasks , fully adapting the model to downstream data by CE results in similar or better performance in data - sufficient scenarios . Lazy CE SupCon MCCL
Visualization
Conclusion
In this paper , we study self - supervised learning for document - level RE . Our method conducts an improved MTB pretraining objective that acquires cheap supervision signals from large corpora without relation labels . To bridge the gap between pretraining and end - task finetuning , we propose a continual contrastive finetuning objective , in contrast to prior studies that typically use classification - based finetuning , and use kNNbased inference . As pretrained representation may form multi - cluster representation , we further propose a multi - center contrastive loss that aligns well with the nature of the pretrained representation . Extensive experiments on two documentlevel RE datasets demonstrate the effectiveness of these key techniques in our method . Future work is adapting our method to other tasks in information extraction , such as n - ary relation extraction , named entity recognition , typing , and linking .
Limitations
The main limitation of MCCL is the requirement of a sufficiently large batch size in training ( 32 documents in our experiments ) , leading to a need for large GPU memory . This is because MCCL uses in - batch entity pairs for contrastive learning , and a small batch size does not provide enough instances to form multiple clusters . In addition , we need to store the entity pair embedding of the whole training set for kNN - based inference , which is less memory - efficient than CE .
and PMI = N ( es , eo ) N ( es ) ×N ( eo ) , to measure the popularity of entity pairs The frequency measures how often e s and e o co - occur . The PMI measures whether e s and e o have a strong association . In pretraining , we first discard the entity pairs with frequency < N threshold , and then use the positive pairs constituted by the top K entity pairs measured by their PMIs . We set the frequency threshold to be 16 and 3 for BioRED and DocRED , respectively , and use the top 5,000 entity pairs in pretraining .
Besides , as MTB is fully self - supervised , the information of whether two relations mentions correspond to the same relation type is not available , but it is assumed that at least entity pairs with different subject or object types are likely to be of different relation types and can therefore be used as negative pairs . Such use of entity types to filter the pairs has indeed been shown a strong feature for RE ( Zhong and Chen , 2021 ; . We only use two entity pairs with different subject or object entity types as negatives . While the entity type based filtering may also discard some hard negatives , our experiment ( see Section C ) shows improved results , meaning that its benefits outweigh the disadvantages .
B Adaptation to Multi - label RE
It is noteworthy that in some RE tasks , such as DocRED , one entity pair may have multiple relation labels , in which case the cross - entropy loss does not apply . Therefore , for multi - label scenarios , we substitute cross - entropy loss ( also the softmax in MCCL ) with the adaptive thresholding loss proposed by . Specifically , denote the logits as l ( the input to softmax in crossentropy loss ) , the set of positive relations as P ( except NA ) , and the set of the remaining relations except for NA as N , the adaptive thresholding loss is formulated as :
L 1 = − r∈P log e lr r ′ ∈P∪ { NA } e l r ′ , L 2 = − log e lNA r ′ ∈N ∪ { NA } e l r ′ , L at = L 1 + L 2 .
This loss encourages the logits of positive relations to be higher than NA , and the logits of other relations to be lower than NA . In prediction , the model returns the relations with higher logits than NA as predictions , or return NA if none of such relations exist .
C More Experiments
Performance w.r.t . number of proxies . We evaluate MCCL with different number of proxies . When no proxy is used , we ignore the relations that do not appear in the current batch .
The F 1 on both BioRED and Re - DocRED in the 1 % low - resource setting is shown in Figure 4 , indicating that adding proxies improves F 1 significantly on both datasets . Using one proxy for each relation achieves an increase of 6.0 % in F 1 on BioRED , and a larger increase of 10.2 % in F 1 on Re - DocRED . Such a difference of increment is due to the fact that Re - DocRED is more longtailed , where 97 % of instances are NA compared to 80 % in BioRED . We also observe that adding more proxies achieves similar or even worse results . These results make sense as the proxies are mainly in the place of long - tail relations that do not appear in the batch , and these relations contain too few instances to form multiple clusters .
Coarse - to - fine evaluation . To give another illustration of showing that MCCL learns multiple clusters , we experiment with it on 1 % of BioRED in a coarse - to - fine setting . Specifically , we merge all relations except NA into one relation in finetuning , and apply kNN inference using the original labels . We find that MCCL achieves an F1 of 30.3 % , which is even better than CE with all relations provided . However , if we remove the instance weights in MCCL to degrade it to onecluster , the F 1 constantly degrades in finetuning . It shows that multi - cluster assumption helps preserve the fine - grained relation information in pretrained representation . Other ablation studies . We analyze the effectiveness of entity type filtering in Section A. Results are shown in Table 5 . Removing entity type filtering degrades performance significantly . It shows that entity type filtering can remove a lot of false negatives in pretraining and greatly improves the pretrained model . Besides , as the main results have demonstrated the effectiveness of MCCL in finetuning , we wonder whether MCCL can also lead to improved pretraining . To do so , we replace the InfoNCE loss in Eq . ( 1 ) by MCCL and regard different entity pairs as different classes . The results are comparable or slightly worse in contrast to using L rel , showing that the multi - cluster assumption of MCCL does not necessarily help pretraining . In section 4 B2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?
In section 4 B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ?
In section 4 B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Not applicable . Left blank .
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? In section 4 B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . In section 4 C Did you run computational experiments ?
In section 4 C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? In section 4
Appendices A Data Preparation
We acquire positive and negative pairs from the document corpus . We regard two entity pairs ( e s 1 , e o 1 ) , ( e s 2 , e o 2 ) in different documents as a positive pair if they share the same subject and object entities , respectively ( i.e. , e s 1 = e s 2 , e o 1 = e o 2 ) , and otherwise negative .
However , for a large corpus , the number of such positive pairs is enormous . For instance , in biomedical RE pretraining , we extract 37 billion positive pairs in total . Using all these pairs in pretraining is computationally infeasible . Therefore , we select positive pairs as follows . Denote the number of documents mentioning an entity e or an entity pair ( e s , e o ) as N ( e ) and N ( e s , e o ) , respectively , we use two metrics , frequency = N ( e s , e o )
Introduction
Task Selection
Probing Baselines
Experiment setup
Main Results and Discussion
Quantitative results . Table 2 summarizes the quantitative results for text - davinci-002 . We include additional results and discussion for text - davinci-003 , PaLM and Flan - PaLM in Appendix A.3 . LLMs can achieve surprisingly high performance when provided with invalid reasoning steps for the demonstrations ( 1 ) . In particular , under Inter . Recall / Inter . F1 , i.e. , intrinsic evaluation , which is arguably a more faithful measurement of the rationale quality ( § 3.3 ) , all LLMs we tested can retain over 90 % of the performance achieved under CoT prompting .
Relevance matters more than coherence for bridging objects . Providing incoherent bridging objects ( 2 ) achieves better performance than providing irrelevant bridging objects ( 3 ) , especially on the more challenging GSM8 K dataset ( 39.2 v.s. 26.2 Inter . F1 ) . which indicates that it is important for the bridging objects to be relevant , but not as important to have them in the right order to guide the LLM along the reasoning process . We quantitatively measure the coverage of bridging objects from the query for the generated rationales , and find that the settings with no relevance for bridging objects ( 3 , 7 ) do have significantly lower coverage ( below 60 % ) than other settings ( around 80 % ) .
We use Recall / F1 of the bridging objects as the metrics for intrinsic evaluation of the generated rationales . While the metrics do n't take into account the quality of the language templates , we examine the predicted rationales for 20 random examples under each setting we tested except standard prompting ( which does not generate any rationale ) , and find that for all the examples , whenever the LLM reaches a correct bridging object , the corresponding language template within the step is also correct . This suggests that overall , the correctness of bridging objects is a very good indicator of the quality of the reasoning steps .
We present a data - and training - efficient approach to build a multilingual VLP model mCLIP , by aligning the pretrained monolingual VLP model CLIP and a multilingual text encoder XLM - R to the same multimodal multilingual space . Despite the strong multimodal and multilingual abilities inherited from both models , the proposed mCLIP also inherits the societal impacts including some negative ones of the original CLIP and XLM - R , e.g. , societal biases ( Radford et al . , 2021 ) and misuse of language models ( Tamkin et al . , 2021 ) . The implicit biases are expected to be removed by debiasing either the dataset or the model ( Meade et al . , 2022 ; Zhou et al . , 2022 ) . Besides , our proposed method makes it simpler to retrieve malicious or offensive content ( Welbl et al . , 2021 ) Then we perform the TriKD in Section 3.2 . We use the LAMB optimizer ( You et al . , 2020 ) . The learning rate is linearly warmed up to 0.01 within the first 500 steps and then decayed to 0 . The batch size is 16,384 . The temperature for the ITC loss is initialized as 0.07 and then learned by gradient descent , while the temperature of TTC loss is fixed as 0.07 ( Jain et al . , 2021 ) . The models are pretrained for 15 epochs when the smaller dataset CC3 M is used , while for 3 epochs when CC12 M is used .
For all experiments in all pretraining and finetuning stages , we use the inverse square root learning rate scheduler and conduct experiments on 8 NVIDIA V100 GPUs . We use the same dropout method as XLM - R ( Conneau et al . , 2020 ) . The dropout ratios are set as 0.3 . The detailed hyperparameters of different stages are listed in Table 9 .
This project was supported by National Natural Science Foundation of China ( No . 62106138 ) and Shanghai Sailing Program ( No . 21YF1412100 ) . Jia Pan and Wenping Wang are partially supported by Centre for Transformative Garment Production . We thank the anonymous reviewers for their insightful feedbacks on this work .
