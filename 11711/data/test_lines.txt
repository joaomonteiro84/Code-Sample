Do Trajectories Encode Verb Meaning ?
Distributional models learn representations of words from text , but are criticized for their lack of grounding , or the linking of text to the nonlinguistic world . Grounded language models have had success in learning to connect concrete categories like nouns and adjectives to the world via images and videos , but can struggle to isolate the meaning of the verbs themselves from the context in which they typically occur . In this paper , we investigate the extent to which trajectories ( i.e. the position and rotation of objects over time ) naturally encode verb semantics . We build a procedurally generated agent - object - interaction dataset , obtain human annotations for the verbs that occur in this data , and compare several methods for representation learning given the trajectories . We find that trajectories correlate as - is with some verbs ( e.g. , fall ) , and that additional abstraction via self - supervised pretraining can further capture nuanced differences in verb meaning ( e.g. , roll vs. slide ) .
Introduction
While large distributional language models such as BERT ( Devlin et al . , 2019 ) and GPT ( Radford , 2020 ; Brown et al . , 2020 ) have had empirical success in deriving representations of words and sentences from large text corpora , most of these models lack grounding , or a connection between the words and their real - world referents . Grounding , in addition to being necessary for multimodal tasks like video recognition , has been argued to lie at the core of language understanding ( Bender and Koller , 2020 ) . Work on grounded language learning associates language with the non - linguistic world , typically by learning from large - scale image ( Bruni et al . , 2011 ) or video ( Sun et al . , 2019 ) datasets .
Much prior work on language grounding has focused on concrete nouns ( objects ) and adjectives ( attributes ) , which are captured well by patterns of pixels . Verbs , however , have received less attention , despite being essential for building models that can interact in realistic 3D environments ( Shridhar et al . , 2020a ; Bisk et al . , 2020 ) . Verbs are especially challenging to model , given that they take place over time . Image and video data alone is insufficient to fully capture verb semantics , as demonstrated by prior work ( Yatskar et al . , 2016 ) , in many cases failing to isolate the meaning of the verb from context in which it typically occurs . For example , Chao et al . 2018 show that an image of a person laying in the snow next to a snowboard is labeled " standing on a snowboard " . Moreover , recent work has introduced datasets and benchmarks based on situated 3D environments ( Gan et al . , 2020 ; Deitke et al . , 2020 ; Ebert and Pavlick , 2020 ; Shridhar et al . , 2020a ) that demonstrate the challenges of learning task - oriented behavior , which demands a combination of object and verb grounding .
In this paper , we test the hypothesis that the semantics of ( concrete ) verbs are grounded in the 3D trajectories of objects : i.e. , the absolute and relative paths objects take through 3D space . We investigate if and when verb meanings appear to be a product of raw perception of objects in 3D space , and when differentiating verb meanings requires additional abstraction and representation beyond what is available via direct perception . To study this , we collect a clean dataset of 3D object trajectories in simulation . We collect human descriptions of these perceived world dynamics , i.e. , to determine whether or not a given event constitutes a fall or a tumble . We then propose a self - supervised pretraining approach , whereby we train a time - series prediction model to obtain representations of trajectories in a 3D environment without any linguistic input . We evaluate the learned representations on how well they encode verb semantics for specific verbs . We show that the pretrained model learns to represent events in a way that aligns well with the meaning of English verbs , e.g. differentiating slide from roll . In summary , our primary contributions are :
1 . We introduce a new , clean dataset of 3D object trajectories paired with human judgments about whether or not each trajectory falls within the extension of each of 24 different verbs . To the best of our knowledge , this is the first dataset of its kind , and provides a valuable resource for empirical studies of lexical semantics . Our data is available at https :
Related Work
Grounded Language with Deep Learning .
Our contributions add to a large body of work on grounded representation learning . Much of this work augments language modeling objectives with images ( Silberer and Lapata , 2012 ; Lazaridou et al . , 2015 ; Kiela et al . , 2017 ) and videos ( Sun et al . , 2019 ) . In this work , we focus on representations that encode verb semantics . Prior work on verb learning has been conducted in the computer vision community , typically described as " humanobject interactions " ( Regneri et al . , 2013 ; Chao et al . , 2018 ; Ji et al . , 2019 ) . Most closely related to our approach , which focuses on trajectory data , is work on learning affordances for human - robot communication . For example , Kalkan et al . ( 2014 ) ; Ugur et al . ( 2009 ) learn affordance representations based on the state changes of objects , but do not encode the full trajectory between states . Also related is work in grounded language in text - only models which investigates models ability to reason about objects through space and time ( Aroca - Ouellette et al . , 2021 ) . Outside of NLP , models have been trained on trajectory data for applications like human motion path forecasting ( Giuliari et al . , 2021 ) and human activity recognition ( Wang et al . , 2018 ) . Our work lies at the intersection of grounded language learning and spatiotemporal machine learning , using representations of trajectory data to study verb semantics .
Grounding and Lexical Semantics . Prior work in formal semantics attempts to build feature - based representations of verb meaning in terms of the 3D trajectories and state transitions entailed by those verbs ( Pustejovsky and Krishnaswamy , 2014 ; Siskind , 2001 ; Steedman , 2002 ) . Such work is related more generally to the idea of mental simulation as a means for representing and reasoning about linguistic concepts ( Feldman , 2008 ; Bergen et al . , 2007 ; Bergen , 2012 ) . We view our contribution as consistent with and complementary to this formal semantics program . While the prior work has sought to codify the precise truth conditions of motion verbs , we investigate whether such representations could emerge organically from datadriven processes .
While we focus on concrete verbs in this paper , other work has argued that motor processing and mental simulation plays a more general role in language processing . For example , GÃ¤rdenfors ( 2019 ) makes a case for grounded distributional " conceptual spaces " as the foundation for modeling linguistic concepts . Dorr and Olsen ( 2018 ) discusses the role of metaphor in modeling abstract uses of words like push . Borghi and Riggio ( 2009 ) argues for the notion of a " motor prototype " as a key component of recognizing and processing objects , and Mazzuca et al . ( 2021 ) presents evidence that the sensorimotor system ( in particular the interactive aspects ) drive acquisition of abstract concepts .
Dataset 3.1 Overview
To carry out the proposed study , we require a dataset that contains continuous 3D recordings of an agent interacting with an object . While our representation learning methods will not use linguistic supervision , we require verb labels in order to evaluate our models . Thus , in our data , we require that each recording is annotated with verbs describing the motion of the object . For example , if the agent throws a bouncy ball across a room , we 'd expect the recording to be annotated with a verb sequence such as be thrown , fall , bounce , bounce , bounce , roll , stop .
To produce such data , we build a simple Markovian agent which interacts with a variety of objects in a 3D virtual environment . We record the result - Figure 1 : The Simulated Spatial Dataset consists of procedurally generated motion data of a virtual agent interacting with an object . In this sequence the agent ( red sphere ) pushes the object ( blue sphere ) . At t=0 and t=1 , the agent approaches the ball . Then , in t=2 and t=3 , the agent pushes to ball . Finally , at t=4 , the ball is rolling away from the agent .
ing trajectory of the object and then , using crowdsourcing , ask humans to determine which verbs could accurately describe which portions of the object 's movement . An example sequence from the dataset is shown in Figure 1 .
Data Generation and Terminology
In this section we provide details on how we generate the data , and introduce terminology that will be used throughout the rest of the paper .
Environment . The dataset is generated in Unity , a game engine seeing increased use by researchers ( Deitke et al . , 2020 ; Gan et al . , 2020 ) for its accessible rendering and physics simulation via the underlying Nvidia PhysX physics engine . The dataset and simulation source code are publicly available . 1 Trajectory data . We define trajectory data as the position and rotation of entities in space , represented with three - dimensional XY Z coordinates and four - dimensional XY ZW quaternions respectively . We choose to focus on only these features , ignoring other possibilities like object shape or identity , in order to focus on learning generalizable aspects of verb semantics that are independent of the object .
Sessions . The dataset is generated in 3 - minute continuous segments we refer to as sessions . Within each session , several parameters are randomized , including object shape , mass , drag , friction , and bounciness .
Action Primitives . The data generation is driven by a Markov Chain with a set of randomly parameterized action primitives . In this Markov Chain , the States are whether the object is Held , On - Counter and OnGround . The transitions between these states are action primitives like PickUp , Put - Down , or Throw . For example , when the object is in the state OnCounter , the agent may execute a PickUp , after which the object is Held . These action primitives , combined with the physics of the objects ( e.g. , their shape , mass , friction , bounciness , etc ) are intended to produce a wide range of object motions corresponding to a range of verbs , and we do not expect that the primitives will map directly to the verbs that one would use to describe the resulting object behavior . For example , when we simulate a Throw primitive , the result might be that the object flies across the room , hits the wall , falls to the floor , and bounces until it comes to a rest . We parameterize the execution of each action with action - specific parameters , e.g. the force of a throw . The combination of session - and actionlevel parameters can result in a wide variety of object motion from each primitive action . A full description of the parameters for each action can be found in Appendix A .
Verbs . We highlight a distinction between action primitives and the high - level actions or verbs that emerge from them . For example , if the object is pushed , it may then slide , bounce , roll , tumble , or any combination thereof . We refer to all of these as verbs , though only push is an action primitive . We highlight this distinction because we are most interested in studying the nuanced verbs that emerge from the simulation , rather than the action primitives that drive it explicitly .
Frames . Our atomic unit is frames , also referred to as timesteps , which represent a single point in time . Our dataset is collected at 60 fps , or 10,800 frames per session . For each frame , we record the position and rotation of the object , as well as the position of the agent . This is sufficient to reconstruct and render the scene from an arbitrary perspective as needed . We choose this high framerate because it 's relatively fast and inexpensive to rapidly produce trajectory data , which can be subsampled as needed for rendering or modeling .
Crowdsourced Annotation
We collect labels for which verbs occur in the data , and when they occur . To do this , we extract short clips from the dataset , and ask crowdworkers to provide binary judgments on whether the clip falls in the extension of the verb .
Clips . We extract short clips from the dataset using Hierarchical Dynamic Clustering with Motion energy - based pooling ( Zhang et al . , 2018 ) , a selfsupervised action segmentation framework that can be summarized as follows :
1 . The 3D space is divided into clusters using the provided trajectory data . The framework uses Hierarchical Dynamic Clustering , which is similar to k - means but shown to outperform it on human motion parsing tasks . 2 . A sliding window is applied to the cluster labels for a given positional sequence . The number of transitions between clusters in a window are defined as its motion energy . 3 . The subsequent motion energy curve is smoothed using a Gaussian kernel with a tuned smoothing factor . 4 . The peaks of the motion energy curve are considered motion segments , with lengths varying with respect to the width of the peak .
This algorithm is shown to perform well on human motion parsing , which we find transfers well to our dataset when applied to object position . This yields easily identifiable patterns of motion , e.g. from the time the object is thrown to when it slows to a stop . We find that , in contrast to a random sliding window , this approach avoids cutting clips in the middle of salient patterns of motion .
In our case , a disadvantage of this approach is that the extracted segments are variable - length . To simplify our pipeline , we filter to only segments of length 72 to 96 , then crop the segment to length 90 , or 1.5 seconds . We call each 1.5s segment a clip . We choose this length to make the clip as short as possible to avoid crowdworker fatigue , but give sufficient time for a human observer to recognize what 's happening .
Verbs . We produce 24 queries , each corresponding to a verb , e.g. Does the object bounce ? To do this , the authors curate a list of 24 verbs 2 of interest which are likely to occur in the simulated data and range from general descriptions ( e.g. , fall ) to more subtle descriptions of object motion ( e.g. , tumble ) . When asking annotators whether a verb applies to a clip , we always frame the question with the object as the subject . That is , when a carry event occurs , annotators are asked " is the object carried " .
We then consider every possible ( clip , query ) pair a potential crowdsourcing task . We apply conservative heuristics to filter out ( clip , query ) pairs that are guaranteed to have a negative label . For example , if the Held state was never present in a clip , we do n't ask if the object is carried . This results in approximately 110k tasks , from which we sample 100 tasks per query , for a total 2400 crowdsourcing tasks , such as the one shown in Figure 2 .
Labels . For each crowdsourcing task , we obtain responses from five workers , then take the majority response as the label for that clip . The same clip is shown for all applicable queries , resulting in a supervised dataset of 24 - dimensional vectors , representing binary verb labels for each clip . 3 The dataset and all unaggregated annotations are available for download . 4 Figure 3 : Crowd annotation agreement by verb . Workers agree most on when verbs of gravity occur , such as fall , drop , bounce , and least on when verbs of rotation occur , i.e. turn , spin , tip .
Dataset Analysis
In this section , we analyze trends in the dataset annotations , including worker agreement , and comparisons between semantically related verbs .
Agreement
Annotation agreement on a clip is the proportion of responses that match the majority label for that clip . Figure 3 shows annotation agreement by verb . A noticeable trend is that agreement is higher for particular semantic categories . Specifically , verbs that involve gravity , i.e. fall , fall off , drop , and bounce have higher agreement . On the other hand , verbs of rotation , i.e. turn , spin , tip , flip have lower agreement , alongside abstract verbs start and stop . For start in particular , we even received feedback from crowdworkers that they were n't sure whether the object started moving during the clip or not .
Co - occurrence
Figure 4 shows co - occurrence : specifically , given that a clip is labeled by at least one worker as verb v 1 , how often is it labeled by other workers as verb v 2 ? Co - occurrence allows us to answer questions like how often is a toss considered a throw ? and vice - versa . We highlight some interesting verb relationships .
General co - occurrence . Verb co - occurrence is high in general . The average number of verbs used to describe a given clip is 4 ( where a verb is considered " used " if at least three workers use it ) . This highlights the challenge of verb learning , as opposed to more concrete nouns and adjectives . Verbs are applicable to a wide variety of behavior , even if it is n't a prototypical instance of that verb .
Lexical entailments . All dogs are animals but not all animals are dogs . These types of semantic containment relationships are also ascribed to verbs . Analyzing our collected data , in some cases , we observe the opposite of what 's expected . For example , according to WordNet ( Fellbaum , 2010 ) , toss is a type of throw . However , using the majority labels , we find throws to be annotated as tosses more often tosses than are annotated as throws . That is , p ( toss|throw ) = .67 < p ( throw|toss ) = .75 .
Frequent co - occurrences . Hit , push , and bump stand out as the most frequently co - occurring verbs , having over 90 % co - occurrence with each other . These likewise occur when many other verbs do , but not reciprocally . For example , most slaps are hits , but only 41 % of hits are slaps . In many cases , this can be explained by other verbs being immediately preceded by the agent making contact with the object , which gets labeled hit , push , and bump .
Fine - grained distinctions . Workers distinguish roll from slide -only 50 % of rolls are also considered slides , and vice - versa . This validates that verbs with similar trajectories , which may be challenging for models , are indeed differentiated by humans . Additionally , verbs with similar but nuanced meanings are differentiated . For example , tip , tumble , fall over , and topple tend to co - occur around 70 - 80 % of the time . These also fall into " verbs of rotation " category , which have the lowest annotator agreement . noise .
Experiments
Our hypothesis is that representation learning in the 3D visuospatial world ( without language supervision ) can yield concept representations that align to English verb semantics - i.e. can the representations capture nuanced distinctions like throw vs. toss or slide vs. roll ? To test this , we pretrain a self - supervised model on a time - series prediction task , and then use a perceptron classifier to evaluate its learned representations .
We evaluate four approaches , described in detail below . First , we train a simple perceptron to evaluate the representational capacity of the trajectory data as - is , as a comparative baseline . Second , we train a fully supervised model to determine a soft upper bound on the task without pretraining . Third , we evaluate our self - supervised model . And finally , we fine - tune the self - supervised model to determine an upper bound with pretraining .
Experimental Setup
For all approaches , we evaluate representation quality with a multi - way verb classification task . Specif - ically , we predict the verb labels for the 1.5s clips gathered through the crowdsourcing task described in Section 3.3 .
Each input sample X t 1 .. 90 is a 90x10 matrix of position and rotation data , corresponding to 90 frames per clip and 10 spatial features 5 per frame . The output Y is a 24 - dimensional multi - hot vector indicating the whether each of our 24 verb classes apply to the clip .
Approaches
Perceptron . We wish to evaluate the representational capacity of the raw trajectory data itself . To do so , we train a single 24 - dimensional dense layer with sigmoid activation , equivalent to a perceptron for each class . While very simple , this approach gives an idea of how well trajectory data represents verbs as - is , and provides a naive comparative baseline against which to evaluate our more complex pretraining techniques .
Fully Supervised . The fully supervised approach is similar to the perceptron , but adds a dense layer and LSTM layer in - between . This is equiv - Figure 5 : Our pretraining setup . During pretraining , the model learns to encode and represent input timesteps for time - series prediction . To evaluate these learned representations , a perceptron probe is trained on the LSTM outputs , without propagating gradients to the pretrained model . alent to the model shown in Figure 5 , but trained end - to - end without pretraining . The purpose of this approach is to provide an upper bound to the experimental setup without pretraining .
Self - supervised Pretraining . To evaluate the capacity of self - supervised models to represent trajectory data , we pretrain a time - series prediction model on a large unlabeled dataset of 400k sessions . That is , given n input frames X t 1 .. n , the model is trained to predict k output frames Y t n+1 .. n+k . The model consists of a dense layer followed by an LSTM layer unrolled k timesteps , as shown in Figure 5 . We use a discounted mean squared error loss as shown in Equation 1 , which discounts loss by how far it is into the future by factor Î³ .
Î³MSE = n+k t = n Î³ t ( y t âÅ· t ) 2
( 1 )
We tune discount factor Î³ , output length k , model width , and batch size using a grid search on validation performance , resulting in values of 0.85 , 60 , 128 , and 1024 , respectively . Input length n is fixed at 90 to match the length of clips . We consider the concatenated LSTM outputs as the representation of a clip . To evaluate this representation compared to raw trajectory data , we freeze the weights of the pretrained model and , as when evaluating the raw trajectory data , train a perceptron for each class . Fine - tuning . To provide an upper bound for our experimental setup with pretraining , we fine - tune the self - supervised model . This is the same as the previous approach , but allows the gradients in the perceptron step to pass through the entire model .
Results
We report Mean Average Precision on unseen test data for each approach in Table 1 . We compare these to random stratified predictions that are based on the class distribution of the training data .
Perceptron . The perceptron approach evaluates the representational capacity of raw trajectory data as - is , with a lower bound of random stratified and soft upper bound of fully supervised . The perceptron performs relatively well for its simplicity , being only 7 points below the fully supervised upper bound . This suggests that the trajectory data itself encodes a significant amount of verb meaning , but leaves plenty of room for improvement . , the fully supervised model ( which represents a soft upper bound on how well one can do given raw state information without any pretraining ) is visualized as 0 , and all other models are visualized relative to that . Human performance is inner - annotator agreement ( % of workers who agree on the majority label ) .
Self - supervised pretraining . The pretraining + probe approach evaluates the ability of selfsupervised models to encode verb meaning from trajectory data . This is equivalent to the perceptron approach , but with learned hidden representations as input rather than raw trajectory data . The pretrained model does outperform the perceptron , as well as the fully supervised approach . Fine - tuning only improves on this slightly , highlighting that self - supervised pretraining can yield representations that successfully encode verb meaning .
Breakdown by verb . Figure 6 shows a comparison of average precision for each verb . There are some patterns worth highlighting . In particular , we can categorize verbs into three main groups : trivial , tractable , and hard .
Trivial verbs are verbs that can are wellrepresented by trajectory data as - is , i.e. those with high performance with the perceptron approach . These include fall , fall off , fall over and pick up . 6 . Many of these have high agreement , and may be explained by the object 's change in height .
Tractable verbs are those that see significant benefit from pretraining , including slide , roll , throw , toss , put down , turn , flip , and stop . An intuition behind this is that these verbs involve manner distinctions , and in particular , rotations of the object relative to itself . Such information does n't fall directly out of raw state descriptions , but is likely to be well modeled by a pretraining objective that tries to predict the object 's future position .
Hard verbs are those with low performance that do n't benefit much from pretraining . These include bounce , drop , tip , topple , and spin . Many of these are verbs which have lower agreement . Bounce , slap and spin appear to benefit a bit from both pretraining and fine - tuning , suggesting that they may be tractable with similar but more robust pretraining . Tip and topple have fairly high performance , and may almost be categorized as trivial , perhaps being explained by the object 's change in rotation . However , they are noticeably lower than other trivial verbs , despite seeing no benefit from pretraining , suggesting that there is nuance to their meaning in the dataset , which is n't captured by any approach . Finally , drop is a great example of a hard verb , as it is similar to trivial verbs like fall . However , drop involves interaction between the agent and object that is highly agreed upon by annotators , but does n't appear to be captured by our approaches , despite the model receiving both object and agent data . More challenging examples may be able to unveil a similar story for other verbs of interaction like pick up and put down .
Discussion and Conclusion
We test the hypothesis that verb meanings can be grounded in 3D trajectories , i.e. , the position of objects over time . Specifically , we investigate the extent to which representations of object trajectories , learned without any linguistic supervision , naturally encode concepts that align to English verb semantics . Our primary contributions are twofold . First , we build a procedurally generated agent - object - interaction dataset for which we collect crowdsourced annotations . This is the first dataset of its kind , and provides a rich inventory of human judgments about the extensions of 24 verbs of motion . Second , we compare a variety of representation learning approaches , specifically contrasting approaches which operate directly on perceptual inputs to approaches which learn abstractions over the raw perception ( via pretraining ) . We find that some verbs meanings ( e.g. , fall and push ) are captured easily by the raw state information , while others ( e.g. , roll and turn ) require additional processing to be represented well . This work is a first step toward exploring ways to capture fine - grained distinctions in grounded verb semantics that are trivial for humans , but challenging for models . Recent benchmarks at the intersection of NLP , vision and robotics ( Deitke et al . , 2020 ; Shridhar et al . , 2020b ) illuminate unsolved challenges in AI that demand a more robust understanding of verb semantics and spatial reasoning . As these benchmarks continue to be developed , and rich multimodal datasets from technologies like virtual reality become increasingly abundant , we envision that future work in this vein will be especially relevant .
In the future , we plan to explore more sophisticated models for self - supervised pretraining , and evaluate how well these models transfer to more naturalistic language learning settings ( Ebert and Pavlick , 2020 ) . Beyond this , there is a large body of related research questions to be explored . For example , can representations of trajectory data be fused with visually - grounded representations to yield better encodings of verb semantics ? Collaborative efforts will be key to addressing these next milestones in natural language understanding .
Acknowledgments
This work was supported by the DARPA GAILA program . Thank you to George Konidaris , Carsten Eickhoff , Roman Feiman , Jack Merullo , Charles Lovering , Gabor Brody , and the members of the Brown LUNAR lab for helpful discussion and feedback .
A Dataset parameters
The following tables describe the session - level and action - level parameters for our procedural data generation protocol described in Section 3.2 . Friction when object is not moving ( 0 , 1 ) Bounciness Energy retained on bounce ( 0 , 1 )
Peek Across : Improving Multi - Document Modeling via Cross - Document Question - Answering
The integration of multi - document pre - training objectives into language models has resulted in remarkable improvements in multi - document downstream tasks . In this work , we propose extending this idea by pre - training a generic multi - document model from a novel crossdocument question answering pre - training objective . To that end , given a set ( or cluster ) of topically - related documents , we systematically generate semantically - oriented questions from a salient sentence in one document and challenge the model , during pre - training , to answer these questions while " peeking " into other topically - related documents . In a similar manner , the model is also challenged to recover the sentence from which the question was generated , again while leveraging cross - document information . This novel multidocument QA formulation directs the model to better recover cross - text informational relations , and introduces a natural augmentation that artificially increases the pre - training data . Further , unlike prior multi - document models that focus on either classification or summarization tasks , our pre - training objective formulation enables the model to perform tasks that involve both short text generation ( e.g. , QA ) and long text generation ( e.g. , summarization ) . Following this scheme , we pre - train our model -termed QAMDEN -and evaluate its performance across several multi - document tasks , including multi - document QA , summarization , and query - focused summarization , yielding improvements of up to 7 % , and significantly outperforms zero - shot GPT-3.5 and GPT-4 . 1 * Work partly done as an intern at AI2 .
Introduction
Among recent NLP research , multi - document processing is gaining increasing attention , due to the need to handle and process an increasing amount of textual data and available documents online . A Figure 1 : Illustration of our pre - training and data generation . Per a considered set of related documents ( 1 ) which we split into context documents ( 2 ) and a held - out document ( 3 ) , we select the most salient sentence ( 4 ) that is used for generating a question - answer pair ( 5 ) . Then , we pre - train a model by generating the proper answer and the salient sentence , given the question and the context documents ( 6 ) . number of prominent applications that are concerned with aggregating information from multiple texts are multi - document summarization ( Fabbri et al . , 2019 ; , query - focused multidocument summarization ( Xu and Lapata , 2020 ; Pasunuru et al . , 2021a ) , and multi - hop question answering ( Yang et al . , 2018 ; Welbl et al . , 2018 ) . These tasks remain challenging mostly since existing NLP models are designed to handle single texts , rather than processing multiple documents at once .
Early solutions for multi - text processing were task - specific and used complex architectures that were difficult to generalize across different multidocument tasks ( Liu and Lapata , 2019 ; Ginzburg et al . , 2021 ) . Efficient LMs ( Tay et al . , 2021 ; Beltagy et al . , 2020 ) recently demonstrated that by simply concatenating multiple documents into a single sequence , the transformer can offload the goal of identifying and connecting relevant information between the documents . Recently , it was suggested that these long - context LMs can be equipped with new pre - training objectives to enable them to process multiple documents more effectively Xiao et al . , 2022 ; Yasunaga et al . , 2022 ) .
These pre - trained models demonstrated state - ofthe - art performance on a variety of multi - document downstream tasks , and outperformed underlying LMs and task - specific architectures . Such models are often pre - trained using a dataset where each instance is a set of related documents ( e.g. , news articles all discussing a specific event ) , which facilitates modeling of cross - text relationships . Existing multi - document pre - training objectives involve unmasking tokens in a document , or generating a salient masked sentence ( Zhang et al . , 2020 ; Xiao et al . , 2022 ) , encouraging the model to recover missing information using other documents . While successful , these models are either limited to classification tasks or primarily designed for summarization ( Zhang et al . , 2020 ; Xiao et al . , 2022 ) .
In this work , we propose a novel pre - training objective that supports both short and long text generation , resulting in a versatile and general multidocument language model . In particular , we hypothesize that using questions and answers involving multiple documents can encourage the model to better learn and incorporate both fine - grained information ( by asking questions about core information units in a specific sentence ) as well as coarsegrained cross - document relationships required to generate a long text such as a summary . We show that this approach holds not only for summarization , but for other multi - document downstream tasks as well .
During the pre - training of existing multidocument language models , the goal is to unmask spans ( for encoder - only models ) or generate masked textual spans ( for encoder - decoder models ) under a multi - document context . To that end , multiple concatenated sequences of related documents are fed during pre - training , thus requiring a large number of sets of related documents for an effective pre - training phase ( Hoffmann et al . , 2022 ) . In a variety of existing multi - document benchmarks , such as multi - document summarization , only small to medium - scale document clusters are readily available . These are acquired either automatically with lexical similarity and retrieval ( Fabbri et al . , 2019 ) or semi - automatically ( Gu et al . , 2020 ) , but generally , this process requires a substantial amount of human effort for filtering instances and generating high quality corpora .
By employing a novel multi - document question - answer generation procedure , we propose an effective method for expanding the multi - document pre - training corpora . Our approach allows us to provide multiple views for every single cluster of documents , thereby artificially increasing the pretraining data size ( in terms of number of instances ) via augmentation . To expose the model to a variety of contexts and diversify the pre - training data , we propose to generate multiple pairs of questions and answers and condition them on a subset of the documents ' cluster . We select a salient sentence in one held - out document and then employ a recent parser to generate a high - quality question - answer pair about one predicate in the selected sentence , using a systematic semantically - oriented approach ( Klein et al . , 2022 ) . This new multi - document pre - training objective challenges the model to generate both the answer to the question as well as the salient sentence , while discarding the held - out document or parts of it ( see Figures 1 , 2 for illustration ) . This procedure exposes the model to a variety of contexts -a question and a different subset of the documents in the cluster per instance , in contrast to prior methods that provide only a single view of the cluster . Our contributions are summarized below :
â¢ A new pre - training approach for multidocument modeling , formulated as a crossdocument question answering task , further directing the LM to model cross - text relationships , focusing on both fine - and coarsegrained information .
Related Work
Long - context efficient text generation transformers ( Tay et al . , 2021 ( Tay et al . , , 2022 extend earlier transformer models ( Vaswani et al . , 2017 ) for processing long sequences , often using a sparse self - attention architecture . Examples include the Longformer Encoder - Decoder ( LED ) ( Beltagy et al . , 2020 ) , and LongT5 . These models demonstrated that single - text approaches be can adapted to multi - document tasks by concatenat - ing multiple documents into a single sequence and processing them using their sparse attention patterns . They sparsify the full self - attention matrix of transformers by using a combination of a localized sliding window ( called local attention ) , as well as a global attention pattern on a few specific input locations . LED is build upon the BART model ( Lewis et al . , 2020 ) by using additional positional embeddings and global attention weights , and introduces the global attention mode that operates over pre - selected tokens . LongT5 extends the T5 model ( Raffel et al . , 2020 ) by using a similar technique introduced in the ETC and BIGBIRD models Zaheer et al . , 2020 ) , relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens .
Further strategies have been proposed for increasing these models ' abilities in multi - document tasks . The Cross - Document Language Model ( CDLM ) suggested pretraining a Longformer - encoder ( Beltagy et al . , 2020 ) over sets of related documents , and showed superior performance results over several multidocument tasks . Following this methodology , the authors of LinkBERT ( Yasunaga et al . , 2022 ) used a similar approach , but utilized Wikipedia 's hyperlinks in order to curate informative pairs of linked documents for LM pre - training .
In order to adopt the multi - document pretraining approach for sequence - to - sequence tasks , PRIMERA ( Xiao et al . , 2022 ) , which is built on top of the Longformer encoder - decoder model ( LED ) , selected salient sentences within clusters of related documents using a pyramid estimation approach , resembling the method presented for pre - training the single - document PEGASUS model ( Zhang et al . , 2020 ) . While this work is the closest to ours , it was pre - trained to generate masked salient sentences without any control , which makes the model potentially hallucinate while generating text , while our model uses a controlled QA - based objective . Furthermore , unlike these works , our method generates significantly more data then used to pre - train PRIMERA , which is possible to obtain by the singledocument QA generation approach . Our QA pretraining formulation allows us to generate multiple contexts per document cluster .
Another related line of work includes methods that incorporate large - scale QA - generated data for pre - training LMs Jia et al . , 2022 ;
( a ) The held - out document is discarded from the context ( c ) The held - out document is included in the context , but the answer in the anchor sentence is masked ( b ) The held - out document is included in the context , but the anchor sentence is masked
Figure 2 : A schematic of our pretraining data modes . The salient sentence which is used for QA generation is colored in yellow . ( a ) The context does not include the held - out document , therefore this mode is the most challenging . ( b ) The held - out document is present in the context , but the salient sentence used for the QA generation is masked ( red ) . ( c ) The held - out document is present in the context , but the answer span within the salient sentence is masked ( red ) . Huber et al . , 2022 ) . These works hypothesize and show that pre - training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other non - QA downstream tasks . Inspired by that , we conjecture that LMs can strongly benefit from infusing QA during pre - training in the multi - document setup , for adding an additional signal for modelling cross - text relationships .
Augmenting the Multi - Document
Pre - training objective
In this section , we provide the required steps for compiling the pre - training dataset for QAMDEN . We next elaborate on the details of the data creation and provide analysis of the resulted corpus .
Recent works have shown that for text summarization , pre - training LMs to generate a " summarylike " sequence , termed pseudo summary , inherently provides gains over general - purpose pre - trained LMs ( PEGASUS , PRIMERA ; Zhang et al . , 2020 ; Xiao et al . , 2022 ) . The data in which the PEGASUS and PRIMERA models were pre - trained on was constructed using the Gap Sentence Generation ( GSG ) method , which suggests masking highly - ranked salient sentences , where salience is pre - determined by a sentence - scoring method of interest . Particularly , in PEGASUS , GSG has been adopted as its pre - training objective , where some sentences in a single document are masked in the input and the model is tasked to generate them .
Formally , for each sentence s i in a given input document D , PEGASUS computes its salience score based on its ROUGE score ( Lin , 2004 ) w.r.t the rest of the sentences within the document ( D / { s i } ) , i.e. Score ( s i ) = ROUGE ( s i , D / { s i } ) . Intuitively , â¦ Pokemon Sword and Shield might have already been announced , but we now know there 's another new Pokemon game on the way from DeNA â¦ QASem QA generation ( Klein et al . , 2022 )
Selected
Figure 3 : A schematic of the process of QA generation using QASEM ( Klein et al . , 2022 ) and the contextualization model from Pyatkin et al . ( 2021 ) . This is an actual sample that was created and used for pre - training QAMDEN , where the document is taken from New - SHead ( Gu et al . , 2020 ) .
this metric assigns a high score to the sentences that have a high overlap and share more lexical information with the rest of the sentences in the document , thus assigning high scores to prominent sentences . PRIMERA has generalized this notion to support the multi - document setup , by applying a GSG variant over a cluster of related documents .
Cross - Document GSG . We propose augmenting the GSG technique to formulate a cross - document question answering pre - training objective for multidocument tasks , instead of the existing pseudo summary generation methods . Our approach supports identification of both fine - and coarse - grained information as we describe below , and results in a substantially larger amount of pre - training examples compared to the preceding methods .
Formally , we are given a cluster of related documents S = D 1 , D 2 , . . . , D |S| in a corpus C. Our cross - document ( CD ) GSG salience score for the i th sentence within the k th document in the set ( s i k ) , is defined by its ROUGE score w.r.t the rest of the sentences within the document ( D k / { s i k } ) as well as the other documents ( S / D k ) , i.e. CD - GSG - Score ( s i k ) = ROUGE ( s i k , S / { s i k } ) . Then , for every document k , following Zhang et al . ( 2020 ) ; Xiao et al . ( 2022 ) we select the top - scored sentence s * k , and then we use this sentence to generate a pair of a question and an answer .
Generating Cross - Document QAs . For generating the cross - document questions and their answers , we employ QASEM , a recent semantic parsing framework for question generation ( Klein et al . ,
D â â ; 2 for n â 1 to |C| do 3 for k â 1 to |Sn| do 4 s * k â arg max i CD - GSG - Score ( s i k ) ; 5 ( q * k , a * k ) â QASEM ( s * k ) ; 6 t * k = [ a * k , s * k ] # target text ; 7 D â D âª { ( [ Sn / D k , q * k ] , t * k ) } # ( a ) ; 8 D â D âª { ( [ Sn / { s * k } , q * k ] , t * k ) } # ( b ) ; 9 D â D âª { ( [ Sn / { a * k } , q * k ] , t * k ) } # ( c ) ; 10 Return D ;
2022 ) . 2 QASEM intended soliciting a manageable , discrete account of information in a text for the sake of building natural language semantic representations . It automatically labels each verbal predicate - argument relation with a questionanswer pair , where a natural language question represents a semantic role , while the answers correspond to the arguments that appear in the input text . QASEM is thus an appealing approach since it is capable of generating multiple high - quality questions given a sentence . We apply QASEM over the sentences withing the pre - training data in order to generate question - answer pairs , and then apply the model from Pyatkin et al . ( 2021 ) which transforms the question into a more natural and clear form , with contextualized arguments ( see example in Figure 3 ) . In order to resemble a summarization task where the generated text is typically long , we select the question - answer pair with the longest argument produced by QASEM . Formally , QASEM ( â¢ ) receives a sentence s * k as an input , and produces question - answer pair ( q * k , a * k ) , where a * k is the longest among the generated answers . See a detailed example and full description in App . A.1 .
Considering the question - answer pair , our goal is to encourage the LM to generate the correct answer as well as the salient sentence in a multi - document context in order to learn cross - text relationships .
Data Generation Process . In order to facilitate the construction of a multi - document context , we propose three different modes , each one is responsible for uncovering information by using different contexts . For all the modes , we first generate a QA pair out of the most salient sentence in the held - out document .
( a ) Excluding the source document . In this mode we disregard the held - out document D k from the context S n given to the model , i.e , S n / D k . Hence , the model is tasked to predict the answer without having access to the source document at all , and is restricted to observe only the other documents in the set . Thus , this mode is considered as the most challenging one .
( b ) Masking the salient sentence . In this mode , the source salient sentence is masked , i.e , S n / { s * k } . The model has access to the surrounding context of the masked sentence in the held - out document , as well as the other documents in the set .
( c ) Masking the answer . In this mode , only the answer span within the salient sentence is masked , i.e , S n / { a * k } . The model has access to the surrounding salient sentence , as well as all the documents in the set .
As part of the new pre - training process of our novel multi - document model , we append the question after the context and instruct the model to generate an answer followed by its salient sentence , i.e. , output = â¨answerâ© , â¨sentenceâ© , inspired by Bohnet et al . ( 2022 ) . Generating the salient sentence introduces a copying mechanism ( allows the model to also learn to copy information from the source directly ) as well as allowing longtext generation , which is crucial for summarization downstream tasks ( Zhang et al . , 2020 ) , as well as outperforming a model which was pre - trained for generating the answer solely -according to the ablations study , this setup yields the best performance results ( Â§ 4.4 ) . In the pre - training evaluation phase , the held - out set was split and the loss was measured separately for each mode of the data . As expected , we observed that the loss for ( a ) was significantly higher than those for the other modes , with ( a ) â» ( b ) â» ( c ) ranking highest . The procedure for generating the pre - training data is summarized in Algorithm 1 and Figure 2 .
The resulted pre - training corpus . We applied our procedure over the NewSHead corpus ( Gu et al . , 2020 ) , which consists of a set of related documents per instance . This is the exact same pre - training corpus used also by our main baseline PRIMERA ( Xiao et al . , 2022 ) ( Xiao et al . , 2022 ) , when concatenating the documents and the question , we add a special document separator token ( < doc - sep > ) between the documents to signal to the model to be aware of the document boundaries . We also assign the global attention mode to these tokens which enables the model to share information across documents . For further hyperparameter and pre - training execution details , see App . B .
Multi - Document Question Answering
Multi - document QA is the task of generating the correct answer , given a set of related multiple documents . For several multi - document QA benchmarks , models are often tasked to implicitly solve multiple sub - tasks or follow intermediate steps , such as comprehending the question , filtering out distracting documents in the context , and stitching pieces of information across the relevant documents ( Geva et al . , 2021 ; . Recall that QAMDEN was pre - trained over a automatically generated multi - document QA dataset . Hence , as a preliminary assessment , we first investigate QAMDEN 's performance over two multi - document QA benchmarks , HopotQAdistractor ( Yang et al . , 2018 ) and WikiHop ( Welbl et al . , 2018 ) ( see more details of the datasets in App . C.1 ) , and compare to other models that were pre - trained using underling un - masking objectives .
Fine - Tuning Format . To follow our pre - training scheme , we append the question to the context and fine - tune the model to generate the correct answer . We use the Longformer Encoder - Decoder ( LED ) ( Beltagy et al . , 2020 ) and PRIMERA ( Xiao et al . , 2022 ) as the baselines , for assesing the contribution of our pre - trainig format . Confirmed by Beltagy et al . ( 2020 ) , we found out that appending the question : and context : prefixes before the question and the context tokens , respectively , resulted in better performance .
Baselines . We compare QAMDEN ( 447 M parameters ) against a set of strong long - context transformer baselines , including LED ( 447 M parameters ) ( Beltagy et al . , 2020 ) , PRIMERA ( 447 M parameters ) ( Xiao et al . , 2022 ) , 4 and LongT5 - xl ( 3B parameters ) 5 ) ( see Â§ 2 ) . 6 Results . The results on multi - document QA are shown in Table 2 . We adopted the F1 and Exact Match ( EM ) evaluation metrics corresponding to the original works . Our QAMDEN outperforms both PRIMERA , LED , and LongT5 , confirming that our pre - training data and input format are beneficial for both capturing cross - document relationships ( QAMDENâ»LED ) as well as exploiting both context and question ( QAMDENâ»PRIMERA ) .
Multi - Document Summarization ( MDS )
This task aims at generating a summary for a given set of topically - related documents . Inherently , end- Results . Tables 3 and 4 present the evaluation results over the Multi - News and Multi - XScience datasets , respectively . Following previous MDS works , we report the ROUGE R-1 , -2 , and -L scores , which are the standard MDS evaluation metrics ( see App . C.2 for details ) . For a fair comparison , we include the results of PRIMERA as well as the results of the previous state - of - the - art methods ( Pasunuru et al . ( 2021b ) and Lu et al . ( 2020 ) , for Multi - News and for Multi - XScience , respectively ) , and LED ( Beltagy et al . , 2020 ) . As shown in the results tables , QAMDEN exhibits the best performance across most of the examined models and benchmarks , especially on the Multi - News dataset , clearly demonstrating its consistent advan- Pasunuru et al . ( 2021b ) 49.2 19.6 24.5 LED ( Beltagy et al . , 2020 ) 47.4 20.7 23.7 LongT5 - xl 47.4 20.7 23.7 PRIMERA ( Xiao et al . , 2022 ) 49 . tage . This excludes the results for Multi - XScience where QAMDEN slightly underperforms the prior work and LongT5 . An explanation which Xiao et al . ( 2022 ) points refers to the fact that the clusters in Multi - XScience have less overlapping information compared to the corpus we used , attributed to the use of abstracts as the input documents in Multi - XScience . In addition , LongT5 advantage over QAMDEN is attributed to significantly larger number of parameters of LongT5 - xl .
Model R-1 R-2 R - L
Query - Focused Multi - Document Abstractive Summarization
The task of Query - focused Multi - Document Summarization ( QMDS ) aims at generating a summary from a set of documents , that answers a specific given query . Unlike MDS , QMDS tries to solve more realistic query - based scenarios , since it suggests summarizing only predefined salient information of interest that best answers the query . Since we proposed pre - trainng under the multi - document question answering setup , we posit that QAMDEN might be effective for QMDS .
We consider the datasets constructed by Pasunuru et al . ( 2021a ) , QMDSCNN and QMDSIR ( see more details of the datasets in App . C.3 ) as well as their strong baseline , and include also the results of PRIMERA and LED .
Baselines . Similar to the previous experiments , we compare QAMDEN against LED , PRIMERA , LongT5 - xl . In addition , we consider also the baseline from Pasunuru et al . ( 2021a ) . ( Beltagy et al . , 2020 ) 32.3 14.3 30.9 LongT5 - xl 35.5 15.9 34.3 PRIMERA ( Xiao et al . , 2022 ) 36 Results . Tables 5 and 6
Ablation Study
Data Generation . We next turn to a broad ablation study , for assessing our configuration and design choices across our suggested pipeline . First , we show the advantage of combining the three proposed data modes , rather than using a subset of them . We evaluate all the resulted models by fine - tuning them over HopotQA - distractor ( Â§ 4.1 ) , Multi - XScience ( Â§ 4.2 ) , and QMDSIR ( Â§ 4.3 ) . For HopotQA - distractor we report the Exact Match ( EM ) score , and for the summarization tasks we report the ROUGE-1 ( R-1 ) score .
Baselines . We pre - train QAMDEN for 100k steps , for using every subset of the set of the set ( superset ) of modes { ( a ) , ( b ) , ( c ) } ( all its possible combinations ) of the generated pre - training data modes presented in Â§ 3 . Note that our QAMDEN model is referred to as using all the modes , i.e. , Results . Figure 4 shows the ablation results . In all tasks , pre - training using all modes yields the best results . Among all modes , mode ( c ) appears to be the most effective for QA , since this is an extractive QA task , and mode ( c ) provides data in this format . Mode ( a ) excels at the summarization tasks , attributed to their abstractive nature as well as the requirement of all the documents for generating appropriate summaries .
Input Format We repeat the previous experiment and ablate the pre - training input format according to the multiple different formats , and compare to the model pre - training format described in Â§ 3 ( with the same pre - training data ) : without questions , with random question , with random context document , with prefixes , placing the question before the context , with question filtering , and without generating the salient sentence . Additionally , we assess the choice of QASEM as our questionanswer generation module by using the generators from Jia et al . ( 2022 ) and Khashabi et al . ( 2022 ) . Finally , we also include the results of PRIMERA , which was further pre - trained for additional 300k steps ( fine - tuning LED for 400k steps in total ) , for a fair comparison to QAMDEN ablated models . See full details regarding all the ablations in App . D .
Results . Overall , our QAMDEN model outperforms the ablation models on most of the tasks , which a significant margin .
Pre - training the model without any questions during or using random questions , negatively impacts the results of downstream tasks . An impor- tant function of the question is to facilitate the model 's ability to generate the appropriate answer and the source sentence . This aligns with the findings from , who showed that pre - training with random documents rather than related ones is sub - optimal . The use of question and context prefixes for positioning input appears to be helpful for QA , but is inferior when applied to summarization tasks due to its unique format , which is well suited for QA but seems to generalize harder for other setups . When the question is placed before the context , performance slightly decreases over query - based tasks , while maintaining the same results for summarization ( where the question location is irrelevant ) .
Using question filtering is found to harm the downstream results of QAMDEN , in accordance to other QA - based pre - training prior works ( Jia et al . , 2022 ) .
Pre - training without generating the attributed source sentence introduces a significant flow to the model , particularly for the summarization downstream tasks . As mentioned before , generating longer sequences , as well as teaching the model to copy text , is beneficial for summarization tasks .
Applying a different question generator rather then QASEM yields inferior results overall , since the other generators produce open - ended questions and answers which are more prone to errors , while QASEM utilizes an existing span in the context as the answer . In addition , QASEM generated local questions , which allows QAMDEN to focus on the fine - grained details , and not only the coarsegrained information in the multi - document context .
When PRIMERA is pre - trained with 400k steps ( to match QAMDEN 's number of further pretraining steps ) , it underperforms QAMDEN and even fails to add any significant improvements over its 100 K checkpoint , possibly due to the small amount of pre - training data it contains .
Comparison with Large Language Models
In order to get insights into how QAMDEN compares with state - of - the - art Generalist Large Language Models ( LLMs ) , we provide a small comparison with two capable models , GPT-3.5 turbo ( Ouyang et al . , 2022 ) and GPT-4 8 ( OpenAI , 2023 ) ( including the 8k input length version ) evaluated on the zero - shot setting .
For a fair comparison , we used the same context window size of 4 K tokens for all models ( and up to 8k for GPT-4 8k ) . Due to the fact that multidocument tasks involve processing long sequences , the cost of API calls is significant for a comprehensive evaluation across all datasets . Therefore , we only evaluate on a sample of 200 instances from the multi - news dataset ( see prompting details in App . E ) . Table 8 depicts the results . We observe that QAMDEN significantly outperforms both GPT-3.5 and GPT-4 models , though the performance of GPT-4 and GPT-3.5 is comparable . We leave more comprehensive comparisons with LLMs to future work .
We further assessed QAMDEN through manual comparison against PRIMERA , GPT-3.5 , and GPT-4 8k . NLP graduate students were shown summaries for a given topic from the three systems and QAMDEN in arbitrary order , along with a corresponding reference summary . Following ( Ernst et al . , 2022 ) , participants were asked to rank the systems based on Content ( overlap with the reference ) , Readability ( the readability of a summary ) , Grammaticality ( avoiding grammar errors ) , and Non - Redundancy ( avoiding repetitions ) , and we extract the pairwise results out of the rankings ( see ( Ernst et al . , 2022 ) for further details ) . In App . F , we provide several examples to system summaries and their corresponding reference summaries .
The results of this study are presented in Table 9 . Under each evaluation criterion , it indicates the percentage of cases where QAMDEN was preferred over both baselines . QAMDEN was favored in all cases except for grammatical errors and readability ( which corresponds to the Reinforcement Learning from Human Feedback phase of the GPT models ) .
Conclusions
In this work , we present a novel pre - training scheme for multi - document tasks . First , our approach suggests to augment the existing multidocument pre - training objectives into a crossdocument question answering task . Second , we generate high - quality large - scale QA pre - training data using a controlled generation approach , in which each QA pair originates from a salient sentence in one of the documents in the set .
During pre - training , we task the the Longformer Encoder - Decoder ( LED ) model to generate the answer and the salient sentence on the basis of the remaining context . This objective encourages the LED model to elicit cross - document relationships , and stitch pieces of information across the input documents , which are relevant for performing multi - document tasks . The resulted model QAMDEN shows significant performance improvements compared to prior models under extensive experimentation over multiple challenging multidocument summarization and QA datasets .
Future work can extend the ideas in this work for equipping decoder - only large LMs with crossdocument modeling using our proposed method , also in the setup of in - context learning and prompt tuning . We foresee that our method should be significant specifically for retrieval - augmented language modeling setups ( Izacard et al . , 2022 ) , where there is a use of related documents as an outsourced external non - parametric knowledge source . Finally , the use of a single document in order to trigger cross - document relationships , as firstly introduced in this work , might be further investigated .
Limitations
While our work tries to focus around reasoning over both fine - and coarse - grained cross - document relationships , QAMDEN , the resulted pre - trained model , might still suffer from factual consistency errors while generating information given a query , and there is no guarantee that it will always generate factual and reasonable content without any further fine - tuning .
The QASEM question generation model that we used may also have been a source of these problems . There is a possibility that QASEM produces inadequate questions that could harm the pre - training process of the model . An attempt was made to filter out noise using a question model , but the results were inferior to non - filtering . Consequently , if the model is not fine - tuned , inconsistency ( hallucinations ) may occur more frequently .
In addition , by using the Newshead corpus as the pre - training data source , we assume that it is comprised of high quality documents . We also take into account the fact that Newshead is limited to documents in the news domain , while some of the benchmarks used for evaluating QAMDEN include another topics of interest . Future work may further assess the quality of the documents , such as checking for duplications or wrong statements , and diversify the corpus domains . This is crucial for productizing models like QAMDEN in interactive multi - text applications ( chatbots ) and semantic search applications which are gaining attraction nowadays ( Hirsch et al . , 2021 ; Eirew et al . , 2022 ) .
Finally , the resulted model QAMDEN was pretrained on sets of related documents , by answering questions that matched their content . As in an out - of - domain scenario , QAMDEN 's use over sets of documents that are not related , or over single documents , might be unexpected . Such settings may be the subject of another research direction in the future .
Ethics Statement
Despite the limited risk associated with our work , similar to existing state - of - the - art generation language models , there is no guarantee that QAM - DEN , our model , will always generate factual information . The model should therefore be used with caution in a practical environment and be carefully tested before deployment . It is possible , for example , that frequent anecdotal events in the pre - training dataset are generated in an unexpected manner .
A Data Creation
As noted in Â§ 3 , we used the NewSHead corpus ( Gu et al . , 2020 ) . We followed the data pre - processing procedure suggested by Xiao et al . ( 2022 ) which supplied each sentence in the NewSHead corpus with their PEGASUS scores ( Zhang et al . , 2020 ) . 9 A.1 QASEM Details QASEM ( Klein et al . , 2022 ) is a unified tool for parsing sentences into a systematic set of QAs that represent each sentence . The following three types of predication are included in this set : verbs , deverbal nominalizations , and informational discourse relations , and they represent the core units of information in a sentence .
For producing the pre - training data for our QAMDEN model , we specifically targeted the verbal predicates for question - answer generation , since their corresponding training examples origin from the Question Answer driven Semantic Role Labeling ( QA - SRL ) dataset ( He et al . , 2015 ) which covers the largest part of the joint QASEM training data , and obtained the best empirical results during evaluation , compared to the other types ( nominalizations and discourse relations ) . Using the QA - SRL formalism , every predicate - argument relation is labeled with a question - answer pair , and so natural language questions represent semantic roles , while answers correspond to arguments .
QASEM first executes sentence - level preprocessing for QA - SRL by running a part - ofspeech tagger to identify verbs . 10 . Then , the parser itself is based on a fine - tuned T5 - small model ( Raffel et al . , 2020 ) which is given a single marked predicate in context at a time , and is trained on the task of producing the full set of question - answer pairs targeting this predicate . 11 The input sequence consists of the unique task prefix , the sentence , special markers for the target predicate , and the basic verbal - form of the predicate . The output is a set of QAs , and we select one pair according to the length of the answer ( Â§ 3 ) . Since QASEM generates " abstractive " questions that replace arguments with placeholders , we follow Pyatkin et al . ( 2021 ) and use their model to convert the generated question into a more natural form , with contextualized arguments . Overall , we observed that this approach generally improves the quality of the questions , in addition to the contextualization utility . Figure 3 shows an example from our dataset ( based on a salient sentence from NewSHead ( Gu et al . , 2020 ) ) that follows the description provided above .
B Pre - training Technical Details
We pretrain QAMDEN for a total number of 400 K steps ( the validation loss kept decreasing along the entire pre - training process ) , batch size of 16 , Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 3e â 5 and with 10k warmup steps and linear decay , all follows prior works ( Beltagy et al . , 2020 ; Xiao et al . , 2022 ) . The pre - training process takes likely eight days on eight 48 GB RTX8000 GPUs . Since the backbone of both QAMDEN and PRIMERA is the Longformer Encoder - Decoder model ( LED ) ( Beltagy et al . , 2020 ) large version , they all have the same number of parameters ( 447 M ) . LED uses a sparse local+global attention pattern in the encoder self - attention side , while using the full attention on decoder and crossattention .
C Benchmarks Description
In this section , we provide further details regarding the datasets we used for the model and baselines evaluation .
C.1 Question Answering Benchmarks
We first describe in detail multi - document question answering tasks , and particularly the task of multi - hop question answering . Multi - hop question answering involves using a model to gather relevant information from multiple documents and combining it to provide the correct answer .
HotPotQA ( Yang et al . , 2018 ) . This question answering dataset consists of questions and 10 paragraphs from various Wikipedia documents , with two of the paragraphs containing the necessary information to correctly answer the question and eight additional paragraphs serving as distractors . The task involves identifying the correct answer span and identifying supporting evidence sentences . ( For more details on the dataset , see Yang et al . ( 2018 ) . )
WikiHop ( Welbl et al . , 2018 ) . WikiHop is a dataset that includes a question , several potential answers ( ranging from 2 to 79 options ) , and supporting contexts ( ranging from 3 to 63 paragraphs ) , and the correct answer . This dataset does not provide any information about the intermediate steps required to arrive to the correct answer , so models are therefore tasked to deduce these steps based on the provided question and context .
C.2 Multi - Document Summarization Benchmarks
We used https : / / github.com / google - research / googleresearch / tree / master / rouge for computing the ROUGE score ( Lin and Rey , 2004 ) with the default stemmer settings during the evaluation .
Multi - News ( Fabbri et al . , 2019 ) . This dataset is a collection of 56,216 pairs of news articles and professional editors - written summaries , all sourced from the web ( newser.com ) . These pairs include trace - back links to the original documents . The authors of the dataset have also compared it to other datasets in terms of coverage , density , and compression , and found that the it is plausibly diverse compared to other similar benchmarks .
Multi - X - Science ( Lu et al . , 2020 ) . This dataset is sourced from Arxiv and Microsoft academic graphs , where the summaries are paragraphs of related work sections , while source documents include the abstracts of the query and referred papers . It is considered to have fewer positional and extractive biases than the Multi - News dataset , transforming it into a more challenging benchmark since the drawback of getting higher scores for a copied sentence at a specific position can be reduced .
C.3 Query - Focused Multi - Document Summarization Benchmarks
In this section , we describe the pair of datasets from Pasunuru et al . ( 2021a ) that were used in our experiments . Similarly to the multi - document summarization experiments ( Appendix C.2 ) , we used https : / / github.com / google - research / googleresearch / tree / master / rouge for computing the ROUGE score ( Lin and Rey , 2004 ) with the default stemmer settings during the evaluation .
QmdsCnn . This dataset is based on the singledocument CNN / Daily Mail ( CNN / DM ) summarizastion dataset ( Hermann et al . , 2015 ) , where its documents are news articles available online and the summaries are their human written highlights . This dataset is transformed to multi - document one by firstly chunking the documents into small documents of paragraphs . Then , the titles of the articles serve as the queries which are fed to a BM25 search engine ( Robertson and Walker , 1994 ) , that returns chunks from the entire dataset that are related to the title , and serve as the context documents .
QmdsIr . In this datasets , the authors suggested using an alternative to the queries that are based on titles of articles -they use instead queries that are issued by actual search engine users , which is more realistic scenario for search use - cases . They collect queries and their top-10 results obtained by the Bing ( www.bing.com ) search engine . The target summary is derived from the answer passage , which is extracted from one of the top - ranked documents by Bing 's production QA system . Next , they omit the document that contains the answer passage from the context documents .
D Ablation Study Details
In this section , we provide details regarding the baselines used during the input format ablation study that we conducted , and was presented in Â§ 4.4 .
The following list includes the detailed descriptions for all the ablations we used :
â¢ Pre - training without questions . Following Jia et al . ( 2022 ) , we omit the generated question , and pre - train the model to predict the answer with no visible question within the context .
â¢ Pre - training using random questions per context documents . Given context documents , we sample a random held - out document from other clusters , and generate an unrelated question which is use for the irrelevant context . It is an alternative to using a question generated by one of the documents in the context . â¢ Pre - training with prefixes . We add the question : and context : prefixes during training and inference . These should further direct the model with the locations of the question and context . While this setup slightly helps for QA , we show that for MDS , the noprefix setup is preferable .
â¢ Pre - training while placing the question before the context . Recall that QAMDEN appends the question tokens to the end of the input sequence , after the context documents . Therefore , we establish a baseline for ablating this setup , and placing the question at the beginning of the input .
â¢ Pre - training with question filtering . The QASEM parser question generation model can be noisy , resulting in a question that can not be answered or with an incorrect answer to a generated question . We therefore follow a recent automatic QA filtering strategy that suggests using a strong QA model to ensure that valid question - answer pairs are present in the dataset ( Alberti et al . , 2019 ; Fang et al . , 2020 ) . pre - training after questionanswer filtering , using the strong UnifiedQA - v2 model ( Khashabi et al . , 2022 ) that follows previous UnifiedQA ( Khashabi et al . , 2020 ) and trains on more supervised datasets . We took the fine - tuned BART - large ( Lewis et al . , 2020 ) as the question filter for a fair comparison with QASEM . We applied UnifiedQA - v2 over the question - context - answer triplets and took only the answerable questions according to the model , which left us with roughly 25 % of the entire pre - training data .
â¢ Pre - training without generating the salient sentence . Recall that we task QAMDEN to generate the salient sentence which was used to produce the question and answer . This should enable the model to generate longer sequences and improve the coping mechanism , which is useful for tasks such as summarization . This hypothesis is assessed by executing the same pre - training procedure but without generating the salient sentence -only the answer of the generated question .
â¢ Using alternative QA generators from recent related works . We pre - train a model based on the QAs generated by two QA generators , based on the BART - large model ( Lewis et al . , 2020 ) : The first is taken from Jia et al . ( 2022 ) 12 , which trained a model over the data from the MRQA 2019 Shared Task ( Fisch et al . , 2019 ) and the second is the QA generator from ( Khashabi et al . , 2022 ) which was trained on eight different QA benchmarks ( see full list and references in Khashabi et al . ( 2022 , Appendix A ) ) .
â¢ Additional pre - training for PRIMERA ( Xiao et al . , 2022 ) -We resume the pre - training of the 100k publicly released checkpoint of PRIMERA , and pre - train for an additional number of 300k steps ( using the same pre - training format and procedure described in Xiao et al . ( 2022 ) ) , to reach the number of steps used for pre - training QAMDEN and its ablations described above .
E API - Based Models Prompting Details
We manually explored several prompts for the GPT-3.5 and GPT-4 chat API - based models , and proceeded with the one that appeared to be the most effective for zero - shot multi - document summarization , as follows .
Per a Multi - News example where we are given k context documents D 1 , D 2 , . . . , D k , we prompt each model to provide an summary using the system format : " You are a helpful assistant that summarizes important information from multiple documents . " , and the user format : " Summarize the following documents into a single summary : Document 1 : D 1 Document 2 : D 2 . . . Prince William and wife Kate Middleton have adopted a new addition to the family : a cocker spaniel puppy . The Telegraph reports that the couple has adopted a baby boy , but it 's not a baby . The puppy is just a few months old and is the son of Kate 's mother 's dog , Ella . " William and Catherine fell in love with the pup instantly and it was n't long before they decided to keep him , " a palace aide tells US Weekly . " He 's now part of the royal fold . "
A rush - hour collision between a Chicago Transit Authority bus and several other vehicles yesterday left one person dead and at least eight others injured , one of them critically , authorities say . The accident occurred around 6 pm in the north Loop . Authorities say the articulated Route 148 Clarendon / Michigan Express bus collided with at least three other vehicles at Michigan Avenue and Lake Street . The bus went onto the sidewalk , and at one point a pedestrian was pinned underneath . She was taken away covered in a sheet , a witness who ran to help tells the Chicago Tribune . NBC Chicago describes the fatality as a 51 - year - old woman . The driver , who was treated for non - life - threatening injuries , was the only person on the bus , and investigators are looking at video from a camera that records the interior of the bus .
A Chicago Transit Authority bus was involved in a serious crash during rush hour , resulting in one fatality and eight injuries . The bus collided with several other vehicles at North Michigan Avenue and East Lake Street . The bus driver has been cited for failing to stop at a red light and for " failure to exercise due care . " The accident is still under investigation . The deceased has been identified as 51 - year - old Aimee Coath of Flossmoor . The eight other individuals , including the bus driver , were hospitalized with non - life - threatening injuries .
A bus crash in downtown Chicago last night left one person dead and eight others injured , including the bus driver , at least 10 ambulances were called to the scene , reports NBC Chicago . The fatality has been identified as 51 - year - old Aimee Coath of Flossmoor , reports the Chicago Tribune . Coath was the only person on the Chicago Transit Authority bus at the time of the crash .
Geez , the French are even sophisticated while performing wanton acts of destruction . The Verge reports a young man was caught on video calmly and methodically wrecking up an Apple Store in France over a refund disagreement . The man used a steel ball -- apparently the kind used in a French lawn game -- to break at least 10 iPhones and a MacBook Air , one at a time , before being arrested outside the store . " Apple is a company that violated European consumers ' rights , " the Daily Dot quotes the man as saying in French during his iPhone smashing . " They refused to reimburse me . I told them : ' Give me my money back . ' They said no . So you know what 's happening ? This is happening ! " An Apple Store in Dijon , France was vandalized by an irate customer who used a steel ball to smash iPhones , MacBooks , and iPads . According to reports , the customer was in a dispute with Apple over a refund and claimed that the company violated European consumers ' rights . He was eventually apprehended by security and arrested after causing significant damage to the store .
A video of an angry man destroying everything in a French Apple Store is making the rounds on the Internet is making headlines , and it 's not for the first time . The video shows a man hurling a steel ball through a store 's windows , smashing everything in sight , and then calmly waiting for security to come and stop him , reports the BBC . The man , who is in his 20s , is identified as a French citizen who lives in the Paris suburb of Montpellier . He was caught on surveillance video at the store on Wednesday .
Acknowledgements
The work described herein was supported by the PBC fellowship for outstanding PhD candidates in data science , in part by grants from the Israel Science Foundation grant 2827 / 21 , and by a grant from the Israel Ministry of Science and Technology .
F System Summary Examples of In Table 10 , we include three examples of system summaries produced by GPT-3.5 and QAMDEN , as well as the corresponding reference ( groundtruth ) summary . In general , QAMDEN 's summaries are more concise , include less redundant information , do not include anecdotal information , and overall were preferred by the human evaluators .
G List of Software and Data Licences Used in this Work
Our code will be released and licensed under the Apache License 2.0 license . Our framework dependencies are :
â¢ PRIMERA : https : / / github.com / allenai / PRIMER / blob / main / LICENSE , under an Apache License 2.0 . â¢ NLTK : https : / / github.com / nltk / nltk , under an Apache License 2.0 .
â¢ NumPy : https : / / github.com / numpy / numpy / blob / main / LICENSE . txt , under a BSD 3 - Clause " New " or " Revised " License .
â¢ seaborn :
https : / / github.com / mwaskom / seaborn / blob / master / LICENSE.md , under a BSD 3 - Clause " New " or " Revised " License .
â¢ openai :
https : / / github.com / openai / openai - python / blob / main / LICENSE , under a MIT License . B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ? Not applicable . Left blank .
B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Not applicable . Left blank .
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Not applicable . Left blank . B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . Section 3 .
C Did you run computational experiments ?
Section 4 .
C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Section 4 , Appendix B .
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022 , with the addition of a question on AI writing assistance .
Just Like a Human Would , Direct Access to Sarcasm Augmented with Potential Result and Reaction
Sarcasm , as a form of irony conveying mockery and contempt , has been widespread in social media such as Twitter and Weibo , where the sarcastic text is commonly characterized as an incongruity between the surface positive and negative situation . Naturally , it has an urgent demand to automatically identify sarcasm from social media , so as to illustrate people 's real views toward specific targets . In this paper , we develop a novel sarcasm detection method , namely Sarcasm Detector with Augmentation of Potential Result and Reaction ( SD - APRR ) . Inspired by the direct access view , we treat each sarcastic text as an incomplete version without latent content associated with implied negative situations , including the result and human reaction caused by its observable content . To fill the latent content , we estimate the potential result and human reaction for each given training sample by [ xEffect ] and [ xReact ] relations inferred by the pre - trained commonsense reasoning tool COMET , and integrate the sample with them as an augmented one . We can then employ those augmented samples to train the sarcasm detector , whose encoder is a graph neural network with a denoising module . We conduct extensive empirical experiments to evaluate the effectiveness of SD - APRR . The results demonstrate that SD - APRR can outperform strong baselines on benchmark datasets .
Introduction
Sarcasm , as subtle figures of speech , serves many communicative purposes in human daily life ( Ivanko and Pexman , 2003 ) , commonly used to criticize an individual . Refer to the formal description of sarcasm from the Oxford English Dictionary : 1 " A way of using words that are the opposite of what you mean in order to be unpleasant to somebody or to make fun of them . "
The sarcastic text is typically characterized as an incongruity between the positive surface and negative situation ( Riloff et al . , 2013 ; . For example , as an obvious sarcasm " I love working for six hours every day for free " , its surface meaning tends to be positive , conveyed by the sentiment word " love " , but it corresponds to a negative situation " work for free " , conveying people 's complaint .
Detecting sarcasm from social media is a significant task due to the universal existence of sarcasm , but its complicated nature makes the task challenging . To resolve this task , the community has recently proposed a number of Sarcasm Detection ( SD ) methods , whose major idea is to capture the incongruity characteristic of sarcasm ( Joshi et al . , 2017 ; Xiong et al . , 2019 ; Pan et al . , 2020 ; Agrawal et al . , 2020 ; Li et al . , 2021b ; Lou et al . , 2021 ) . For example , several early SD studies express the incongruity by extracting positive - negative pairs from observable text content , such as rule - based method ( Joshi et al . , 2017 ) and neural networks with co - attention tricks ( Xiong et al . , 2019 ; Pan et al . , 2020 ) . Unfortunately , those methods can not accurately capture the negative situations , which are mostly implied and associated with contexts and background information . To alleviate this issue , the recent arts of SD express the negative situations with external knowledge bases . From the perspective of sentiments , some SD methods employ auxiliary affective lexicons , e.g. , SenticNet ( Cambria et al . , 2020 ) , to estimate the implied affective correlations among words and phrases of samples ( Agrawal et al . , 2020 ; Lou et al . , 2021 ) . Additionally , the SarDeCK method ( Li et al . , 2021b ) employs the pre - trained commonsense reasoning tool COMET ( Hwang et al . , 2021 ) to infer the relations behind samples as their implied situations . Despite the promising performance , their expressions of implied negative situations are still a bit abstract and impalpable . As complicated figures of speech , we are particularly interested in how do human beings accurately identify sarcasm ? Through referring to the prior psychological , cognitive , and linguistic literature ( Gibbs , 1986 ; W.Gibbs , 2002 ; Ivanko and Pexman , 2003 ) , we are agreeable with two significant viewpoints . First , the negative situations of sarcasm are mostly associated with certain social events ( Pickering et al . , 2018 ) , and human beings can often easily identify the events with the background information in the brain . Second , from the direct access view ( Giora and Fein , 1999 ; W.Gibbs , 2002 ; Ivanko and Pexman , 2003 ) , human beings are likely to directly understand the whole sarcastic text with both literal meanings and implied negative situations , which can be easily captured by them .
Based on the analysis , what we expect is to develop a novel SD method by simulating the way of human thinking . Inspired by the direct access view , we treat each sarcastic text as an incomplete version without latent content associated with implied negative situations . We can use the associated social events to express the negative situations due to their strong connection . Further , we assume the social events can be mainly expressed by the potential results and human reactions that the events produced ( see examples in Table 1 ) . Accordingly , for each given sample we can estimate its potential result and human reaction by pre - trained commonsense reasoning tools ( acted as background information ) , and then integrate the observable text content with them as an augmented sample ( acted as the whole text ) . Finally , we can use those augmented samples to train the sarcasm detector ( just like a human would ) .
Upon these ideas , we propose a novel SD method , namely Sarcasm Detector with Augmentation of Potential Result and Reaction ( SD - APRR ) . Specifically , we estimate the potential result and human reaction for each training sample by [ xEffect ] and [ xReact ] relations inferred by the auxiliary commonsense reasoning tool COMET ( Hwang et al . , 2021 ) , and then integrate the sample with them to generate an augmented one , dubbed as event - augmented sample . By analogy to ( Lou et al . , 2021 ; Liang et al . , 2022 ) , we assume that the syntactic information of eventaugmented samples can intuitively imply the incongruity of sarcasm . Accordingly , we transform each event - augmented sample into a dependency graph ( Nivre , 2003 ) , and suggest a graph - based encoder to generate sample embeddings . Additionally , to resolve the noisy results and reactions inferred by COMET , we suggest a denoising module with the dynamic masking trick ( Yang et al . , 2021 ) , enabling to improve the quality of sample embeddings . With those embeddings , a single - layer MLP is used as the sarcastic classifier finally . To examine the effectiveness of SD - APRR , we conduct extensive experiments on benchmark datasets . The empirical results demonstrate that SD - APRR can outperform the existing baseline methods .
The contributions of this work can be summarized as follows :
â¢ We propose a novel SD method , named SD - APRR , with event - augmented samples formed by the auxiliary commonsense reasoning tool COMET .
â¢ We suggest a graph - based encoder with a denoising module , enabling to generate strong sample embeddings .
â¢ The experimental results indicate that SD - APRR can achieve competitive performance compared with existing baselines .
2 Related Works
Sarcasm Detection
Early SD methods are mostly based on special rules and evidence ( Maynard and Greenwood , 2014 ; Bharti et al . , 2015 ; Riloff et al . , 2013 ) . For instance , the study ( Maynard and Greenwood , 2014 ) treats the hashtag sentiment as the key indicator of sarcasm since the hashtags are usually taken to highlight sarcasm in Tweets ; and other methods employ various evidence , such as parser - based negative phrase matching , interjections ( Bharti et al . , 2015 ) , and positive - negative word pairs ( Riloff et al . , 2013 ) . Some other methods form incongruity - specific embeddings for sarcastic texts , such as shape and pointedness of words ( PtÃ¡Äek et al . , 2014 ) , extensions of words ( Rajadesingan et al . , 2015 ) , and unexpectedness ( Reyes et al . , 2012 ) . Due to the success of neural networks , the mainstream SD methods nowadays apply them to capture the incongruity between positive surface and negative situations within the sarcastic text . Early methods mainly capture the incongruity from the observable text content ( Tay et al . , 2018 ; Xiong et al . , 2019 ; Pan et al . , 2020 ) . For instance , the methods of ( Xiong et al . , 2019 ; Pan et al . , 2020 ) extract positive - negative word pairs and phrase pairs with co - attention tricks . However , those methods can not fully understand the negative situation due to its implicit nature . To resolve this issue , the recent methods employ external resources to capture negative situations and further incongruities of sarcastic texts ( Agrawal et al . , 2020 ; Lou et al . , 2021 ; Li et al . , 2021b ; . For example , the ADGCN method ( Lou et al . , 2021 ) employs the affective lexicon SenticNet ( Cambria et al . , 2020 ) to represent intra - sentence affective relations ; and the DC - Net method exploits sentiment lexicon to separate literal meanings from texts and further estimates sentiment conflicts . Orthogonal to the aforementioned methods , our SD - APRR forms augmented samples by commonsense reasoning and treats the augmented ones as the whole versions of sarcastic texts from the direct access view ( Giora and Fein , 1999 ; W.Gibbs , 2002 ; Ivanko and Pexman , 2003 ) .
Commonsense Knowledge Graph
Large - scale commonsense knowledge graphs ( Lin et al . , 2019 ; Yin et al . , 2022 ) can conduct reasoning for texts to infer the commonsense knowledge behind them , and they have been widely applied to a wide range of natural language processing tasks , such as dialogue generation ( Sabour et al . , 2022 ) , relation classification ( Hosseini et al . , 2022 ) , and emotion recognition ( Li et al . , 2021a ) . To our knowledge , some representatives include Concept - Net ( Speer et al . , 2017 ) , ATOMIC ( Sap et al . , 2019 ) , and TransOMCS ( Zhang et al . , 2020 ) . The Con-
The Proposed SD - APRR Method
In this section , we briefly describe the task definition of SD and the commonsense reasoning tool COMET . We then introduce the proposed SD - APRR method in more detail . For clarity , we summarize the important notations in Table 2 . The outputs of the two relations can be directly used as the auxiliary augmentation in SD - APRR . We declare that the COMET takes the large version of BART ( Lewis et al . , 2020 ) as the backbone , which contains 24 layers , 1024 - dimensional hidden embeddings , and 16 heads for self - attention . 2 Then it was fine - tuned over ATOMIC 20 20 .
Overview of SD - APRR
As depicted in Fig . 1 , our SD - APRR mainly consists of three components . ( 1 ) Event - augmented samples generation : For each raw text s i , we employ COMET to infer its result e r i and human reaction e h i , and then concatenate them to form the corresponding event - augmented sample s e i .
( 2 ) Maksed graph - based encoder : For each event - augmented sample s e i , we transform it into a dependency graph G i , and encode G i as the sample embedding z i by leveraging a graph neural network encoder with dynamic masking . ( 3 ) Sarcastic classifier : With z i , we predict the category label by employing a singlelayer MLP finally . In the following , we introduce each component of SD - APRR in more detail .
e r i = { w i1 , â¢ â¢ â¢ , w iM } and e h i = { w i1 , â¢ â¢ â¢ , w i M }
as the result and human reaction of the implied social event behind s i . We then concatenate them to form its event - augmented version . For semantic coherence , we further leverage two linkers l r and l h , where l r denotes " then may " for e r i and l r denotes " and I feel " for e h i . Accordingly , the final event - augmented sample is formed by
s e i = s i â l e â e r i â l h â e h i ,
Masked Graph - based Encoder
Given event - augmented samples { s e i } N i=1 , we suggest a masked graph - based encoder to induce their embeddings { z i } N i=1 .
Constructing Graphs of Samples
By analogy to ( Lou et al . , 2021 ; Liang et al . , 2022 ) , we assume that the syntactic information of eventaugmented samples can intuitively imply the incongruity of sarcasm . Accordingly , we transform each s e i into an undirected graph G i = { V i , E i } with the off - the - shelf dependency parsing tool , 3 where V i is the set of nodes , i.e. , the tokens occurring in s e i , and E i is the set of edges computed by dependency parsing . Define A i â { 0 , 1 } M e ÃM e as its corresponding adjacency matrix , and 1 / 0 denotes the component corresponds to an edge or not . Besides , each node is with self - loop .
Initializing Node Embeddings
For each G i , we initialize its node embeddings
H ( 0 ) i = [ h ( 0 ) i1 , â¢ â¢ â¢ , h ( 0 )
iM e ] â¤ by leveraging a singlelayer Bi - LSTM ( Hochreiter and Schmidhuber , 1997 ) . Specifically , we represent the nodes
X i = [ x i1 , â¢ â¢ â¢ , x iM e ]
â¤ by the pre - trained GloVe word embeddings , and then feed X i into the Bi - LSTM as follows :
H ( 0 ) i = Bi - LSTM ( X i ; W b ) , ( 1 )
where W b is the trainable parameter of Bi - LSTM .
Learning Sample Embeddings with Dynamic Masking
Given each pair { G i , H
i } , we optimize the node embeddings H
( l ) i = [ h ( l ) i1 , â¢ â¢ â¢ , h ( l )
iM e ] â¤ by a Llayer graph neural network encoder with dynamic masking ( Yang et al . , 2021 ) , and then form the final sample embedding z i by leveraging the readout operator with H i .
To be specific , the learning process of node embeddings for each layer is formulated below :
h ( l ) ij = ReLU W ( l ) n m ( l ) ij h ( lâ1 ) ij â h ( lâ1 ) N ( ij ) , j = 1 , â¢ â¢ â¢ , M e , l = 1 , â¢ â¢ â¢ , L , ( 2 )
where
W n = { W ( l )
n } L l=1 are the trainable parameters ; m ij â [ 0 , 1 ] is the mask weight of the j - th node , used to capture the possible noisy e r i and e h i inferred by COMET ; N ( ij ) denotes the neighbor set of the j - th node ; and h
( lâ1 ) N ( ij ) = kâN ( ij ) m ( lâ1 ) ik h ( lâ1 ) ik
is the weighted sum of the neighbors of the j - th node .
The update process of the mask weights for each layer is formulated below :
m ( l ) ij = Sigmoid W ( l ) mÄ¥ ( lâ1 ) ij â W ( l ) f h ( lâ1 ) N ( ij ) j = 1 , â¢ â¢ â¢ , M e , l = 1 , â¢ â¢ â¢ , L , ( 3 ) where W m = { W ( l ) m } L l=1 and W f = { W ( l ) f } L l=1
are the trainable parameters ; andÄ¥
( lâ1 ) ij = m ( lâ1 ) ij h ( lâ1 ) ij .
After obtaining the node embeddings H ( L ) i of the last layer , we can form the sample embedding z i by leveraging the readout operator as follows :
z i = 1 M e M e i=1 h ( L ) i ( 4 )
Sarcastic Classifier and Training Objective
Given the sample embeddings { z i } N i=1 , we employ a single - layer MLP as the sarcastic classifier . For each z i , we predict its category labelÅ· i by the following equation :
y i = Softmax ( W c z i ) , ( 5 )
where W c is the trainable parameter of the sarcastic classifier .
Consider N training pairs { ( z i , y i ) } N i=1 , we can formulate the full objective of SD - APRR with respect to all trainable parameters
W = { W b , W n , W m , W f , W c } : L ( W ) = N i=1 L CE ( y i , Å· i ) + Î»â¥Wâ¥ 2 , ( 6 )
where L CE is the cross - entropy loss ; â¥ â¢ â¥ denotes the â 2 -norm ; and Î» â [ 0 , 1 ] is the regularization coefficient .
Experiment
Experimental Settings
Datasets . To thoroughly evaluate the performance of SD - APRR , we conduct experiments on four publicly available SD datasets with different scales . Their statistics of are shown in Table 3 , and they are briefly described below :
â¢ SemEval18 is collected in SemEval 2018 Task 3 Subtask A ( Van Hee et al . , 2018 ) .
â¢ iSarcasm ( Oprea and Magdy , 2020 ) consists of tweets written by participants of an online survey and thus is for intented sarcasm detection .
â¢ Ghosh ( Ghosh and Veale , 2016 ) is collected from Twitter and leverages hashtag to automatically annotate samples .
â¢ IAC - V2 ( Abbott et al . , 2016 ) is sourced from online political debates forum . 4 Compared with other datasets , the samples of IAC - V2 are relatively longer and more normative . Baselines . We select a number of recent baseline methods for comparison . They are briefly described below :
â¢ NBOW : A traditional SD method that represents samples by the averages of word embeddings .
â¢ Bi - LSTM : A SD method that sequentially encodes sarcastic texts with Bi - LSTM .
â¢ SIARN and MIARN ( Tay et al . , 2018 ) : Two RNN - based SD methods that capture the incongruity by using the single - dimensional and multi - dimensional intra - sentence attentions , respectively . We implement in - house codes .
â¢ SAWS 5 ( Pan et al . , 2020 ) : A CNN - based SD method that cuts each text sample into snippets and uses self - attention to re - weight them .
â¢ ADGCN 6 ( Lou et al . , 2021 ) : A GCN - based SD method that builds affective and dependency graphs with SenticNet to capture the incongruity in a long distance .
â¢ DC - Net 7 : A BERT - based SD method that respectively encodes literal 5 https : / / github.com / marvel2120 / SAWS 6 https : / / github.com / HLT-HITSZ / ADGCN 7 https : / / github.com / yiyi-ict / dual - channel - for - sarcasm meanings and implied meanings by the external sentiment lexicon .
â¢ SarDeCK 8 ( Li et al . , 2021b ) A BERT - based SD method that uses the COMET to derive dynamic commonsense knowledge and fuses the knowledge to enrich the contexts with attention .
Implementation details . In the experiments , except the BERT - based methods , we apply the 300dimensional GloVe embeddings 9 to represent the words initially . The dimension of the Bi - LSTM output is set to 300 , and the layer number of the masked graph - based encoder is set to 3 . For all neural network - based methods , the batch size is set to 32 . We take Adam as the optimizer , and the learning rate is set to 0.001 . The regularization coefficient Î» is set to 0.01 . Besides , we use the Xavier Uniform to initialize the parameters . For the BERTbased methods , the number of training epochs is set to 6 , while for other methods , the epoch number is fixed to 100 with an early stopping mechanism ( Lou et al . , 2021 ) . In terms of all datasets , the splitting of training and testing is shown in Table 3 . We independently run all comparing methods 5 times and report the average results .
Results and Analysis
The main results of all comparing methods are reported in Table 4 , and we draw the following observations : ( 1 ) First , it can be clearly seen that our SD - APRR can achieve the highest scores of both Accuracy and Macro - F1 in most settings , where it ranks the first over SemEval18 , iSarcasm , and IAC - V2 , and ranks the second over Ghosh .
( 2 ) Second , we observe that SD - APRR mostly outperforms the recent strong baseline SarDeCK , which also employs COMET to generate auxiliary commonsense relations . A major difference between SarDeCK and SD - APRR is that the former integrates training samples with their corresponding commonsense results of COMET at the embedding level , while the latter treats the augmentations of raw training texts and inferred commonsense results of COMET as the whole raw texts . So the improvements to SarDeCK indirectly indicate that the direct access view may be a better perspective for SD . ( 3 ) Third , compared with ADGCN that is also based on graph neural networks , our SD - APRR achieves significant improvements over all datasets . This indicates that leveraging contextually inferred results and reactions can be a more efficient way for SD than leveraging context - free affective lexicons in a static way . ( 4 ) Finally , SD - APRR , ADGCN , DC - Net , and SarDeCK consistently perform better than NBOW , Bi - LSTM , SIARN , MIARN , and SAWS , the methods without external resources . The results support the previous statement that understanding sarcasm heavily relies on human background information .
Ablation Study
We conduct ablation studies to examine the effectiveness of the augmentations of results , augmentations of human reactions , and the denoising module .
The results are reported in Table 5 . Overall , when removing the results ( w / o Result ) and the reactions ( w / o Reaction ) , the performance of SD - APRR show a decline on all datasets . This indicates that the potential results enable the SD - APRR to have extra explainable contexts to understand the negativity inside the negative situations . Meanwhile , human reactions provide explicit emotional clues that can be related to the negative situations during graph learning . However , when removing the denoising module ( w / o Masking ) , the performance of SD - APRR decreases across the IAC - V2 dataset . This is because samples in the Ghosh are short texts , and their syntactical information may not be accurately captured , leading the masked graph - based encoder skips nodes related to the sarcasm by mistake . Additionally , we replace the masked graphbased encoder with BERT ( Devlin et al . , 2019 ) , and further compare this BERT - based version of SD - APRR with its ablative versions ( w / o Result and w / o Reaction ) . Due to the space limitation , we report the results on two datasets , i.e. , Ghosh with relatively more training samples and IAC - V2 with longer text lengths . The results are shown in Table 6 . We can observe that the full version performs the best compared with the ablative versions . These results further indicate the augmentations of results and human reactions inferred by COMET can improve the classification performance even with a different encoder .
The Impact of Layer Numbers of the Masked Graph - based Encoder
We now investigate the impact of the layer number L of the masked graph - based encoder across benchmark datasets . We present the results with different values of L â { 1 , 2 , 3 , 4 , 5 } in Fig 2 . We can observe that SD - APRR performs the best results across the SemEval18 dataset when L = 1 , while achieving the best results across the other datasets when L = 3 . The reason may be that the positive surfaces and the negative situations in the SemEval18 dataset are close to each other on the dependency graph , so the two terms can be associated together through low - order message - passing . While for the other three datasets , SD - APRR requires higher - order message passing to model the incongruity between the two terms . In practice , we suggest L = 3 as the default setting .
Visualization of Mask Weights .
To qualitatively visualize the impact of mask weights , we randomly select several examples and show the words with lower mask weights of the final layer of the masked graph - based encoder . The visualization is shown in Table 7 . We use the red color to demonstrate the word tokens with lower mask weights . From the table , we observe that the encoder can effectively eliminate semantically irrelevant tokens , such as " gets fired " and " see doctor " , and wrong speakers ' reactions , such as the term of " happy " in the second and the third cases .
Besides , we observe that some sarcasm - irrelevant parts in the original texts can also be captured , e.g. , the stop words " on " , " to " , " is " .
Conclusion and Limitations
In this paper , we propose a novel SD method , entitled SD - APRR , which expresses negative situations of sarcasm by the potential results and human reactions of the associated events . We employ the COMET to estimate the results and human reactions , and form event - augmented samples with them . We employ those augmented samples as the whole sarcastic texts from the direct access view . We suggest a masked graph - based encoder , enabling to generate discriminative sample embeddings . Experimental results demonstrate that our SD - APRR can achieve competitive performance compared with the existing baseline methods .
We demonstrate two limitations : ( 1 ) The datasets used in this work are mostly collected from social media . In the future , we plan to collect sarcastic texts from various sources , such as the literature and films , and conduct more experiments with them . ( 2 ) Our exploration of sarcasm theories still has some space to improve . Though the incongruity theory is the mainstream in the community , there are other theories worthy to investigate in the future . dation of China ( No.62076046 ) , and the Young Scientists Fund of the National Natural Science Foundation of China ( No.62006034 ) .
Acknowledgment
We would like to acknowledge support for this project from the National Natural Science Foun-
B2 . Did you discuss the license or terms for use and / or distribution of any artifacts ? Left blank .
B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ?
Left blank .
B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Left blank .
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Left blank .
B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be .
Left blank .
C Did you run computational experiments ?
Left blank .
C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Left blank .
BLINK with Elasticsearch for Efficient Entity Linking in Business Conversations
An Entity Linking system aligns the textual mentions of entities in a text to their corresponding entries in a knowledge base . However , deploying a neural entity linking system for efficient real - time inference in production environments is a challenging task . In this work , we present a neural entity linking system that connects the product and organization type entities in business conversations to their corresponding Wikipedia and Wikidata entries . The proposed system leverages Elasticsearch to ensure inference efficiency when deployed in a resource limited cloud machine , and obtains significant improvements in terms of inference speed and memory consumption while retaining high accuracy .
Introduction
Companies that offer VoIP telephony products with built - in speech and natural language processing features aim to assist the customer support agents with information relevant to the content of their conversations with the customers . To be useful , such assistance should be provided in near realtime of the triggering utterance . In this paper , we demonstrate how we build a near real - time entity linking system at Dialpad 1 to link the entities in business phone transcripts to a knowledge base to provide more semantically - informed assistance .
The entity linking task is usually comprised of three steps : ( i ) detect the mentions in the given text , ( ii ) generate a list of candidate entities relevant to each mention , and finally ( iii ) link each mention to its most relevant entry in the knowledge base ( Ravi et al . , 2021 ) . Note that entity linking systems used in production should provide the optimum performance in terms of both inference speed and memory consumption while being used within a limited computational budget . Since there are millions of entities stored in a knowledge base , the 1 https : / / www.dialpad.com / scaling issue is a major concern while developing a real - time entity linking system .
The goal of this research is to develop a neural entity linking system to efficiently link product and organization type entities in business phone conversations to their respective entries in a knowledge base for information extraction . For that purpose , we present an extended version of the stateof - the - art neural entity linker , the BLINK model ( Wu et al . , 2020 ) . Though BLINK was originally proposed for entity linking on Wikipedia , we extend it for entity linking on Wikidata 2 since unlike Wikipedia , the Wikidata knowledge base contains information related to the entities in a structured way . Thus , it allows effective extraction of relevant information for each entity . More importantly , for production deployment , we also introduce several new techniques that significantly reduce the memory requirements , computational resource usage , and the inference speed of BLINK . More concretely , our major contributions are stated below :
â¢ We tackle the computational complexities in BLINK by saving all pre - trained entity embeddings in Elasticsearch 3 and propose a word matching technique to retrieve the candidate entities faster . We also present an approach to pre - compute the linking between the Wikipedia page of each entity to its respective Wikidata page to reduce the runtime latency .
â¢ Extensive experiments show that our entity linking system significantly reduces the inference time and memory requirements while retaining high accuracy in a computationally inexpensive machine . We also successfully deploy our entity linking system in a 10 GB RAM machine ( without GPU ) whereas the original model requires a machine in our server having 60 GB RAM for inference .
Figure 1 : The Proposed Entity Linking System . First , our Internal NER model detects the mention in the given text . Then we retrieve a list of candidate entities with their embeddings from Elasticsearch . At the same time , we generate the contextualized representation of the input text using the pre - trained BLINK embeddings . Afterward , we utilize the pre - trained BLINK Bi - Encoder to determine the entity that is the most relevant among the candidates and finally we extract information related to that entity from our knowledge base in Elasticsearch .
Related Work
Prior work on entity linking mostly focused on linking named entities to unstructured knowledge bases like Wikipedia , whereas the amount of work that used a structured knowledge base like Wikidata is very limited ( Shen et al . , 2014 ; Sakor et al . , 2020 ) .
Though other knowledge bases like DBpedia ( Auer et al . , 2007 ) or YAGO ( Fabian et al . , 2007 ) have also been studied , the utilization of Wikidata as the knowledge base to extract relevant information has gained lots of attention recently ( Lin et al . , 2021 ; MÃ¶ller et al . , 2021 ) . Detecting mentions ( i.e. , entities ) in the given text ( Huang et al . , 2015 ; Akbik et al . , 2018 ) is an important step for entity linking . In recent years , utilizing the neural network architecture for mention detection has been extensively studied ( Wu et al . , 2020 ; Onoe and Durrett , 2020a ) . More recently , the impressive success of the transformer architecture ( Vaswani et al . , 2017 ; Devlin et al . , 2019 ; Yamada et al . , 2020 ) in a wide range of natural language processing tasks has also inspired researchers to apply transformer models for the entity recognition ( Lin et al . , 2021 ) step in entity linking ( Ravi et al . , 2021 ) , which results in obtaining superior performance over the previously used recurrent neural network - based models ( Peters et al . , 2018 ) .
For the candidate generation step in entity linking , early work mostly utilized various non - neural network approaches such as TF - IDF or alias tables ( Wu et al . , 2020 ) , whereas more recent work utilized dense embeddings learnt via pre - trained transformers to retrieve the relevant candidates ( Wu et al . , 2020 ; Onoe and Durrett , 2020b ) . However , there is an important limitation while generating the candidates via pre - trained embeddings . For instance , the state - of - the - art neural entity linking model BLINK ( Wu et al . , 2020 ) loads the pre - trained embeddings of all entities in Wikipedia into memory . Thus , it becomes inapplicable for deployment in production scenarios where the requirement is to ensure lower memory consumption . In this paper , we address this issue via storing the pre - trained embeddings in Elasticsearch . Moreover , we introduce new techniques that pre - compute the linking between Wikipedia and Wikidata to ensure efficient information retrieval , while also optimize the pre - trained models to meet the goal of deploying the proposed system in a limited computational resource setting .
System Overview
To develop the entity linking system , we adopt BLINK , a neural entity linker that uses the transformer - based BERT model ( Vaswani et al . , 2017 ; Devlin et al . , 2019 ) and trains it on Wikipedia . BLINK connects each mention in a given text with its respective Wikipedia page based on the overall context . Since Wikipedia contains textual data in an unstructured format , it is difficult to extract information from it . Thus , we connect BLINK with a structured knowledge base , Wikidata , to extract information about product and organization type entities . Note that we store our knowledge base as well as the embedding representation of each entity in Elasticsearch . Moreover , we replace the Flair Named Entity Recognition ( NER ) model ( Akbik et al . , 2019 ) originally used by BLINK with an NER model ( we denote it as Internal NER ) trained on transcripts of business phone conversations using DistilBERT ( Sanh et al . , 2019 ) .
We show our entity linking system in Figure 1 . At first , the input text is processed by the NER model to detect the mention . Then , we generate the representation for the input text using the pretrained BLINK embeddings , while we retrieve the relevant candidates with their embeddings from Elasticsearch using the Multi Match Query 4 feature of Elasticsearch . Finally , the embedding representations of the input text and the candidates are sent to the pre - trained BLINK Bi - Encoder to select the most relevant candidate . Below , we first demonstrate our proposed entity linking system : BLINK with Elasticsearch , followed by describing how we deploy our proposed system in production .
BLINK with Elasticsearch
The original BLINK model requires about 25 GB RAM to load all pretrained embeddings into memory . In our proposed system , we instead store these embeddings in an external database . To do so , we store all entity embeddings as dense vectors 5 in our knowledge base in a remote Elasticsearch server along with saving textual information , such as Wikipedia title , description , URL , and etc . of each entity . This allows the model to only load the top K candidate embeddings into the memory that are most relevant to the mention in a given utterance . As mentioned earlier , the BLINK model was trained over Wikipedia , while our goal is to utilize Wikidata for information extraction . Thus , we need to map the Wikipedia URL of each entity to its Wikidata URL such that we can utilize Wikidata to extract relevant information . Below , we first describe how we add Wikidata URL of each entity to our knowledge base . Then , we demonstrate how we retrieve the relevant candidates from our knowledge base .
Pre - computing Wikipedia to Wikidata Linking
We pre - compute the mapping between Wikipedia and Wikidata using the Wikimapper 6 API and add the Wikidata URL of each entity to our knowledge base in Elasticsearch ( see Figure 2 ) . This allows our entity linking system to reduce the runtime latency . Note that during the pre - computation step , other information from Wikidata for each entity can also be added to the knowledge base ( for our case , we add the instance of property as the entity type ) .
Multi Match Query for Candidate Retrieval
We find that the whole word or subword ( s ) in the product or organization type entity names usually appear in the Wikipedia title and description fields . Thus , to retrieve the most relevant candidates , we utilize the multi match query feature of Elasticsearch for each entity mention in the input text and apply it to the title and description fields in our knowledge base ( see Figure 3 ) . For multi match query , we give more weight to the title field to make it two times more important than the description field . In this way , we retrieve the top k = 250 candidates from Elasticsearch and send to the BLINK Bi - Encoder to select the most relevant entity .
Model Deployment
We deploy our entity linking system in containers 7 in a Kubernetes 8 cluster with 2 CPUs and 10 GB RAM . The deployed system architecture is shown in Figure 4 . For production deployment , we also apply some optimization techniques to reduce the size of the pre - trained Bi - Encoder , as well as our knowledge base . We describe these below .
Pre - trained Bi - Encoder Optimization
We noticed that the binary file of the pre - trained BLINK Bi - Encoder had two type of tensors : one for context encoding ( for the input representation ) , and the other for the candidate encoding . However , the candidate encoding is only required during the training phase and it is not required during the inference stage since all the candidate embeddings are already stored in our knowledge base in Elasticsearch . Thus , we remove the unnecessary candidate encoding tensors from the binary file which results in reducing the file size from 2.5 GB to 1.2 GB ( 50 % reduced space ) to improve memory efficiency .
Knowledge Base Optimization
The original version of the pre - trained BLINK model ( Wu et al . , 2020 ) learns the embedding representations of 59,03,527 Wikipedia entities . In total , the size of these pre - computed embeddings is about 23 GB . As our goal is to detect the Product and Organization type entities in business conversational data , we apply some filtering techniques to optimize the knowledge base such that it mostly contains the entities that are relevant to our NER system . In order to do that , we utilize the Instance Of property in Wikidata of each entity and remove entities that are of Person , Disambiguation , Location , etc . In this way , the size of the Knowledge base is reduced from 23 GB to 12 GB ( about 50 % reduced space ) , while the total number of entities has been reduced from 59,03,527 to 27,84,042 .
Experimental Details
In this section , we demonstrate the datasets used in our experiments and the implementation details .
Datasets
To demonstrate the effectiveness of our proposed approach , we conduct a series of experiments on seven academic datasets as well as on a sample of 287 utterances collected from business conversation data . Below , we describe these datasets .
Business Conversation Dataset
As our goal is to develop an entity linking system that can link entities in conversational data from business domains , we sample some real world business phone conversation transcripts . After data collection , we use domain experts ( in - house scientists ) to annotate the utterances to label the mentions ( i.e. , product and organization type entities ) . Our annotated business conversation data consists of 287 utterances that we use in our experiment for evaluation .
Academic Datasets
Since our goal is to develop an entity linking system to extract information for product and organization type entities , at first we pre - process the academic datasets such that our model only links product and organization type entities during experiments . Similar to the original BLINK model ( Wu et al . , 2020 ) , we also did not leverage the training data and only used the test data of each dataset for zero - shot entity linking . In our experiment , we use the AIDA - YAGO2 - CONLL dataset ( testa and testb ) from Hoffart et al . ( 2011 ) that contains newswire articles from the Reuters Corpus ; the ACE 2004 , AQUAINT , and MSNBC datasets from ( Guo and Barbosa , 2018 ) that were constructed from news articles ; and the WNED - CWEB ( Guo and Barbosa , 2018 ) and the WNED - WIKI ( Gabrilovich et al . , 2013 ) datasets that were constructed from CWEB and Wikipedia respectively .
Implementation
Recall that instead of using the Flair NER model ( Akbik et al . , 2019 ) used by the original BLINK model , we train an NER model on phone transcripts as our goal is to build the entity linking model for real world business conversation data . For this purpose , we adopt the pre - trained DistilBERT model ( Sanh et al . , 2019 ) and fine - tune it on a business conversational dataset collected from some phone transcripts in Dialpad that contains 516124 training samples ( 16124 instances were annotated by humans while 500k instances were pseudo labels generated by the pre - trained LUKE NER model ( Yamada et al . , 2020 ) ) . There were also 2292 human annotated samples in the validation set while 4497 human annotated samples in the test set . We use the HuggingFace 9 library ( Wolf et al . , 2020 ) to implement the distilbert - base - cased 10 model and utilize it for the sequence labeling task with the following hyperparameters : learning rate = 2e-5 , total number of epoch = 15 , and batch size = 32 .
To implement the BLINK model for inference , we use its original source code 11 .
Results and Discussions
We denote our entity linking model that utilizes Multi Match Query ( MMQ ) on Elasticsearch ( ES ) as BLINK + ES MMQ . Here , we first discuss its performance on our business conversation data . Then we conduct experiments on some academic datasets to demonstrate its generalized effectiveness .
Performance on Business Conversation Data
Below , we present some baselines that we use to compare the performance of our proposed model . For this experiment , we use the following evaluation metrics , ( i ) average inference time : it refers to how much time it takes on average per utterance for entity linking , ( ii ) accuracy : it computes the correctness of linking the named entities to the Wikidata knowledge base , ( iii ) memory : it refers to the RAM configuration of the Machine that had to be used to run the model in Google Cloud Platform ( GCP ) 13 .
Since the utilization of GPUs significantly increases the computational cost , we did not leverage any GPU in our experiments to mimic the production environment . We show our experimental results in Table 1 and find that our proposed model significantly reduces the inference time while achieving high accuracy . Moreover , we were able to run our proposed model in GCP on an n1 - standard-4 machine having 15 GB RAM with 4 CPUs whereas BLINK models with Pywikibot had to be run on an n1 - standard-16 machine having 60 GB RAM with 16 CPUs ( we failed to run the model for inference due to memory leaks in other n1 - standard machines in GCP that had less RAM ) .
From Table 1 , we also observe that the performance of BLINK + ES CS model is the poorest among all models . One possible explanation behind this could be because the BLINK model did not leverage cosine similarity during its training phase and so zero - shot cosine similarity between the embedding of the candidate entity and the input embedding for candidate entity retrieval led to poorer accuracy . Moreover , we observe that the cosine similarity between embeddings is also very slow in comparison to MMQ . Furthermore , we find that our Internal NER is more effective than the Flair NER ( about 46 % ) and combining it with the MMQ leads to the highest accuracy score of 93.03 .
Performance on Academic Datasets
In this section , we further analyze the performance of our proposed BLINK + ES MMQ model via conducting experiments on seven academic datasets . We particularly conduct this experiment to investigate the generalized effectiveness of multi match query . For this analysis , we use the BLINK + ES CS model as the baseline where cosine similar- ity has been used instead of multi match query . As our goal is to deploy our model in a limited computational resource setting to ensure less memory consumption , we only use the models in this experiment that can be run in a machine that do not require more than 16 GB RAM . For this reason , we use the models that leverage Elasticsearch instead of Pywikibot ( we have already demonstrated in our previous experiment on business conversation data how our proposed method is more effective in terms of both accuracy and efficiency than other baseline models that utilized Pywikbot ) .
We show the results of our experiments in Table 2 to find that in 5 out of 7 datasets , our proposed method that uses multi match query instead of cosine similarity outperforms its counterparts . The only two datasets where our model could not outperform the baseline are the AIDA - YAGO2 - CONLL dataset ( testa ) and the AQUAINT dataset where cosine similarity outperforms multi match query by 7.94 % and 0.65 % respectively . In other datasets , our proposed BLINK + ES MMQ model outperforms the BLINK + ES CS model by 2.98 % , 10.42 % , 10.52 % , 11.22 % , and 5.75 % in AIDA - YAGO2 - CONLL ( testb ) , ACE 2004 , MSNBC , WNED - CWEB , and WNED - WIKI datasets respectively . Furthermore , we find during our experiments that our proposed method outperforms its counterpart in terms of inference speed in all 7 datasets ( on average , 8 times faster ) . These findings further validate the effectiveness of our proposed BLINK + ES MMQ model for real world deployment in computationally limited resource settings . So far , we discuss the effectiveness of our entity linking system in terms of both accuracy and efficiency based on extensive experiments in business conversation data , as well as in benchmark academic datasets . Below , we conduct a case study to analyze how the top K candidates retrieval from Elasticsearch impacts the overall performance .
Case Study
For the case study ( see Table 3 ) , we conduct experiments with some additional values of K for candidate retrieval to investigate its effect on accuracy and inference speed . For that purpose , in addition to the original value of K = 250 for the BLINK + ES MMQ model , we use the following values : K = 100 and K = 500 . We find that even though reducing the value of K to 100 for candidate retrieval leads to a faster inference speed , the accuracy is decreased by 3.74 % . Moreover , increasing the value of K to 500 provides an opposite impact , as it improves the accuracy by 1.13 % but makes the candidate retrieval speed slower by taking more than 2 seconds per utterance . This trade - off implies that the retrieval value for K can be tuned based on the requirement .
Ablation Study
To further investigate the effectiveness of our proposed approach of combining BLINK with Elasticsearch via leveraging MMQ for candidate retrieval , we do an ablation test . In our ablation test , we remove BLINK and only utilize the MMQ of Elasticsearch to retrieve the most relevant candidate . In this way , only one top matched candidate entity is retrieved from Elasticsearch . The result of our experiment is given in Table 4 .
From Table 4 , we observe that even though removing BLINK led to a great improvement in terms of the inference speed , there is a significant drop in accuracy ( by 20.22 % ) . This makes the model without BLINK inapplicable in production scenarios where the requirement is to ensure high accuracy .
Conclusion
In this paper , we introduce an efficient , scalable version of the BLINK model and extend it for entity linking on Wikidata . With extensive experiments , we show that our proposed system is usable for production environments within a limited budget setting since it significantly reduces memory requirements , computing resource usage , as well as the inference time while retaining high accuracy . We also effectively deploy our proposed entity linking system in a 10 GB RAM machine without using any GPU for near real - time inference . In the future , we will investigate how to make our entity linking system more efficient such that it can give inference in real - time ( e.g. , within one second ) . Moreover , we will study how different BERT - based ( Sanh et al . , 2019 ; Devlin et al . , 2019 ; Liu et al . , 2019 ; Lan et al . , 2019 ) sentence similarity models ( Garg et al . , 2019 ; Laskar et al . , 2020aLaskar et al . , , b , 2021 for candidate retrieval can impact the performance , while also exploring different techniques such as dimensionality reduction ( Wang et al . , 2016 ) to optimize the space used in Elasticsearch as well as the computing resource requirements .
Ethics Statement
The business phone conversational data used for entity linking experiments is annotated by the inhouse Scientists for which the annotations were acquired for individual utterances . Whereas to annotate the conversation dataset to train our internal NER model , Appen was used ( https : / / appen . com / ) for data annotation and the annotators were provided with adequate compensation ( above minimum wages ) . There is a data retention policy available for all users so that data will not be collected if the user is not consent to data collection . To protect user privacy , sensitive data such as personally identifiable information ( e.g. , credit card number , phone number ) were removed while collecting the data . Since our model is doing classification to link the named entities to their corresponding entries in a publicly available knowledge base for information extraction , incorrect predictions will not cause any harm to the user besides an unsatisfactory experience . We also maintain the licensing requirements accordingly while using different tools , such as Wikidata , WikiMapper , PyWikiBot , Elasticsearch , HuggingFace , BLINK , etc .
Acknowledgements
We gratefully appreciate the reviewers for their excellent review comments that helped us to improve the quality of this paper .
Discovering Differences in the Representation of People using Contextualized Semantic Axes
A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances . In particular , past work has compared embeddings against " semantic axes " that represent two opposing concepts . We extend this paradigm to BERT embeddings , and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations . We validate and demonstrate these axes on two people - centric datasets : occupations from Wikipedia , and multi - platform discussions in extremist , men 's communities over fourteen years . In both studies , contextualized semantic axes can characterize differences among instances of the same word type . In the latter study , we show that references to women and the contexts around them have become more detestable over time .
Introduction
Warning : This paper contains content that may be offensive or upsetting .
Quantifying and describing the nature of language differences is key to measuring the impact of social and cultural factors on text . Past work has compared English embeddings for people to adjectives or concepts ( Garg et al . , 2018 ; Mendelsohn et al . , 2020 ; Charlesworth et al . , 2022 ) , or projected embeddings against axes representing contrasting attributes ( Turney and Littman , 2003 ; Kozlowski et al . , 2019 ; Field and Tsvetkov , 2019 ; Mathew et al . , 2020 ; Kwak et al . , 2021 ; Lucy and Bamman , 2021b ; Fraser et al . , 2021 ; Grand et al . , 2022 ) . Static representations for the same word can also be juxtaposed across corpora that reflect different time periods ( Gonen et al . , 2020 ; Hamilton et al . , 2016 ) . This paradigm of using embedding distances to uncover socially meaningful patterns has also transferred over to studies that measure biases in contextualized embeddings , such as Wolfe and Caliskan ( 2021 ) 's finding that BERT beautiful ugly â¦ gorgeous equipage , ornately attired â¦ â¦ a grotesque , monster - like costume â¦ â¦ there are beautiful women who are willing â¦ â¦ literally filled with garbage women â¦ Figure 1 : An axis is constructed using embeddings of adjectives in selected contexts . These contexts are predictive of synonyms , but not antonyms , of the target adjective during masked language modeling . Tokenlevel embeddings for people are then projected onto this axis .
embeddings of less frequent minority names are closer to words related to unpleasantness . The use of " semantic axes " is enticing in that it offers an interpretable measurement of word differences beyond a single similarity value ( Turney and Littman , 2003 ; Kozlowski et al . , 2019 ; Kwak et al . , 2021 ) . Words are projected onto axes where the poles represent antonymous concepts ( such as beautiful - ugly ) , and the projected embedding 's location along the axis indicates how similar it is to either concept . Semantic axes constructed using static , type - based embeddings have been used to analyze socially meaningful differences , such as words ' associations with class ( Kozlowski et al . , 2019 ) , or gender stereotypes in narratives ( Huang et al . , 2021 ; Lucy and Bamman , 2021b ) .
Our work investigates the extension and application of semantic axes to contextualized embeddings . We present a novel approach for constructing semantic axes with English BERT embeddings ( Figure 1 ) . These axes are built to encourage selfconsistency , where antonymous poles are less conflated with each other . They are able to capture semantic differences across word types as well as variation in a single word across contexts . Their ability to differentiate contexts makes them suitable for studying how a word changes across domains or across individual sentences . These axes are also more self - consistent and coherent than ones created using GloVe and other baseline approaches .
We demonstrate the use of contextualized axes on two datasets : occupations from Wikipedia , and people discussed in misogynistic online communities . We use the former as a case where terms appear in definitional contexts , and characteristics of people are well - known . In the latter longitudinal , cross - platform case study , we examine lexical choices made by communities whose attitudes towards women tend to be salient and extreme . We chose this set of online communities as a substantive use case of our method , in light of recent attention in web science on analyzing online extremism and hate at scale ( e.g. Ribeiro et al . , 2021b , a ; Aliapoulios et al . , 2021 ) . There , we analyze language change and variation along axes through a sociolinguistic lens , emphasizing that speakers use language that reflects their social identities and beliefs ( CH - Wang and Jurgens , 2021 ; Huffaker and Calvert , 2017 ; Card et al . , 2016 ; Lakoff and Ferguson , 2006 ) .
Our code , vocabularies , and other resources can be found in our Github repo : https : / / github.c om / lucy3 / context_semantic_axes .
Constructing semantic axes
Static embeddings . Several formulae for calculating the similarity of a target word to two sets of pole words have been proposed in prior work on static semantic axes . These differ in whether they take the difference between a target word 's similarities to each pole ( Turney and Littman , 2003 ) , calculate a target word 's similarity to the difference between pole averages Kwak et al . , 2021 ) , or calculate a target word 's similarity to the average of several word pair differences that represent the same antonymous relationship ( Kozlowski et al . , 2019 ) . We build on the approach of and Kwak et al . ( 2021 ) , because it does not require us to curate multiple paired antonyms for each axis , and it draws out the difference between two concepts before a target word is compared to them , rather than after . We define an axis V containing antonymous sets of adjective vectors , S l = { l 1 , l 2 , l 3 , ... , l n } and S r = { r 1 , r 2 , r 3 , ... , r m } , as the following :
V = 1 n n i=1 l i â 1 m m j=1 r j .
Relying on single - word poles for axes can be unstable to the choice of each word Antoniak and Mimno , 2021 ) . creates a pole 's set of words using the nearest neighbors of a seed word , which may risk conflating unintended meanings or antonymous neighbors ( MrkÅ¡iÄ et al . , 2016 ; Sedoc et al . , 2017 ) . For example , one axis uses the opposite seed words green and experienced , but green 's nearest neighbors include red rather than inexperienced . Instead of using this nearest neighbors approach , we construct poles using WordNet antonym relations . Each end of an axis aggregates synonymous and similar lemmas in WordNet synsets , which are expanded using the similar to relation ( Miller , 1992 ) .
Our type - based embedding baseline , GLOVE , uses 300 - dimensional GloVe vectors pretrained on Wikipedia and Gigaword ( Pennington et al . , 2014 ) . We only keep poles where both sides have at least three adjectives that appear in the GloVe vocabulary , and we also exclude acronyms , which are often more ambiguous in meaning . We start with 723 axes , where poles have on average 9.63 adjectives each .
Contextualized embeddings . Static embeddings , however , present a number of limitations . Such embeddings can not easily handle polysemy or homonymy ( Wiedemann et al . , 2019 ) , and even when they are trained on different social or temporal contexts , they require additional steps to be aligned ( Gonen et al . , 2020 ) . Context - specific embeddings also need enough training examples of target words to create usable representations . These limitations prevent the analysis of token - based semantic variation , such as measuring how one mention of a word is more or less beautiful than another . Our main contribution of contextualized axes uses the same WordNet - based formulation as our GloVe baseline . Rather than each word in S l or S r being represented by a single GloVe embedding , we obtain BERT embeddings over multiple occurrences of each adjective . We use BERT - base , as this model is small enough for efficient application on large datasets and is popular in previous work on semantic change and differences ( e.g. Hu et al . , 2019 ; Lucy and Bamman , 2021a ; Giulianelli et al . , 2020 ; Zhou et al . , 2022 ; Coll Ardanuy et al . , 2020 ; Martinc et al . , 2020 ) . It is also used in tutorials for researchers outside of NLP , which means it has high potential use in computational social science and cultural analytics ( Mimno et al . , 2022 ) .
For contextualized axes , we obtain a potential pool of contexts for adjectives sampled over all of Wikipedia from December 21 , 2021 , preprocessed using Attardi ( 2015 ) 's text extractor . This sample contains up to 1000 sentences , or contexts , that contain each adjective , and we avoid contexts that are too short ( over 10 tokens ) or too long ( over 150 tokens ) . 1 We experiment with two methods of obtaining contextualized BERT embeddings for each adjective : a random " default " ( BERT - DEFAULT ) and one where contexts are picked based on word probabilities ( BERT - PROB ) . For BERT - DEFAULT , we take a random sample of 100 contextualized embeddings across the adjectives in each pole . Since words can be nearest neighbors with their antonyms in semantic space ( MrkÅ¡iÄ et al . , 2016 ; Sedoc et al . , 2017 ) , our main approach , BERT - PROB , aggregates word embeddings over contexts that highlight contrasting meanings of axes ' poles .
To select contexts , we mask out the target adjective in each of its 1000 sentences , and have BERT - base predict the probabilities of synonyms and antonyms for that masked token . We remove contexts where the average probability of antonyms is greater than that of synonyms , sort by average synonym probability , and take the top 100 contexts . One limitation of our approach is that predictions are restricted to adjectives that can be represented by one wordpiece token . If none of the words on a pole of an axis appear in BERT 's vocabulary , we backoff to BERT - DEFAULT to represent that axis .
For each axis type , we also have versions where words ' embeddings are z - scored , which has been shown to improve BERT 's alignment with humans ' word similarity judgements ( Timkey and van Schijndel , 2021 ) . For z - scoring , we calculate mean and standard deviation BERT embeddings from a sample of around 370k whole words from Wikipedia . As recommended by Bommasani et al . ( 2020 ) , we use mean pooling over wordpieces to produce word representations when necessary , and we extend this approach to create bigram representations as well . These embeddings are a concatenation of the last four layers of BERT , as these tend to capture more context - specific information ( Ethayarajh , 2019 ) .
Internal validation
We internally validate our axes for self - consistency .
For each axis , we remove one adjective 's embeddings from either side , and compute its cosine similarity to the axis constructed from the remaining adjectives . For BERT approaches , we average the adjective 's multiple embeddings to produce only one before computing its similarity to the axis . In a " consistent " axis , a left - out adjective should be closer to the pole it belongs to . That is , if it belongs to S l , its similarity to the axis should be positive . We average these leave - one - out similarities for each pole , negating the score when the adjective belongs to S r , to produce a consistency metric , C .
Table 1 shows C for different axis - building methods . 2 An axis is " consistent " if both of its poles have C â¥ 0 . GLOVE 's most inconsistent axis poles often involve directions , such as east â west , left - handed â right - handed , and right â left . These concepts may be difficult to learn from text without grounding . We find that the various BERT approaches ' most inconsistent axes include direction - related ones as well , but they also struggle to separate concepts such as lower - class â upper - class .
The best method for producing consistent axes is z - scored BERT - PROB , with a significant difference in C from z - scored BERT - DEFAULT and GLOVE ( Mann - Whitney U - test , p < 0.001 ) . It also produces the highest number of consistent axes . GLOVE presents itself as a formidable baseline , 3 and BERT - DEFAULT struggles in comparison to it .
External validation
Previous work on static semantic axes validates them using sentiment lexicons , exploratory anal- yses , and human - reported associations Kwak et al . , 2021 ; Kozlowski et al . , 2019 ) . We perform external validation of self - consistent axes on a dataset where people appear in a variety of well - defined and known contexts : occupations from Wikipedia . We conduct two main experiments . In the first , we test whether contextualized axes can detect differences across occupation terms , and in the second , we investigate whether they can detect differences across contexts .
Data
We For each occupation 's singular form , we extract sentences in its page that contains it . In total , we have 3,015 sentences for 300 occupations .
Term - level experiment ( occupations )
Each occupation is represented by a pre - trained GloVe embedding or a BERT embedding averaged over all occurrences on its page . If an axis uses z - scored adjective embeddings , we also z - score the occupation embeddings compared to it . We assign poles to occupations based on which side of the axis they are closer to via cosine similarity . embeddings ' proximity can reflect any type of semantic association , not just that a person actually has the attributes of an adjective . For example , adjectives related to unhealthy are highly associated with Health occupations , which can be explained by doctors working in environments where unhealthiness is prominent . Therefore , embedding distances only provide a foggy window into the nature of words , and this ambiguity should be considered when interpreting word similarities and their implications . This limitation applies to both static embeddings and their contextualized counterparts .
We conduct human evaluation on this task of using semantic axes to differentiate and characterize occupations . Three student annotators examined the top three poles retrieved by each axisbuilding approach and ranked these outputs based on semantic relatedness to occupation categories ( Appendix B ) . These annotators had fair agreement , with an average Kendall 's W of 0.629 across categories and experiments . Though GLOVE is a competitive baseline , z - scored BERT - PROB is the highest - ranked approach overall ( Table 3 ) . This suggests that more self - consistent axes also produce measurements that better reflect human judgements of occupations ' general meaning .
Context - level experiment ( person )
The identity of a word , and prior associations learned from BERT 's training data , have the potential to overpower its in - context use ( Field and Tsvetkov , 2019 ) . Thus , we may want to discount word associations originally learned by BERT when we examine the use of a target word in a narrower context . Prior work has shown that words with higher frequency in BERT 's training data tend to encode more context - specific information in their embeddings ( Ethayarajh , 2019 ; Zhou et al . , 2021 ; Wolfe and Caliskan , 2021 ) . To investigate whether contextualized axes can measure context changes for people , we replace all occupation bigrams and unigrams with person , a very common word . This also makes contexts across different words comparable to each other , a property which we will leverage later in Section 5.4 .
Each person embedding is averaged over one occupation 's contexts . The identity of person tends to overpower its similarity to axes across contexts , in that the top - ranked poles are similar across occupation categories . So , in contrast to the previous occupation experiment , additional steps are needed to draw out meaningful differences in how person is used in one group of contexts from its typical use . To do this , we estimate the average cosine similarity to axes of n person embeddings in occupational contexts using 1000 bootstrapped samples , where n is the number of terms in an occupation category . We take the axes with the highest statistically significant ( p < 0.001 , one - sample t - test ) difference in cosine similarity .
We assume that occupations ' Wikipedia pages mention them within definitional contexts , so topranked poles should reflect the original occupation replaced by person . These top poles are less intuitive than those outputted by the earlier term - level experiment ( Table 2 ) . Still , in some cases , such as for Government and Math & Statistics occupations , we uncover relative differences that distinguish one category from others . We only show three adjectives in the top two poles in Table 2 due to space considerations , but moving further down the list for z - scored BERT - PROB uncovers additional meaningful poles . For example , the pole spry , gymnastic , sporty is the third most prominent shift and highest similarity increase ( + ) in the person experiment for Sports occupations . In addition , human evaluators preferred BERT - PROB over other approaches ( Table 3 , Appendix B ) .
Measuring change and variation
Now that we have contextualized semantic axes that can measure differences across words and contexts , we apply them onto a domain that can showcase salient and socially meaningful variation . NLP research on harmful language often employs methods that focus on the target group , such as measuring their association with other words ( Zannettou et al . , 2020 ; Garg et al . , 2018 ; Tahmasbi et al . , 2021 ; Field and Tsvetkov , 2019 ) , or with biases in models ( Wolfe and Caliskan , 2021 ; Ghosh et al . , 2021 ) . We illustrate the application of self - consistent z - scored BERT - PROB axes onto the manosphere , which is a collection of communities with mostly male users who hold alternative beliefs around relationships and gender . We use the same axes we presented earlier , which were created using Wikipedia data , because Wikipedia provides more normative coverage of a variety of adjectives than topic - specific communities . This way , we examine how entities in the manosphere orient themselves against typical adjectival uses and meanings .
The manosphere has been linked to acts of violence in the physical world ( Hoffman et al . , 2020 ) , and most members believe that men are systemically disadvantaged in society ( Van Valkenburgh , 2021 ; Marwick and Caplan , 2018 ; Lin , 2017 ; Ging , 2019 ) . These communities focus on heterosexual relationships and masculinity , and feature a dynamic linguistic landscape . Much prior work on the manosphere has been qualitative , such as ethnographies ( Lin , 2017 ; Lumsden , 2019 ; Van Valkenburgh , 2021 ) . There have been a few quantitative analyses of their language , usually focusing on phrase and word frequencies in a few communities ( Farrell et al . , 2019 ; Gothard et al . , 2021 ; LaViolette and Hogan , 2019 ; Jaki et al . , 2019 ) . As an example involving word vectors , Farrell et al . ( 2020 ) uses static embeddings identify the meanings of incels ' neologisms by inspecting words ' nearest neighbors .
Our case study extends beyond prior work with its methodology and scale . We use contextualized semantic axes to tackle one question : how have references to women and contexts around them changed over fourteen years ?
Data
We use a taxonomy of subreddits and external forums described by Ribeiro et al . ( 2021a ) , who show that the manosphere began with ideologies such as pick - up artists ( PUA ) and Men 's Rights Activists ( MRA ) , and evolved into more extreme ones such as The Red Pill ( TRP ) , incels ( short for involuntary celibate ) and Men Who Go Their Own Way ( MGTOW ) , with users moving from older to newer ideologies . We call this dataset EXTREME_REL , because it contains extreme views of relationships .
We use Reddit posts and comments from March 2008 to December 2019 from subreddits listed in Ribeiro et al . ( 2021a ) 's study , downloaded from Pushshift ( Baumgartner et al . , 2020 ) . We slightly modify their taxonomy by separating out incel subreddits where the intended userbase are women ( femcels ) , and also include a newer set of subreddits focused on " Female Dating Strategy " ( FDS ) , a women - led community analogous to TRP ( Holden , 2020 ; Clark - Flory , 2021 ) . Therefore , we have 60 subreddits in seven ideological categories : Incels , MGTOW , PUA , MRA , TRP , FDS , and Femcels 4 ( Appendix C ) . This Reddit subset of EX - TREME_REL contains over 1.3 billion tokens .
We also include seven external forums provided by Ribeiro et al . ( 2021a ) . These public forums include A Voice for Men ( AVFM ) , Master Pick - up Artist ( MPUA ) Forum , The Attraction , incels.co , MGTOW Forum , RooshV , and Red Pill Talk . 5 This forum subset of EXTREME_REL contains over 800 million tokens spanning November 2005 to June 2019 , and we remove duplicates and quoted text from posts . Some experiments use a subset of Reddit that shares a similar topical focus as EXTREME_REL , but may have more mainstream views of women and relationships .
We use a list 6 of common " Relationship " subreddits : r / relationships , r / dating , r / relationship_advice , r / dating_advice , and r / breakups .
We call this dataset GEN - ERAL_REL , and it contains 1.2 billion tokens from September 2009 to December 2019 . For Reddit data , we do not use posts and comments written by usernames who have bot - like behavior , which we define as repeating any 10 - gram more than 100 times .
Vocabulary
We use a mix of NER , online glossaries , and manual inspection to curate a unique vocabulary of people ( details in Appendix D ) . This vocabulary has 2,434 unigrams and 4,179 bigrams , tokenized using BERT 's tokenizer without splitting words into wordpieces ( Devlin et al . , 2019 ; Wolf et al . , 2020 ) . These terms appear at least 500 times in EXTREME_REL .
Since gender is central to the manosphere , we infer these labels based on terms ' social gender in a dataset . For example , accuser is not semantically gendered like girl and woman , but its social gender , estimated using pronouns , is more feminine in EXTREME_REL than GENERAL_REL . We use two stages of gender inference to account for pronoun sparsity and noise . First , we use a list of semantically gendered nouns , and second , we use feminine and masculine pronouns linked to terms via coreference resolution ( details in Appendix E ) . We label each vocabulary term based on its fraction of cooccurring feminine pronouns in EXTREME_REL and GENERAL_REL , separately . We are able to label 72.5 % of the vocabulary in EXTREME_REL and 67.0 % of it in GENERAL_REL .
Term - level change
Contextualized semantic axes can reveal how word and phrase types change over time . Here , our analyses focus on 1,482 feminine ( genderleaning > 0.75 ) terms in EXTREME_REL . To capture broad snapshots of words ' use , we randomly sample up to 500 sentence - level occurrences of each term in each platform and ideology ( e.g. a specific forum or Reddit category ) in each year . Overall z - scored BERT embeddings for each vocab word are averages over this stratified sample of its contexts .
The history of the manosphere is characterized by waves of different ideological communities ( Ribeiro et al . , 2021a ) . To reflect this characterization through language , we segment our vocabulary based on when terms peak in popularity . We cluster normalized frequency time series 7 for each term using K - Spectral Centroid clustering ( KSC ) ( Yang and Leskovec , 2011 ) . We use their default parameters , including K = 6 . In contrast to their original approach , our symmetric distance measure d is invariant to scaling by Î± but not to the translation of the time series , so that peaks earlier in time are not clustered with those later in time :
d ( x , y ) = ||x â Î±y|| ||x|| ,
where Î± = x T y / ||y|| 2 . " Waves " of term types for people correspond to ideological change . Figure 2 shows examples of feminine terms , but the top masculine terms are often labels of ideological groups , such as mgtow and incels , which we use to estimate which clusters align with ideological up and downturns . 8 Cluster A and cluster D tend to have terms that have widespread use .
We examine the shifts of high variance , substantive axes across temporal clusters . High variance axes include those related to gender , appearance , and desirability ( Table 4 ) . For example , the lovable versus detestable pole contrasts beautiful girls with degenerate whores . As another example , the axis for clean versus dirty contrasts loyal wife with harlots . Prior studies using toxicity detection and lexicon - based approaches found that hate and misogyny rose with the arrival of later MG - TOW and incel communities ( Farrell et al . , 2019 ; Ribeiro et al . , 2021a ) . Similarly , we find that lexical choices for women are more detestable and dirty in later waves associated with MGTOW and incels ( Figure 3 ) . Often , low and high frequency words share similar patterns in each wave .
Context - level change
Contextualized semantic axes can reveal how the contexts around people have changed over time . Women in online communities can be referenced in a variety of ways ( Figure 2 ) . To compare overall changes around women between mainstream and extremist communities , we examine the contexts around feminine ( gender - leaning > 0.75 ) words . We use instances of 287 unigram types , since bigrams can include modifiers that would be considered " context " . As discussed earlier , word identities impact measurements of contextual changes across them ( Section 4.3 ) . We replace each target word with person or people depending on whether it is singular or plural , estimated through the Python INFLECT package . We choose Figure 4 : Contexts around singular ( person ) or plural ( people ) feminine words over time in EXTREME_REL and GENERAL_REL along three axes . Time series include 95 % CI , and dotted lines mark the peak of major ideological communities ( gray labels ) . These vertical lines are months that have the highest normalized frequencies of words used to refer to their members : puas , mras , trpers , mgtows , and incels .
replacements to respect singular / plural forms to ensure ecological validity and not perturb BERT 's sensitivity to grammaticality ( Yin et al . , 2020 ) . We use reservoir sampling to obtain up to 1000 occurrences of person - or people - replaced feminine words in each month on EXTREME_REL and GEN - ERAL_REL .
In comparison to GENERAL_REL , EX - TREME_REL has more detestable , sickening , and dirty contexts for women ( Figure 4 ) . Both GENERAL_REL and EXTREME_REL discuss relationship issues , but contextualized axes reveal how contrasting and changing attitudes toward women can influence context . Negative associations especially peak during the height of the incels ' movement around late 2017 to mid 2019 . These persist despite Reddit 's ban of r / incels in November 2017 and the quarantine of r / braincels and r / theredpill in September 2018 . Thus , the widespread efficacy of community - level moderation is worthy of closer study ( e.g. Copland , 2020 ; Ribeiro et al . , 2021b ) . An advantage of computing scores at the token - level rather than at the type - level is interpretability . That is , one can see which contexts land at the extreme ends of axes ( as illustrated in Table 5 ) .
Contextualized semantic axes can also illuminate differences among lexical variables , or different linguistic forms that share the same referential meaning ( Nguyen et al . , 2021 ; Labov , 1972 ) .
As prominent examples , men - led communities use the lexical innovations femoids and foids , which are shortenings of female humanoids , as dehumanizing words for all women ( Chang , 2020 ; PraÅ¼mo , 2020 FDS , use moids as an analogous way to refer to men . Prior work studying three manosphere subreddits showed that the lemmas woman and girl are constructed negatively as immoral , deceptive , incapable and insignificant ( Krendel , 2020 ) . We hypothesize that the contexts of community - specific variants should have even more dehumanizing connotations along similar dimensions . In this experiment , we replace all terms ( men , moids , foids , femoids , and women ) with people .
We sample up to 100 occurrences of each variant in each platform and ideology per year , limiting time ranges to when domain - specific variants are widely used by their home community . We examine the use of variants for men by Femcels and FDS in 2018 - 2019 , and the use of variants for women by all other communities in EXTREME_REL in 2017 - 2019 . Unlike in the person experiment for occupations , we have substantial pools of occurrences to compare . Thus , to find axes that distinguish one variant from another , we use axis scores as features in random forest classifiers ( Pedregosa et al . , 2011 ) , and perform binary classification of word identity : women versus foids or femoids , and men versus moids ( Appendix G ) . We rank axes based on their feature importance , and select three highly ranked and relevant axes to show in Figure 5 . Shifts along these axes confirm our hypothesis that communityspecific variants are more dehumanized than their widely - used counterparts .
Conclusion
In this work , we examine the capability of contextualized embeddings for discovering differences among words and contexts . Our method uses predicted word probabilities to pinpoint which contexts to include when aggregating BERT embeddings to construct axes . This approach creates more self - consistent axes that better fit different occupation categories , in comparison to baselines . We further demonstrate the use of these axes in a longitudinal , cross - platform case study . Overall , contextualized embeddings offer more flexibility and granularity compared to static ones for the analysis of content across time and communities . That is , rather than train static word embeddings for various subsets of data , we can characterize change and variation at the token - level .
Though we focus on analyzing associations between adjectives and people , our approach can generalize to other types of entities as well . Measuring and comparing the contexts of other entity types should include many of the same considerations we did , such as reducing the conflation of antonyms , controlling for word identity by replacing target words with a shared hypernym , and experimenting with z - scoring . Future work includes understanding why some opposing concepts are conflated in large language models , and how a word embed - ding 's identity influences its encoding of contexts .
Limitations
Aside from computing power requirements ( Appendix H ) , we outline a few additional limitations of our methodology and its application not discussed in the main text . Domain shift . The use of pretrained BERT on a niche set of communities makes our approaches susceptible to domain shift , such as rare words having less robust embeddings ( Zhou et al . , 2022 ( Zhou et al . , , 2021 , or target words carrying over learned associations from a broader corpus that are less applicable in a narrower one . Domain shift is difficult to avoid without retraining or further pretraining BERT , which is resource - intensive , may risk catastrophic forgetting , and inaccessible to some disciplines in computational social science ( Gururangan et al . , 2020 ; Ramponi and Plank , 2020 ; Goodfellow et al . , 2014 ) . Also , training a large language model on text with toxic and misogynistic origins introduces additional risk of dual use ( Kurenkov , 2022 ) . We suggest some potential workarounds that lessen the severity of domain shift , such as replacing target words with common ones for context - focused analyses .
WordNet . WordNet is a popular lexical resource for NLP , but its senses for words can be overly finegrained ( Pilehvar and Camacho - Collados , 2019 ) and not suitable for all domains . We use WordNet version 3.0 , which is included in NLTK , and this version was last updated in 2006 . Since English is constantly changing , some synonym and antonym relations may be outdated .
Errors . Our method for drawing out differences in words is better than common baselines yet still imperfect , and some of the opposing concepts in embedding space that BERT struggles to separate may be important for an application domain . Therefore , domain expertise is needed to recognize spurious patterns from real ones and fill these gaps .
In the main text we mention that embeddings offer a " foggy window " into how two concepts may be associated or related , and the exact type of relation is not always clear . For example , if contexts for women are closer to unpleasant , does it mean that the text discusses unpleasant events that affect women , or that the writers believe that women are unpleasant , or both ? Some of this uncertainty could be resolved qualitatively by inspecting sentences at poles ' extremes . We compare embeddings for people to axes , but it is also possible to include relation - based approaches such as dependency parsing and compare words that share specific relations with people to axes ( e.g. Lucy and Bamman , 2021b ) . One trade - off of doing this is that informative verbs and adjectives connected to mentions of target groups can be sparse . Our method is able to find that mathematician replaced with person is highly similar to calculable in a variety of sentence structures , such as this one modified off Wikipedia : A person is someone who uses an extensive knowledge of mathematics in their work , typically to solve mathematical problems .
Ethical considerations
User privacy . Online data opens many doors for research , but its use raises concerns around user privacy . For our use case , we believe that the benefits of our work outweigh privacy - related harms . Consent is infeasible to obtain for large datasets ( Buchanan , 2017 ) , and in the manosphere , it is unlikely that users would give consent , especially if the researchers using their data believe that their ideologies are harmful and wrong . Obtaining consent would pose risks to the safety of the researcher ( Conway , 2021 ; Doerfler et al . , 2021 ) .
All online discussions included in our work were public when downloaded by their original curators , mainly Baumgartner et al . ( 2020 ) and Ribeiro et al . ( 2021a ) . Some forums and online glossaries were relocated , shutdown , banned , or made private later on . A user 's " right to be forgotten " confronts researchers who have interests in documenting and studying the histories of communities . We truncate the examples shown in our paper rather than use them in full verbatim ( Bruckman , 2002 ) .
Communities may expect their posts to stay within their in - group , but the content in our work was posted on public platforms . This publicness and increased visibility plays a key role in how this content impacts others , such as those who view this information and propagate it elsewhere , or those who are direct targets of hate . Common targets such as women and people of color carry a bigger burden when participating in online spaces ( Hoffmann and Jonas , 2017 ) , and our broader research agenda aims to mitigate this issue .
Social biases in models and resources . We use WordNet to group similar adjectives into semantic axes , but we observe some socially harmful asso - ciations in this resource . For example , gross and fat are listed as similar lemmas . As another example , WordNet conflates gender and sexuality when androgynous and bisexual are also listed as similar lemmas . The BERT language model , like all large , pretrained models , is also susceptible to social biases in its training data ( Bender et al . , 2021 ) .
Gender inference . In this paper 's main case study , we perform gender inference for word and phrase types . This step was necessary to study how women are portrayed over time , which is a key question due to the centrality of misogyny in these communities . However , perfect prediction of each word 's perceived gender in our dataset using pronouns is impossible ( Cao and DaumÃ© III , 2021 ) . Not all mentions of people co - occur with pronouns , pronouns do not equate gender , and coreference resolution systems can produce errors . So , we approximate the social gender of terms by aggregating coreference patterns over all instances of that term . Since it is difficult to separate noisy errors from meaningful word - level pronoun variation at scale , we had to use a score threshold to pinpoint what words were feminine - leaning enough to be included in our analyses .
Restricting pronouns to the traditional binary of feminine and masculine is limiting , since individuals use other pronouns as well . They / them pronouns are predominantly used to reference plural terms in this dataset , and the coreference model we use does not handle neopronouns . The manosphere and the typical framing under which it is studied is heavily cisheteronormative . We use a frequency cutoff to determine our vocabulary ( Appendix D ) , so references to transgender and nonbinary people may be filtered out . Vocab terms retained for transgender people are outdated or typically offensive terms such as transsexuals and transgenders , and no vocab term includes non - binary , nb , or nonbinary .
A Wikipedia page titles
Table 6 lists the categories of occupations , the titles of Wikipedia pages that list them , and the number of terms in each category . These lists were retrieved in February 2022 .
B Human evaluation for occupations
We recruited three student volunteers with familiarity with NLP coursework and tasks to rank the top poles provided by each axis - building method for our occupation and person experiments . We used Qualtrics to design and launch the survey . Since we were not asking about personal opinions but rather evaluating models , we were determined exempt from IRB review by the appropriate office at our institution . Each question pertains to a specific occupation category , and within each experiment , question order and answer option order are randomly shuffled . Each model option is presented with its top three poles , in order of most to less Hi ! Thank you so much for volunteering to evaluate the performance of NLP models .
Please read these instructions carefully .
In this task , you will judge how much lists of adjectives from WordNet outputted by models are semantically related to occupational differences described in Wikipedia . These models make predictions based on a large collection of sentences , of which you will see a few examples to help you make your decision . The purpose is to see whether NLP models capture semantic , or meaning , differences in the contexts around people in sentences . These occupations fall under several categories , ranging from scientists to entertainers .
You are deciding which models ' outputs are typically more related to occupations , which may not reflect your personal opinions about occupations .
There are two sets of questions , and 11 questions in each set .
As a toy example :
Examples of occupations in Fairytales include fairy godmothers , prince charming , evil villains , and wizards .
You are given the sets of adjectives below . Adjective sets include " MORE " and " LESS " labels based on how people in the category above are more or less related to them , in comparison to other people who work as artists , government workers , and scientists : The above three models are ranked from most related to the occupation category to least related . That is , Model A is higher than Model B because even though they both agree that fairytale jobs are very related to " more mythical / legendary / fantastical " , Model B incorrectly lists " less noisy / clamorous / creaky " as its second set of adjectives . Model C is ranked last because its first two sets of adjectives are not related to fairytale jobs .
Try to be consistent in your rankings . That is , in the example above , you should not rank Model C after A and before B because A and B agree on the first set and overall share two valid adjective sets . Model C is more of an outlier , with only one valid third adjective set . relevant . Figure 6 shows screenshots of instructions . In the toy example , the options are labeled with " Model A " , " Model B " , " Model C " , to allow explanation clarity , but in the actual task questions , options are not labeled with model letters to avoid biasing the evaluators towards a specific model . Some annotators expressed that the task was difficult , and for some occupations , different approaches output similar axes , just in different order .
C Reddit communities
We used a list of subreddits 9 for the manosphere provided by ( Ribeiro et al . , 2021a ) in their detailed , data - driven sketch of the manosphere . Five of the subreddits included in Ribeiro et al . ( 2021a ) 's taxonomy of the Reddit manosphere ( r / malecels , r / lonelynonviolentmen , r / 1ncels , r / incelbrotherhood , r / incelspurgatory ) were not on Pushshift 's dump of Reddit . We curated the list of communities for our new ideological category , Female Dating Strategy ( FDS ) , using a now removed list of FDS 's " sister communities " on the subreddit r / FemaleDatingStrategy 's sidebar :
r / PinkpillFeminism , r / AskFDS , r / FDSSuperFans , r / PornFreeRelationships , and r / FemaleLevelUpStrategy . The Femcels set of subreddits include : r / Trufemcels , r / TheGlowUp , and r / AskTruFemcels . Though the main user base of the manosphere are men , there are also small populations of women in other ideologies as well , such as r / redpillwomen . We mainly portion out FDS and Femcels due to their role in Section 5.4 's lexical variant experiment as communities who use moids .
In total we have 12 subreddits in TRP , 11 in MRA , 7 in PUA , 22 in Incels , 3 in MGTOW , 4 in Femcels , and 6 in FDS . The complete list of subreddits and their categories is also in our Github repo .
D Vocabulary creation
First , we extract nominal and proper persons using NER , keeping ones that are popular ( occur at least 500 times in EXTREME_REL ) , and unambiguous , where at least 20 % of its instances in these datasets are tagged as a person . Gathering a substantial number of labels from our domain to train an indomain NER system from scratch is outside the scope of our work , so we experimented with three models trained on other labeled datasets : ACE , contemporary literature , and a combination of both . We evaluated these models on a small set of posts and comments labeled by one author , after retrieving 25 examples per forum or Reddit ideological category using reservoir sampling . The annotator only labeled spans for nominal and named PERSON entities . Table 7 shows the performance of each model on EXTREME_REL . Based on these evaluation results , we chose to use the model trained on contemporary literature .
We extract bigrams and unigrams from detected spans , excluding determiners and possessives whose heads are the root of the span . Named entities that refer to types of people rather than specific individuals were estimated through their co - occurrence with the determiner a , e.g. a Chad .
Then , one author consulted community glossaries and examined in - context use of words to manually correct the list of automatically extracted terms . We include additional popular and unambiguous words not tagged sufficiently often enough by NER , but defined as people in prior work and online resources .
Table 8 lists the sources and glossaries for vocabulary words and the ideologies they include . Some of these sources , such as the Shedding of the Ego , are created by insiders in the community , while some , such as academic papers and news articles , are by outsiders . For each of these glossaries and lists of terms , we manually separated them into two categories : 269 people ( singular and plural forms ) and 1776 non - people . Two of these sites , Shedding of the Ego and Pualingo , no longer exists , but were publicly available until at least late 2020 . We include 93 terms for people that were initially filtered out in our NER pipeline in our final vocabulary , excluding ambiguous ones that also occur often as non - human entities , such as tool ( a fool who is taken advantage of ) and jaw ( short for just another wannabe ) .
The resulting vocabulary contains niche language , where 20.7 % of unigrams are not found in WordNet , and 85.1 % of those missing are also not in the Internet resource Urban Dictionary . 10 The full list is also available in our Github repo .
E Gender inference
This section includes additional details around our gender inference process .
Our list of semantically gendered terms , or words gendered by definition , expands upon the one used by Hoyle et al . ( 2019 ) We include the following additional semantically gendered terms : male , males , dude , dudes , guy , guys , boyfriend , boyfriends , bf , female , females , chick , chicks , girlfriend , girlfriends , gf , gal , gals , bro , transmen , transwomen , she , he .
We check if any of the above words appear in a unigram or bigram vocabularly term . Around 29.9 % of our vocabulary in EXTREME_REL is gendered through this word list approach .
To infer gender for the remaining words using pronouns , we ran coreference resolution on EX - TREME_REL , and extracted all pronouns that are clustered in coreference chains with terms in our vocabulary ( Clark and Manning , 2016 ) . We label the masculine to feminine leaning of vocab terms by calculating the proportion of feminine pronouns ( she , her , hers , herself ) over the sum of feminine and masculine pronouns ( he , him , his , himself ) . We only consider a word to have a usable gender signal if it appears in at least 10 coreference clusters with feminine or masculine pronouns . Since plural words do not usually appear with he / she pronouns , we have plural words take on the gender leaning of their singular forms . We pair plural and singular forms using the Python INFLECT package . 11 We also transfer unigrams ' gender to bigrams , after examining the modifiers ( the first token ) in bigram terms to check that they are not differently and semantically gendered . Around 20.9 % of our vocabulary in EXTREME_REL is gendered through pronouns alone , an additional 12.6 % is gendered through plural to singular mapping , and an additional 9.1 % is gendered through bigram to unigram mapping .
F High variance axes
Table 9 shows the top vocabulary terms that correspond to the poles of high variance axes .
G Classification of lexical variants
Our main goal here is to tease out which axes differentiate the contexts of lexical variants , rather than find the best model that performs well on a classification task . Therefore , we choose to use a random forest classifier for its interpretability : it outputs weights that indicate what features were most important across its decisions . We use scikitlearn 's implementation , and perform randomized search with 5 - fold cross validation and weighted F1 scoring to select model parameters ( Table 10 ) . Table 11 shows the most important axis features of these models . In general , the set of most important features did not change much with parameter choices and roughly aligns with axes that showcase the largest mean differences between each pair of variants . That is , the three axes we show in the main text in Figure 5 are also among the top ten ordered by mean difference for men vs. moids and women vs. femoids .
H Runtime and infrastructure
We only use BERT - base for inference , but the overall runtime cost is high due to the size of our corpora : English Wikipedia and social media discussions . We use one Titan XP GPU with 8 CPU cores for most of the paper , and occasionally expanded to multiple machines with 1080ti and K80 GPUs in parallel when handling social media data . We use BERT for two main purposes : predicting word probabilities to select contexts for constructing axes , and obtaining word embeddings . On one 10 : Parameter choices for random forest classification . Symbols mark selected parameters for each task , where â  refers to men vs. moids , â¡ refers to women vs. femoids , and * refer to women vs. foids . These models had weighted F1 scores of 0.670 , 0.759 , and 0.781 , respectively . Titan XP GPU , the former takes â¼1 hour for one million sentences containing one masked target word each , and the latter takes â¼2.5 hours for one million sentences , including wordpiece aggregation .
Acknowledgements
We thank Manoel Horta Ribeiro for sharing his dataset and materials for our case study , and Sam Robertson , Alexus Lopez , and Harold Cha for evaluating model outputs . In addition , we are grateful for feedback provided by Nicholas Tomlin , Kaitlyn Zhou , and our anonymous reviewers . This research was supported by funding from the National Science Foundation ( DGE-1752814 , IIS-1813470 , and IIS-1942591 ) .
Title2Event : Benchmarking Open Event Extraction with a Large - scale Chinese Title Dataset
Event extraction ( EE ) is crucial to downstream tasks such as new aggregation and event knowledge graph construction . Most existing EE datasets manually define fixed event types and design specific schema for each of them , failing to cover diverse events emerging from the online text . Moreover , news titles , an important source of event mentions , have not gained enough attention in current EE research . In this paper , We present Title2Event , a large - scale sentence - level dataset benchmarking Open Event Extraction without restricting event types . Title2Event contains more than 42,000 news titles in 34 topics collected from Chinese web pages . To the best of our knowledge , it is currently the largest manuallyannotated Chinese dataset for open event extraction . We further conduct experiments on Ti - tle2Event with different models and show that the characteristics of titles make it challenging for event extraction , addressing the significance of advanced study on this problem . The dataset and baseline codes are available at https : / / open-event-hub.github.io / title2event .
Introduction
Event extraction ( EE ) is an essential task in information extraction ( IE ) , aiming to extract structured event information from unstructured plain text . Extracting events from news plays an important role in tracking and analyzing social media trending , and facilitates various downstream tasks including information retrieval ( Basile et al . , 2014 ) , news recommendation system ( Raza and Ding , 2020 ) and event knowledge graph construction ( Gottschalk and Demidova , 2018 ; . Figure 1 shows an example of extracting events from multiple news titles . Based on the extracted events , news reporting the same event could be aggregated and sent to users to provide comprehensive views from different sources . Event extraction can be categorized into two levels : sentence - level EE and document - level EE . Sentence - level EE identifies event entities and attributes in a single sentence ( Ahn , 2006 ) , while document - level EE aims to extract entities of the same event scattered across an article ( Sundheim , 1992 ) . In scenarios such as news aggregation , human - written news titles often preserve the core information of the news event , while news articles may contain too many trivial details . Therefore , performing sentence - level EE on news titles is more efficient than document - level EE on news articles to aggregate relevant news .
However , most EE models trained on traditional sentence - level datasets could not reach ideal performance when extracting events from titles ( Chen et al . , 2015 ; Nguyen and Nguyen , 2019 ; Wadden et al . , 2019 ; Du and Cardie , 2020 ; Li et al . , 2020a ; Lu et al . , 2021 ; Lou et al . , 2022 ) . On the one hand , these models request predefined event types and a specific schema for each of them . Each event schema consists of manually designed argument roles such as event trigger , person , time , and location . Then the extraction of events will The first event is actually the subject of the second event , which indicates these two events are associated . be decomposed into sub - tasks of extracting each argument role separately . Despite the success in traditional EE , the manual design of specific event schema is costly and time - consuming , and the limited predefined event types could not handle a great variety of events emerging from the Internet where most news titles nowadays are derived from . On the other hand , extracting events from Chinese titles could be more challenging than traditional sentence - level EE such as the ACE 2005 benchmark . 1 This is because some unique writing styles are observed in news titles on Chinese social media , as shown in Figure 2 . First , the writing of many titles does not strictly obey the correct grammar . For example , some titles will omit the agent when describing an action for brevity , while others may place the action before the first mention of the agent for emphasis . Second , the role overlap problem , i.e. , the same entity may play different roles in multiple events , usually occurs when the events in the text have certain associations with each other .
Requirement of Domain Knowledge
Although there are about 10 % events in ACE 2005 having this problem , it has not gained enough research attention for quite a long time ( Yang et al . , 2019 ) . However , the role overlap problem is much We write detailed annotation guidelines and conducted two rounds of expert review for quality control . To the best of our knowledge , Title2Event is currently the largest manually annotated Chinese dataset for OpenEE .
3 . It is the first sentence - level dataset with a special focus on titles with its unique values and challenges that little attention has been paid to . We believe Title2Event could further facilitate current EE research in real - world scenarios .
We experiment with different methods on Ti - tle2Event and analyze their performance to address the challenges of this task .
Related Work
Event Extraction Datasets . Automatic Content Extraction ( ACE 2005 ) ( Doddington et al . , 2004 ) is one of the most widely - used corpora in event extraction . It contains 599 documents with 8 event types , 33 event subtypes , and 35 argument roles in English , Arabic and Chinese ( Li et al . , 2021b ) . TAC KBP 2017 2 is a dataset of the event tracking task in KBP which contains 8 event types and 18 event subtypes in English , Chinese and Spanish . MAVEN ( Wang et al . , 2020 ) collects 4,480 Wikipedia documents , 118,732 event mention instances and constructs 168 event types . Despite the large scale , MAVEN merely focuses on event triggers without annotating event arguments . All of the above datasets manually define event types and schema , struggling to handle newly emerging event types in real - world applications .
Open Information Extraction . Open information extraction ( OpenIE ) aims to extract facts in the form of relational tuples from unstructured text without restricting target relations , relieving human labor of designing complex domaindependent schema ( Niklaus et al . , 2018 ) . Due to the release of large - scale OpenIE benchmarks such as OIE2016 ( Stanovsky and Dagan , 2016 ) and CaRB ( Bhardwaj et al . , 2019 ) , neural OpenIE approaches become popular ( Zhou et al . , 2022 ) . Existing neural OpenIE models can be categorized into sequence tagging models ( Stanovsky et al . , 2018 ; Kolluru et al . , 2020a ; Zhan and Zhao , 2020 ) and generative sequence - to - sequence models ( Cui et al . , 2018 ; Kolluru et al . , 2020b ) . We adopt the formulation of OpenIE and represent events as triplets since the event mentions in news titles tend to be brief without complex substructures .
Chinese Event Extraction . Chinese event extraction can be regarded as a special case of EE due to its unique linguistic properties and challenges ( Li et al . , 2021b ) . However , the resources of Chinese EE data are relatively scarce and lack sufficient coverage comparing with EE data in English , which greatly hinders existing research ( Zeng et al . , 2016 ; Lin et al . , 2018 ; Ding et al . , 2019 ; Xiangyu et al . , 2019 ; Cui et al . , 2020 ) . Apart from multilingual datasets with Chinese corpora such as ACE 2005 and TAC KBP 2017 , Chinese Emergency Corpus ( CEC ) 3 collects 6 types of common emergency events . Doc2EDAG and FEED ( Li et al . , 2021a ) are two Chinese financial EE datasets built upon distant supervision . DuEE ( Li et al . , 2020b ) is a document - level EE dataset with 19,640 events categorized into 65 event types , collected from news articles on Chinese social media . Compared with DuEE , our Ti - tle2Event dataset is larger in scale and does not restrict event types .
Dataset Construction
This section describes the process of data collection and annotation details .
Data Collection
We broadly collect Chinese web pages from January to March 2022 using the web crawler logs of the search engine of Tencent as well as a proven business tool to select web pages containing event mentions ( most of them are from news websites ) . Afterwards , the titles of the selected web pages are extracted and automatically tagged with our predefined topics , and titles containing toxic contents are all removed . To ensure the diversity of events , we conduct data sampling every ten days during the crawling period , reducing the occurrence of events belonging to the top frequently appeared topics to make the distribution of topics more balanced . Eventually , around 43,000 instances are collected .
Annotation Framework
Annotation Standard . We summarize some essential parts of our annotation standard . In general , we expect each event could be represented by a ( Subject , Predicate , Object ) triplet where the subject and object could be viewed as the argument roles of the event triggered by the predicate . Multiple event triplets may be extracted from a single title , and they may have some overlaps . However , the predicate of an triplet is considered as the unique identifier of an event , thus multiple triplets of a single title will not share the same predicate . Some important specifications are listed below :
1 ) We define event as an action or a state of change which occurs in the real world . Some statements such as policy notifications or some subjective opinions are not considered as events . Also , if an title is not clearly expressed , or is concatenated by several unrelated events ( e.g. news round - up ) , then it should be labeled as " invalid " by annotators .
2 ) We find the identification of predicates in Chinese is complex , so we specify some rules to unify them . First , if an event tends to emphasize the state change of the subject , e.g. " åé³å¤§æ¡¥é è½¦ " ( Nanyang Bridge opens to traffic ) , then the predicate will be labeled as " éè½¦ " ( open - to - traffic ) instead of " é " with " è½¦ " as the object . Second , for phrases with serialized verbs and dual objects , we integrate the direct target of the action ( i.e. the Patient ) into the predicate expression while taking the indirect patient ( i.e. the Affectee ) ( Thompson , 1973 ) as the object of the event . For example , in " éå­©å­å»å­¦æ ¡ " ( send kids to school ) the predicate will be labeled as " éå»å­¦æ ¡ " ( send - toschool ) with " å­©å­ " ( kids ) as the object . Moreover , we find the colon ( " ï¼ " ) frequently plays the role of predicate in titles , representing the meaning of " say " , " announce " or " require " , etc . We view this as a feature of news titles and allow annotators to label it as the predicate .
3 ) We expect the fine - grained annotations of argument roles , which are intact yet not redundant . All determiners and modifiers of entities are kept only if they largely affect the understanding of events . All triplets are required to have a subject and a predicate , while the object could be omitted as in the original text .
Crowdsourced Annotation . We cooperate with crowdsourcing companies to hire human annotators . After multi - rounds of training in three weeks , 27 annotators are selected . We pay them ï¿¥1 per instance . Meanwhile , four experts are participated in two rounds of annotation checking for quality control . For each instance , a human annotator is asked to write all expected event triplets independently . To reduce the annotation difficulty , we provide some auxiliary information along with the raw title , including the tokenization outputs , to help annotators quickly capture the entities and concepts present in the titles . Note that we do not force annotators to strictly obey the tokenization outputs , as we find that many of them do not match the desired granularity of triplet elements under our criteria . Instead , the annotation is conducted in a < text , label > pair paradigm rather than a token - level tagging paradigm . Moreover , we provide automatic extraction outputs as references . During the initial phase , we design an unsupervised model to extract triplets . After 20,000 labeled instances are collected , we train a better sequence tagging model for the rest of annotation process . Both models are introduced in Section 5 . Meanwhile , as titles often contain some domain knowledge which the annotators may not be familiar with , we allow them to refer to search engines . To ensure the quality , we also allow them to label an instance as " not sure " if they are not confident enough . The crowdsourced annotation is conducted in batches . Every batch of annotated instances undergoes two rounds of quality checking before being integrated into the final version of our dataset . We also develop a browserbased web application to accelerate the annotation process , see Appendix A .
First - round Checking . Each time the crowdsourced annotation of a batch is completed , it is sent to four experts to check whether they meet the requirements of our annotation standard . Instances which do not pass the quality check will be sent back for revision , attached with the specific reasons for rejection . This process repeats until the acceptance rate reaches 90 % .
Second - round Checking . Each batch of annotated instances passing the first - round checking is sent to the authors for dual check . The authors will randomly check 30 % of the instances and send unqualified instances back to the experts along with the reasons for rejection . Slight adjustments on annotation standard also take place in this phase . This process repeats until the acceptance rate reaches 95 % .
Our annotation process encourages positive interactions among the authors , the experts and the crowdsourced annotators , which effectively helps the annotators to understand the annotation standard and provide timely feedback .
Data Analysis on Title2Event
This section describes the statistics and characteristics of Title2Event from various perspectives . Table 1 shows the overview of the dataset .
Topic Distribution . The titles in the dataset can be categorized into 34 topics , 24 of which contain more than 100 instances . Figure 3 Event Distribution . As shown in Table 1 , most of the titles contain more than one event , and the maximum number of events per title is six . We further investigate the distribution of instances containing different numbers of triggers ( i.e. predicates for Title2Event ) , and compare our dataset with the ACE2005 Chinese dataset ( denoted as ACE05 - zh ) 4 as shown in Figure 4 . It can be observed that the phenomenon of multiple events per instance is more common in Title2Event compared with ACE05 - zh , which brings additional challenges in event extraction .
Predicate Distribution . We also investigate the distribution of predicates in Title2Event . Figure 5 shows the distribution of the 30 most frequent predicates in the dataset .
Challenge Distribution . We further analyze to what extent are the observed challenges described in Figure 2 covered in Title2Event . To do this , we randomly sample 1,000 instances and manually annotate 1 ) Whether the instance omits or inverts some event arguments which makes itself not strictly obeying the grammatical norms .
2 ) Whether there 's a text span appearing at multiple events of the instance . ( 3 ) Whether some domain knowledge is crucial in understanding the instance that without these knowledge one might not correctly identify the event arguments . The annotation result shows that 9.70 % of sampled instances are observed with unconventional writing , 21.50 % instances have role overlap problem ( 10 % for ACE 2005 for comparison ) , and 2.80 % instances requires domain knowledge for correct event understating . We believe such statistics are a good identification of the challenging nature of Title2Event .
Methods
Formally , given a sequence of tokens S = < w 1 , w 2 , . . . , w n > , Open EE aims to output a list of triplets T = < t 1 , t 2 , . . . , t m > where each triplet t i = < s i , p i , o i > represents an event occurred in S and s i , p i , o i denote the subject , predicate and object of the event respectively . The object of an event could be empty , and the total number of events per sentence m is not fixed . Open EE can also be aligned with traditional EE task formulation by considering the predicate as the event trigger as well as a unique event type , while the subjects and objects both taken as event arguments .
Based on the task formulation , we first implement an unsupervised method using an existing toolkit . Then , we split the task into trigger extraction and argument extraction , and implement different supervised methods on them .
Unsupervised Method
Since the formulation of Open EE is similar to some traditional tasks such as dependency parsing ( DP ) and semantic role labeling ( SRL ) , we investigate the performance of existing triplet extraction methods on Open EE . Each title will be segmented and tokenized first , then the extraction is conducted as a token - wise sequence - labeling task . Each token will first be labeled by a SRL module on whether it belongs to a semantic role which appears in one of the S - P - O , S - P , P - O semantic tuples . If not , it will be relabeled by a DP module on whether it appears in a syntactical tuple of the above structures . The entire method is implemented using the LTP toolkit ( Che et al . , 2020 ) .
Trigger Extraction
Since the number of triggers per sentence is neither fixed nor given as input , we adopt a token - level sequence tagging model to extract all event triggers in a given sentence based on the inductive bias that event triggers ( i.e. , predicates ) will not overlap with each other ( see Section 3.2 ) . Sequence tagging model requires a set of tags where each tag , aligned with a token , represents a part of an event element ( i.e. , a triplet element ) or a non - event element . Then , the model learns the probability distributions of tags for each given sentence , and outputs triplets based on the predicted tags . We adopt the BIO tagging scheme where a token is tagged B - trg i ( I - trg i ) if it is at the beginning of ( inside ) the i th trigger , or O if it is outside any trigger . The subscript is used to distinguish between different triggers as they might be discontinuous tokens . Since Title2Event is not annotated on tokenlevel ( see 3.2 ) , we perform automatic tagging by locating each annotated event element at the source sentence to get its offset . We use BERT ( Devlin et al . , 2019 ) as the sentence encoder to get the contextualized representations of tokens , and each token representation will be fed to a classification layer to compute the probability distribution of the tags .
Argument Extraction
Argument extraction models take the source sentence and the given triggers as input and output the arguments of each given trigger respectively . Due to the role overlap problem , a token might appear in multiple event arguments and thus has multiple tags , which does not match the common setting of sequence tagging task . Therefore , we iterate over the extracted triggers and extract the arguments of each event trigger separately . We implement three methods for argument extraction .
Sequence Tagging . The first method is a tokenlevel sequence tagging model similar to the trigger extraction model , which also uses BIO tagging scheme for subject and object tokens . During each forward process , to specify the current trigger , we adopt the method proposed by Yang et al . ( 2019 ) . Specifically , the input of BERT encoder is the sum of WordPiece embeddings , position embeddings and segment embeddings , and we set the segment ids of current trigger tokens being one while others being zero to explicitly encode the current trigger . Span MRC . The second method is a span - level tagging model which formulates argument extraction as a machine reading comprehension ( MRC ) task , inspired from Du and Cardie ( 2020 ) and . For each given sentence as well as a specified trigger , the subject and object are extracted separately by prepending a question , e.g. " å¨ä½ < trigger > çä¸»ä½æ¯ ï¼ " ( What is the subject of < trigger > ) ? , into the sentence to form a context like " [ CLS ] question [ SEP ] sentence [ SEP ] " , then the model is asked to extract the answer span from the context for the given question by predicting a start position and an end position . We also use BERT as the context encoder .
Seq2Seq MRC . The third method is a sequenceto - sequence MRC model with same the question design as Span MRC . However , instead of extracting the answer spans from the context , it directly generates a sequence of tokens as the output with the given context by maximizing the conditional probability P (
Y | S ) = m i=1 p ( y i | y 1 , y 2 , . . . , y iâ1 ; S )
, where Y = < y 1 , . . . , y m > is the golden answer . We adopt mT5 ( Xue et al . , 2021 ) , a multilingual text - to - text transformer model as the context encoder as well as the answer decoder .
Experiments
We conduct experiments on Title2Event with the methods described in Section 5 and analyze their performance .
Evaluation Metrics
We adapt the evaluation metrics used in previous works on traditional EE tasks ( Li et al . , 2021b ) to Open EE . We first define the matching criteria : an event trigger or argument is correctly identified if it exactly matches the golden answer , and an event triplet is correctly identified only if all of its elements are correctly identified . We then compute the precision ( P ) , recall ( R ) , and F1 - score ( F1 ) for trigger extraction , argument extraction and triplet extraction respectively .
Evaluation Model
We summarize all the models we implement for experiments here : Unsuper . The unsupervised triplet extraction method implemented by the LTP toolkit using the Chinese - ELECTRA - small ( Cui et al . , 2021 ) model . SeqTag . A pipeline tagging - based model consisting of a trigger extractor and an argument extractor , both are based on the token - level sequence tagging model using BERT - base - Chinese as the encoder , and are trained separately . During inference , the argument extractor predicts the arguments based on the triggers predicted by the trigger extractor . ST - SpanMRC . A pipeline model using a tokenlevel sequence tagging model as the trigger extractor , and a span - level MRC model as the argument extractor , both are based on BERT - base - Chinese . ST - Seq2SeqMRC . A pipeline model which replaces the argument extractor with a sequence - tosequence MRC model using mT5 - base .
Overall Experimental Results
Table 2 shows the results of all Open EE methods experimented on Title2Event . It can be observed that : 1 ) For trigger extraction , the sequence tagging model significantly outperforms the unsupervised model . 2 ) For argument extraction and triplet extraction , ST - Seq2SeqMRC outperforms the other tagging - based models . A large part of the reason is that the unconventional writing styles of titles make it difficult to locate token - level tags or span offsets in the source text , while sequence - to - sequence models are free from these restrictions .
Analysis on Error Propagation
Table 3 shows the results of argument extraction with predicted triggers and with golden triggers . All three models ' performance improve by approximately 20 % if provided with golden triggers , indicating the huge impact of correct triggers on argument extraction and the urgent need to alleviate the propagating error brought by pipeline architecture in future works .
Analysis on Multiple Event Extraction
Figure 4 shows that containing multiple events per instance is an important feature of Title2Event , thus we further investigate the models ' performance on multiple event extraction , as shown in Figure 6 . We can see that as the number of events per instance increases , all models on trigger extraction , argument extraction , and triplet extraction show a decrease in performance , which indicates that multiple events per instance brings additional challenges to open event extraction .
Analysis on Different Topics
We also investigate the results of trigger extraction and argument extraction on different topics of Ti - tle2Event , see Appendix A for details . It can be observed that the F1 - scores of " Weather " are higher than other topics , probably because news titles on weather ( forecast ) usually have a fixed template which makes extraction easier .
Analysis on Error Cases
We summarize three typical challenges observed in Title2Event in Section 1 . Here , we analyze some error cases of the model outputs to further demonstrate the issues . Figure 7 ( a ) shows an error output in trigger extraction , where the given title is unconventionally written by concatenating two predicates . As a result , SegTag is unable to distinguish the two different predicates . Figure 7 ( b ) shows an instance with multiple events and all the models mix up the argument roles . Figure 7 ( c ) shows a sport news title , without the background that Real Madrid and PSG are both football clubs , none of the models properly understand the event that PSG is defeated by Real Madrid . All of the above cases clearly address the challenges present in Ti - tle2Event , which are also common in real - world scenarios , and require advanced study to be better solved .
Conclusions
In this paper , we present Title2Event , a Chinese title dataset benchmarking the task of open event extraction . To the best of our knowledge , Ti - tle2Event is the largest manually - annotated Chinese dataset for sentence - level event extraction . We experiment with different methods and conduct detailed analysis to address the challenges observed in Title2Event , which are rather scarce in existing datasets yet common in real - world scenarios . We believe Title2Event could further facilitate advanced research in event extraction .
Limitations
We summarize the limitations of Title2Event as follows : Evaluation Metrics . We make Title2Event a benchmark for open event extraction with a hope that it could evaluate the performance of domaingeneral EE models . We adapt the formulation of Open IE and represent events in a universal triplet format while adopting traditional EE metrics which is based on exact match . However , we observe that the narrative of events in Chinese titles are extremely diverse . To unify them into the triplet format without losing the core event information , we design detailed annotation guidelines which results in the fact that the a large amount of triplet elements are text spans instead of one or two tokens which is common in traditional EE datasets such as ACE 2005 . Therefore , using exact match in Title2Event might be too strict for model outputs which are just one or two tokens different from the golden text span . We leave the design of finegrained evaluation approaches to future work .
Methods . Some characteristics of Title2Event such as unfixed number of events per instance and the role overlap problem bring difficulties to the model design . We adopt a pipeline architecture which suffers from the error propagation problem as discussed in Section 6.4 . We also adapt some end - to - end models in traditional EE such as TEXT2EVENT proposed by Lu et al . ( 2021 ) to our Open EE benchmark , but find the performance is unexpectedly poor . We conduct preliminary analysis and find that the length of text span in triplets ( as mentioned above ) as well as the relatively complex linearized event structures ( largely due to the multiple events per instance issue ) are the potential factors of the limited performance . Therefore , we do not provide a good end - to - end model as baseline , which might make the model comparison in Section 6 less comprehensive . However , we hope that future works could pay more attention to the design of text - to - structure models except from traditional tagging - based models .
Ethics Statement
As Title2Event is an Open EE dataset which broadly collects contents of various categories on the Internet , keeping the corpus without bias is extremely difficult . However , we put large efforts in cleaning the toxicity of data . First , all crawled web paged are automatically removed if they contain toxic contents using an existing system . During annotation , all instances will be dual checked by the human annotators and manually deleted if not passing the check . Moreover , in our annotation standard , we ask annotators to label only factual events while ignoring all subjective opinions , as we hope Title2Event could be factual and unbiased .
A Appendix Annotation Tool . Figure 8 shows a screenshot of our annotation web page . The raw title are given with auxiliary information , the annotators will first determine whether to abandon this case as well as is this case easy to annotate or not . Then , they will type all plausible events in the text boxes following our annotation guidelines . Results on Different Topics . Figure 9 shows the F1 - scores of trigger extraction ( using SeqTag model ) and argument extraction with golden triggers ( using SeqTag , SpanMRC , and Seq2SeqMRC models ) on the top-10 topics in Title2Event .
Hyper - parameter Settings in Training . For all models , we use the batch size of 32 and train them for 30 epochs on the training set of Title2Event . All models are trained on a single Tesla A100 GPU . We use the linear learning rate scheduler and AdamW as the optimizer . For models based on Bert - base - Chinese , we set the learning rate to be 5e-5 ; For models based on mT5 - base , we set the learning rate to be 1e-4 . All supervised models are implemented using the Huggingface - transformers library .
Summarizing Community - based Question - Answer Pairs
Community - based Question Answering ( CQA ) , which allows users to acquire their desired information , has increasingly become an essential component of online services in various domains such as E - commerce , travel , and dining . However , an overwhelming number of CQA pairs makes it difficult for users without particular intent to find useful information spread over CQA pairs . To help users quickly digest the key information , we propose the novel CQA summarization task that aims to create a concise summary from CQA pairs . To this end , we first design a multi - stage data annotation process and create a benchmark dataset , CO - QASUM , based on the Amazon QA corpus . We then compare a collection of extractive and abstractive summarization methods and establish a strong baseline approach DedupLED for the CQA summarization task . Our experiment further confirms two key challenges , sentencetype transfer and deduplication removal , towards the CQA summarization task . Our data and code are publicly available . 1 * Work done while at Megagon Labs . 1 https : / / github.com / megagonlabs / qa - summarization Q : Is this actually a rigid board or more of a floppy mat ? A : It is rigid.the main board is rigid , the two sides are semi . A : The main area is very sturdy . Then there are two work area pads that are more flexible so when moving those I keep two hands on them . A : 16 inches wide ( there are two ) .A : most certainly will . â¦ â¦ ( omitted 27 QAs ) Summary : This puzzle board comes with a rigid main board . You can arrange pieces in the middle and on two side pieces , and then pick up those side pieces to place them atop the middle area before folding the wings in . The dimension of the puzzle space is 32 " x21.75 " . The closed unit is almost the same size as the puzzle workspace ( 32 " x21.75 " ) . There are two 16 " wide side inserts . The mat holds most 1000 pieces puzzles . It is too big to use on you lap and definitely needs a table . ( a ) . QAs for a puzzle board product ( Input ) ( b ) . Summary of QAs ( Output ) Q : what is the storage size when case is fully closed for storage ? A : Closed size is 32.25 x 22.75 " . Q : What size is the closed unit ? A : Closed is almost the same size as the puzzle workspace . 32.25 x 22 .
Introduction
Community - based Question Answering ( CQA ) enables users to post their questions and obtain answers from other users . With the increase in online services , CQA has become essential for various purposes , including online shopping , hotel / restaurant booking , and job searching . Many online platforms implement CQA features to help users acquire additional information about entities ( e.g. , products , hotels , restaurants , and companies ) of their interests . CQA complements customer reviewsanother type of user - generated content , which provide additional information about the entity but mostly focusing on user experiences and their opinions . While CQA greatly benefits users in decisionmaking , digesting information from original question and answer pairs ( QA pairs 2 ) also has become increasingly harder . Due to the community - based nature , CQA tends to have a large number of heavily repetitive QA pairs , which make it difficult for users , especially those who do not have specific intent ( i.e. , questions ) , to find and digest key information .
Existing summarization efforts for CQA ( Liu et al . , 2008 ; Deng et al . , 2020a ; Fabbri et al . , 2021b ) primarily focus on summarizing answers for a given question , which still assumes that the user has a certain intent . We believe that information spread over QA pairs can be summarized into a more concise text , which helps any users grasp the key points of discussions about a target entity . Therefore , we take a step beyond the scope of answer summarization and propose a novel task of CQA summarization , which aims to summarize a collection of QA pairs about a single entity into a concise summary in declarative sentences ( shown in Figure 1 ) .
The CQA summarization task has the following two unique challenges . First , CQA summarization needs to solve sentence - type transfer as questions in interrogative sentences have to be converted into declarative sentences to make a concise summary . This challenge is not trivial as existing summarization tasks assume that input and output are both written in declarative sentences . Second , CQA contains duplicated questions and answers . That is , different users can post similar questions . A question can have multiple answers , many of which contain duplicate information . Also , unlike questionanswering forums ( e.g. , Quora ) , CQA in online services is less incentivized to remove such redundancy . Slightly different questions / answers can provide detailed and useful information not mentioned in other questions / answers . Having more similar answers supports the information is more reliable . Those properties make existing summarization solutions unsuitable for CQA summarization .
To enable further study of the CQA summarization task , we create a corpus COQASUM by collecting reference summaries on QA pairs from the Amazon QA dataset ( Wan and McAuley , 2016 ; McAuley and Yang , 2016 ) . Reference summary annotation is challenging for CQA summarization , as a single entity ( i.e. , a product for the dataset ) can have so many questions and answers that the annotator can not write a summary directly from them . Furthermore , the sentence - type difference ( i.e. , interrogative vs. declarative ) obstructs summary writing . To make this annotation feasible , we designed a multi - stage annotation framework . Sampled seed QA pairs are given to the annotator to convert into declarative sentences , which are then rewritten into gold - standard summaries by expert writers . At the last step , we collected semantically similar QA pairs to to make the annotated corpus more realistic .
We conduct a comprehensive experiment that compares a collection of extractive and abstractive summarization solutions and establish a strong baseline approach , DedupLED , for the CQA summarization task . Specifically , DedupLED finetunes the entire LED model for summary generation while additional classifier attached to the encoder is optimized to extract representative QA pairs . Leveraging the strengths of both abstractive and extractive summarization objectives , as well as the pre - trained language model checkpoints , Dedu - pLED significantly outperforms the other alternative methods . Our experiment also confirms that DedupLED is suitable for CQA summarization , as the model implements the functionality for both ( 1 ) sentence - type transfer and ( 2 ) duplicate removal .
Our contributions of the paper are as follows :
â¢ We propose the novel task of CQA summarization , which takes QA pairs about a single entity as input and make a summary written in declarative sentences ( Section 2 ) . â¢ We designed a multi - stage annotation framework and collected reference summaries to build the first benchmark corpus for CQA summarization . The corpus is based on the Amazon QA corpus ( Wan and McAuley , 2016 ) and consists of reference summaries for 1 , 440 entities with 39 , 485 QA pairs from 17 product categories . ( Section 3 ) . â¢ We conduct comprehensive experiments on a collection of extractative and abstractive summarization methods and develop a strong baseline DedupLED , which implements key characteristics of sentence - type transfer and duplication removal functions . ( Section 4 and Section 5 ) .
Problem definition
Let D denote a dataset of questions and answers on individual entities { e 1 , e 2 , ... , e |D| } ( e.g. , products or hotels ) . For every entity e , we define a set of question - answer pairs QA e = { ( q i , a i ) } |QAe| i=1 , where the question q i and the answer a i are sequences of tokens q i = ( w 1 , ... , w n ) and a i = ( a 1 , ... , a m ) 3 . Given a set of QA pairs for an entity e , the CQA summarization task is to generate a natural language summary S e from QA e .
The COQASUM Corpus
We first describe the multi - stage annotation framework to collect gold - standard reference summaries from input QA pairs and then describe our benchmark dataset COQASUM .
A Multi - stage Annotation Framework
Reading and summarizing a set of QA pairs is challenging and error - prone for three reasons : ( 1 ) a large number of QA pairs , ( 2 ) the heavy repetition and noise in both questions answers , and ( 3 ) the difficulty of converting questions and answers into declarative summaries . Thus , it is infeasible to collect high - quality reference summaries by simply showing a set of QA pairs and asking annotators to write a summary . In this paper , we design a multistage annotation framework that first simplifies this complex annotation task into more straightforward annotation tasks and then enriches the collected annotations .
Figure 2 depicts the schematic procedure of the multi - stage annotation framework . For each entity and its corresponding QA pairs in the original corpus , we first select representative seed QA pairs and ask annotators to rewrite them into declarative sentences , which are then concatenated into a raw summary . Next , we ask highly - skilled annotators to polish the raw summary into a more fluent summary . In the last step , we enrich the seed QA pairs by selecting semantically similar QA pairs from the original corpus .
Step 1 : QA Pair Selection and Rewriting
In this step , we use a drastic strategy to remove duplicate QA pairs and simplify the annotation task for human annotators . A natural way to deduplicate QA pairs is by manually comparing existing QA pairs ' semantics and only keeping the unique ones . However , we found this approach less practical because asking human annotators to perform the comparison is extremely expensive . It is also hard to validate the quality because selecting a representative QA from a set of semantically similar ones is a subjective process .
Thus , we use a heuristic - based strategy to select representative QA pairs from the original corpus . Specifically , we use the following two rules to filter out QA pairs that are not suitable for creating reference summaries : ( 1 ) length rule : QA pairs with less than 5 or more than 150 tokens ; ( 2 ) pronoun rule : QA pairs that include first - person pronouns . We found that long questions / answers tend to contain their background information ( e.g. , personal stories ) , which is irrelevant to the entity . First - person pronouns are also a strong indicator for such questions / answers . After the filtering , we randomly sample k seed QA pairs from the remaining ones . In addition , to avoid redundancy , we only sample seed QA pairs of different questions . 4 For each of the k seed QA pairs , we ask human annotators to rewrite them into declarative sentences . We recruited three crowd workers from Amazon Mechanical Turk 5 to annotate every QA pair and chose the highest - quality annotation based on 6 criteria : ( 1 ) length of LCS against the original QA pair , ( 2 ) use of yes / no , ( 3 ) use of interrogative determiner ( e.g. , What ) , ( 4 ) use of first - person pronouns , ( 5 ) use of the item name at the beginning , ( 6 ) the ignorance of the question information . We also blocked MTurk workers with consistently low - quality annotations to ensure the quality of annotations .
Step 2 : Summary Writing
We form a raw summary by concatenating annotations ( i.e. , declarative sentences rewritten from QA pairs ) obtained in the first step for each entity . The raw summaries are not necessarily fluent and coherent as different pieces are annotated independently . They may also contain redundant information . To address these issues , we use another annotation task to polish and write a summary from the raw summary . To ensure the quality , we hired highly - skilled writers from Upwork 6 by conducting screening interviews for this annotation task . For each entity , we show annotators the raw summary and ask them to write a fluent and concise summary .
Step 3 : Enriching Input QA Pairs Recall that in Step 1 , we select k seed QA pairs for each entity . The seed QA pairs are less redundant because of the filtering and sampling strategy . This does not reflect the real - world scenario , where similar questions are asked multiple times , and each question often contains several answers .
To align the benchmark with more realistic settings , we enrich the input QA pairs in Step 3 . In Sampled SeedQA â  SeedQA Selection and Rewriting Q : how wide is each Side piece ? " A : Each side piece is 16 inches wide ( there are two ) .
Q : Can this be used in your lap while seated in a chair or sofa ? A : Its too big to use on your lap . Definitely needs a table . Q : will this mat hold 1000 piece puzzle ? A : most certainly will .
There are two side pieces , each one is 16 inches wide . This product is too big to use on your lap while seated in a chair or sofa . You need a table for it . This puzzle mat can hold 1000 puzzles .
Rewritten SeedQA
( in declarative language ) There are two side pieces , each one is 16 inches wide . This product is too big to use on your lap while seated in a chair or sofa . You need a table for it . This puzzle mat can hold 1000 puzzles . This puzzle mat can hold 500 puzzles .
â¡ Summary Writing
Raw summary
( concatenated from rewritten QAs ) particular , we add all answers to every question in the seed QA pairs . Besides , we retrieve questions that are semantically similar to seed questions and add all the answers to the input QA pairs . For semantic similarity calculation , we use BERT embeddings and word overlap to find the candidates , followed by an additional crowd - sourcing task using Appen 7 for manual validation . The validation step ensures that our reference summaries can be created from the enriched input QA pairs .
Dataset Statistics
Using the multi - stage annotation framework , we created the COQASUM benchmark based on the Amazon QA dataset ( Wan and McAuley , 2016 ; McAuley and Yang , 2016 ) Table 1 shows the statistics of COQASUM . We 7 https : / / appen.com / confirm that the average word count of input QA pairs / raw summaries / reference summaries is consistent for different categories . The novel n - gram distributions also confirm that COQASUM offers a fairly abstractive summarization task . Some product categories such as " Office Products " and " Patio Lawn and Garden " have lower novel n - gram ratios , indicating that the task becomes relatively extractive . The word count difference between the raw summary and the reference summary supports the value and quality of the summary writing task in
Step 2 , indicating that the raw summary still contains some redundant information .
Models
To examine the feasibility and explore the challenges of CQA summarization , we tested several summarization solutions on COQASUM . The models are grouped into Extractive , Extractive - Abstractive and Abstractive methods .
Extractive
Extractive methods extract salient sentences from input QA pairs as the output summary . We consider unsupervised ( LexRank ) and supervised ( Bert- SumExt ) models in addition to a simple rule - based baseline that filters the original seed input QA . We evaluate those methods to understand how well selecting sentences without sentence - type transfer performs on the task .
SeedQAs : This method concatenates the seed QA pairs used in the first annotation task of the multi - stage annotation framework . This is an oracle method as we can not tell which QA pairs were used as seed QA pairs for annotation . We use this method to verify the performance of simply extracting QA pairs .
LexRank ( Erkan and Radev , 2004 ) : This is an unsupervised extractive method , which uses the similarity between words to build a sentence graph and compute the centrality of sentences for selecting top - ranked sentences as summary .
BertSumExt ( Liu and Lapata , 2019 ) : Bert - SumExt is a supervised model , which fine - tunes BERT ( Devlin et al . , 2019 ) to extract sentences by solving multiple sentence - level classifications .
In our experiment , we use BertSumExt to extract salient QA pairs from the input , where the goldstandard labels are acquired by greedily select QA pairs that maximize the ROUGE scores to the goldstandard summary 8 .
Extractive - Abstractive
While extractive methods can remove duplication from the input , they can not transfer interrogative 8 https : / / github.com / nlpyang / BertSum sentences ( i.e. , questions ) into declarative sentences . To handle this better , we combine extractive and abstractive models to implement two - stage solutions . We also test an existing two - stage algorithm in addition to another summarization model that learns to extract and rewrite in an end - to - end manner .
LexRank+LED This method is a select - thenrewrite hybrid model . Using a sentence - type transfer model , the model rewrites each of the QA pairs extracted by LexRank into declarative sentences , which are then concatenated as an output summary . For the sentence - type transfer model , we fine - tune the LED model ( Beltagy et al . , 2020 ) on the seed QA pairs and their rewritten texts collected in Step 1 of the multi - stage annotation pipeline ( Section 3.1 ) .
LED+LexRank This method is a rewrite - thenselect hybrid model that swaps the steps of LexRank+LED . It uses LexRank to extract salient sentences from input QA pairs rewritten by the same sentence - type transfer model .
Bert - SingPairMix ( Lebanoff et al . , 2019 ) Bert - SingPairMix is a select - then - rewrite - style model that first selects salient sentences from the input and then summarizes the selected sentences into the summary . In our experiment , we use our goldstandard summaries to train both the content selection model and the abstractive summarizer . FastAbstractiveSum ( Chen and Bansal , 2018 ) FastAbstractiveSum also implements select - thenrewrite summarization via reinforcement learning .
The model learns to select representative sentences with the extractor and rewrite the selected sentences with the abstractor . We train a FastAbstractiveSum model on the gold - standard summaries .
Abstractive
As the final group of models , we explore abstractive models that directly summarize input QA pairs . Specifically , we use LED and its variants , which can take long - document as input . Our DedupLED is a variant of LED and falls into this group . HierLED ( Zhu et al . , 2020 ; Zhang et al . , 2021 ) Hierarchical LED ( HierLED ) is a variant of LED , which has two encoders for token - level and QAlevel inputs to handle the structure of QA pairs better . We use the same architecture as Hierarchical T5 ( Zhang et al . , 2021 ) , replacing T5 with LED . We fine - tune the model in the same manner as LED .
DedupLED While pre - trained encoder - decoder models , including LED , are known to be powerful summarization solutions , they do not explicitly implement deduplication functionality . Inspired by BertSumExt , we consider incorporating a classifier layer optimized to extract the original seed QA pairs into an LED model and fine - tuning the LED model via multi - task learning , which we refer to as DedupLED . Figure 3 depicts the model architecture . The classifier layer is trained to select the original seed QA pairs , so the shared encoder learns to detect duplicate information while the decoder is optimized to generate a summary . In the training time , DedupLED uses the original seed QA pair information in addition to the gold - standard summaries in the training data . We would like to note that DedupLED does not require any additional information other than input QA pairs in the summary generation phase .
Evaluation
We conduct comparative experiments to evaluate those models for the CQA summarization task on the COQASUM dataset . We randomly split the data into train / validation / test sets , which consist of 1152 / 144 / 144 entities , respectively . For LexRank , we limit the output length based on the average reference summary length in the training set . For LED and its variations , we fine - tuned the allenai / led - base-16384 checkpoint using the Hugging Face Transformers library . 9 We report the performance of the best epoch ( based on ROUGE-1 F1 ) chosen on the validation set for all the supervised models .
Automatic Evaluation
For automatic evaluation , we use ROUGE ( Lin , 2004 ) F1 10 and BERTScore ( Zhang et al . , 2019 ) F1 11 with the default configuration . The performance and required supervision of all models described in Section 4 are shown in Table 2 .
Extractive : SeedQAs , which simply selects the original QA pairs , performs badly . This is expected because while with high recall ( 88.45 R1 - recall ) , the Oracle method suffers badly from low precision , largely due to the sentence - type inconsistency ( i.e. , interrogative vs. declarative ) and duplication in input QA pairs . LexRank , the unsupervised summarization baseline , performs slightly better than SeedQAs thanks to its ability to select more concise QAs for the output summary . BertSumExt , while leveraging gold - standard summaries , achieves similar performance with LexRank . We believe the discrepancy between interrogative and declarative sentences in input QA pairs and gold - standard summaries is the primary cause of the performance .
Extractive - Abstractive : Extractive - abstractive models achieve better performance than extractive models . The sentence - type transfer helps LexRank+LED / LED+LexRank achieve a much higher R1 score while comparative R2 / RL / BS scores against the original LexRank . This implies the limitation of sentence selection before / after sentence - type transfer . Also , the sentence - type transfer model was trained on seed QA pairs and their corresponding declarative sentences , not the gold - standard summaries . Thus , another factor may be the difference between the rewritten QA pairs and the gold - standard summaries . Both FastAbstractiveSum and BERT - SingPairMix , which are directly supervised by the gold - standard summaries , show significantly better performance than the extractive models . The results confirm that those models can learn to perform both sentence - style transfer and duplication removal directly from gold - standard summaries .
Abstractive : All three models achieve strong performance on the CQA summarization task . The vanilla LED outperforms extractive / extractiveabstractive models . By incorporating the hierarchical structure into the model , HierLED improves the performance against the vanilla LED . Furthermore , DedupLED achieves the best performance for all the evaluation metrics . This confirms that by adding an auxiliary objective and using another supervision ( i.e. , seed QA pair selection ) , DedupLED appropriately learns to deduplicate while learning to summarize input QA pairs . Takeaway : From the results , we confirm that both sentence - style transfer and duplication removal are crucial for the CQA summarization task . In addition , fine - tuning pre - trained language models using the gold - standard summaries offers strong performance , better than manually - crafted twostage summarization models . Finally , by incorporating the duplication removal functionality into the model via multi - task learning , we show that DedupLED establishes a strong baseline for the CQA summarization task .
Human evaluation
We further conducted human evaluation to judge the quality of generated summaries by different models . For every entity in the test set , we showed summaries generated by four models ( LexRank , FastAbstractiveSum , BERT - SinglePairMix , and DedupLED ) to three human judges 12 to choose the best and worst summaries for three criteria : informativeness ( Inf . ) , coherence ( Coh . ) , and conciseness ( Con . ) . Then , we computed the performance of the models using the Best - Worst Scaling ( Louviere et al . , 2015 ) . Table 3 shows that DedupLED consistently achieves the best performance in all three criteria . On the other hand , LexRank , as expected , performs the worst among all the methods we tested . The human evaluation performance trend aligns with the automatic evaluation performance , validating the quality of COQASUM as a Inf .
Coh .
Con . benchmark for the CQA summarization task .
6 Analysis
Choice of Pre - trained Language Models
To justify our observation that pre - trained language models have strong abilities we test and compare three additional pre - trained language models on COQASUM : PEGASUS ( Zhang et al . , 2020 ) , T5 ( Raffel et al . , 2020 ) , and BART ( Lewis et al . , 2019 ) . We confirm that all models perform better than the extractive and extractive - abstractive models . While PEGASUS and T5 show similar ( 24.81 and 24.61 RL , respectively ) , they are less effective than BART and LED ( 26.89 and 26.01 RL , respectively ) .
Learning Curve Analysis
Since collecting reference summaries is costly and time - consuming , we investigate the models ' performance with limited training data . We tested the models ' performance when trained with 20 % , 40 % , . . . , 100 % of the training data . Figure 4 shows the ROUGE - L F1 scores of DedupLED and FastAb - stractiveSum when trained on different size of training data . By leveraging a pre - trained checkpoint , DedupLED performs consistently and substantially better than FastAbstractiveSum , which is trained from scratch . DedupLED also shows a faster learning curve and reaches the plateau in performance when trained with 60 % and more data . This supports that the annotation size of COQASUM is sufficient for fine - tuning pre - trained language models , while it may be insufficient for non - pre - trained models . it on the five categories . For each category , we split entities into train / dev / test sets in 0.8 / 0.1 / 0.1 ratios .
Related Work
Opinion summarization ( Amplayo et al . , 2022 ) aims to create a summary from multiple customer reviews . While opinion summarization is relevant to CQA summarization as it summarizes consumergenerated text , customer reviews are significantly different from QA pairs in CQA as they are selfcontained and tend to contain more subjective information . Recent opinion summarization models have adopted pre - trained LMs ( LED ) for summarizing multiple reviews ( Oved and Levy , 2021 ; Iso et al . , 2022 ) .
A line of work studies on summarizing answers in CQA , which can be categorized into extractive models ( Liu et al . , 2008 ; Chan et al . , 2012 ; Deng et al . , 2020a , b ) and abstractive models ( Fabbri et al . , 2021b ; Chowdhury et al . , 2021 ) . Among them , Chowdhury and Chakraborty ( 2019 ) created a benchmark by selecting the best answer as the reference summary , and Fabbri et al . ( 2021b ) has collected professionally written reference summaries for answer summarization . Our CQA summarization differs from answer summarization as we con- ( Zhang et al . , 2021 ) , customer support conversations ( Feigenblat et al . , 2021 ) , conversations in multiple domains ( Fabbri et al . , 2021a ) , and forum discussions ( Khalman et al . , 2021 ) . CQA summarization is similar to those tasks in creating abstractive summaries from multiple turntaking conversations between more than one user . Meanwhile , we also found that CQA summarization tends to contain more duplication in the input by nature as the compression ratio ( i.e. , input length / summary length ) of COQASUM is 10.88 % , which is smaller than EmailSum ( 29.38 % ) and Fo - rumSum ( 11.85 % ) . We also tested HierLED , a variant of the strongest baseline for E - mail thread summarization , and confirmed that DedupLED performs better than HierLED , indicating that CQA summarization offers unique challenges that are not in E - mail summarization .
Conclusion
We propose the CQA summarization task to summarize QA pairs in Community - based Question Answering . We develope a multi - stage annotation framework and created a benchmark COQASUM for the CQA summarization task . Our multi - stage annotation framework decomposes a complex annotation task into three much simpler ones , thus allows higher annotation quality . We further compare a collection of extractive and abstractive summarization methods and establish a strong baseline method DedupLED for the CQA summarization task . Our experiment also confirms two key challenges , sentence - type transfer and duplication removal , towards the CQA summarization task .
Limitations
As we propose and tackle a challenging summarization task , the paper has certain limitations . First , our benchmark is in a single domain ( E - commerce ) in a single language ( English ) , which not necessarily ensuring the generalizability for other domains and languages . Second , the quality of our annotations relies on the initial selection of seed QA pairs . As we discussed in the paper , we filtered high - quality seed QA pairs to minimize the risk . Nevertheless , it may not accurately replicate the summarization procedure by experts . Third , we use rules and heuristics to ensure the quality of the freetext annotation . Despite being able to detect and eliminate a significant ratio of low - quality annotation , our rules and heuristics do not provide perfect guarantee , meaning that COQASUM may still contain noisy and low - quality annotations . With those limitations , we still believe that the paper and the benchmark are benefitial for the community to take a step beyond the scope of existing summarization tasks .
Ethics Statement
For the annotation tasks , we paid $ 10 hourly wage for the crowd workers on MTurk and Appen ( Steps 1 and 3 ) and $ 15 to $ 30 hourly wage for the Upwork contractors ( Step 2 ) , making sure to pay higher than the minimum wage in the U.S. ( i.e. , $ 7.25 per hour ) . Our COQASUM is based on the publicly available Amazon QA dataset . To our knowledge , the dataset does not contain any harmful content .
TAGPRIME : A Unified Framework for Relational Structure Extraction
Many tasks in natural language processing require the extraction of relationship information for a given condition , such as event argument extraction , relation extraction , and taskoriented semantic parsing . Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks . In this work , we propose to take a unified view of all these tasks and introduce TAGPRIME to address relational structure extraction problems . TAGPRIME is a sequence tagging model that appends priming words about the information of the given condition ( such as an event trigger ) to the input text . With the self - attention mechanism in pre - trained language models , the priming words make the output contextualized representations contain more information about the given condition , and hence become more suitable for extracting specific relationships for the condition . Extensive experiments and analyses on three different tasks that cover ten datasets across five different languages demonstrate the generality and effectiveness of TAGPRIME .
Introduction
There are many tasks in natural language processing ( NLP ) that require extracting relational structures from texts . For example , the event argument extraction task aims to identify event arguments and their corresponding roles for a given event trigger ( Huang et al . , 2022 ; Wang et al . , 2019 ) . In entity relation extraction , the model identifies the tail - entities and head - entities that forms specific relations Yu et al . , 2020 ) . In taskoriented semantic parsing , the model predicts the slots and their semantic roles for a given intent in an * The authors contribute equally .
utterance ( TÃ¼r et al . , 2010 ; . These tasks are beneficial to a wide range of applications , such as dialog systems , question answering ( Yasunaga et al . , 2021 ) , and narrative generation ( Chen et al . , 2019a ) . Prior works usually design models to specifically address each of the tasks Miwa and Bansal , 2016 ; Fu et al . , 2019 ; Zhang et al . , 2018 ) . However , less attention is paid to the commonality among these tasks and having a unified framework to deal with them and provide a strong baseline for every task .
In this work , we take a unified view of these NLP tasks . We call them relational structure extraction ( RSE ) tasks and formulate them as a unified task that identifies arguments to a given condition and classifies their relationships . The condition could be a textual span , such as an event trigger for event argument extraction , or a concept , such as an intent for task - oriented semantic parsing .
We present TAGPRIME , a simple , unified , and strong model , which follows a sequence tagging paradigm with a priming technique , which is proposed by Fincke et al . ( 2022 ) . TAGPRIME inherits the strength of sequence tagging models to unifiedly address RSE by converting the relational structure into a sequence of predictions by sequentially labeling tokens in the input passage . TAGPRIME further improves this framework 's performance by better incorporating information about the given condition via priming . Traditional sequence tagging models usually leverage learnable feature embeddings to incorporate information about the given condition before the tags are assigned , as illustrated in Figure 1 ( a ) . With the priming mechanism , TAGPRIME augments the input text with condition - specific contexts , as illustrated in Figure 1 A conventional sequence tagging model that embeds conditional information by adding learnable features to the output representation from a pre - trained language model . In the shown example , the conditional features contain two parts : one is the token embedding representing the conditional word " military " , and the other is an entity type embedding . ( b ) TAGPRIME with condition priming : The conditional information is further applied to the input sequence to induce the output representation from the pre - trained language model to become condition - aware . ( c ) TAGPRIME with condition & relationship priming : Our approach that further append the verbalized relationship to TAGPRIME with condition priming model . For this case , the goal of the tagging model is to make predictions specific to the relationship type in the input . We omit CRF layers after MLP layers in this figure for better readability .
priming technique comes from the nature of the self - attention mechanism in pre - trained language models . Augmenting input text with conditionspecific contexts makes the sentence representations condition - specific directly . Thus , it unlocks the capability of sequence tagging methods for relational structure extraction better than the commonly used feature embedding approach , as shown in Section 5 .
Our contributions can be summarized as follows .
( 1 ) We take a unified view of NLP tasks that requires extracting relational structures , including end - to - end event extraction , end - to - end relation extraction , and task - oriented semantic parsing . Then , we present TAGPRIME , a unified sequence tagging model with priming that can serve as a strong baseline to various relational structure extraction problems . ( 2 ) Thorough experiments on three different tasks show that TAGPRIME achieves competitive performance than the current state - of - the - art on ten datasets in five different languages . ( 3 ) We propose a novel efficient approximation to speed up TAG - PRIME during inference time without sacrificing too much performance .
Our code will be publicly accessible at https : / / github.com / PlusLabNLP / TagPrime .
Related Work
Many natural language processing applications require extracting relational structures , including event extraction , relation extraction , coreference resolution , etc . The prevalence of these applications makes us hard to exhaustively list them in this short summary , hence , we mainly focus on related works for the applications we experiment on .
Event extraction . Early works in event extraction mostly consider a pipelined approach ( Nguyen and Grishman , 2015 ; Wang et al . , 2019 ; to deal with event extraction . Some followup works argue that pipelined design leads to error propagation issues and hence propose end - to - end approaches to better capture dependencies between each prediction Li et al . , 2013 ; Nguyen et al . , 2016 ; Hsu et al . , 2022b ; Lu et al . , 2021 ; Huang and Peng , 2021 ) . However , recently , some empirical studies ( Hsu et al . , 2022b ; Zhong and Chen , 2021 ; Fincke et al . , 2022 ) also show that when an abundant amount of data is used to learn representations for each pipelined task , it is hard to conclude that joint learning approaches always provide a stronger result . This aligns with our discovery in experiments -even though we apply a pipelined approach with a simple sequence tagging framework on event extraction , with the help of priming to learn more condition - aware contextualized representation , we can still achieve very strong performance on multiple datasets .
Relation extraction . End - to - end relation extraction can usually be solved using two categories of approaches . The first one is to directly perform joint inference on named entities and their relation ( s ) ( Zheng et al . , 2017 ; Wang and Lu , 2020 ; Katiyar and Cardie , 2017 ; Miwa and Bansal , 2016 ; Fu et al . , 2019 ) . The second category is to perform a pipeline that first extracts named entities , and then performs relation classification ( Wu and He , 2019 ; Hsu et al . , 2022a ; Lyu and Chen , 2021 ; Peng et al . , 2020 ; Zhou and Chen , 2021a ; Lu et al . , 2022 ) , which assumes that both the head - entity and tail - entity are given . Yet , in our unified formulation for relational structure extraction tasks , we extract tail - entities and their corresponding relation types for a given head - entity , which is more similar to a less frequently studied framework called cascading approaches Yu et al . , 2020 ) . Despite being a less popular formulation to deal with end - to - end relation extraction , TAGPRIME presents a strong performance compared to prior studies , showcasing the practicality and effectiveness of our unified formulation . Task - oriented semantic parsing . Task - oriented semantic parsing , which focuses on intent classification and slot filling , has a long history of development ( TÃ¼r et al . , 2010 ; Gupta et al . , 2018 ; Zhang et al . , 2018 ; Louvan and Magnini , 2020 ) . Recently , some more advanced neural network - based approaches have been proposed , such as MLP - mixer ( Fusco et al . , 2022 ) or sequence - to - sequence formulation ( Desai et al . , 2021 ) . Among them , JointBERT ( Chen et al . , 2019b ) , a sequence - tagging - based model that is trained to jointly predict intent and extract slots , serves as a widely - used baseline due to its simplicity . Our approach benefits from the same simplicity as JointBERT and can further improve its performance .
Method
We first introduce our view to unify RSE problems and then discuss how TAGPRIME approaches this problem under a unified framework of sequence tagging model with priming . 1 , r c 2 , ... , r c l ] towards the condition c , where r c i â A and A is the set of all possible relationships or attributes . Many NLP tasks can be formulated as an RSE task . We showcase how this formulation can be applied to event extraction , entity relation extraction , and taskoriented semantic parsing below .
A Unified Formulation of RSE
End - to - end event extraction . End - to - end event extraction aims to extract events from given texts ( Ma et al . , 2020 ; Hsu et al . , 2022b ; ) . An event contains a trigger , which is the textual span that best represents the occurrence of an event , and several arguments , which are the participants involved in the event with different argument roles . We consider a pipeline solutionafter the event triggers are identified , an argument extraction model extracts the event arguments and their corresponding roles for each given event trigger . Under the RSE formulation , the condition c is the given event trigger , and the target spans s c and the relationships r c are the arguments and their argument roles , respectively .
End - to - end relation extraction . Relation extraction identifies entities and their relations from texts , and it is usually solved by pipeline approachesfirst extracting named entities and then predicting relations for each entity - pair ( Wu and He , 2019 ; Zhong and Chen , 2021 ) . Under the new formulation , an RSE model is used to predict tail - entities and the relations for each extracted named entity that serves as the head - entity . For example , in Figure 1 ( b ) , we extract the tail - entities ( " Iraqi " and " base " ) and their relation ( " Part - Whole " and " ART " ) for the head - entity , " military " . In this way , each given head - entity is the condition c , and the extracted tail - entities are s c , with relations , r c . Task - oriented semantic parsing . Task - oriented semantic parsing aims to classify the intent and parse the semantic slots in an utterance ( to a taskoriented dialog system ) Gupta et al . , 2018 ) . To fit into our formulation , we first predict the intent and then use a relational structure extraction model to predict the slots ( s c ) as well as their semantic roles ( r c ) for the given intent ( c ) .
Sequence Tagging Model for RSE
We hereby introduce the typical way of applying a sequence tagging model to unifiedly solve relational structure extraction . The goal of our sequence tagging model for relational structure extraction is to predict the BIO - tag sequence y = [ y 1 , y 2 , ... , y n ] , where each y i is the corresponding tag for each token x i in the input text . The BIOtag sequence can then be decoded to represent the extracted spans s c ( and their relationships r c ) .
Specifically , given an input text , we obtain the contextualized representation z i for each token x i by passing the passage to a pre - trained language model . 1 To embed the information of the condition c , one commonly - used technique is to add conditional features to z i ( Ma et al . , 2020 ; Yu et al . , 2020 ) , as shown in Figure 1 ( a ) . For example , in Ma et al . ( 2020 ) , they use a token embedding of the given event trigger word and a learnable event type feature as the conditional features for the task of event argument extraction . In such case , the feature of c will contain the contextualized word representation z j , if x j is the token that represents the given condition , i.e. , event trigger . In our experimental setup , if the given condition can be represented as an input span , we will include the span embeddings as the conditional features together with the type embeddings , such as the cases for event extraction and relation extraction . If the condition is only a concept , such as the task - oriented semantic parsing case , the conditional features will only contain type embeddings . Augmented with these conditional features , the final representation for token x i is further fed into multi - layer perceptrons and a conditional random field ( CRF ) layer ( Lafferty et al . , 2001 ) to predict the BIO - tag sequence y , as illustrated in Figure 1 ( a ) .
TAGPRIME
TAGPRIME follows the sequence tagging paradigm but utilizes the priming technique for better leverage information about the input condition .
Condition Priming . Motivated by previous work ( Fincke et al . , 2022 ) , we consider priming to inject the information of the condition c to further improve the sequence tagging model . The priming mechanism informs the model of the conditional information by directly appending conditional information to the input text . However , unlike Fincke et al . ( 2022 ) that uses an integer string to represent features in a categorical style , we use a naturallanguage - styled indicator to better exploit the semantics of the condition . The indicators can be obtained by verbalizing the conditional information .
Take Figure 1 ( b ) as an example , when extracting the tail - entities and the relationships for the " military " head - entity ( condition c ) , we first verbalize the entity type of " military " , i.e. , from " Org " to " Organization " . Then , the string " military " and " Organization " are appended to the input text , which serves as the information about the condition c .
The priming technique leverages the selfattention mechanism in pre - trained language models and makes the token representation z i conditionaware . Hence , the representation of every z i is more task - specific than the one in the model described in Section 3.2 . More precisely , for tagging models without priming , the representation z i usually captures more general information that focuses on the context of input text . For models with priming , the representation z i is affected by the additional verbalized words when computing attention . Hence , z i becomes more task - specific and more suitable for addressing the task ( Zheng and Lapata , 2022 ; Zhong and Chen , 2021 ) . Additionally , the priming method can be easily combined with conditional features described in Section 3.2 . More discussion on this will be shown in Section 5 .
Relationship Priming . The same idea of condition priming can also be extended to relationship . Specifically , we decompose a relational structure extraction task into several extraction subtasks , each of them only focusing on one single relationship r ( r â A ) . Similar to the condition priming , we verbalize the relationship information and append related strings to the input text as well . Therefore , the representation z i is aware of the relation - ship r and specific for predicting spans with relationship r to the condition c .
For example , in Figure 1 ( c ) , for the given relationship " Part - Whole " , we first verbalized it into " is part of " . Then , the string " is part of " is appended to the input text together with the condition priming strings . The BIO - tag sequence can be decoded into those tail - entities s c that form " Part - Whole " relationship ( s ) with the given head - entity " military " .
Discussion . A similar idea of appending tokens in the pre - trained language model 's input to affect the output text representation has also been leveraged in Zhou and Chen ( 2021b ) ; Zhong and Chen ( 2021 ) . Yet , different from their works that only focus on relation classification and apply instancespecific information , our TAGPRIME with relationship priming method focuses on using task - specific information , because we decompose relational extraction into sub - tasks . We want that different taskspecific representation can be learned for different sub - tasks , hence proposing relationship priming .
An underlying advantage of TAGPRIME with relationship priming is its ability to handle cases containing multi - relationships . After we decompose a relational structure extraction task into several extraction subtasks , we do not perform any filtering to address conflict relationship predictions between the same condition and extracted span . This is to enlarge our model 's generality to different scenarios .
Experiments
To study the effectiveness of TAGPRIME , we consider three NLP tasks : ( 1 ) end - to - end event extraction , ( 2 ) end - to - end relation extraction , and ( 3 ) taskoriented semantic parsing . All the results are the average of five runs with different random seeds .
End - to - End Event Extraction
Datasets . We consider the two most widely - used event extraction datasets , ACE-2005 ( Doddington et al . , 2004 ) and ERE ( Song et al . , 2015 ) . For ACE-2005 ( ACE05 - E ) , we experiment on the English and Chinese portions and keep 33 event types and 22 roles , as suggested in previous works ( Wadden et al . , 2019 ; Hsu et al . , 2022b ) . For ERE , we consider the English and Spanish annotations and follow the preprocessing of to keep 38 event types and 21 roles .
Baselines . We consider the following end - to - end event extraction models , including DyGIE++ ( Wad - den et al . , 2019 ) , TANL ( Paolini et al . , 2021 ) , Text2Event ( Lu et al . , 2021 ) , OneIE , and DEGREE ( Hsu et al . , 2022b ) . Since TAGPRIME requires trigger predictions , we simply take the trigger predictions made by a simple sequence tagging model trained with multi - tasking on trigger detection and named entity recognition .
For TAGPRIME , DyGIE++ , and OneIE , we consider BERT - large ( Devlin et al . , 2019 ) for ACE05 - E ( en ) and ERE ( en ) , and consider XLM - RoBERTalarge ( Conneau et al . , 2020 ) for ACE05 - E ( zh ) and ERE ( es ) . For generation - based models , we consider BART - large ( Lewis et al . , 2020 ) for DEGREE , T5 - base ( Raffel et al . , 2020 ) for TANL , and T5large ( Raffel et al . , 2020 ) for Text2Event , as suggested by their original papers . language models with the dropout rate being 0.2 . We use AdamW optimizer . For parameters that are not pre - trained we set the learning rate to 10 â3 and the weight decay to 10 â3 . For parameters that are not pre - trained we set the learning rate to 10 â5 and the weight decay to 10 â5 . We consider the linear scheduler with a warm - up , where the warm - up epoch is 5 . The number of epochs is 90 . The training batch size is set to 6 . For conditional token features and learnable features , the dimension is set to 100 . It takes around 6 hours to train with a NVIDIA RTX A6000 with 48 GB memory .
Evaluation metrics . Following previous works ( Wadden et al . , 2019 ; , we measure the correctness of arguments based on whether the offsets of the argument span match or not . We Table 1 : Results of end - to - end event extraction . All values are micro F1 - score , and we highlight highest scores with boldface . TAGPRIME with conditional and relationship priming achieves more than 1.4 Arg - C F1 - score improvements in three out of four datasets . * We reproduce the results using their released code . consider argument identification F1 - score ( Arg - I ) , which cares about only the offset correctness , and argument classification F1 - score ( Arg - C ) , which cares about both offsets and the role types . We also report trigger classification F1 - score ( Tri - C ) , although it is not our main focus as the triggers are provided via other models and we just use their predictions to simulate the end - to - end scenarios .
Results . Table 1 shows the results of end - to - end event extraction on various datasets and languages . Although simple , TAGPRIME surprisingly has decent performance and achieves better results than the state - of - the - art models in terms of argument F1scores . We attribute the good performance to the design of priming , which leverages the semantics of the condition and makes the representations more task - specific . It is worth noting that considering relationship priming further improves the results , which again shows the importance of task - specific representations .
End - to - End Relation Extraction
Datasets . We consider two popular end - toend relation extraction datasets , ACE04 and ACE05 ( Doddington et al . , 2004 ) , denoted as ACE04 - R and ACE05 - R. Both datasets consider 7 named entity types and 6 different relations . We follow the same procedure in Zhong and Chen ( 2021 ) to preprocess the data and split the datasets . We refer readers to their papers for more details about the datasets .
Baselines . We compare to the following end - toend relation extraction models : Table - Sequence ( Wang and Lu , 2020 ) , PFN , and Cascade - SRN ( both late fusion and early fusion ) ( Wang et al . , 2022 ) . Additionally , we consider PURE ( Zhong and Chen , 2021 ) , which also takes a pipelined approach to solve end - to - end relation extraction . To fairly compare with prior works , we use PURE 's named entity predictions on the test set for TAGPRIME to perform relational structure extraction . 4 In order to be consistent with our other tasks , we adopt the single sentence setting ( Zhong and Chen , 2021 ) for our model . However , we also list baselines with cross - sentence settings , such as PURE 's and UniRE ( Wang et al . , 2021 ) 's results with cross - sentence context as input . All the models use ALBERT - xxlarge - v1 ( Lan et al . , 2020 ) as the pre - trained language models . Implementation details . The followings are the training details for all baselines :
â¢ Table - Sequence ( Wang and Lu , 2020 ) : we report the numbers from the original paper .
â¢ Cascade - SRN ( Wang et al . , 2022 ) : we report the numbers from the original paper .
â¢ PURE ( Zhong and Chen , 2021 ) : we report the numbers from the original paper .
â¢ PFN : we report the numbers from the original paper .
â¢ UniRE ( Wang et al . , 2021 ) : we report the numbers from the original paper .
â¢ TAGPRIME ( ours ) : We fine - tune pre - trained language models with the dropout rate being 0.2 . We use AdamW optimizer . For parameters that are not pre - trained we set the learning rate to 10 â3 and the weight decay to 10 â3 . For parameters that are not pre - trained we set the learning rate to 2 Ã 10 â5 and the weight decay to 10 â5 . We consider the linear scheduler with a warm - up , where the warm - up epoch is 5 . The number of epochs is 30 . The training batch size is set to 32 . For conditional token features and learnable features , the dimension is set to 100 . It takes around 20 hours to train with a NVIDIA RTX A6000 with 48 GB memory .
Model ACE05 - R ACE04 - R Ent Rel Rel+ Ent Rel Rel+
Evaluation metrics . We follow the standard evaluation setting with prior works ( Bekoulis et al . , 2018 ; Zhong and Chen , 2021 ) and use micro F1score as the evaluation metric . For named entity recognition , a predicted entity is considered as a correct prediction if its span and the entity type are both correct . We denote the score as " Ent " and report the scores even though it is not our main focus for evaluation . For relation extraction , two evaluation metrics are considered : ( 1 ) Rel : a predicted relation is considered as correct when the boundaries of head - entity span and tail - entity span are correct and the predicted relation type is correct ; ( 2 ) Rel+ : a stricter evaluation of Rel , where they additionally required that the entity types of head - entity span and tail - entity must also be correct .
Results . The results of end - to - end relation extraction are presented in Table 2 . From the table , we observe that TAGPRIME has the best performance on ACE05 - R and outperforms most baselines on ACE04 - R. This shows the effectiveness of TAG - PRIME . Similar to the results of event extraction , considering relationship priming makes the representations more relationship - aware and leads to performance improvement .
Task - Oriented Semantic Parsing
Datasets . We choose MTOP , a multilingual dataset on semantic parsing for taskoriented dialog systems . We specifically consider data in English ( en ) , Spanish ( es ) , French ( fr ) , and German ( de ) .
Baselines . We consider JointBERT ( Chen et al . , 2019b ) , the commonly used baseline for task - oriented semantic parsing . We directly use the predicted intents by JointBERT as the condition of TAGPRIME for a fair comparison . Both TAGPRIME and JointBERT are trained with XLM - RoBERTalarge ( Conneau et al . , 2020 ) . Unlike event extraction and relation extraction , the condition of taskoriented semantics parsing ( intent ) does not include the word span , therefore , only a type feature embedding is contained in the conditional features for TAGPRIME in this experiment .
Implementation details . The followings are the training details for all baselines :
â¢ JointBERT ( Chen et al . , 2019b ) : we use the training script 5 with the default parameters .
â¢ TAGPRIME ( ours ) : We fine - tune pre - trained language models with the dropout rate being 0.2 . We use AdamW optimizer . For parameters that are not pre - trained we set the learning rate to 10 â3 and the weight decay to 10 â3 . For parameters that are not pre - trained we set the learning rate to 10 â5 and the weight decay to 10 â5 . We consider the linear scheduler with warm - up , where the warm - up epoch is 5 . The number of epochs is 90 . The training batch size is set to 6 . For conditional token features and learnable features , the dimension is set to 100 . It takes around 4 hours to train with a NVIDIA RTX A6000 with 48 GB memory .
Evaluation metrics . We following MTOP to consider slot identification ( Slot - I ) and slot classification ( Slot - C ) F1 - scores . Even though we focus on the performance of slot filling , we also report the intent classification accuracy . Table 4 : The ablation study results for three different tasks . The average column calculates the average scores of the stricter evaluation metrics ( i.e , Arg - C , Slot - C , and Rel+ ) for each dataset . From the table , we demonstrate priming technique is the key attribute that make our sequence tagging model stronger than models with learnable features , which is the typical way of using sequence tagging models for relational structure extraction .
Results . As demonstrated in
achieves a better performance than the baselines . Again , considering relationship priming leads to further improvement . It is worth noting that TAG - PRIME is effective for different languages , which shows the generality of TAGPRIME .
Summary
We show the superiority of TAGPRIME on three different tasks ( including ten different datasets across five different languages ) . Although being a unified and simple model , the results suggest that TAG - PRIME can achieve competitive results for tasks requiring extracting relational structures .
Analysis
In this section , we study two questions : ( 1 ) What is the effectiveness of priming techniques compared to learnable features ? ( 2 ) Relationship priming boosts the performance of TAGPRIME , but the task decomposition could slightly slow down the inference speed . Can we mitigate this issue ?
To answer the first question , we conduct ablation experiments on sequence tagging models using different combinations of learnable features or / and adding information through the priming technique ( Section 5.1 ) . For the second question , we propose a simple modification to TAGPRIME so that we can flexibly control the number of layers to fuse priming information to contextualized representations .
The modified TAGPRIME can serve as an efficient approximation of TAGPRIME ( Section 5.2 ) .
Ablation Study
We focus on the setting where we alter the choices on how to include the type information of the condition c and the relationship information r. Table 4 demonstrates our experimental results .
Comparing the first four cases in Table 4 , we observe that the addition of type features is useful in general , and using the priming technique is a more effective way to incorporate conditional information . For models in case 5 to case 8 , the relationship decomposition formulation described in Section 3.3 is applied . Comparing case 2 to case 5 , we can see that simply applying the relationship decomposition formulation for solving relational structure extraction does not lead to improvements if the way to embed the relationship r is only through learnable features . However , comparing case 3 to case 6 and case 4 to case 7 , we show that the relationship priming approach makes the representation z i well capture the attribute of the queried relationship , thus , better exploiting the advantage of the relationship decomposition formulation and gaining improvements . Note that we conducted preliminary experiments that use pretrained language models ' representations of the same verbalized token to be the initialization of the learnable type feature embedding , but the method shows similar results with the random initialization , hence , we stick to random initialization on the learnable type features .
Efficient approximation of TAGPRIME
To make TAGPRIME to inference faster , we perform two modifications to TAGPRIME : ( 1 ) We first separate the pre - trained language model , which contains L layers , into two halves -one with the first k layers , the other one is the remaining layers .
( 2 ) We copy the first half of the language model to another module . When an input passage is fed into the model . We use the original first half to encode the input text as well as the verbalized condition , and we use the copied first half to encode the verbalized relation . Finally , the encoded representations will be fed into the second half layers , as illustrated in Figure 2 . The value of k is adjustable , where when k = 0 , it represents the TAGPRIME with condition and relationship priming , and when k = L , it is TAGPRIME with condition priming . Since the encoding stage of the input text and the verbalized relationship is separated , we can accelerate the inference time of our modified TAG - PRIME through parallel encoding . More precisely , our modified TAGPRIME can aggregate instances that share the same passage and verbalized condition . For those instances , TAGPRIME only needs to perform the encoding once on their input passage part , 6 and paired with several separated embedded verbalized relationships , which could be parallelly encoded together .
We conduct experiments on the ACE05 - E ( en ) dataset to test our modification . In order to better analyze the results and isolate the influence from the pipelined errors , we report the results on the event argument extraction when gold event triggers are given . The experimental results are shown in Figure 3 . First , we investigate the performance influence of our modification . We find that when k â¤ 10 , the performance of our modified TAGPRIME is strong in general and is comparable with TAGPRIME with the condition and relationship priming . To compare the efficiency of the model , we benchmark the inference time by performing inference on the whole testing dataset fifty times and calculate the average speed , which is measured by checking how many instances can be processed per second . The red line in Figure 3 shows the results . We observe that for our modified TAGPRIME with k = 10 , its inference speed is around 30 % faster than the TAGPRIME with the condition and relationship priming , but they perform similarly .
Conclusion
In this work , we take a unified view of tasks requiring extracting relational structures and present TAG - PRIME , a simple , unified , effective , and general sequence tagging model . The key idea is applying priming , a small trick to make the representations task - specific by appending condition - related and relationship - related strings to the input text . Our experimental results demonstrate that TAGPRIME is general to different tasks in various languages and can serve as a strong baseline for future research on relational structure extraction .
Acknowledgments
We thank anonymous reviewers for their helpful feedback .
Limitations
As we point out in Section 5 , one of the limitations in TAGPRIME is the inference speed . When we perform TAGPRIME with condition and relationship priming , we requires more turns of sequence tagging processes than typical sequence tagging models . Observing this , we propose a simple way to mitigate such issue and increase the inference speed with only a small performance drop . Despite such effort , it is still slightly slower than the model requires only one pass of sequence labeling .
The other potential limitation of our method is that we assume the condition and relationship can be verbalized . However , in practice , there could be cases that the verbalization is hard to be done . Considering this , we do conduct preliminary experiments of applying TAGPRIME with special tokens priming rather than verbalized tokens . However , our preliminary results show that such method 's performance is less stable and weaker than we can achieve with TAGPRIME .
Ethics Considerations
TAGPRIME fine - tunes the pre - trained language models ( Devlin et al . , 2019 ; Lan et al . , 2020 ) . There have been works showing the potential bias in pretrained language models . Although with a low possibility , especially after our finetuning , it is possible for our model to make counterfactual , and biased predictions , which may cause ethical concerns . We suggest carefully examining those potential issues before deploying the model in any real - world applications . B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ? Section 4 B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Not applicable . Left blank .
A Detailed Results
B5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and linguistic phenomena , demographic groups represented , etc . ? Not applicable . Left blank . B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . Section 4 C Did you run computational experiments ? Section 4 C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? Section 4 and Appendix A
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022 , with the addition of a question on AI writing assistance .
Multimodal Intent Discovery from Livestream Videos
Individuals , educational institutions , and businesses are prolific at generating instructional video content such as " how - to " and tutorial guides . While significant progress has been made in basic video understanding tasks , identifying procedural intent within these instructional videos is a challenging and important task that remains unexplored but essential to video summarization , search , and recommendations . This paper introduces the problem of instructional intent identification and extraction from software instructional livestreams . We construct and present a new multimodal dataset consisting of software instructional livestreams and containing manual annotations for both detailed and abstract procedural intent that enable training and evaluation of joint video and text understanding models . We then introduce a multimodal cascaded cross - attention model to efficiently combine the weaker and noisier video signal with the more discriminative text signal . Our experiments show that our proposed model brings significant gains compared to strong baselines , including large - scale pretrained multimodal models . Our analysis further identifies that the task benefits from spatial as well as motion features extracted from videos , and provides insight on how the video signal is preferentially used for intent discovery . We also show that current models struggle to comprehend the nature of abstract intents , revealing important gaps in multimodal understanding and paving the way for future work . 1
Introduction
Instructional videos have become increasingly ubiquitous as users generate diverse " how - to " , DIY , and tutorial videos . A Pew Research Center 2018 survey of U.S. adult YouTube users ( Smith et al . , 2018 ) found that over half of surveyed users use video content to learn how to do things they had not done before . These instructional videos convey both abstract and specific intent for physical tasks such as cooking where e.g. , an abstract culinary intent is " let 's bring out the flavor " and a detailed intent is " add a pinch of nutmeg " . Thus , a key task in instructional video understanding is to discover both abstract and detailed intents . By discovering these intents , we can enable or improve important tasks such as semantic indexing of videos ( Kofler et al . , 2016 ) , knowledge graph creation for video search and recommendations ( Pei et al . , 2011 ; Kofler et al . , 2014 ) , intent highlighting , and video summarization ( Nalla et al . , 2020 ) .
An important domain with rich and complex examples of both abstract and detailed intent types are software training videos for creative tasks such as making photo or video effects . These types of software training videos have been shown to be effective for enhanced learning ( Van der Meij , 2017 ) and are also considered a valuable resource in the era of online learning ( Meyer , 2015 ) . Existing video and phrase datasets such as HowTo100 M ( Miech et al . , 2019 ) cover a wide variety of tutorials for visual tasks demonstrated by humans ; however , software - based instructional videos are not a part of such corpora . Hence , in this paper , we present a new corpus of software - instructional videos containing instructional intents , which are derived from Behance Livestreams demonstrating the use of Adobe Photoshop software . 2 Intent detection has been well - studied in dialogue systems , but is less explored for instructional video content , especially emerging livestream content ( Fraser et al . , 2019 ) . While rich in complex procedural instruction and intent , the interactive and social nature of livestreams poses unique challenges . Analyzing language features alone will provide only limited information about the actual instructional intent and the tools and commands used . For instance , the phrase " flipping the canvas " in " Are you flipping the canvas ? " indicates a tool intent , but a closer look at the video clip reveals that it is in fact part of livestream chit - chat and does not take place on - screen . Incorporating both language and video modalities can enhance intent extraction of such ambiguous intents . Hence , in this paper , we present a new joint language - video intent discovery task and a multimodal dataset consisting of : Behance Intent Discovery , and the Behance Livestream video and transcript corpus that intents are found in . We frame intent discovery as a sequence labelling task ; each sample in the intent discovery dataset contains a transcribed phrase annotated with token - level tags for abstract and detailed intents , and an associated video timestamp . Our goal is to predict the instructional intents from the transcript in each video .
To perform intent discovery within instructional videos , we propose a multimodal cascaded crossattention model to predict both the abstract and detailed procedural intents that are present . Additionally , we use late fusion of multimodal embeddings to prevent the visual modality from overwhelming the textual signal , and show significant improvements on the video - based intent detection task using unimodal and multimodal pretrained models like HERO ( Li et al . , 2020 ) . Further , we compare the performance of various video feature extractors as well as different video lengths , and present benchmark results on the proposed dataset . We find that discovery of tool intents benefit from sparsely - sampled spatial features while creative intents benefit from densely - sampled motion features . In the absence of motion features , most models struggle to utilize the video signal for identification of creative intents . Further , visualization of crossattention and visual gate modules in the late fusion model suggests strong and meaningful interaction between the two modalities . Our contributions are :
â¢ We introduce and explore the novel task of video - based multimodal intent discovery , and present an annotated dataset consisting of nearly 20 K sentences from 66 livestreams for extraction of procedural intents from instructional videos .
â¢ We release a large corpus of software - based instructional videos ( 2,049 sessions , 3,128 hours total ) , accompanied by timestamped transcripts , that can be used for pretraining multimodal models .
â¢ We propose the multimodal cascaded crossattention model and demonstrate the effectiveness of late fusion of multimodal embeddings in this task .
â¢ We present empirical results for the proposed dataset using unimodal and multimodal approaches , and provide insights from analysis of modelling choices for future research .
Related Work
Intent discovery has been widely studied in the context of dialog modelling and generation wherein it has been framed as a binary or multi - class classification problem . The SNIPS ( Coucke et al . , 2018 ) and ATIS ( Dahl et al . , 1994 ) datasets consist of concise single - sentence texts containing intents with constrained vocabulary and attributes . Several works have explored intent classification of internet posts in the context of racial / radicalized intent ( Agarwal and Sureka , 2016 ) , purchase intents ( Gupta et al . , 2014 ; Wang et al . , 2015 ) , discussion forums ( Chen et al . , 2013 ) and health queries ( Cai et al . , 2017 ) . Vedula et al . ( 2019 ) propose open intent discovery with unconstrained vocabulary as a sequence tagging task . Using this framework , we present our dataset on instructional intents .
In the wake of exploding visual social - media content , several image - based multi - modal intent datasets have been previously proposed . Kiela et al . ( 2020 ) ; Aprosio et al . ( 2020 ) study abusive language and hateful intent in memes and photo posts . Jia et al . ( 2021 ) explore intent categories derived from social psychology and use object localization to integrate visual context in task models . Instagram posts are another interesting source for multimodal content ( Chen and Hsieh , 2020 ; Kruk et al . , 2019 ) . We introduce the task of video - based multimodal intent discovery , which has been unexplored .
Several tasks have been proposed in the recent years to probe joint video and text understanding . Lei et al . ( 2018 ) ; ; Maharaj et al . ( 2017 ) ; Jang et al . ( 2017 ) ; Tapaswi et al . ( 2016 ) and introduce video - based question answering datasets created from various sources of creative visual content , i.e. movies , TV shows , GIFs etc . Lei et al . ( 2020b ) and Lei et al . ( 2020a ) propose the task of video - moment retrieval and next frame prediction respectively , based on query subtitles , while present the multimodal version of natural language inference . Early models for performing these tasks involve combining pretrained image representations from sparsely sampled videos , and text encodings from pretrained encoders ( Devlin et al . , 2019 ; Liu et al . , 2019 ) in architectures for modelling global - local interactions ( Zhu and Yang , 2020 ; , temporal localization Zhang et al . , 2020 ) , graph - based reasoning etc . More recent attempts involve pretraining models on large video+text corpora ( Miech et al . , 2019 ) and finetuning on downstream tasks ( Sun et al . , 2019 ; Cho et al . , 2021 ; Tang et al . , 2021 ; . We explore late - fusion of video and text embeddings for intent detection in pretrained and non - pretrained multimodal settings .
Behance Datasets
Dataset Collection . We first obtain 2,049 videos along with their transcripts and tool timelines from the Behance platform . The tool timeline contains a time - stamped record of the tools used in the software during the tutorial . The average session length is 80 minutes with an average of 587 transcribed phrases per session ( see Table 1 ) . The tool timelines contain 282 distinct tools with varying frequencies ; Color , Select Brush , Select Layer are some of the most frequent ones . The instructional software - based domain of this dataset is significantly different from existing large corpora drawn from YouTube instructional videos ( Miech et al . , 2019 ) and TV content ( Lei et al . , 2018 ( Lei et al . , , 2020b ) , but it is an important learning resource . Hence , we include the unlabelled Behance Livestreams corpus as an addition to the pool of video+text corpora that can be leveraged for continued pretraining of multimodal models and finetuning on downstream tasks relevant to software - based livestream videos .
In order to prepare the intent discovery dataset , we extract candidate intent phrases from the transcripts of the Behance Livestream corpus . Following Vedula et al . ( 2019 ) , we define an intent as a text phrase consisting of : ( i ) an action word or phrase , which constitutes a definite task , goal or activity and ( ii ) an object , which represents those words or phrases that the action is going to act or operate upon . We generate the dependency graph of sentences , and extract the VERB node as action and the direct object of the VERB as the object , along with all other children nodes ( see example in Fig . 2 ) . 3 Through manual analysis , we identified two major categories of meaningful intent : tool and creative . Tool intents are low - level intents that can be typically mapped to a single tool in the software . Creative intents are abstract intents used to describe a high - level creative goal that consist of a complex set of actions or tool intents . For instance , in Fig . 3 , " make a new layer " is a tool intent that can be mapped to the tool Create Layer , while " add more plants and stuff " is a creative intent . All other intents in the corpus , predominantly from chit - chat statements , are irrelevant to our task . We frame the task of intent discovery as a sequencetagging problem and tag the intent phrases within each sentence with IOB ( inside , outside , beginning ) span annotations for the two classes : tool and creative intents . Each sample consists of a timestamped sentence with span annotations and the video session it is extracted from . Based on the above defined framework , we col- Dataset Analysis . We analyzed tool and creative intents to find the most frequent , unique verbs and nouns mentioned in the phrases . While there are action verbs which are distinctly tool - specific , such as merge , select , and duplicate , there are many verbs which are common to both tool and creative intents such as add , make , and paint . Hence , the task model needs to learn the difference between tool and creative intents to be able to classify intents with similar action verbs into the correct categories . Further , we examined the unique nouns occurring in the intents and found lesser overlap between the two intent classes . Creative intents contain abstract and subjective visual concepts which pose a unique and interesting challenge to multimodal models . See Appendix for probing experiments .
Methods for Intent Discovery
Intuitively , as in a lot of instructional sources like text books , the text or audio serves as the primary mode of high - level information transfer , while the video / image signal provides detailed context or demonstration . Thus , we start our exploration using text models , which are built on two pretrained models : RoBERTa ( Liu et al . , 2019 ) and GPT2 ( Radford et al . , 2019 ) . There are several previous works focusing on a limited set of intents ( Xia et al . , 2018 ) , and thus , treat the problem of intent discovery as a classification problem . In our case , given the vast possibilities of potential intents in our sources , we cast the problem as a span detection problem , and design our models accordingly .
Unimodal Sequence Labelling
Our text models are designed similar to Named - Entity - Recognition models with a pretrained embedding layer and a sequence classification layer on top . Each phrase in the transcript is annotated separately in the intent dataset , leading to efficient processing . Although it is possible to process longer spans of text , in our annotations , we found out that each sentence usually gives enough information to extract the intent inside it , and extra context ( neighboring sentences ) does not significantly help the decision . We denote an input sentence as X = [ x 0 , ... ,
Multimodal Sequence Labelling : NaÃ¯ve Fusion
Seeking to leverage the video information , in our first attempt , we tried a simple feature fusion between the text signal and the video signal in the sequence labelling framework . We add a crossattention layer on top of the pretrained text encoder in this naÃ¯ve joint video - text model and use the output of the cross - attention layer for sequence label classification . Let 's denote the video features as V .
Our model ( see Fig . 4 ( a ) ) is described as follows :
Z = sof tmax ( W c * f self ( f cross ( E , V ) ) + b c )
where E , f self and f cross are text encodings , self - attention and cross - attention layers respectively . This naÃ¯ve fusion model , however , does not provide any significant improvements compared to text - only baselines ( see Sec . 7 ) . Analysis of the results revealed that the textual features dominate the final decision , especially in the creative intent classes . To understand this behaviour , we performed a pilot task in which a human annotator looks through the video segments and tries to guess the intent without any transcript or audio . Our annotator found the task very difficult , and only possible after watching a very long context window , which partially explains the low performance of this model . The video signal is much more ambiguous than the text signal , and when presented with two sources where one is vastly less informative than the other , the model learns to rely only on the text , leading to no improvement compared to the text - only baseline . Joining two sources of features with different predictive utility is difficult . Given the fact that the video feature extractor is not trained on similar data , the video feature might not contain enough information for a direct intent detection task . Fortunately , our pilot task also reveals an important insight , i.e. , the video signal is good at identifying whether an intent is present or not . Many intent candidates identified by the text models are not creative or tool intents , but are chitchat utterances from the instructor interacting with the audience . In these cases , we posit that the inactivity presented in the video signal is a strong indication that a creative / tool intent does not occur at the current time window . Using this idea , we propose a cascaded model with deeper interaction between video and text signal .
Cascaded Cross - Attention & Late Fusion
Using the intuition that the text signal would provide candidates for the vision model , which is subsequently used for filtering out the cases without intent , we design the cascade cross - attention model as follows : First , we extract the set of contextualized embeddings E from the text encoder f enc and transform it through two self - attention layers to create a two - stream architecture ( see Fig . 4 ) . In the first stream , the text encodings are processed through a single - layer of self - attention to produce E 1 . In the second stream , the output from selfattention i.e. E 2 , is combined with video embeddings through a cascaded cross - attention module .
Let
V = [ v 1 , v 2 , ... , v k ]
be the input sequence of video embeddings . The cascaded module contains three cross - attention layers : video - to - text f v2 t ( â¢ ) , text - to - video f t2v ( â¢ ) and text - to - text cross - attention f t2 t ( â¢ ) , with outputs computed as :
S 1 = f v2 t ( W m V + b m , E 2 ) S 2 = f t2 t ( E 2 , S 1 ) S 3 = f t2i ( E s2 , W m V + b m )
where W m , b m are the parameters of a linear layer for transforming video embeddings . Next , the outputs from cross - attention layers are concatenated , linearly mapped and transformed into 0 - 1 values using a sigmoid , to generate the visual gate ( see Fig . 4 ( c ) ) . Finally , the output from cross - attention layer is multiplied with this gate , i.e .
S gate = sigmoid ( W g [ S 2 ; S 3 ] + b g ) S clf = [ S gate * S 3 ; E s1 ]
The visual gate is dynamically computed using the contextualized video representations and is used to trim the video signal to the relevant bits . This helps in regulating the contribution of the two modalities for the final prediction as per the input . The concatenation represents the late - fusion of text - only embeddings and video - contextualized text embeddings . This merged representation is then sent to the classifier layer for classification i.e. Z = sof tmax ( W c * S clf + b c ) .
Sequence Labelling with Joint Video - Text Pretraining
In order to leverage joint modelling of video and text modalities through large - scale pretraining , we adapt the pretrained HERO ( Li et al . , 2020 ) and ClipBERT for global contextualization . Thus , the output is :
S temp = f temp ( [ V cross ; W q emb ] ) S out = S temp [ N v : ( N v + N t ) , : ]
where N v and N t are the number of frames and tokens in video and query respectively . The output of f temp is masked to select the representations pertaining to the query only . In the naÃ¯ve fusion setting , S out is then sent to the classifier layer .
ClipBERT . Similarly , the output S out from the Cross - modal Transformer f cross in ClipBERT is masked and sent to the classifier layer for prediction i.e. S out = f cross ( [ V ; W q emb ] ) [ : N t , : ] . Late Fusion . We integrate the late fusion approach into HERO and ClipBERT as follows :
S gate = sigmoid ( W g * S out + b g ) S clf = [ S gate * S out ; W q emb ]
where the visual gate is computed as in Sec . 5.3 ( see Fig . 4 ) and S clf is sent to the classifier layer .
Experiments
Evaluation . Since the transcribed phrases in Behance Livestreams are the result of an automatic speech recognition ( ASR ) system , the exact span match metrics might be distorted by ASR errors . Hence , we use a more lenient 75 % partial matchbased Precision / Recall / F - score metric i.e. , if there is more than 75 % overlap between the ground truth and predicted span , we consider it as a match .
Video Representations . We experiment with 3D ResNext-101 ( Xie et al . , 2017 ) ( f ps=6 ) , SlowFast ( Feichtenhofer et al . , 2019 ) ( clip length=2s ) and 2D ResNet-152 ( He et al . , 2016 ) ( clip length=2s ) following preprocessing steps in Li et al . ( 2020 ) .
Models . We use the RoBERTa LARGE ( Liu et al . , 2019 ) models for the unimodal experiments , as well as the multimodal experiments that are based on unimodal pretrained models . We use the pretrained HERO ( Li et al . , 2020 ) and ClipBERT in the remaining experiments ; their language encoders are initialized from pretrained RoBERTa BASE and BERT BASE ( Devlin et al . , 2019 ) models . Each model is trained end - toend using fully - supervised training and is subjected to grid - search based hyper - parameter optimization . The best checkpoints are selected based on overall F - Score . See Appendix for bounds .
Results
In this section , we discuss results from various models on the Behance Intent Discovery dataset ( see Table 4 ) .
The text baselines . Starting with the text - only baselines , we see the best performance from the RoBERTa models , i.e. , 58 % and 27 % partial match F - scores on the tool and creative intents , respectively ( rows 2 and 3 in Table 4 ) . Notably , the tool intent predictor is biased with high recall but low precision performance i.e. it retains too many candidates , many of which do not correspond to any intents . These results also demonstrate that large pretrained language models like RoBERTa and GPT2 struggle to comprehend the abstract ideas represented in creative intents .
The NaÃ¯ve Fusion models . The NaÃ¯ve Fusion approach with pretrained RoBERTa yields upto 2 % improvement over the text - only baselines . In some cases , such as the 3D ResNext representations , this approach degrades the performance , especially in the harder creative intent set . We attribute this to the difference in informativeness between the text and the video signal , as discussed in Sec . 5.2 .
The Late Fusion models . With the Late Fusion approach , we see significant improvements in almost all cases . Compared to the corresponding NaÃ¯ve Fusion models , Late Fusion models mainly improve precision for tool intents . This result supports our hypothesis that the video signal is most useful as a gate to filter out non - intent candidates from the textual signal . The SlowFast representations prove especially beneficial for creative intents , as seen in row 9 in Table 4 . With the use of multimodal pretrained models like HERO and ClipBERT , we observe significant improvements in prediction of tool intents and smaller improvements for creative intents with a simple adaptation of the prediction head for sequence labelling ( see Sec . 5.4 ) . HERO uses video representations from pretrained encoders while ClipBERT operates on raw videos ; both approaches work well with the software - based video domain yielding upto 3 % and 1 % improvement on tool intents respectively ( rows 10 , 12 in Table 4 ) over the unimodal RoBERTa models . Larger improvements are seen from further augmenting these models with late fusion i.e. 1 % improvement on tool intents ( rows 11 , 12 in Table 4 ) . The late fusion RoBERTa model using SlowFast features ( row 9 in Table 4 ) performs best for creative intents , with 3 % improvement over the text - only baseline .
We see similar trends from experiments on the validation set of the Behance Intent Discovery dataset . See results in Appendix .
Analysis & Discussion
In this section , we perform qualitative analysis of the late fusion approach and examine the effect of video clip length . We also discuss a semiautomated approach to creating annotations for intent extraction and use the data in combination with manual annotations for improved results . See Appendix for more analyses .
Qualitative Analysis
In order to understand the inner workings of the late fusion architecture , we examine the cross - attention and visual gate modules of the RoBERTa+Late Fusion model trained with 2D ResNet features . Each row of the attention score matrix M â R nÃf ( for n tokens and f video segments ) in text - to - video cross - attention module corresponds to the temporal attention over video clips ( represented by a sequence of ResNet feature vectors ) for a given token . We plot this score matrix for the 12 attention heads in the RoBERTa model in Figures 5 ( a ) and 7 ( a ) . The attention heads are activated in the intent region suggesting a strong interaction between two modalities in important segments of the video .
To understand how the video signal helps the prediction , we first plot the mean and standard deviation of visual gate values ( S gate ) for each token in Figures 5 ( b ) and 7 ( b ) . Results show that the visual gate preferentially relies on the video modality for tokens outside the intent span . Furthermore , in Fig . 6 , we show example phrases where the text only model classifies wrongly as intent while the joint model does not . The phrases themselves appear to be intent but the lack of action in the visual frame indicates that these are chit - chat interactions . Both analyses support our hypothesis that the late fusion model utilizes the video signal to filter intent candidates and improve precision .
Video Clip Length
As we discuss in Sec . 4 , the video clip durations for the tool and creative intents are not specified . We observe that the intended action can span anywhere between 1 second to several minutes . Longer clip lengths are relevant for many creative intents like " make it into something fantasy " , " add the arm to this little guy " , etc . Hence , we experiment with various clip lengths ( 10 , 20 and 60 secs ) , but find that larger clip lengths do not lead to further improvements . In fact , with 60 second clips the performance of RoBERTa+Late Fusion model drops below the performance of text - only RoBERTa . This issue could be alleviated with long - range video understanding models ( Sener et al . , 2020 ) .
Semi - automated Intent Annotations
Since manual annotation of procedural intents is time - intensive and expensive , we explore a semiautomatic pipeline for creation of intent annotations . The Behance Livestreams corpus contains tool timelines for each livestream , which enumerates the tools used within the software at different points in the livestream . We compute the tf - idf scores for co - occurrence of 896 , 287 action - object phrases ( from dependency parses of sentences ) and corresponding tools in the tool timelines , in order to find the phrases that are frequently used for describing particular tool actions , such as " grab the smudge tool " . After filtering the phrases for those with high tf - idf scores , the pool of intent candidates was further cleaned manually , resulting in a final set of 3,697 tool intent candidates . Using this pool of candidates , 24,300 phrases from the Behance Livestreams corpus were identified as tool intent samples . Since it is not straightforward to extract creative intents using similar methods , we first identified key phrases for creative intents from the set of action - object phrases with high term frequency . We then subjected it to manual cleaning ( two annotators per sample ; Îº=0.986 ) followed by embedding similarity to select creative intents ( see Appendix for full pipeline ) . Using this method , we recovered 7,135 phrases containing creative intents .
We use these semi - automatically collected annotations as additional training data in our experiments with Late Fusion RoBERTa models . Since the manually annotated Behance Intent Discovery dataset is skewed towards negative samples i.e. < 25 % samples contain intent , we balance the training data by adding 5,000 samples ( containing tool or creative intents ) from the aforementioned semi - automatically annoated dataset to it . With this balanced data , we see upto 2 % improvement in the Late Fusion RoBERTa models . See Appendix .
Conclusion
In this paper , we explore the novel task of videobased multimodal intent discovery . We present the unlabelled Behance Livestream corpus consisting of instructional videos for software tools , and the Behance Intent Discovery dataset annotated with tool and creative intents . We propose a late - fusion approach for integration of the video signal with the text signal in a controlled manner for this task , and show significant improvements with unimodal and multimodal pretrained models .
Acknowledgements
We would like to thank Tracy King for her detailed feedback and Hailin Jin for making the Behance transcript available . We would also like to thank the reviewers for their useful feedback . This work was partially done while AM was interning at Adobe Research and later extended at UNC , where it was supported by ARO Award W911NF2110220 and DARPA KAIROS Grant FA8750 - 19 - 2 - 1004 . The views contained in this article are those of the authors and not of the funding agency .
Ethics / Broader Impacts
From an ethics standpoint , we provide a detailed overview of the methods used to create the Behance Livestreams corpus and Behance Intent Discovery dataset in Sec . 4 and more details in the Appendix . We also provide some analyses of the data in Table 3 . All of the language data consists of simple English sentences . The dataset comprises livestreamed video tutorials by users of the Behance platform . Behance users grant full usage rights of their content and agree to not hold copyright claims on content in the livestreams videos or transcripts . This content is being made available for free distribution for academic research purposes only and does not allow for redistribution . Aside from the name of the instructor in each video ( which is public information ) , real names of livestream session users or other identifying information does not appear in any of the transcripts . We provide full descriptions of the models used in this paper in Sec . 5 . Detailed hyperparameters and bounds for hyperparameter search are included in the Appendix .
Video - based intent discovery serves to enhance the information exploration experience of users on any video - based platform . Since we focus on extracting procedural intent relevant to the goal of the video and in the software domain , we do not anticipate this technology to cause any harm to users , or have any unintended consequences .
B Experiments
For HERO and ClipBERT models , we use the recommended hyperparameters for finetuning in their Github repository . 6,7 For RoBERTa - based models , see the hyperparameters common to all models in Table 8 . We performed grid - search based optimization of the variable hyperparameters using the bounds in Table 8 . The best performing batch size for all models was found to be 32 .
6 https : / / github.com / linjieli222 / HERO 7 https : / / github.com / jayleicn / ClipBERT
C Results
See partial match results for the validation split of Behance Intent Discovery in Table 6 .
D Analysis
D.1 Finetuned Video Representations
We see large improvements with sparsely - sampled 2D ResNet video embeddings ( see Table 4 which are extracted from ResNet pretrained on the Im - ageNet dataset . This begs the question , if larger improvements can be had by finetuning the feature extractors on the domain of Behance Livestreams .
To facilitate this , we create a dataset of 10,000 images containing snapshots of video livestreams and classified them into one of 50 tool categories using the tool timeline . We finetune ResNet-152 on this dataset with a resulting classification accuracy of 47 % . We use the finetuned ResNet to extract sparsely sampled video embeddings and re - run the late fusion experiment with RoBERTa . We see 2 % improvement for tool intents and 1 % drop in performance on creative intents . This suggests that finetuning feature extractors on the target domain can be beneficial for low - level intents .
D.2 Semi - automated Intent Annotations
As discussed in Sec . 8 . semi - automatically data , we drastic decline in the precision of the model for both tool and creative intents ( see row 3 in Table 7 ) . With the use of better methods for filtering out the useful signal from the noisy data , there might be better results with semiautomatically created annotations . This line of research is important because it promotes scalable annotations which can cover a diverse population of livestreamers from many livestream videos .
A Dataset
For the semi - automatically created annotations described in Sec . 8.3 , we empirically select a window of 10 seconds for computing the scores and retain intent phrases with a term frequency of 5 or higher in the corpus and tf - idf scores of 0.3 or higher with one or more tools . See the full semi - automated pipeline of dataset creation in Fig . 8 .
Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation *
We introduce a new method to improve existing multilingual sentence embeddings with Abstract Meaning Representation ( AMR ) . Compared with the original textual input , AMR is a structured semantic representation that presents the core concepts and relations in a sentence explicitly and unambiguously . It also helps reduce surface variations across different expressions and languages . Unlike most prior work that only evaluates the ability to measure semantic similarity , we present a thorough evaluation of existing multilingual sentence embeddings and our improved versions , which include a collection of five transfer tasks in different downstream applications . Experiment results show that retrofitting multilingual sentence embeddings with AMR leads to better state - of - the - art performance on both semantic textual similarity and transfer tasks . Our codebase and evaluation scripts can be found at https : / / github.com / jcyk / MSE - AMR .
Introduction
Multilingual sentence embedding ( MSE ) aims to provide universal sentence representations shared across different languages ( Hermann and Blunsom , 2014 ; Pham et al . , 2015 ; Schwenk and Douze , 2017 ) . As an important ingredient of crosslingual and multilingual natural language processing ( NLP ) , MSE has recently attracted increasing attention in the NLP community . MSE has been widely adopted to bridge the language barrier in several downstream applications such as bitext mining ( Guo et al . , 2018 ; Schwenk , 2018 ) , document classification ( Eriguchi et al . , 2018 ; Singla et al . , 2018 ; Yu et al . , 2018 ) and natural language inference ( Artetxe and Schwenk , 2019 ) . Prior work typically borrows fixed - size embedding vectors from multilingual neural machine models ( Schwenk and Douze , 2017 ; Yu et al . , 2018 ) or trains siamese neural networks to align the semantically similar sentences written in different languages ( Wieting et al . , 2019 ; Feng et al . , 2020 ) .
Despite the recent progress , the current evaluation of multilingual sentence embeddings has focused on cross - lingual Semantic Textual Similarity ( STS ) ( Agirre et al . , 2016 ; Cer et al . , 2017 ) or bi - text mining tasks ( Zweigenbaum et al . , 2018 ; Artetxe and Schwenk , 2019 ) . Nevertheless , as pointed out by Gao et al . ( 2021 ) , the evaluation on semantic similarity may not be sufficient because better performance on STS does not always indicate better embeddings for downstream tasks . Therefore , for a more comprehensive MSE evaluation , it is necessary to additionally evaluate downstream tasks , which is largely ignored in recent work ( Chidambaram et al . , 2019 ; Reimers and Gurevych , 2020 ; Feng et al . , 2020 ) . In this paper , we collect a set of multilingual transfer tasks and test various existing multilingual sentence embeddings . We find that different methods excel at different tasks and the conclusions drawn from the STS evaluation do not always hold in the transfer tasks and vice versa . We aim to establish a standardized evaluation protocol for future research in multilingual sentence embeddings .
To improve the quality of existing MSE models , we explore Abstract Meaning Representation ( AMR ) ( Banarescu et al . , 2013 ) , a symbolic semantic representation , for augmenting existing neural semantic representations . Our motivation is twofold . First , AMR explicitly offers core concepts and relations in a sentence . This helps prevent learning the superficial patterns or spurious correlations in the training data , which do not generalize well to new domains or tasks ( Poliak et al . , 2018 ; Clark et al . , 2019 ) . Second , AMR reduces the variances in surface forms with the same meaning . This helps alleviate the data sparsity issue as there are rich lexical variations across different languages .
On the other hand , despite that AMR is advocated to act as an interlingua ( Xue et al . , 2014 ; Damonte and Cohen , 2018 ) , little work has been done to reflect on the ability of AMR to have impact on subsequent tasks . In order to advance research in AMR and its applications , multilingual sentence embedding can be seen as an important benchmark for highlighting its ability to abstract away from surface realizations and represent the core concepts expressed in the sentence . To our knowledge , this is the first attempt to leverage the AMR semantic representation for multilingual NLP .
We learn AMR embeddings with contrastive siamese network ( Gao et al . , 2021 ) and AMR graphs derived from different languages ( Cai et al . , 2021 ) . Experiment results on 10 STS tasks and 5 transfer tasks with four state - of - the - art embedding methods show that retrofitting multilingual sentence embeddings with AMR improves the performance substantially and consistently .
Our contribution is three - fold . â¢ We propose a new method to obtain high - quality semantic vectors for multilingual sentence representation , which takes advantage of languageinvariant Abstract Meaning Representation that captures the core semantics of sentences .
â¢ We present a thorough evaluation of multilingual sentence embeddings , which goes beyond semantic textual similarity and includes various transfer tasks in downstream applications .
â¢ We demonstrate that retrofitting multilingual sentence embeddings with Abstract Meaning Representation leads to better performance on both semantic textual similarity and transfer tasks .
Related Work
Universal Sentence Embeddings Our work aims to learn universal sentence representations , which should be useful for a broad set of applications . There are two lines of research for universal sentence embeddings : unsupervised approaches and supervised approaches . Early unsupervised approaches ( Kiros et al . , 2015 ; Hill et al . , 2016 ; Gan et al . , 2017 ; Logeswaran and Lee , 2018 ) design various surrounding sentence reconstruction / prediction objectives for sentence representation learning . Jernite et al . ( 2017 ) exploit sentencelevel discourse relations as supervision signals for training sentence embedding model . Instead of us - ing the interactions of sentences within a document , Le and Mikolov ( 2014 ) propose to learn the embeddings for texts of arbitrary length on top of word vectors . Likewise , Chen ( 2017 ) ; Pagliardini et al . ( 2018 ) ; Yang et al . ( 2019b ) calculate sentence embeddings from compositional n - gram features . Recent approaches often adopt contrastive objectives ( Zhang et al . , 2020 ; Giorgi et al . , 2021 ; Wu et al . , 2020 ; Meng et al . , 2021 ; Carlsson et al . , 2021 ; Kim et al . , 2021 ; Yan et al . , 2021 ; Gao et al . , 2021 ) by taking different views - from data augmentation or different copies of models - of the same sentence as training examples .
On the other hand , supervised methods ( Conneau et al . , 2017 ; Reimers and Gurevych , 2019 ; Gao et al . , 2021 ) take advantage of labeled natural language inference ( NLI ) datasets ( Bowman et al . , 2015 ; , where a sentence embedding model is finetuned on entailment or contradiction sentence pairs . Furthermore , Wieting and Gimpel ( 2018 ) ; Wieting et al . ( 2020 ) demonstrate that bilingual and back - translation corpora provide useful supervision for learning semantic similarity . Another line of work focuses on regularizing embeddings Su et al . , 2021 ; Huang et al . , 2021 ) to alleviate the representation degeneration problem . Very recently , Opitz and Frank ( 2022 ) combine the strengths of AMR metrics and embedding similarities for accurate and explainable sentence similarity rating .
Multilingual Sentence Embeddings Recently , multilingual sentence representations have attracted increasing attention . Schwenk and Douze ( 2017 ) ; Yu et al . ( 2018 ) ; Artetxe and Schwenk ( 2019 ) ( Bromley et al . , 1993 ) with contrastive objectives using parallel corpora . Reimers and Gurevych ( 2020 ) train a multilingual model to map sentences to the same embedding space of an existing English model . Different from existing work , our work resorts to multilingual AMR , a languageagnostic disambiguated semantic representation , for performance enhancement .
Evaluation of Sentence Embeddings Traditionally , the mainstream evaluation for assessing the quality of English - only sentence embeddings is based on the Semantic Textual Similarity ( STS ) tasks and a suite of downstream classification tasks . The STS tasks ( Agirre et al . , 2012 ( Agirre et al . , , 2013 ( Agirre et al . , , 2014 ( Agirre et al . , , 2015 ( Agirre et al . , , 2016Marelli et al . , 2014 ; Cer et al . , 2017 ) calculate the embedding distance of sentence pairs and compare them with the human - annotated scores for semantic similarity . The classification tasks ( e.g. , sentiment analysis ) from SentEval ( Conneau and Kiela , 2018 ) take sentence embeddings as fixed input features to a logistic regression classifier . These tasks are commonly used to benchmark the transferability of sentence embeddings on downstream tasks . For multilingual sentence embeddings , most previous work has focused on crosslingual STS ( Agirre et al . , 2016 ; Cer et al . , 2017 ) and the relevant bi - text mining tasks ( Zweigenbaum et al . , 2018 ; Artetxe and Schwenk , 2019 ) . The evaluation on downstream transfer tasks has been largely ignored ( Chidambaram et al . , 2019 ; Reimers and Gurevych , 2020 ; Feng et al . , 2020 ) . Nevertheless , as pointed out in Gao et al . ( 2021 ) in English scenarios , better performance on semantic similarity tasks does not always indicate better embeddings for transfer tasks . For a more comprehensive evaluation , in this paper , we collect a set of multilingual transfer tasks and test various existing multilingual sentence embeddings . We aim to establish a standardized evaluation protocol for future research in multilingual sentence embeddings .
Preliminaries
Contrastive Siamese Network
Siamese network ( Bromley et al . , 1993 ) has attracted considerable attention for self - supervised representation learning . It has been extensively adopted with contrastive learning ( Hadsell et al . , 2006 ) for learning dense vector representations of images and sentences ( Reimers and Gurevych , 2019 ; . The core idea of contrastive learning is to pull together the representations of semantically close objects ( images or sentences ) and repulse the representations of negative pairs of dissimilar ones . Recent work in computer vision ( Caron et al . , 2020 ; Grill et al . , 2020 ; Chen and He , 2021 ; Zbontar et al . , 2021 ) has demonstrated that negative samples may not be necessary . A similar observation was made in NLP by who adopted the BYOL framework ( Grill et al . , 2020 ) for sentence representation learning . In this work , we adopt the framework in ( Gao et al . , 2021 ) with in - batch negatives Henderson et al . , 2017 ) .
Formally , we assume a set of training examples
D = { ( x i , x + i , x â i ) } N i=1
, where x + i and x â i are semantically close and semantically irrelevant to x i , respectively . The training is done with stochastic mini - batches . Each mini - batch consists of M examples and the training objective is defined as :
i = â log e s ( x i , x + i ) / Ï M j=1 e s ( x i , x â j ) / Ï + M j=1 e s ( x i , x + j ) / Ï
( 1 ) where s ( â¢ , â¢ ) measures the similarity of two objects and Ï is a scalar controlling the temperature of training . As seen , other objects in the same minibatch ( i.e. , { x â j } j = i and { x + j } j = i ) are treated as negatives for x i . More concretely , s ( â¢ , â¢ ) computes the cosine similarity between the representations of two objects :
s ( x i , x j ) = h T i h j h i â¢ h j
where h i and h j are obtained from a neural encoder f Î¸ ( â¢ ) : h = f Î¸ ( x ) . The model parameters Î¸ are then optimized using the contrastive learning objective .
Multilingual AMR Parsing
AMR ( Banarescu et al . , 2013 ) is a broad - coverage semantic formalism originally designed for English . The accuracy of AMR parsing has been greatly improved in recent years Lam , 2019 , 2020a ; Bevilacqua et al . , 2021 ; Bai et al . , 2022 ) . Because AMR is agnostic to syntactic and wording variations , recent work has suggested the potential of AMR to work as an interlingua ( Xue et al . , 2014 ; Damonte and Cohen , 2018 ) . That is , we can represent the semantics in other languages using the corresponding AMR graph of the semantic equivalent in English . A number of crosslingual AMR parsers ( Damonte and Cohen , 2018 ; Blloshmi et al . , 2020 ; Sheth et al . , 2021 ; Procopio et al . , 2021 ; Cai et al . , 2021 ) have been developed to transform non - English texts into AMR graphs . Most of them rely on pre - trained multilingual language models and synthetic parallel data . In particular , Cai et al . ( 2021 ) proposed to learn a multilingual AMR parser from an English AMR parser via knowledge distillation . Their single parser is trained for five different languages ( German , Spanish , Italian , Chinese , and English ) and achieves state - of - the - art parsing accuracies . In addition , the one - for - all design maintains parsing efficiency and reduces prediction inconsistency across different languages . Thus , we adopt the multilingual AMR parser of Cai et al . ( 2021 ) in our experiments . 1 It is worth noting that the multilingual parser is capable of parsing many other languages , including those it has not been explicitly trained for , thanks to the generalization power inherited from pre - trained multilingual language models ( Tang et al . , 2020 ; . In Section 4.2 , we further extend the training of the multilingual parser to French , another major language , for improved performance .
Proposed Method
We first introduce how we learn AMR embeddings and then describe the whole pipeline for enhancing existing sentence embeddings .
Learning AMR Embeddings
Linearization & Modeling Given AMR is graph - structured , a variety of graph neural networks ( Song et al . , 2018 ; Beck et al . , 2018 ; Ribeiro et al . , 2019 ; Guo et al . , 2019 ; Cai and Lam , 2020b ; Ribeiro et al . , 2019 ) have been proposed for the representation learning of AMR . However , recent work Mager et al . , 2020 ; Bevilacqua et al . , 2021 ) has demonstrated that the power of existing pre - trained language models based on the Transformer architecture ( Vaswani et al . , 2017 ) , such as BERT ( Devlin et al . , 2019 ) , GPT2 ( Radford et al . , 2019 ) and BART , can be leveraged for achieving better performance . Following them , we also take BERT as the backbone model .
Since Transformer - based language models are designed for sequential data , to encode graphical AMR , we resort to the linearization techniques in ( Bevilacqua et al . , 2021 ) . Figure 1 illustrates the linearization of AMR graphs . For each AMR graph , a DFS traversal is performed starting from the root node of the graph , and the trajectory is recorded . We use parentheses to mark the hierarchy of node depths . Bevilacqua et al . ( 2021 ) also proposed to use special tokens for indicating variables in the linearized graph and for handling reentrancies ( i.e. , a node plays multiple roles in the graph ) . However , the introduction of special tokens significantly increases the length of the output sequence ( almost 50 % increase ) . We remove this feature and simply repeat the nodes when revisiting happens . This significantly reduces the length of the output sequence and allows more efficient modeling with Transformer - based language models . The downside is that reentrancy information becomes unrecoverable . However , we empirically found that the shortened sequences lead to better performance . The linearizations of AMR graphs are then treated as plain token sequences when being fed into Transformer - based language models . Note that AMR linearization introduces additional tokens that are rarely shown in English ( e.g. , " ARG2 " and " belong-01 " ) . These tokens may not be included in the original vocabulary of existing language models and could be segmented into sub - tokens ( e.g. , " belong-01 " â " belong " , " - " , " 01 " ) , which are less meaningful and increase the sequence length . To deal with this problem , we extend the original vocabulary of existing language models to include all the relation and frame names occurring at least 5 times in the AMR sembank ( LDC2017T10 ) . ( Wu et al . , 2020 ; Meng et al . , 2021 ) or introducing some random noise ( e.g. , dropout ( Srivastava et al . , 2014 ) ) to the modeling function f Î¸ ( Gao et al . , 2021 ) . On the other hand , negative examples x â i are usually sampled from other sentences . However , prior work ( Conneau et al . , 2017 ; Gao et al . , 2021 ) has demonstrated that entailment / contradiction sentence pairs in supervised natural language inference ( NLI ) datasets ( Bowman et al . , 2015 ; are better positive / negative pairs for learning sentence embeddings . Following ( Gao et al . , 2021 ) , we borrow the supervisions from two NLI datasets , namely SNLI ( Bowman et al . , 2015 ) and MNLI . In the NLI datasets , given one premise , there are one entailment hypothesis and another contradiction hypothesis accompanying . Therefore , in each training example
( x i , x + i , x â i )
, x i is the premise , x + i is the entailment hypothesis , and x â i is the contradiction hypothesis . Specifically , we use the multilingual AMR parser described in Section 3.2 to parse sentences into AMR graphs . Because the sentences in the NLI datasets are in English , the resultant AMR graphs are all derived from English . This is in contrast to downstream applications where an AMR graph may be derived from a foreign language . To reduce the discrepancy between training and testing , we use OPUS - MT ( Tiedemann and Thottingal , 2020 ) 2 , an off - the - shelf translation system , to translate English sentences in the NLI datasets to other languages . The translations in other languages are then parsed by our multilingual AMR parser . In this way , we extend the training of AMR embeddings to multilingual scenarios as well .
Mixed Training To better cover both the monolingual and cross - lingual settings in downstream applications , the training aims to capture the interactions between AMR graphs derived from the same language as well as those derived from different languages . To this end , we mix up AMR graphs from different languages during training . Moreover , to alleviate the drawback of imperfect parsing and avoid catastrophic forgetting of pre - trained language models , we also mix up AMR graphs and original English sentences during training . The details are shown in Algorithm 1 .
We hypothesize that the noise introduced by automatic translation could negatively affect the performance but a suitable amount of noise might also serve as a helpful regularizer . Unfortunately , due to the lack of gold translations , we could not perform a rigorous quantitative comparison . In our preliminary experiments , we also tried another automatic translation system , mBART - mmt ( Tang et al . , 2020 ) 3 , other than OPUS - MT . We found that mBARTmmt leads to worse performance in general , likely 2 https : / / huggingface.co / docs / transformers / model_doc / marian 3 https : / / huggingface.co / facebook / mbart - large-50 - many - to - many - mmt Algorithm 1 : Learning AMR Embeddings .
Input : Dataset : D = { ( xi , x + i , x â i ) } N i=1
, Systems : AMR parser parse ( â¢ ) and English - to - l translator translate ( â¢ , l ) , Maximum training steps : T , Batch size M , Language set : L .
1 for t â 1 to T do 2 Draw a mini - batch B = { ( xi , x + i , x â i ) } M i=1 from D 3 foreach sentence x in B do
Incorporating AMR Embeddings
The learned AMR embeddings can be used to augment any existing sentence embedding model . For any input sentence x , it is processed through two channels : ( 1 ) the sentence is first parsed into an AMR graph y = parse ( x ) . The graph is then fed into our AMR encoder : h = f Î¸ ( y ) . ( 2 ) the sentence is directly encoded by an off - the - shelf sentence embedding model g ( â¢ ) : s = g ( x ) . Lastly , we combine the text and graph embeddings ( s and h ) to produce the final sentence representation .
Parsing Theoretically , the multilingual AMR parser introduced in Cai et al . ( 2021 ) can parse 50 different languages as it inherits the multilingual encoder pre - trained on these languages from Tang et al . ( 2020 ) . However , the original parser has only been explicitly trained for German ( de ) , Spanish ( es ) , Italian ( it ) , Chinese ( zh ) , and English ( en ) . We hypothesize that including more languages in training can help improve the overall parsing accuracy . Therefore , we add French ( fr ) , another major language , to the training of the parser . 4 Integration We explore four different choices for the integration of the text embedding s and the AMR embedding h : s â h , s + h , , s s â h h , s s + h h , where â denotes the concatenation of two vectors . Empirically , we find that s s â h h generally works best .
Evaluation Benchmark
To provide a more comprehensive evaluation of multilingual sentence representations , In addition to traditional semantic textual similarity tasks , we also introduce a set of downstream transfer tasks .
Semantic Textual Similarity
Multilingual STS The goal of semantic textual similarity ( STS ) is to assign for a pair of sentences a score indicating their semantic similarity . For example , a score of 0 indicates not related and 5 indicates semantically equivalent . We use the datasets in Reimers and Gurevych ( 2020 ) , which is an extended version of the multilingual STS 2017 dataset ( Cer et al . , 2017 ) . The evaluation is done by comparing the distance in the embedding space and the human - annotated scores in the dataset .
Transfer Tasks
We evaluate the quality of the multilingual sentence embeddings on the following cross - lingual sentence / sentence - pair classification benchmarks :
XNLI The Cross - lingual Natural Language Inference benchmark is used to estimate the capability of cross - lingual / multilingual models in recognizing textual entailment . The evaluation sets of XNLI are created by manually translating the development corpus and the testing corpus of MultiNLI to 15 other languages .
PAWS - X The Cross - lingual Paraphrase Adversaries from Word Scrambling benchmark ( Yang et al . , 2019a ) consists of golden English paraphrase identification pairs from PAWS ( Zhang et al . , 2019b ) and around 24k human translations of PAWS evaluation sets ( i.e. , development set and testing set ) in English , French , Spanish , German , Chinese , Japanese ( ja ) , and Korean ( ko ) .
QAM The Question - Answer Matching task aims to predict if the given ( question , passage ) pair is a QA pair . We use the multilingual QAM dataset from XGLUE ( Liang et al . , 2020 ) , which provides the labeled instance ( question , passage , label ) in English , French , and German , to evaluate the effectiveness of multilingual sentence embeddings . MLDoc The Multilingual Document Classification benchmark ( Schwenk and Li , 2018 ) is a multilingual corpus with a collection of news documents written in English , German , Spanish , French , Italian , Chinese , Japanese , and Russian ( ru ) . The entire corpus is manually classified into four groups according to the topic of the document .
MARC The Multilingual Amazon Review Corpus ( Keung et al . , 2020 ) is a large - scale collection of Amazon user reviews for multilingual rating classification . The corpus covers 6 languages , including English , German , French , Spanish , and Chinese , Japanese .
6 Experiments
Experimental Setup
For STS tasks , following previous work ( Gao et al . , 2021 ) , we define the similarity score as the cosine similarity of sentence embeddings and compute the Spearman 's rank correlation between the computed score and the gold score .
For downstream transfer tasks , we follow the conventional zero - shot cross - lingual transfer setting ( Liang et al . , 2020 ; Hu et al . , 2020 ) , where annotated training data is provided in English but none is provided in other languages . We fit a logistic regression classifier on top of fixed sentence representations and follow default configurations in Conneau and Kiela ( 2018 ) ; Gao et al . ( 2021 ) . To faithfully reflect the multilinguality of multilingual sentence embeddings , we train exactly one model for each task . The union of the development sets in different languages is adopted for model selection .
Implementation Details
We initialize our AMR encoder with BERT ( Devlin et al . , 2019 ) ( uncased ) and take the [ CLS ] representation as the sentence embedding . By default , the AMR encoder is trained on English , German , Spanish , Italian , Chinese , French , and Arabic ( ar ) . Each model is trained for a maximum of 9 epochs with a learning rate of 5e â 5 and a batch size of 512 . The temperature in Eq . ( 1 ) is set to be 0.05 . For model selection , we use the STS - B development ( Cer et al . , 2017 ) . We train a multilingual AMR parser on English , German , Spanish , Italian , Chinese , and French using the same recipe in Cai et al . ( 2021 ) . We release our code at https : / / github.com / jcyk / MSE-AMR .
Baseline Systems
We evaluate the following systems : mBERT / XLM - R We use the mean pooling of the outputs from the pre - trained mBERT ( Devlin et al . , 2019 ) and XLM - R ( Conneau et al . , 2020 ) , which are pre - trained on multilingual data . However , no parallel or labeled data was used .
mUSE Multilingual Universal Sentence Encoder ( Chidambaram et al . , 2019 ) uses a dual - encoder transformer architecture and adopts contrastive objectives . It was trained on mined question - answer pairs , SNLI data , translated SNLI data , and parallel corpora over 16 languages .
LASER LASER ( Artetxe and Schwenk , 2019 ) trains a sequence - to - sequence encoder - decoder architecture on parallel corpora for machine translation . The sentence representation is obtained via max - pooling over the output of the encoder . LASER was trained over 93 languages .
LaBSE Language - agnostic BERT Sentence Embedding ( LaBSE ) ( Feng et al . , 2020 ) was trained similar to mUSE with a translation ranking loss . It fine - tunes a dual - BERT architecture with 6 Billion translation pairs for 109 languages .
Xpara Reimers and Gurevych ( 2020 ) fine - tunes XLM - R to imitate SBERT - paraphrases ( Reimers and Gurevych , 2019 ) , a RoBERTa model trained on more than 50 Million English paraphrase pairs , with massive bilingual sentence pairs over 50 languages .
Model Variants
To study the effect of each modeling choice , we implement a series of model variants .
â¢ # 1 : To show if learning from English data suffices , we train the AMR encoder with only English sentences and the AMR graphs derived from them .
â¢ # 2 : To study the effect of extending the training of the multilingual AMR parser to French , we use the original parser in Cai et al . ( 2021 ) , which does not include French .
â¢ # 3 : To measure the help of involving more languages when training the AMR encoder , we train the AMR encoder without the AMR graphs derived from French and Arabic .
â¢ # 4 : To validate the usefulness of adding the English sentences to the training of the AMR encoder , we train the AMR encoder without English sentences .
â¢ # 5 : The standard model as described in Section 6.2 .
For each model variant , we report the average performance over five different runs ( different random seeds ) throughout this paper .
Results
Multilingual STS Table 2 and Table 3 show the evaluation results on 3 monolingual STS tasks and 7 cross - lingual STS tasks respectively . As seen , the best - performing models in the literature are mUSE and Xpara . Thus , we present the results of augmenting mUSE and Xpara with our AMR embeddings , denoted by mUSE++ and Xpara++ respectively . Using AMR embeddings substantially improves both two models across the monolingual ( up to +2.31 on avg . ) and cross - lingual settings ( up to +2.22 on avg . ) , greatly advancing the state - of - theart performance . with our AMR encoders . We hypothesize that it is because Xpara is trained on paraphrase corpus , which diminishes the ability of AMR to group different expressions of the same semantics .
One interesting finding is that model variant # 2 performs best on monolingual settings while model variant # 5 attains the best results on cross - lingual settings . We believe that adding more languages to the training of the AMR parser helps the generalization to other languages and reduces the parsing inconsistency across different languages . Thus , the AMR graphs from different languages are better aligned , leading to a better - aligned vector space . On the other hand , adding more language may decrease the parsing accuracies on individual languages due to the fixed model capacity . Note that all other model variants except # 2 underperform # 5 , confirming the effectiveness of the proposed mixed training strategy .
Transfer Tasks Table 4 shows the evaluation results on transfer tasks . For each task , we report the macro - average scores across different languages . The results for each language can be found in Appendix . Different to previous work , our AMR encoders are only trained with a few languages ( en , de , es , it , zh , fr , and ar ) at most . To isolate the effect on unseen languages , we separate the results on those seen languages from all languages ( seen / all ) . First of all , we find that the rankings of existing models are quite different to the results on STS tasks . LASER and LaBSE achieve the best results on most transfer tasks except for QAM , and outperforms mUSE and Xpara by large margins in most cases . The results demonstrate the limitation of solely testing on sentence similarity measurement .
Next , we augment the best - performing models , LASER and LaBSE , with our AMR embeddings ( LASER++ and LaBSE++ ) . For seen languages , our methods substantially boost the performance of these two models across different tasks ( up to +2.02 on avg . ) . The performance gains over LASER are greater than those over LaBSE . Note that LASER is trained with an encoder - decoder architecture and both LaBSE and our AMR encoders are trained with a Siamese network . Therefore , we believe the AMR embeddings are more complementary to LASER .
When considering all languages , the improvements over LASER are also considerable ( up to +1.11 on avg . ) . However , according to the average scores over different tasks , the AMR embeddings seem to fail to improve LaBSE ; We even observe a performance drop for model variants # 1- # 3 . Nevertheless , the performance drop largely comes from XNLI while the scores on other tasks are instead boosted . This is because the test sets of XNLI include some distant languages ( e.g. , Swahili and Urdu ) that our multilingual AMR parser can not handle well ( see the results on individual languages in Table 6 in Appendix ) . We conjecture that further extending the multilingual AMR parser to more languages can alleviate this problem . The comparison among different model variants provides a basis for the above speculation . As we can see , model variant # 2 ( exclude French from the training of the multilingual AMR parser ) performs worst . Also , model variants # 1 ( drop all non - English AMR graphs for training ) and # 2 ( drop the AMR graphs derived from French and Arabic ) are the other two variants that negatively impact the average performance . Another interesting observation is that model variant # 4 performs best on MLDoc and QAM , suggesting English sentences might not be necessary .
Conclusion
This paper presented a thorough evaluation of existing multilingual sentence embeddings , ranging from traditional text similarity measurement to a new variety of transfer tasks . We found that different methods excel at different tasks . We then proposed to improve existing methods with universal AMR embeddings , which leads to better performance on all tasks .
Limitations
Although our work provides an effective solution for improving multilingual sentence embeddings with AMR , we acknowledge some limitations of this study and further discuss them in the following :
( 1 ) Our framework treats the text encoder as a black box and does not care too much about its implementation . Although it is flexible and straightforward to apply our framework to any multilingual sentence embedding model , designing more specific interaction mechanisms for different text encoders is supposed to be better and we leave it as future work .
( 2 ) The improvement from our framework is higher in seen languages than unseen languages . Further extending the language coverage in the training phases of both the multilingual AMR parser and the AMR encoder is presumably beneficial to the cross - lingual generalization capability of the AMR embeddings . However , due to the limit of computational resources , we only consider a few languages in the experiments .
A Transfer task
We provide the detailed results on each language for each transfer task in
Training Language Models with Memory Augmentation
Recent work has improved language models ( LMs ) remarkably by equipping them with a non - parametric memory component . However , most existing approaches only introduce memories at testing time or represent them using a separately trained encoder , resulting in suboptimal training of the language model . In this work , we present TRIME , a novel yet simple training approach designed for training LMs with memory augmentation . Our approach uses a training objective that directly takes inbatch examples as accessible memory . We also present new methods for memory construction and data batching , which are used for adapting to different sets of memories - local , longterm , and external memory - at testing time . We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings . Concretely , TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103 , by effectively leveraging a large memory set from the training corpus . Compared to standard LM training , TRIME adds negligible computational overhead and is compatible with different neural architectures , making it a versatile solution for training memory - augmented LMs . 1
Introduction
Memory augmentation has become a remarkable approach to enhance language modeling performance without significantly increasing the amount of parameters and computation . By accessing memory units such as a neural cache of recent inputs ( Merity et al . , 2017 ; Grave et al . , 2017b ) and an external look - up table ( Khandelwal et al . , 2020 ) , a memory - augmented language model ( LM ) enjoys increased memorization capacity and sets * TL currently works at Google Research . The collaboration was initialized before TL joined Google .
1 Our code and pre - trained models are publicly available at https : / / github.com / princeton-nlp / TRIME . new state - of - the - art records in various language modeling benchmarks .
A major limitation of existing approaches , however , is that the memory units are either introduced at testing time ( Grave et al . , 2017b , a ; Khandelwal et al . , 2020 ) or taken from a separately trained model . As a consequence , they are not directly optimized during the training process , resulting in a missed opportunity to achieve even stronger results . In this paper , we pioneer and present a novel yet simple training approach TRIME ( Training with In - batch Memories ) 2 , that is well - suited for memory augmentation in language modeling . Our approach makes two major departures compared to standard language model training :
Training objective Inspired by contrastive representation learning , we propose a training objective that directly leverages in - batch examples as accessible memory ( Figure 1 ) . Our training ob - jective is closely connected to neural cache models ( Grave et al . , 2017b ; Merity et al . , 2017 ) and nearest - neighbor language models ( Khandelwal et al . , 2020 ) , where the next - token probabilities are calculated by comparing encoder outputs against static token embeddings and memory representations . However , previous work only considers incorporating memories at testing time , while we do for both training and testing .
In - batch memory construction With this training objective in mind , the key challenge is how to construct memories effectively during training while keeping it efficient . We identify three types of memories that can be leveraged at testing time and have been explored in the literature : ( a ) local memory denotes the words that appear in the recent past and are modeled using attention ( Vaswani et al . , 2017 ) ; ( b ) long - term memory 3 denotes longrange context from the same document but can not be directly accessed due to the limit of input length ; ( c ) external memory is used to store the entire training set or any additional corpus ( Khandelwal et al . , 2020 ; Borgeaud et al . , 2021 ) .
To better leverage these memories at testing time , we devise new data batching strategies to improve the construction of training memories ( Â§ 4 ) . By packing consecutive segments from the same document in one training batch , our model can access long - term memories beyond the attention context . We pack segments from other documents that have high lexical overlap as a proxy to all external memory units . Importantly , these working memories are generated on the fly during training , allowing us to back - propagate to all memory representations .
We instantiate TRIME in three models by considering different sets of training and testing memories ( Table 1 ) and evaluate them on multiple language modeling and machine translation benchmarks . We highlight our results as follows :
â¢ We first show that we can simply optimize a language model using our training objective without long - term and external memory . Without any other modifications , we demonstrate that a 247 M Transformer - based model can achieve an improved perplexity from 18.70 to 17.76 on WIKITEXT-103 ( Merity et al . , 2017 ) for vanilla language models .
â¢ By training with consecutive segments in the same batch , our approach is capable of leveraging very long context at testing time - up to 15k-25k tokens on WIKITEXT-103 and ENWIK8 ( Mahoney , 2009 ) . Our approach achieves at least competitive performance as previous works ( Dai et al . , 2019 ; Martins et al . , 2022 ; Ji et al . , 2022 ) that modify the Transformer architecture to incorporate memories from previous segments , yet our solution is conceptually simpler and computationally cheaper .
â¢ Finally , we train language models by incorporating all other segments in the same batch as memories . Our model works better with a large datastore at testing time and improves over the kNN - LM model ( Khandelwal et al . , 2020 ) by reducing the test perplexity from 16.23 to 15.41 on WIKITEXT-103 . We also demonstrate significant improvements over the kNN - MT baseline ( Khandelwal et al . , 2021 ) on an IWSLT'14 De - En machine translation task .
In summary , we propose a simple approach TRIME for optimizing language models with memory augmentation and demonstrate consistent and significant gains in multiple experimental settings . Our approach only uses memories at the final prediction step , and hence adds little computational overhead and can be combined with different model architectures such as recurrent networks and other attention variants ( Lei , 2021 ; Dai et al . , 2019 ; Rae et al . , 2020 ) . We hope that our work can encourage the research community to think about better training objectives for language models , given their significant societal impacts ( Brown et al . , 2020 ; Chowdhery et al . , 2022 ; .
Preliminaries
Language Modeling
In this paper , we mainly focus on improving language models , although our solutions may extend to most text generation tasks ( see one example of machine translation in Â§ 5.4 ) . Neural language models take a sequence of tokens as context c t = x 1 , . . . , x tâ1 and map it to a vector representation f Î¸ ( c t ) â R d , where f Î¸ ( â ) is parameterized by a neural network . The next - token probability is :
P ( w c t ) â exp ( E âº w f Î¸ ( c t ) ) , ( 1 )
where E w â R d denotes the output embedding of token w â V. The parameters are optimized to minimize the negative log - likelihood of ground truth x t during training .
Memory Augmentation
We consider memory as a set of context - target pairs { ( c i , x i ) } following Grave et al . ( 2017b ) ; Khandelwal et al . ( 2020 ) . These context - target pairs can be aggregated to obtain the next - token probability weighted by the similarity between hidden representations . 4 We formalize three types of contexttarget memories as follows :
Local memory The local memory is simply the preceding tokens in the same input . Specifically , for c t = x 1 , . . . , x tâ1 , it is defined as :
M local ( c t ) = { ( c j , x j ) } 1â¤jâ¤tâ1 .
( 2 ) Grave et al . ( 2017b ) use the local memory at testing time , denoted by the " continuous cache " model . However , it has been argued less effective for Transformer - based models because they can already learn to leverage recent tokens in the selfattention layers ( Khandelwal et al . , 2020 ) . Interestingly , we show that using local memory is still beneficial if we consider it during training .
Long - term memory Long - term memory denotes long - range context from the same document , but they can not be directly accessed by attention . For example , if a document contains 10 K tokens , only a short segment of text ( e.g. , 100 - 3 K tokens ) can be fed into a Transformer model because the complexity scales quadratically with the input length . Formally , we divide a document into consecutive segments s ( 1 ) , . . . , s ( T ) , where a segment
s ( i ) contains L contexts s ( i ) = { c ( i ) 1 , . . . , c ( i ) L } . The long - term memory for c ( i ) t is : M long ( c ( i ) t ) = { ( c ( k ) j , x ( k ) j ) } 1â¤k < i,1â¤jâ¤L . ( 3 )
Previous works ( Dai et al . , 2019 ; Rae et al . , 2020 ; Martins et al . , 2022 ; Ji et al . , 2022 ; Lei , 2021 ) leverage hidden representations from previous segments with modified Transformer architectures to learn long - range dependency . Our approach does not modify the model architecture and is compatible with these neural architectures . 5 External memory Finally , external memory assumes a large corpus D and the external memory set can be defined as :
M ext = { ( c j , x j ) â D } . ( 4 )
D can be simply the training corpus , or a domainspecific corpus when the testing domain shifts ( Â§ 5.3 ) . Note that M ext is usually several orders of magnitude larger than previous two types ( e.g. , 10 8 ) ; accessing all the memories is computationally expensive and requires approximate nearest neighbor search ( Johnson et al . , 2019 ) . defines the next - token probability distribution as :
P ( w c ) â exp ( E âº w f Î¸ ( c ) ) + ( c j , x j ) âM train â¶x j = w exp ( sim ( g Î¸ ( c ) , g Î¸ ( c j ) ) ) . ( 5 )
Here , f Î¸ ( c ) is the output representation of a Transformer model and E w is the token embedding . g Î¸ ( â ) denotes the representations that can be used to compute similarity between c and all the contexts c j in the memory M train . It is possible to simply take g Î¸ = f Î¸ ; however , we find that taking g Î¸ to be the input of the final feed - forward layer in Transformer works better , which is consistent with the observation in Khandelwal et al . ( 2020 ) . In addition , sim ( â , â ) is a similarity function and we found using the scaled dot - product sim ( q , k ) = qâk â d ( Vaswani et al . , 2017 ) leads to stable training and better performance in our preliminary experiments .
This training objective can be viewed as a contrastive loss ( Hadsell et al . , 2006 ) : for a contexttarget pair ( c , w * ) , the goal is to align the query representation f Î¸ ( c ) ( and g Î¸ ( c ) ) with the static token representation E w * , and contextualized representations that share the same next token i.e. , g Î¸ ( c j ) for x j = w * . Our objective handles rare words nicely - if w * does not appear in the training memory , the objective will fall back to aligning f Î¸ ( c ) with only the word embedding E w * . Similar to the vanilla training loss ( Eq . 1 ) , our TRIME loss is optimized to minimize the negative log - likelihood of next token w * and all the parameters Î¸ and E w are updated during training .
Our training objective is also inspired by the success of contrastive learning in dense retrieval . As we will show in Â§ 6 , it can help improve retrieving contexts that share the same next token effectively when the set of testing memories is large . Our objective is also closely connected to the objective used in Grave et al . ( 2017b ) ; Khandelwal et al . ( 2020 ) , which linearly interpolates the distribution of standard language modeling , and a distribution defined by cache / external datastore , e.g. , P ( w c ) = ( 1âÎ» ) P lm ( w c ) +Î»P kNN ( w c ) . Our work differs from previous works that we use this objective during training ( and testing ) , while they only used it at testing time - the key is how to construct training memories that we will elaborate next . 6
Adaption to Different Memories
Inference We are interested in incorporating the three types of memories defined in Â§ 2.2 and their combinations at testing time . The testing objective is basically the same as the training objective ( Eq . 5 ) except that we take testing memories as a combination of M local , M long and M ext . As M ext can be very large , we approximate it by retrieving the top - K closest terms to g Î¸ ( c ) . We tune a temperature term Ï to adjust the weight of the memory component ( see Appendix A for details ) .
Notation Throughout this section , we use L to denote segment length , B to denote the total number of segments used in the one training batch , and m to denote the number of consecutive segments from each document in the batch . Correspondingly , each batch will contain b â B m different documents . L , B and m are hyper - parameters that we will choose for training , and will vary as we consider different memories during inference .
A key challenge is that the testing memories can be very large ( e.g. , M long â¼ 10 4 and M ext â¼ 10 8 in our experiments ) and it is computationally infeasible to keep training memories the same as testing memories . In the following , we will discuss three ways of constructing training memories and data batching , aiming to reduce the discrepancy between training and testing . Along the way , we will also present three major model instantiations : TRIMELM , TRIMELM long , TRIMELM ext ( Table 1 ) , which combine the training strategies and different sets of testing memories .
Local Memory
M local only considers all the previous tokens in the same segment . It is straightforward that we can simply use M train = M local . As shown in Fig . 2 ( a ) , we basically do not need to make any modifications compared to standard language model training . All we need is to replace the training objective of Eq . 1 by our objective in Eq . 5 , by incorporating ( c j , x j ) , âj < t in the memory during both training and testing . The computational overhead is also negligible compared to running neural encoders on the segment x 1 , . . . , x L itself . We denote this model as TRIMELM , which can be viewed as a lightweight replacement for vanilla language models . As we will show in the experiments , simply incorporating local memory provides a notable gain on multi - ple LM benchmarks , showing the effectiveness of training with memories explicitly .
Long - term Memory
In order to enable long - term memory augmentation , we pack multiple consecutive segments from the same document in a training batch ( i.e. , m > 1 ) . For a context - target pair ( c , w ) in the training batch , its accessible memory M train includes tokens from previous segments as well as the preceding tokens in the same segment . We denote this model as TRIMELM long . It shares a similar motivation with many previous works which aim to leverage memory from previous segments through attention recurrence ( Dai et al . , 2019 ; Ji et al . , 2022 ) , or memory compression ( Rae et al . , 2020 ; Martins et al . , 2022 ; . However , our solution deviates significantly from previous approaches . First , previous works need to store the hidden representations ( of every layer ) from previous segments and modify the self - attention layers to incorporate them . Our approach does not modify the architecture and only uses the outputs from the last layer . Additionally , previous works use stale memory representations and do not back - propagate gradients to the rep - resentations of previous segments , whereas our batching method enables gradient propagation to the memory and previous segments . 7 As we will show in the experiments , our approach is competitive with previous works while being conceptually simpler and computationally cheaper .
External Memory
Finally , we consider external memory M ext . Since M ext contains the context - target pairs in a large corpus such as the entire training set , we need to retrieve top - K pairs from M ext measured by sim ( g Î¸ ( c ) , g Î¸ ( c j ) ) through ( approximate ) similarity search ( more details are given in Â§ 5.2 ) .
Since the retrieved contexts at testing time are expected to be similar to the query context , we propose a simple heuristic for constructing training memories M train by packing segments that have large lexical overlap into the same batch using BM25 scores ( Robertson and Zaragoza , 2009 ) . Specifically , we start with a single segment and repeatedly add segments with highest BM25 scores into the same batch ( Appendix B ) . A high BM25 score indicates that two segments have high lexical overlap and can serve as a good proxy to nearest neighbors in the external memory , which improves our model predictions at testing time . M train contains all tokens from other segments as well as the previous tokens in the same segment ( Figure 2 ( c ) ) . We set m = 1 during training as many segments from the same document tend to have high lexical overlap and denote this model by TRIMELM ext .
In practice , when considering tokens from both the current segment and other segments in the batch , we observe that the model tends to leverage local memory more and ignore other segments .
To encourage the use of information from other segments , we exclude the local memory from M train with a probability of p during training ( we find that p = 90 % works the best , see Appendix H ) . This significantly improves performance when the model is evaluated with a large set of external memory .
Experiments
Datasets and Tasks
We evaluate our approach on two popular language modeling benchmarks : WIKITEXT-103 ( Merity et al . , 2017 ) , ENWIK8 ( Mahoney , 2009 ) , and a machine translation benchmark : IWSLT'14 De - En . We also evaluate domain - adaptation performance on the BOOKSCORPUS dataset ( Zhu et al . , 2015 ) .
WIKITEXT-103 is a word - level language modeling dataset consisting of 103 M training tokens . We evaluate on two model configurations : one uses a 247 M Transformer model and a segment length L = 3 , 072 and another one uses a 150 M Transformer model with a segment length L = 150 .
ENWIK8 is a character - level language modeling dataset that contains a total of 100 M characters . We use a 12 - layer Transformer model with a hidden dimension 512 and segment length L = 512 .
BOOKSCORPUS is a word - level language modeling dataset . We build our own train / dev / test splits which consist of 100M / 250K / 250 K tokens . On this dataset , we evaluate the models trained on WIKITEXT-103 to study how our approach can adapt to new domain without re - training .
IWSLT'14 De - En is a machine translation task , which consists of 170 K translation pairs . We use a Transformer encoder - decoder model . See Appendix C for how we adapt our approach to the machine translation task .
See Appendix C for data statistics and task setups and Appendix D for model configurations .
Training and Inference Details
We implement our approach using the Fairseq library ( Ott et al . , 2019 ) . For TRIMELM long and TRIMELM ext , we tune the number of segments used in M long on the development set during evaluation . Our TRIMELM ext model requires building a large datastore at testing time and we use the FAISS library ( Johnson et al . , 2019 ) for approximate nearest neighbor search ( details in Appendix D ) .
We first train our model with the standard LM objective ( Eq . 1 ) for the first 5 % updates . Without this warmup stage , we observe the training process to be unstable probably due to a large variance in the estimated distributions . We use different memories when evaluating different instantiations of TRIME , as shown in Table 1 . We find that when a large set of external memory M ext is considered during inference , the performance can be improved by linearly interpolating the output distribution and a distribution over the memory , similarly to kNN - LM ( Khandelwal et al . , 2020 ) . Thus , we apply an additional linear interpolation to our output probability distribution when considering external mem- cache to the vanilla Transformer model and find it to underperform our model , demonstrating the importance of joint training using our approach . Compared to previous methods which explicitly leverage hidden representations from previous segments ( Dai et al . , 2019 ; Rae et al . , 2020 ; Martins et al . , 2022 ; Ji et al . , 2022 ; Lei , 2021 ) , our approach achieves better or at least competitive performance . Different from these approaches which need to store all the hidden representations of every layer and modify the model architecture , we only incorporate the outputs from the last layerrequiring less computations and GPU memory . Our approach is orthogonal and can be applied on top of these models . To verify this , we adapt our approach to SRU++ ( Lei , 2021 ) though the memory representations are optimized on one domain , our approach does not overfit , and building an external memory using the target domain dataset enables the model to perform well with domain shifts .
Results : Machine Translation
To showcase the generality of our training approach TRIME to other generation tasks , we evaluate our approach on the IWSLT'14 de - en translation task .
Since it is a sentence - level task , we do not use any local or long - term memory ( M local , M long ) , as there are few repetitive tokens . We denote our model as TRIMEMT ext .
As shown in Table 6 , our approach improves the vanilla Transformer by 1.15 BLEU score and outperforms kNN - MT ( Khandelwal et al . , 2021 demonstrates that our approach is able to improve the performance on other language generation tasks with different memory access .
Analysis
We conduct ablation studies and analysis to further understand individual components of our approach . Due to the limited computation budget , some experiments on WIKITEXT-103 are conducted with a small 7 M Transformer model ( 8 layers , hidden dimension 128 ) in this section and the trends are generally similar for smaller models ( see Appendix D and Appendix F for details ) .
Memory construction We first study how different data batching and memory construction strategies affect the performance when different testing memories are used . We compare our three models ( TRIMELM , TRIMELM long , TRIMELM ext ) in Table 7 . This ablation study clearly shows that packing consecutive segments and segments with high BM25 scores in the same training batch and constructing memories properly can improve the performance when the long - range and external memories are used . This demonstrates the importance of closing the gap between training and inference .
Leveraging long - range contexts We study if our model is able to handle large long - term memory . As Figure 3 shows , our model is able to effectively handle long - range context ( more than 10k tokens ) , which goes beyond typical attention context . Compared to continuous cache ( Grave et al . , 2017b , a ) , the improvement of our approach becomes larger when more long - term memory is incorporated . This suggests that our model is able to leverage long - range context much more effectively . Additional analysis We conduct more ablation studies and analysis in Appendix G. We summarize them as follows . ( 1 ) Our ablation studies show using BM25 batching method and enabling back - propagation to update memory representations are important for our approach ( Table 11 ) .
( 2 ) TRIMELM is able to leverage local memory effectively to improve performance with different segment lengths L ( Table 12 ) . ( 3 ) TRIMELM ext outperforms kNN - LM in terms of top - K retrieval accuracy given the external memory set ( Table 13 ) .
( 4 ) We study the perplexity of tokens in different frequency groups and find that TRIMELM and TRIMELM long achieve larger improvements on rare words while TRIMELM ext improves results across the board ( Table 14 ) .
Related Work
Memory - augmented language models We have discussed continuous cache , kNN - LM and models that leverage representations from long - range context in the previous sections . also aim to combine several types of memories by learning an adaptive gating function ; however , their external memory uses a pre - trained vanilla language model . Borgeaud et al . ( 2021 ) demonstrate a remarkable performance by augmenting LMs with an external datastore of trillion of tokens and their datastore is built based on chunks of text using off - the - shelf BERT embeddings ( Devlin et al . , 2019 ) . Our approach differs from prior works in the following aspects , which help our model achieve superior performance with little overhead : ( 1 ) we update the memory representations through back - propagation from the end loss ;
( 2 ) our model does not modify the base architecture ;
( 3 ) we consider different types of memories in a unified framework . GNN - LM ( Meng et al . , 2022 ) augments LMs with a graph neural network to aggregate information of retrieved items from external memory , which makes an orthogonal contribution to our paper .
Transformers for long inputs A large body of research has investigated how to scale self - attention mechanism to long contexts , either through sparse attention ( Liu et al . , 2018 ; Child et al . , 2019 ; Beltagy et al . , 2020 ; Zaheer et al . , 2020 ) or subquadratic - time attention Choromanski et al . , 2020 ; Peng et al . , 2021 ; Katharopoulos et al . , 2020 ) . See Tay et al . ( 2020 ) for a comprehensive survey of efficient Transformers . Our approach is orthogonal , as we only change the training objective and data batching to enable models to use large contexts during inference .
Memory - augmented models for downstream tasks While our paper focuses on improving language models with memory augmentation , other works improve models for downstream tasks with a retrieval component , such as question answering ( Kumar et al . , 2016 ; de Masson D'Autume et al . , 2019 ; Guu et al . , 2020 ; Zemlyanskiy et al . , 2021 ; Chen et al . , 2022 ; Izacard and Grave , 2021 ; Singh et al . , 2021 ) , dialogue , and other knowledge - intensive NLP tasks Petroni et al . , 2021 ) .
Conclusion
In this work , we propose TRIME , a training approach for language modeling . We present three model instantiations TRIMELM , TRIMELM long , TRIMELM ext : Through carefully - designed data batching and memory construction during training , we show that our models can leverage long - range contexts and external memory effectively at testing time . Our approach adds little computational overhead and does not modify model architectures , making it compatible with other neural models and techniques . For future work , we are interested in training TRIME with large language models and other text generation tasks .
Limitations
We discuss limitations of our research as follows .
â¢ Despite the strong performance achieved by our approach when incorporating a large set of external memory , it results in a reduced inference efficiency at the same time due to the nearest neighbor search . For example , the model is 10Ã slower when incorporating external memory . This issue can be more crucial when the external memory is even larger . Potential solutions to this issue include ( 1 ) constructing the memory using a coarser granularity ( e.g. , text blocks ) ( Borgeaud et al . , 2021 ) ;
( 2 ) compressing the external memory set and reducing the dimension of memory representations ( He et al . , 2021 ) .
â¢ We mainly experiment with Transformerbased models and additionally adapt our approach to SRU++ ( Lei , 2021 ) . We believe our approach is compatible with other architectures or techniques such as Transformer - XL ( Dai et al . , 2019 ) and Compressive Transformer ( Rae et al . , 2020 ) . We plan to explore them as future work .
â¢ We evaluate our approach on machine translation to test the generality of TRIME to other generation tasks . However , due to compute limitation , we only evaluate it on a small dataset ( i.e. , IWSLT'14 ) , which consists of 4 M tokens in the external memory . We leave the evaluation on larger machine translation datasets as future work .
â¢ Our paper mainly studies language modeling tasks and machine translation tasks . Although we believe our approach is compatible with all language generation tasks , how to adapt TRIME to natural language understanding tasks such as text classification still remains an open question .
â¢ The biggest model we experimented with consists of 247 M parameters due to our compute limit . The state - of - the - art auto - regressive LMs contain hundreds of billions of parameters ( Brown et al . , 2020 ) . We hope to see future efforts in scaling up our approach and evaluating the effectiveness on large LMs .
Ethical Considerations
Our proposed approach leverages external memory to achieve strong results on multiple language modeling benchmarks . In our experiments , we construct the external memory using the corpus on which the model is trained , while it can be constructed using any corpus . In general , we suggest practitioners constructing external memory using a public corpus , as retrieving from the external datastore can cause information leakage from the corpus . We acknowledge this ethical consideration and caution those who apply our approach to privacy - sensitive domains . domain adaptation ( Appendix 5.3 ) . Table 8 shows the statistics . WIKITEXT-103 ( Merity et al . , 2017 ) is a wordlevel language modeling dataset consisting of 103 M training tokens . Following standard practice , we use adaptive softmax and adaptive token embeddings in our model and report perplexity . In order to better compare with previous work , we evaluate on two model configurations - one uses a 247 M Transformer model and a segment length L = 3 , 072 following ; Khandelwal et al . ( 2020 ) and another one uses a 150 M Transformer model with segment length L = 150 following Dai et al . ( 2019 ) . More details are provided in Appendix D .
ENWIK8 ( Mahoney , 2009 ) is a character - level language modeling dataset that contains a total of 100 M characters . Following previous work , we report bit - per - character ( bpc ) on this dataset . We use a 12 - layer Transformer model with a hidden dimension 512 and segment length L = 512 .
We also evaluate the IWSLT'14 DEâEN machine translation task , which consists of 170 K translation pairs . Following Khandelwal et al . ( 2021 ) , we build an external memory by taking all the translation contexts and the corresponding target token ( ( x , y < t ) , y t ) on the training set . We use the output representation as f ( ( x , y < t ) ) and the input representation of last FFN layer as g ( ( x , y < t ) ) to compute the loss . Similarly , we use BM25 to batch training data -we encourage two target sentences with a high BM25 score to be in the same training batch ( see Algorithm 1 ) . We use the default model configuration in the Fairseq library ( Ott et al . , 2019 ) , and sacrebleu ( Post , 2018 ) to compute BLEU scores ( Papineni et al . , 2002 ) .
We evaluate our approach for domain adaptation on the BOOKSCORPUS dataset ( Zhu et al . , 2015 ) , which is a word - level language modeling dataset . The complete BOOKSCORPUS dataset consists of 0.7B tokens . We build our own train / dev / test splits which consist of 100M / 250K / 250 K tokens respectively . The train set is only used to build external memory . On this dataset , we evaluate the models trained on WIKITEXT-103 to study how our approach can adapt to new domain without re - training or fine - tuning . The model we used on this dataset is the 247 M Transformer model with a segment length L = 3,072 .
D Model Configurations and Hyperparameters
Table 9 shows the model configurations and hyperparameters that we used in our experiments . Following , during training , we train the model with fixed - length segments ; during evaluation , we evaluate on the tokens at the end of the segment ( i.e. , an evaluation segment can overlap with others ) . When evaluating with large external memory , we always retrieve top - K ( K = 1,024 ) context - target pairs for language modeling . For machine translation , we tune K = { 1 , 2 , 4 , 8 , 16 , 32 , 64 } following Zheng et al . ( 2021 ) .
E Applying TRIMELM long to SRU++
We apply our approach to SRU++ ( Lei , 2021 ) and we believe our approach is also compatible with other architectures such as Transformer - XL ( Dai et al . , 2019 ) . SRU++ is a language model which combines recurrent units and the attention mechanism . SRU++ use hidden representations from the previous segment at attention layers to incorporate long - range contexts , similarly to Dai et al . ( 2019 ) .
To apply our approach to SRU++ , we follow their data - batching method as it is required due to the recurrence of the model architecture . We construct the training memory using all the contexts in the current segment ( i.e. , local memory ) and all contexts in the previous segment ( i.e. , long memory ) . Note that the memory representations from the previous segment will be stale , thus we do not back - propagate to that part . 16 . For other hyper - parameters and the optimizer , we follow the default ones in their implementation .
During inference , we can use more contexts to construct memory . We train with different segment lengths , i.e. , L = 512 or L = 2048 . For the model trained with L = 512 , it can leverage a long - term memory of a size 6,144 during inference ; for the model trained with L = 2048 , it can leverage a long - term memory of a size 12,228 .
G Additional Analysis
Ablation study on TRIMELM ext We study the importance of packing segments with high BM25 scores in the same training batch , as well as the effectiveness of enabling back - propagation to memory representations during training . As shown in Table 11 , when we random batch training segments ( instead of using BM25 scores ) , the perplexity increases to 45.71 ( +4.21 ) . Also , enabling backpropagation to memory is crucial for our approach -the performance is much worse if we disable it .
Effectiveness of using local memory We study the effectiveness of our model TRIMELM that uses only local memory with different segment lengths L. As shown in Table 12 , our model significantly outperforms the baselines in all the settings . This suggests that our model can leverage local memory very effectively to improve performance .
Retrieval performance on external memory
When external memory is used in our experiments , we perform nearest - neighbor search over the entire memory set M ext to retrieve the top K keys ( we use K = 1024 ) . Table 13 compares the retrieval accuracy of our approach and kNN - LM ( Khandelwal et al . , 2020 ) for different K. Our approach outperforms kNN - LM in terms of retrieval results ; this explains how our final perplexity surpasses kNN - LM when incorporating external memory .
Perplexity breakdown for different frequencies
We aim to understand which type of memories improves perplexity of tokens in different frequency groups . We group tokens into 5 buckets according to their frequency on the development set . Table 14 shows the results for different models . TRIMELM and TRIMELM long improve the perplexity of rare words ( i.e. , frequency â¤ 1k ) while achieving similar or slightly worse results for frequent words compared to the Transformer baseline . TRIMELM ext improves perplexity in all the buckets . Interestingly , kNN - LM with continuous cache does not perform significantly better compared to TRIMELM and TRIMELM long although these two models do not use external memory . This suggests that jointly training memory representations and the language model particularly help improve the performance of rare words .
H Tuning p for training with external memory
When training the model with local and external memory , to avoid the model to only relies on highquality local memory , we disable the local memory with a probability of p. Here we study how p will affect the final performance of our model . The results of using different p are shown in Table 15 . We find that when p = 0 , the model performs poorly with external memory as the model learns to only leverage local memory and ignores external memory during training . By increasing p , this issue is mitigated . We set p = 0.9 in our main experiments .
Acknowledgments
We thank Jane Pan , Howard Chen , Alexander Wettig , Tianyu Gao , Kaiyu Yang , Mengzhou Xia , Jinhyuk Lee , and the members of Princeton NLP group for helping with proofreading and providing valuable feedback . This research is partially supported by the James Mi * 91 Research Innovation Fund for Data Science and a gift from Apple . ZZ is also supported by a JP Morgan PhD fellowship .
A Inference Method
Testing objective Formally speaking , our testing objective is basically the same as the training objective ( Eq . 5 ) : P ( w c ) â exp ( E âº w f Î¸ ( c ) ) + ( c j , x j ) âM eval â¶x j = w exp ( sim ( g Î¸ ( c ) , g Î¸ ( c j ) ) Ï ) ,
except that we take M eval as a combination of M local , M long and M ext . As M ext can be very large , we approximate it by retrieving the top - K closest terms to g Î¸ ( c ) . Formally , M eval of three instantiations of TRIME is constructed as follows , Linear interpolation when using M ext We find that when a large set of external memory M ext is considered during inference , the performance can be improved by calibrating a separated distribution over the memory and interpolating the output distribution and the memory distribution , similarly to kNN - LM ( Khandelwal et al . , 2020 ) . We think this is because the distribution of the similarity values has been significantly shifted during inference , while the relative ranking preserves . As a result , having values from two different distributions in one softmax normalization is sub - optimal compared to computing two separated probabilities and interpolating them .
Thus , we apply an additional linear interpolation to our output probability distribution . Specifically , we first use Eq . 6 to compute the distribution P ( w c ) . Then , we compute a probability distribution over the tokens in memory P â² ( w c ) as follow ,
We linearly interpolate these two probability distributions with a coefficient Î» and get the final output P final ( w c ) :
We tune the temperature terms and Î» on the development set .
B Packing Segments Using BM25 Scores
In Â§ 4.3 , we construct training memories M train by packing segments that have large lexical overlap into the same batch using BM25 ( Robertson and Zaragoza , 2009 ) . Algorithm 1 shows the process to pack segments into training batches . We start with a single segment and repeatedly add segments with highest BM25 scores into the same batch .
C Dataset Statistics and Tasks
We evaluate our approach on three benchmarks : WIKITEXT-103 , ENWIK8 , and IWSLT'14 . We also evaluate our approach on BOOKSCORPUS for
EmpHi : Generating Empathetic Responses with Human - like Intents
In empathetic conversations , humans express their empathy to others with empathetic intents . However , most existing empathetic conversational methods suffer from a lack of empathetic intents , which leads to monotonous empathy . To address the bias of the empathetic intents distribution between empathetic dialogue models and humans , we propose a novel model to generate empathetic responses with humanconsistent empathetic intents , EmpHi for short . Precisely , EmpHi learns the distribution of potential empathetic intents with a discrete latent variable , then combines both implicit and explicit intent representation to generate responses with various empathetic intents . Experiments show that EmpHi outperforms state - ofthe - art models in terms of empathy , relevance , and diversity on both automatic and human evaluation . Moreover , the case studies demonstrate the high interpretability and outstanding performance of our model . Our code are avaliable at https : / / github.com / mattc95 / EmpHi .
Introduction
Empathy is a basic yet essential human ability in our daily life . It is a capacity to show one 's caring and understanding to others . Many types of research have been conducted on empathetic expression to enhance the empathy ability of Artificial Intelligence , e.g. , computational approach for empathy measurement ( Sharma et al . , 2020 ) , empathetic expression understanding in newswire ( Buechel et al . , 2018 ) , online mental health support ( Sharma et al . , 2021 ) , etc . In this work , we focus on the task of generating empathetic responses ( Rashkin et al . , 2019 ; Lin et al . , 2019 ; Majumder et al . , 2020 ) in open - domain conversation .
Existing empathetic dialogue models pay more attention to the emotion - dependent response generation ( Lin et al . , 2019 ; Majumder et al . , 2020 ) .
I just started college again , and while I 'm doing great in school , it has lead me to feel very lonely with a lack of social life .
I am sorry to hear that ! I know I broke up with my ex , but I ca n't help but feel irritated when he talks about going on dates .
A while back my cat knocked over and broke my mother 's urn .
Oh my goodness , I have a cat , I know how you feel .
I am sorry to hear that . I hope you find someone to help you .
That is so annoying . I would be upset too .
Acknowledging
Sympathizing Agreeing
EmpHi Output
Figure 1 : EmpHi generates empathetic responses with human - like empathetic intents ( text in blue box ) , while existing empathetic dialogue models generate responses with contextually irrelevant and monotonous empathy ( text in orange box ) .
However , using emotion alone to generate responses is coarse - grained , and the model needs to incorporate empathetic intent information . On the one hand , the speaker often talks with a particular emotion while the listener shows their empathy with specific empathetic intents , e.g. , Acknowledging , Agreeing , Consoling and Questioning etc ( Welivita and Pu , 2020 ) . On the other hand , see in Figure 1 , when the user expresses sadness , existing models tend to generate sympathetic responses like " I 'm sorry to hear that . " However , empathy is not the same as sympathy , so the models should not only generate responses with Sympathizing intent . We demonstrate this phenomenon elaborately with a quantitative evaluation in Section 2 . In real life situation , humans could reply with various empathetic intents to the same context which depends on personal preference . For example , given a context , " I just failed my exam " , an individual may respond " Oh no , what happened ? " with Questioning intent to explore the experience of the user , or " I understand this feeling , know how you feel " with Agreeing intent . These types of empathy are more relevant , interactive , and diverse . To address the above issues , we propose a new framework to generate empathetic responses with human - like empathetic intents ( EmpHi ) which could generate responses with various empathetic intents , see examples in Figure 1 . Specifically , Em - pHi learns the empathetic intent distribution with a discrete latent variable and adopts intent representation learning in the training stage . During the generation process , EmpHi first predicts a potential empathetic intent and then combines both implicit and explicit intent representation to generate a response corresponding to the predicted intent . Our main contributions are :
â¢ We discover and quantify the severe bias of empathetic intents between existing empathetic dialogue models and humans . This finding will lead subsequent researchers to pay more attention to fine - grained empathetic intents .
â¢ To address the above problem , we propose EmpHi , which generates responses with human - like empathetic intents . Experiments have proved the effectiveness of our model through the significant improvement in both automatic and human evaluation .
â¢ According to the quantitative evaluation and analysis , EmpHi successfully captures humans ' empathetic intent distribution , and shows high interpretability in generation process .
Related Work
Empathetic Response Generation . Providing dialogue agents the ability to recognize speaker feelings and reply according to the context is challenging and meaningful . Rashkin et al . ( 2019 ) propose the EmpatheticDialogues for empathetic response generation research . Most subsequent empathetic conversation researches are evaluated on this dataset , including ours . They also propose Multitask - Transformer , which is jointly trained with context emotion classification and response generation . To further capture the fine - grained emotion information , Lin et al . ( 2019 ) introduce MoEL , a transformer with a multi - decoder . Each of them is responsible for the response generation of one specific emotion . Majumder et al . ( 2020 ) propose MIME , which mimics the speaker emotion to a varying degree . All these models focus on emotion - dependent empathetic response generation , whereas we pay more attention to the empathetic intents and propose to generate a response that is not only emotionally appropriate but also empathetically humanlike .
One - to - many Response Generation . Given dialogue history , there could be various responses depends on personal preference . Zhao et al . ( 2017 ) propose to learn the potential responses with continuous latent variable and maximize the loglikelihood using Stochastic Gradient Variational Bayes ( SGVB ) ( Kingma and Welling , 2014 ) . To further improve the interpretability of response generation , Zhao et al . ( 2018 ) propose to capture potential sentence - level representations with discrete latent variables . MIME ( Majumder et al . , 2020 ) introduces stochasticity into the emotion mixture for various empathetic responses generation .
Different from the previous works , we propose a discrete latent variable to control the empathetic intent of response and achieve intent - level diversity .
Empathetic Expression Bias
Although existing empathetic conversational methods have shown promising progress , we reveal there is a severe bias of empathetic expression between them and humans according to quantitative evaluation .
Empathy plays a vital role in human conversation , Welivita and Pu ( 2020 ) their empathy naturally by Questioning , Acknowledging , and Agreeing intents etc . However , there are no empirical experiments have shown how empathetic dialogue models express their empathy ? To further study , we finetune a BERT classifier ( Devlin et al . , 2019 ) on the released EmpatheticIntents 1 dataset ( Welivita and Pu , 2020 ) . Our classifier achieves 87.75 % accuracy in intent classification and we apply it to identify the empathetic intents of responses generated by the state - of - the - art empathetic dialogue model MIME ( Majumder et al . , 2020 ) . As shown in Figure 4 , the severe empathetic intent distribution bias emerges while comparing humans to MIME . Given context with sad emotion , existing models usually generate " I am sorry to hear that " with Sympathiz - ing intent , which is not human - like and contextually relevant . In addition , we can tell that the empathetic expression of MIME is monotonous . We also quantify the intent distribution of other empathetic dialogue models in the Appendix A. The results are similar to Figure 4 .
We believe this phenomenon is caused by that existing models only generate responses according to context emotion and lack fine - grained empathetic intent modeling . Therefore , we propose EmpHi , which generates empathetic responses with humanlike empathetic intents .
EmpHi Method
Task Definition and Overview
Given the context ,
C = [ c 1 , c 2 , â¢ â¢ â¢ , c m ]
, which consists of m words for single or multiple utterances . We aim to generate empathetic response ,
X = [ x 1 , x 2 , â¢ â¢ â¢ , x n ]
, with human - like empathetic intent . The whole model architecture is shown in Figure 3 .
EmpHi learns the potential empathetic intent distribution with a latent variable z , which could be seen in Figure 5 . Conditional Variational AutoEncoder ( CVAE ) ( Yan et al . , 2016 ; Zhao et al . , 2017 ; Gu et al . , 2019 ) is trained to maximize the conditional log likelihood , log p ( X|C ) , which involves an intractable marginalization over z. We train the CVAE efficiently with Stochastic Gradient Variational Bayes ( SGVB ) ( Kingma and Welling , 2014 ) by maximizing the variational lower bound of the log likelihood : p ( X|C , z ) denotes response reconstruction probability , q ( z|X , C ) is recognition probability and p ( z|C ) is prior probability . Our method mainly consists of three aspects :
log p ( X|C ) â¥E q ( z|X , C ) [ log p ( X|C , z ) ] â KL ( q ( z|X , C ) ||p ( z|C ) ) , ( 1 )
Z X C C X p ( | ) q ( | , ) p ( | , ) p ( | ) ( b ) ( a )
â¢ To capture the explicit relationship between the latent variable and the intent , we propose an intent representation learning approach to learn the intent embeddings .
â¢ We construct an intent predictor to predict potential response intent using contextual information and then use this intent for guiding the response generation .
â¢ During the generation process , EmpHi combines both implicit intent embedding and explicit intent keywords to generate responses corresponding to the given intents .
Learning Intent Representation
To achieve more interpretability , we choose a discrete latent variable that obeys categorical distribution with nine categories , each corresponding to one empathetic intent . Directly maximizing Eq.1 would cause two serious problems : the relation between the latent variable and intent is intractable ; the vanishing latent problem results in insufficient information provided by the latent variable during generation . ( Bowman et al . , 2016 ; Zhao et al . , 2017 ; Gu et al . , 2019 ) . To solve the above issues , we separately train a recognition network q r ( z|X ) to encourage intent variable z to capture context - independent semantics , which is essential for z to be interpretable ( Zhao et al . , 2018 ) . The task of the recognition network is to provide the accurate intent label of the response , which corresponds to an intent embedding . Then , by maximizing likelihood p ( X|C , z ) , the embedding captures corresponding intent representation automatically . The recognition network q r ( z|X ) does not need additional training . We utilize the BERT intent classifier mentioned above , which achieves 87.75 % accuracy in intent classification . In addition , as the sample operation easily brings noise for the intent representation learning when sampling a wrong intent , we use argmax operation to avoid the noise , the response reconstruction loss is :
L 1 = â log p ( X|C , z k ) ( 2 ) z k = arg max z k q r ( z k |X ) ( 3 ) k â { 0 , 1 , 2 , â¢ â¢ â¢ , 8 }
, each integer corresponds to a specific empathetic intent as in Figure 2 .
Intent Predictor and Emotion Classifier
The intent predictor is based on the prior network p i ( z|C ) , which predicts the distribution of response intent by the given context . During inference , we sample potential intents from this distribution in order to generate human - like empathetic responses .
Specifically , the context is encoded with gated recurrent units ( GRU ) ( Chung et al . , 2014 ) :
h t = GRU ( h tâ1 , E ( c t ) ) , ( 4 )
where h t is the hidden state of GRU encoder , E ( c t ) denotes the word embedding of the t - th word in context , we use h m as context embedding , then the prior network is :
p i ( z|C ) = Softmax ( FFN z ( h m ) ) , ( 5 )
where FFN represents Feed - Forward Network with two layers . The prior intent distribution is supervised by recognition distribution with KLdivergence in Eq.1 :
L 2 = KL ( q r ( z|X ) ||p i ( z|C ) ) = K k=1 q r ( z k |X ) log q r ( z k |X ) p i ( z k |C ) . ( 6 )
Since the context emotion is proved to be beneficial to empathetic dialogue generation ( Rashkin et al . , 2019 ; Lin et al . , 2019 ; Majumder et al . , 2020 ) , we also employ an emotion classifier to classify the emotion of context :
P = Softmax ( FFN e ( h m ) ) ]
p e i = P [ i ] ( 7 )
Given the ground truth emotion label e t , the emotion classifier is trained with cross - entropy loss :
L 3 = â log p e t . ( 8 )
Response Generator
As for the response generation p ( X|C , z ) , we consider implicit intent embedding for the high - level abstraction of an intent . In addition , we also introduce intent keywords for explicitly utilizing intent knowledge during the generation process . Implicit . To generate response with an empathetic intent , the most intuitive approach is taking the intent embedding as additional input to decoder during the generation process . We also consider emotion embedding as traditional empathetic dialogue models :
s t = GRU ( s tâ1 , [ E ( x tâ1 ) ; v ( z ) ; v ( e ) ; c att ] ) , ( 9 )
where s t is the state of GRU decoder , c att denotes the context attention value which contains key information of context ( Bahdanau et al . , 2015 ) . v ( z ) is intent embedding and v ( e ) is emotion embedding , both will not change during the generation process . However , this may sacrifice grammatical correctness ( Zhou et al . , 2018 ; Ghosh et al . , 2017 ) . Therefore we add a gate operation to capture intent and emotion dynamically :
Input = FFN i ( [ E ( x t ) ; c att ; s t ] ) , Gate = Sigmoid ( Input ) , v ( z ) = Gate â v ( z ) , ( 10 )
where â represents element - wise product . Each time step , the intent representation is used appropriately according to current word , state , and context value . The gate operation for emotion is the same as above .
Explicit . The empathetic expression is quite distinct over vocabularies , e.g. , ' know ' , ' understand ' , ' agree ' , are indicative of the empathetic intent Agreeing . Therefore , we employ the copy mechanism to explicitly utilize intent keywords for intent conditional generation . See in Appendix B for more details about intent keywords .
Î± t = Sigmoid ( v â¤ s s t ) , p ( x t = w g ) = Softmax ( W g s t ) , p ( x t = w i ) = Softmax ( W i s t ) , p ( x t ) = ( 1 â Î± t ) â¢ p ( w g ) + Î± t â¢ p ( w i ) , ( 11 )
where { s t , v s } â R dÃ1 , { W g , W i } â R V Ãd , d is hidden size and V denotes the vocabulary size . The copy rate Î± t is used to balance the choice between intent keywords and generic words , it is trained with binary cross entropy loss :
L 4 = n t=1 q t â¢ log Î± t + ( 1 â q t ) â¢ log ( 1 â Î± t ) , ( 12
n is the amount of words in response , q t â { 0 , 1 } indicates that whether x t is a intent keyword .
Loss Function
To summarize , the total loss is :
L = Î» 1 L 1 + Î» 2 L 2 + Î» 3 L 3 + Î» 4 L 4 , ( 13 )
In order to join all losses with weighting method , we add 4 hyperparameters in total loss , Î» i , where each Î» i is corresponding to L i . L 1 , L 2 , L 3 , L 4 denote the losses of response reconstruction , intent prediction , emotion classification and copy rate prediction respectively .
Experiments
Dataset
We evaluate our method and compare with others on EmpatheticDialogues 2 ( Rashkin et al . , 2019 ) which contains 25k open domain dialogues . Follow the same setting as the authors of this dataset , the proportion of train / validation / test data is 8 : 1 : 1 . Each dialogue consists of at least two utterances between a speaker and listener . There are 32 emotion situations in total , which are uniformly distributed .
Baselines
We compare our model with the three latest empathetic conversational models :
â¢ Multitask Transformer ( Multi - TRS ) . A transformer model trained by the response generation task and the context emotion classification task ( Rashkin et al . , 2019 ) .
â¢ Mixture of Empathetic Listeners ( MoEL ) .
An enhanced transformer model with 32 emotion - specific decoders to respond appropriately for each emotion ( Lin et al . , 2019 ) .
â¢ MIMicking Emotions for Empathetic Response Generation ( MIME ) . The state - ofthe - art empathetic dialogue model allows the generator to mimic the context emotion to a varying degree based on its positivity , negativity , and content . Furthermore , they introduce stochasticity into the emotion mixture and achieves one - to - many generation ( Majumder et al . , 2020 ) .
Evaluation
Automatic Metrics
â¢ BLEU . We choose BLEU ( Papineni et al . , 2002 ) for relevance evaluation which measures the n - gram overlaps with reference and compute BLEU scores for n â¤ 4 using smoothing techniques ( Chen and Cherry , 2014 ) . Since the state - of - art model MIME and ours are both one - to - many generators , we calculate BLEU recall and BLEU precision ( Zhao et al . , 2017 ; Gu et al . , 2019 ) . For each test case , we sample 5 responses from latent space and use greedy search for MIME and EmpHi , use beam search for MoEL and Multitask - Transformer .
â¢ Distinct . Distinct ( Li et al . , 2016 ) is a widely used metric for diversity evaluation . Specifically , we compute the number of distinct unigrams ( Distinct-1 ) and bigrams ( Distinct-2 ) , then scale them by the total number of unigrams and bigrams .
Human Ratings
First , we randomly sample 100 dialogues and their corresponding generations from the three baseline models and EmpHi . Then , we invite five volunteers with master degrees to do the human evaluation .
The annotators mark each response from 1 to 5 for empathy , relevance , and fluency .
To clarify the marking criteria , we provide an explanation for each metric :
â¢ Empathy . Whether the response shows that the listener understands and shares the speaker 's feeling . Can the listener imagine what it would be like in the speaker 's situation ?
â¢ Relevance . Whether the response is relevant to the context .
â¢ Fluency . Whether the response is easy to read and grammatically correct .
Human A / B Test
Following ( Lin et al . , 2019 ; Majumder et al . , 2020 ) , we construct this evaluation task to directly compare our model with each baseline . We randomly sample 100 dialogue responses from EmpHi vs { Multitask - Trans , MoEL , MIME } . Given randomly ordered responses from above models , four annotators select the better response , or tie if they think the two responses have the same quality . The average score of four results is calculated , and shown in Table 6 .
Implement Detail
For MIME 3 ( Majumder et al . , 2020 ) and MoEL 4 ( Lin et al . , 2019 ) , we reproduce their results using their open - source codes and their default hyperparameters . According to the log - likelihood in the validation dataset for Multitask - Transformer , we use grid search for the best head number , layer number , and feed - forward neural network size . The best set is 2 , 10 , and 256 , respectively . EmpHi uses a two - layer Bi - GRU as the encoder and a two - layer GRU as the decoder , Î» is set as [ 1 , 0.5 , 0.5 , 1 ] respectively . All the feed - forward neural networks in EmpHi have two layers , 300 hidden units and ReLU activations . For the sake of fairness , we use pretrained Glove vectors ( Pennington et al . , 2014 ) with 300 dimensions as the word embedding for all models , the batch size is 16 , and the learning rate is set to 1e â4 .
6 Results and Discussions
Results Analysis
In this section , we mainly testify :
â¢ human - like empathetic intent boost EmpHi 's performance in terms of empathy , relevance , and diversity .
â¢ EmpHi successfully captures the empathetic intent distribution of humans .
Human Evaluation
As shown in metric in empathetic dialogue generation . EmpHi outperforms the previous SOTA on empathy by 9.43 % , which directly indicates that human - like empathetic intents are beneficial to the empathy ability of the dialogue model . Last but not least , a decent fluency score proves that our generated response could be understood by humans easily , where our model has an improvement of 9.87 % from MoEL . In addition , the human A / B test results in Table 2 also confirm that the responses from our model are preferable to baselines . Overall , EmpHi successfully generates empathetic , relevant , and fluent responses .
Automatic Evaluation
As seen in Table 1 , the automatic evaluation is consistent with human evaluation . The BLEU recall and F1 score are improved by 14.2 % and 8.34 % , respectively . However , we only have a slight im- provement on BLEU precision , which is similar to ( Zhao et al . , 2017 ; Gu et al . , 2019 ) because the precision is penalized when the model generates diverse responses . Also , the distinct value of unigrams and bigrams are 32.04 % and 19.32 % higher than the previous SOTA , respectively . As shown in Figure 4 and Figure 6 , the empathy intents of EmpHi 's responses are more diverse than existing models , so the distinct scores improve significantly .
Our method enhances the relevance and diversity simultaneously , which proves the effectiveness of human - like intent in empathetic response generation .
Empathetic Intent Distribution
We apply the same approach in Section 3 and quantify the empathetic intent distribution of EmpHi 's responses to prove that EmpHi accurately captures humans ' empathetic intent distribution . Comparing Figure 4 and Figure 6 , the difference between them illustrates that our model successfully reduces the bias of empathetic expression . The KL - divergence of intent distributions between models and humans are 0.025 for EmpHi , 1.949 for MIME , 1.545 for MoEL , and 4.570 for Multitask - Transformer ( See in Appendix A ) .
Ablation Study
We evaluate each component of EmpHi using BLEU and ACC , where ACC indicates the accuracy of predicted empathethetic intent of generated response . Since each conversation could have multiple empathetic responses , the ACC of 26.8 % is pretty ideal . As seen in Table 3 , there is a dra-
Emotion
Nostalgic
Context
When my wife and i started dating , our second date ended up lasting like 12 hours .
Reference
That is a pretty long date ! Where did you guys go ? Multitask - Trans That is so sad , I 'm sorry to hear that .
MoEL
That is great ! I am sure she will be fine .
MIME
That is a good thing to do . I am sure you will do great .
EmpHi
That 's awesome ! How long have you been together ? Emotion Angry Turn-1 I just moved to this neighborhood and some dumb criminals shot one of my neighbors and ran into the woods ! Turn-2 That 's not good . Do you own a gun ? Turn-3 I do ! I want to be able to protect my son .
Reference
That is always number one goal . matic drop in the performance of EmpHi without any intent information ( both implicit embedding and explicit keywords ) . Therefore , this proves the effectiveness of empathetic intents and the intent representation learning approach . As for implicit gate control , it improves both response quality and intent accuracy since it helps EmpHi dynamically capture intent information during generation . Same conclusion has been made in ( Zhou et al . , 2018 ) .
Multitask - Trans
The copy mechanism provides EmpHi the ability to explicitly use intent keywords and thus contributes to the intent accuracy .
Case Study
Intent - level diverse generation . Through sampling intents in the discrete latent space , EmpHi generates different responses with empathetic intents . As in Figure 7 , the speaker shows an exciting emotion for getting a better job . EmpHi
generates empathetic yet contextually relevant responses as humans . Besides , EmpHi predicts the potential intent distribution and shows successful conditional generation based on the corresponding intents , which improves the interpretability and controllability of empathetic response generation . See Appendix C for error analysis .
Compare with existing models . For the first instance in Table 4 , even though baseline models show naive empathy in their response , it is hard for the speaker to feel empathy because the response is not relevant to the topic . In contrast , EmpHi shows its understanding of the speaker 's feelings and asks a relevant question to explore the speaker 's experience . For second case , all baselines express contextually irrelevant empathy , while EmpHi truly understands the dialogue history and put itself into speaker 's situation , then further reply : " Maybe you should go to the police " with the Suggesting intent .
Conclusion
Overall , we reveal the severe bias of empathetic expression between existing dialogue models and humans . To address this issue , this paper proposes EmpHi to generate empathetic responses with human - like empathetic intents . As a result , both automatic and human evaluation prove that EmpHi has a huge improvement on empathetic conversation . According to the anlaysis and case studies , EmpHi successfully learns the emapthetic intent distribution of human and shows high interpretability and controllability during the generation process . We will try large pretrained language models with empathetic intent in our future work .
Ethical Statement
Since this paper involves subjects related to human conversation , we have ensured that all the experiments will cause no harm to humans . The dataset EmpatheticDialogues is collected by ( Rashkin et al . , 2019 ) , all the participants join the data collection voluntarily . Also , the dataset provider filters all personal information and obscene languages . Therefore , we believe that the dataset Empathetic - Dialogues used in our experiments are harmless to users , and the model trained on this dataset is not dangerous to humans .
A Empathetic Expression Gap
For more comprehensive recognization of the severe emathy expression bias between existing empathetic dialogue models and humans , we further quantify the bias of Multitask - Transformer ( Rashkin et al . , 2019 ) in Figure 8 and MoEL ( Lin et al . , 2019 ) in Figure 9 , the intent index is consistent with Figure 2 . The results are similar with MIME ( Majumder et al . , 2020 ) , we can see the large intent distribution bias and the monotony of empathetic expression of existing models .
B Intent Keywords Collection
The keywords are retrieved from the training set of Empathetic Intents dataset ( Welivita and Pu , 2020 ) by using TF - IDF method . Empathetic Intents has a training set of 5490 responses , where each intent group has 610 responses . Based on the labeled intent for each response in the training set , we concatenate all the responses which are in the same group and remove all the stop words . Finally , we apply TF - IDF to obtain the top k keywords for each intent group , we set k to 30 in our experiments . See Table 5 for top ten keywords for each intent .
C Error Analysis
Although EmpHi achieves huge improvement in terms of empathy , relevance , and diversity in empathetic dialogue generation , there is still some flaws . At first , the generation task of EmpHi is far difficult than existing models , because it needs to generate response condition on both context and the predicted intent , while other models generate response only condition on the context , therefor the exposure bias of EmpHi is more severe . See in Table 6 , although the predicted intent of EmpHi is the same as reference and its corresponding response is great , EmpHi also gives high probability for Questioning intent and the corresponding response is not very contextually relevant , EmpHi knows it is suitable for asking more details to show its caring , but it does not know how to ask under this context , thus EmpHi needs better understanding for context information . We believe this issue could be mitigated when using more dialogue data for pretraining .
Acknowledgements
We thank the anonymous reviewers for their insightful comments and suggestions . This research was supported in part by the National Key Research and Development Program of China ( No . 2020YFB1708200 ) , the Guangdong Basic and Applied Basic Research Foundation ( No . 2019A1515011387 ) and the Shenzhen Key Laboratory of Marine IntelliSense and Computation under Contract ZDSYS20200811142605016 .
Analyzing Encoded Concepts in Transformer Language Models
We propose a novel framework ConceptX , to analyze how latent concepts are encoded in representations learned within pre - trained language models . It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human - defined concepts . Our analysis on seven transformer language models reveal interesting insights : i ) the latent space within the learned representations overlap with different linguistic concepts to a varying degree , ii ) the lower layers in the model are dominated by lexical concepts ( e.g. , affixation ) , whereas the core - linguistic concepts ( e.g. , morphological or syntactic relations ) are better represented in the middle and higher layers , iii ) some encoded concepts are multi - faceted and can not be adequately explained using the existing human - defined concepts . 1
Introduction
Contextualized word representations learned in deep neural network models ( DDNs ) capture rich concepts making them ubiquitous for transfer learning towards downstream NLP . Despite their revolution , the blackbox nature of the deep NLP models is a major bottle - neck for their large scale adaptability . Understanding the inner dynamics of these models is important to ensure fairness , robustness , reliability and control .
A plethora of research has been carried out to probe DNNs for the linguistic knowledge ( e.g. morphology , syntactic and semantic roles ) captured within the learned representations . A commonly used framework to gauge how well linguistic information can be extracted from these models is the Probing Framework ( Hupkes et al . , 2018 ) , where they train an auxiliary classifier using representations as features to predict the property of interest . The performance of the classifier reflects the amount of knowledge learned within representations . To this end , the researchers have analyzed what knowledge is learned within the representations through relevant extrinsic phenomenon varying from word morphology ( Vylomova et al . , 2016 ; Belinkov et al . , 2017a ) to high level concepts such as syntactic structure ( Blevins et al . , 2018 ; Marvin and Linzen , 2018 ) and semantics ( Qian et al . , 2016 ; Reif et al . , 2019 ; Belinkov et al . , 2017b ) or more generic properties ( Adi et al . , 2016 ; Rogers et al . , 2020 ) .
In this work , we approach the representation analysis from a different angle and present a novel framework ConceptX. In contrast to relying on the prediction capacity of the representations , we analyze the latent concepts learned within these representations and how knowledge is structured , using an unsupervised method . More specifically , we question : i ) do the representations encode knowledge inline with linguistic properties such as word morphology and semantics ? ii ) which properties dominate the overall structure in these representations ? iii ) does the model learn any novel concepts beyond linguistic properties ? Answers to these questions reveal how deep neural network models structure language information to learn a task .
Our inspiration to use the term concept comes from " concept based explanation " in computer vision ( Kim et al . , 2018 ; Ghorbani et al . , 2019 ; Chen et al . , 2020 ) . Stock ( 2010 ) defined a concept as " a class containing certain objects as elements , where the objects have certain properties " . We define an encoded concept as a cluster of contextaware latent representations of words , where the representations are encoder layer outputs .
Our framework clusters contextualized representations using agglomerative hierarchical clustering ( Gowda and Krishna , 1978 ) . The resulting clusters represent encoded concepts , captured within the learned representations ( Please see Figure 1 for illustration ) . We then use a novel align - Figure 1 : ConceptX : i ) Extract representations from trained model , ii ) Cluster the representations to obtain encoded concepts , iii ) Align the concepts to human - defined concepts ment function that measures the amount of overlap between encoded concepts and a range of predefined categories ( that we call as human - defined concepts in this paper ) . We experimented with affixes , casing , morphological , syntactic , semantic , WordNet ( Miller , 1995 ) , and psycholinguistic concepts ( LIWC Pennebaker et al . ( 2001 ) ) . The use of such a diverse set of human - defined concepts enables us to cover various abstractions of language . In Figure 3 we present a few examples of human - defined concepts that were aligned with the encoded concepts .
We carry out our study on seven pre - trained transformer models such as BERT ( Devlin et al . , 2019 ) and XLM - RoBERTa ( Conneau et al . , 2020 ) , with varying optimization functions , architectural details and training data . Some notable findings emerging from our analysis are as follows :
â¢ Shallow concepts such as lexical ngrams or suffixes are predominantly captured in the lower layers of the network .
â¢ WordNet and psycholinguistic - based concepts ( LIWC ) are also learned in the lower layers .
â¢ Middle and higher layers encode concepts that capture core linguistic properties such as morphology , semantics and syntax .
â¢ Roughly 50 % of the encoded concepts adhere to our suite of human - defined linguistic concepts .
â¢ The models learn novel concepts that are multi - faceted and can not be adequately explained using the existing human - defined concepts .
Our contributions in this paper are as follow : i ) We present ConceptX , a framework that interprets encoded concepts in the learned representation by measuring their alignment to the human - defined concepts . ii ) We provide a qualitative and quantitative evidence of how knowledge is structured within deep NLP models with respect to a large suite of human - defined concepts .
Related Work
Most of the work done on interpretability in deep NLP addresses two questions in particular : ( i ) what linguistic ( and non - linguistic ) knowledge is learned within contextualized representations , Concept Analysis and ( ii ) how this information is utilized in the decision making process , Attribution Analysis . The former thrives on post - hoc decomposability , where we analyze representations to uncover linguistic phenomenon that are captured as the network is trained towards any NLP task ( Adi et al . , 2016 ; Conneau et al . , 2018 ; Liu et al . , 2019a ; Tenney et al . , 2019 ; and the latter characterize the role of model components and input features towards a specific prediction ( Linzen et al . , 2016 ; Gulordava et al . , 2018 ; Marvin and Linzen , 2018 ) . Our work falls into the former category . Previous studies have explored visualization methods to analyze the learned representations ( Karpathy et al . , 2015 ; KÃ¡dÃ¡r et al . , 2017 ) , attention heads ( Clark et al . , 2019 ; Vig , 2019 ) , language compositionality ( Li et al . , 2016 ) etc . A more commonly used framework analyzes representations by correlating parts of the neural network with linguistic properties , by training a classifier to predict a feature of interest ( Adi et al . , 2016 ; Belinkov et al . , 2017a ; Conneau et al . , 2018 ) . Several researchers used probing classifiers for investigating the contextualized representations learned from a variety of neural language models on a variety of character- , word- ( Liu et al . , 2019a ) or sub - sentence level ( Tenney et al . , 2019 ) linguistic tasks . Rather than analyzing the representations as a whole , several researchers also explored identifying salient neurons within the model that capture different properties ( Dalvi et al . , 2019a ; Suau et al . , 2020 ; Mu and Andreas , 2020 ) or are salient for the model irrespective of the property ( Bau et al . , 2019 ; Wu et al . , 2020 ) .
Our work is inline with ( Michael et al . , 2020 ; Dalvi et al . , 2022 ) , who analyzed latent concepts learned in pre - trained models . Michael et al . ( 2020 ) used a binary classification task to induce latent concepts relevant to a task and showed the presence of linguistically motivated and novel concepts in the representation . However , different from them , we analyze representations in an unsupervised fashion . Dalvi et al . ( 2022 ) used human - in - the - loop to analyze latent spaces in BERT . Our framework uses human - defined concepts to automatically generate explanations for the latent concepts . This enabled us to scale our study to many transformer models .
In a similar work , Mamou et al . ( 2020 ) applied manifold analysis technique to understand the amount of information stored about object categories per unit . Our approach does away from the methodological limitations of probing framework such as complexity of the probes , effect of randomness etc ( Belinkov , 2021 ) . However , it is important to mention that the two frameworks are orthogonal and complement each other .
Methodology
A vector representation in the neural network model is composed of feature attributes of the input words . We group the encoded vector representations using a clustering approach discussed below . The underlying clusters , that we term as the encoded concepts , are then matched with the human - defined concepts using an alignment function . Formally , consider a Neural Network ( NN ) model M with L encoder layers { l 1 , l 2 , ... l l , ... , l L } , with H hidden nodes per layer . An input sentence consisting of M words w 1 , w 2 , ... w i , ... , w M is fed into a NN . For each input word i , we compute the node output ( after applying the activation func - tions ) y l h ( w i ) of every hidden node h â { 1 , ... , H } in each layer l , where â â y l ( w i ) is the vector representation composing the outputs of all hidden nodes in layer l for w i . Our goal is to cluster representations â â y l , from a large training data to obtain encoded concepts . We then align these with various human - defined concepts to obtain an explanation of them to build an understanding of how these concepts are represented across the network .
Clustering
We use agglomerative hierarchical clustering ( Gowda and Krishna , 1978 ) , which we found to be effective for this task . It assigns each word to a separate cluster and then iteratively combines them based on Ward 's minimum variance criterion that minimizes intra - cluster variance . Distance between two representations is calculated with the squared Euclidean distance . The algorithm terminates when the required K clusters ( aka encoded concepts ) are formed , where K is a hyperparameter . Each encoded concept represents a latent relationship between the words present in the cluster . Appendix C presents the algorithm .
Alignment
Now we define the alignment function between the encoded and human - defined concepts . Consider a human - defined concept as z , where a function z ( w ) = z denotes that z is the human - defined concept of word w. For example , parts - of - speech is a human - defined concept and each tag such as noun , verb etc . represents a class / label within the concept , e.g. z ( sea ) = noun . Similarly , suffix is a human - defined concept with various suffixes representing a class , e.g. z ( bigger ) = er . A reverse function of z is a one - to - many function that outputs a set of unique words with the given humandefined concept , i.e. , z â1 ( z ) = { w 1 , w 2 , . . . , w J } , like z â1 ( noun ) = { sea , tree , . . . } , where J is the total number of words with the human - defined concept of z. Following this notation , an encoded concept is indicated as c , where c ( w ) = c is a function of applying encoded concept on w , and its reverse function outputs a set of unique words with the encoded concept of c , i.e. , c â1 ( c ) = { w 1 , w 2 , . . . , w I } , where I is the set size .
To align the encoded concepts with the humandefined concepts , we auto - annotate the input data that we used to get the clusters , with the humandefined concepts . We call our encoded concept ( c ) to be Î¸ - aligned ( Î Î¸ ) with a human - defined concept ( z ) as follows :
Î Î¸ ( z , c ) = 1 , if w â² âz â1 wâc â1 Î´ ( w , w â² ) J â¥ Î¸ 0 , otherwise ,
where Kronecker function Î´ ( w , w â² ) is defined as
Î´ ( w , w â² ) = 1 , if w = w â² 0 , otherwise
We compute c and Î Î¸ ( z , c ) for the encoder output from each layer l of a neural network . To compute a network - wise alignment , we simply average Î¸agreement over layers .
4 Experimental Setup
Dataset
We used a subset of WMT News 2018 2 ( 359 M tokens ) dataset . We randomly selected 250k sentences from the dataset ( â5 M tokens ) to train our clustering model . We discarded words with a frequency of less than 10 and selected maximum 10 occurrences of a word type . 3 The final dataset consists of 25k word types with 10 contexts per word .
Pre - trained Models
We carried out our analysis on various 12 - layered transformer models such as BERT - cased ( BERTc , Devlin et al . , 2019 ) , BERT - uncased ( BERT - uc ) , RoBERTa ( Liu et al . , 2019b ) , XLNet ( Yang et al . , 2019 ) and ALBERT ( Lan et al . , 2019 ) . We also analyzed multilingual models such as multilingualbert - cased ( mBERT ) and XLM - RoBERTa ( XLM - R , Conneau et al . , 2020 ) where the embedding space is shared across many languages . This choice of models is motivated from interesting differences in their architectural designs , training data settings ( cased vs. un - cased ) and multilinguality .
Clustering and Alignment
We extract contextualized representation of words by performing a forward pass over the network using the NeuroX toolkit ( Dalvi et al . , 2019b ) . We cluster representations in every layer into K groups .
To find an optimum value of K , we experimented with the ELbow ( Thorndike , 1953 ) and Silhouette ( Rousseeuw , 1987 ) methods . However , we did not observe reliable results ( see Appendix C ) . Therefore , we empirically selected K = 1000 based on finding a decent balance between many small clusters ( over - clustering ) and a few large clusters ( under - clustering ) . We found that our results are not sensitive to this parameter and generalize for different cluster settings ( See Section 5.4 ) . For the alignment between encoded and human - defined concepts , we use Î¸ = 90 % i.e. , we consider an encoded concept and a human - defined concept to be aligned , if they have at least 90 % match .
Human - defined concepts
We experiment with the various Human - defined concepts , which we categorize into four groups :
â¢ Lexical Concepts : Ngrams , Affixes , Casing , First and the Last Word ( in a sentence ) â¢ Morphology and Semantics : POS tags ( Marcus et al . , 1993 ) and SEM tags ( Abzianidze et al . , 2017 ) â¢ Syntactic : Chunking tags ( Tjong Kim Sang and Buchholz , 2000 ) and CCG super - tags ( Hockenmaier , 2006 ) â¢ Linguistic Ontologies : WordNet ( Miller , 1995 ) and LIWC ( Pennebaker et al . , 2001 ) At various places in this paper , we also refer to Morphology , Semantics and Syntactic concepts as core - linguistic concepts . We trained BERT - based classifiers using gold - annotated training data and standard splits for each core - linguistic concepts and auto - labelled the selected news dataset using these . 4
Analysis
In this section , we analyze the encoded concepts by aligning them with the human - defined concepts .
Overall Alignment
First we present to what extent the encoded concepts in the entire network align with the humandefined concepts . We compute the overall score as the percentage of the aligned encoded concepts to the human - defined concepts across layers using the function described in Section 3.2 . We BERT - c BERT - uc mBERT XLM - R RoBERTa ALBERT XLNet Figure 2 presents the results .
Lexical Concepts Pre - trained models encode varying amount of lexical concepts such as casing , ngrams and suffixes . We found between 7 - 11 % encoded concepts that align with the casing concept ( title case or upper case ) . We observed that most of these encoded concepts consist of named entities , which were grouped together based on semantics .
Comparing suffixes and ngrams While affixes often have linguistic connotation ( e.g. , the prefix anti negates the meaning of the stem and the suffix ies is used for pluralization ) , the ngram units that become part of the vocabulary as an artifact of statistical segmentation ( e.g. , using BPE ( Sennrich et al . , 2016 ) or Word - piece ( Schuster and Nakajima , 2012 ) ) often lack any linguistic meaning . However , models learn to encode such information . We found a match ranging from 1 % ( BERT - cased ) up to 25 % ( XLM - R ) when comparing encoded concepts with the suffix concept . A similar pattern is observed in the case of the ngram concept ( which is a superset of the suffix concept ) where a staggering 48 % matches were found . Figure 6a shows an ngram cluster found in layer 2 of Morphology and Semantics We found that the encoded concepts based on word morphology ( POS ) consistently showed a higher match across all models in comparison to the other abstract concepts , aligning a quarter of the encoded concepts in the case of mBERT . The alignment with semantic concepts is relatively lower , with at most 16 % match across models . This reflects that while the models learn both linguistic properties , morphological ontology is relatively preferred compared to the semantic hierarchy .
Syntactic These concepts capture grammatical orientation of a word , for example Chunking : B - NP is a syntactic concept describing words in the beginning of a noun phrase . CCG : PP / NP is a concept in CCG super tagging , describing words that takes a noun phrase on the right and outputs a preposition phrase for example " [ in [ the US ] ] " . We found relatively fewer matches , a maximum of 7 % and 14 % matching encoded concepts for Chunking and CCG concepts respectively . The low matches for syntactic concepts suggest that the models do not encode the same syntactic hierarchy suggested by these human - defined syntactic tasks .
Linguistic Ontologies Comparing the encoded concepts with static linguistic ontologies , we found WordNet concepts to be the second most aligned concept ( 11 - 21 % ) with the human - defined concepts . LIWC also shows a relatively higher alignment compared to the other human - defined concepts in a few models ( e.g. , BERT - c ) . However , this observation is not consistent across models and we found a range between 5 - 16 % matches . These results present an interesting case where several models prefer the distinction of lexical ontology over abstract linguistic concepts such as morphology . Figure 3 shows examples of encoded concepts aligned with WordNet and LIWC . We see that these concepts are built based on a semantic relationship e.g. , the clusters in Figure 3b , 3c and 3d group words based on religious , facial anatomy , and specific motion - related vocabulary respectively .
Comparing Models
The results of multilingual models ( mBERT , XLM - R ) are intriguing given that their encoded concepts are dominated by ngrambased concepts and POS concepts , and their relatively lesser alignment with the linguistic ontologies . On the contrary , several monolingual models ( BERT - c , ALBERT ) showed a better match with linguistic ontologies specially WordNet . The higher number of matches to the ngram ( and suffix ) concepts in the multilingual models is due to the difference in subword segmentation . The subword models in XLM - R and mBERT are optimized for multiple languages , resulting in a vocabulary consisting of a large number of small ngram units . This causes the multilingual models to aggressively segment the input sequence , compared to the monolingual models 7 and resulted in highly dominated ngram - based encoded concepts , especially in the lower layers . This may also explain the relatively lower match that multilingual models exhibit to the linguistic ontologies . We discuss this further in the context of layer - wise analysis in Section 5.2 .
Comparing BERT cased vs. uncased , interestingly BERT - uc consistently showed higher matches for the core - linguistic concepts ( See Figure 2 ) . We speculate that in the absence of casing information , BERT - uc is forced to learn more linguistic concepts , whereas BERT - c leverages the explicit casing information to capture more semantically motivated concepts based on linguistic ontologies .
The higher matches in multilingual models in comparison to the monolingual models , and BERTuncased in comparison to BERT - cased suggest that the training complexity is one factor that plays a role in a model 's ability to learn linguistic nuances . For example , multilingual models need to optimize many languages , which is a harder task compared to learning one language . Similarly , the absence of capitalization in training data makes the learning task relatively harder for BERT - uc compared to BERT - c models , thus resulting in higher matches for BERT - uc . We speculate that the harder the training task , the more language nuances are learned by a model . made a similar observation , where they showed that the linguistic knowledge learned within the encoder - decoder representations in NMT models correlates with complexity of a language - pair involved in the task .
Layer - wise Alignment
Now we study the alignment of human - defined concepts across layers to understand how concepts Figure 4 : Layer - wise concept alignment . Y - axis is the normalized number of aligned concepts . The number within brackets of each human - defined concept , e.g. Casing ( 166 ) , shows the maximum layer - wise match evolve in the network . Figure 4 shows results for selected models . 8 The y - axis is the normalized number of aligned concepts across layers .
Overall Trend
We observed mostly consistent patterns across models except for ALBERT , which we will discuss later in this section . We found that the shallow concepts ( such as ngram and suffixes ) and the linguistic ontologies ( LIWC and WORD - NET ) are better represented in the initial layers and exhibit a downward trend in the higher layers of the network . On the contrary the core linguistic concepts ( POS , Chunking , etc . ) are better repre-8 See Figure 10 in the Appendix for complete results . sented in the higher layers ( layer 8 - 10 ) . The last layers do not show any consistently dominating human - defined concepts considered in this work . We can generalize on these trends and hypothesize on how encoded concepts evolve in the network : the initial layers of the pretrained models , group words based on their lexical and semantic similarities where the former is an artifact of subword segmentation . With the inclusion of context and abstraction in the higher layers , these groups evolve into linguistic manifolds . The encoded concepts in the last layers are influenced by the objective function and learn concepts relevant to the task . also made similar observation when analyzing linguistic concepts in pre - trained models that are fine - tuned towards different GLUE tasks .
Concept - wise Trend
In the following , we discuss different concepts in detail . As we mentioned earlier , the high presence of ngram and suffix concepts in the lower layers is due to subword segmentation . At the higher layers , the models start encoding abstract concepts , therefore get better alignment with the core linguistic concepts . Casing shows an exception to other lexical concepts and has similar trend to POS and SEM . Upon investigating we observed that the words appearing in these clusters have a hybrid connotation . For example , more than 98 % of the encoded concepts that match with Casing are named entities , which explains the trend . The syntactic concepts observe peak in the higher - middle layers and a downward trend towards the end . These findings resonate with the earlier work on interpreting neural network representations for BERT . For example Liu et al . ( 2019a ) also showed that probes trained with layers 7 - 8 give the highest accuracy when trained towards predicting the tasks of Chunking and CCG tagging . Although here , we are targeting a slightly different question i.e. how the latent concepts are encoded within the representations and how they evolve from input to output layers of the network .
We observed a downward trend in linguistic ontologies ( WordNet , LIWC ) as we go from lower layers to higher layers as opposed to the core linguistic concepts ( POS , CCG , etc . ) . This is because of the context independent nature of these concepts as opposed to the core - linguistic concepts which are annotated based on the context . The embedding layer is non - contextualized , thus shows a high match with linguistic ontologies . With the availability of context in contextualized layers , the encoded concepts evolve into context - aware groups , resulting in higher matches with core - linguistic concepts .
Comparing Models While the overall trend is consistent among BERT - uc , mBERT and XLNet ( and other studied models -Figure 10 in Appendix ) , the models somewhat differ in the last layers : see the large drop in core - linguistic concepts such as POS and Chunking for XLNet and mBERT in comparison to BERT . This suggests that BERT retains much of the core - linguistic information at the last layers . observed a similar pattern in their study , where they showed BERT to retain linguistic information deeper in the model as opposed to XLNet where it was more localized and predominantly preserved earlier in the network .
While the overall layer - wise trends of multilingual models look similar to some monolingual models ( mBERT vs. XLNet in Fig 4b , c ) , the former 's absolute layer - wise matches ( numbers inside the brackets in Figure 4 e.g. Casing ( 166 ) ) are generally substantially higher than the monolingual counterparts . For example , the POS and SEM matches of mBERT are 38.9 % and 30 % respectively which are 18 % and 15 % higher than BERT - uc . On the contrary , the number of matches with linguistic ontologies is often lower for multilingual models ( mBERT LIWC alignment of 65 vs. BERT - uc alignment of 186 ) . We hypothesize that the variety of training languages in terms of their morphological and syntactic structure has caused the multilingual models to learn more core - linguistic concepts in order to optimize the training task . Although , the knowledge captured within linguistic ontologies is essential , it may not be as critical to the training of the model as the linguistic concepts .
ALBERT showed a very different trend from the other models . Note that ALBERT shares parameters across layers while the other models have separate parameters for every layer . This explains the ALBERT results where we see relatively less variation across layers . More interestingly , the encoded concepts in the last layers of ALBERT showed presence of all human - defined concepts considered here ( see the relatively smaller drop of ALBERT alignment curves in Figure 4 ) .
Unaligned Concepts
In Table 1 we observed that at least 27.6 % ( in XLM - R ) and up to 56.4 % ( in XLNet ) encoded concepts did not align with the human - defined concepts . What concepts do these unaligned clusters contain ? In an effort to answer this question , we analyzed these clusters and observed that many of them were compositional concepts that involves more than one fine - grained categories of the human defined concepts . Figure 5a shows an example of the unaligned concept which partly aligns with a semantic category ( SEM : geopolitical entity ) and a morphological category ( POS : adjective ) . Similarly , Figure 5b is a verbs related to cognitive processes and Figure 5c shows an unaligned cluster that is composed of different verb forms ( past , present and gerunds ) . The alignment with multiple human- defined concepts can be used to generate explanations for these unaligned concepts . For example , Figure 5a can be aligned as a mix of geopolitical entities and adjectives . We also quantitatively verified the number of unaligned encoded concepts that can be explained using composition of different concepts ( See Appendix E : Table 9 ) and found that a majority of the clusters can be explained using a combination of three pre - defined concepts .. Moreover , note that encoded concepts are often multifacet i.e. , they represent more than one relationship . For example , the encoded concept in Figure 5c consists of different forms of verbs but at the same time , these verbs are semantically similar . The semantic relationship present here is not adequately captured using the human - defined concepts used in this work . These are the novel concepts that require richer annotations or human - in - the - loop setup to generate adequate explanations .
Generalization of Results
Do the results generalize over different dataset selection and using different number of clusters ? We ran experiments using different split of the news dataset for several models , and also performed alignment using different values of K , the number of clusters . The results are consistent across the board . Please see Appendix F for details .
Conclusion
We presented ConceptX , a novel framework for analyzing the encoded concepts within deep NLP models . Our method uses unsupervised clustering to discover latent concepts within the contextualized representations and then aligned these concepts with a suite of human - defined concepts to generate explanations for them . Our results illuminate how DNNs structure language information . A few notable findings are : i ) lower layers capture shallow linguistic concepts , ii ) whereas the abstract linguistic concepts such as morphology and semantics are preserved higher in the network , iii ) the extent of alignment varies across different models and different human - defined concepts , iv ) we found that novel explanations and an improved coverage of concepts can be achieved via compositionality .
Appendix
A Human - defined concept labels A.1 Lexical Concepts : Ngrams , Affixes , Casing , First and the Last Word .
A.2 Morphology and Semantics :
POS tags : We used the Penn Treebank POS tags discussed in ( Marcus et al . , 1993 ) , which consists of 36 POS tags and 12 other tags ( i.e. , punctuation and currency symbols ) . In Table 2 , we provide POS tags and their description .
SEM tags : ( Abzianidze et al . , 2017 ) consists of 73 sem - tags grouped into 13 meta - tags . In Table 3 , we provide a detailed information of the tagset , and in Table 5 , we provide fine and coarse tags mapping .
A.3 Syntactic :
Chunking tags : For Chunking we used the tagset discussed in ( Tjong Kim Sang and Buchholz , 2000 ) , which consists of 11 tags as follows : NP ( Noun phrase ) , VP ( Verb phrase ) , PP ( Prepositional phrase ) , ADVP ( Adverb phrase ) , SBAR ( Subordinate phrase ) , ADJP ( Adjective phrase ) , PRT ( Particles ) , CONJP ( Conjunction ) , INTJ ( Interjection ) , LST ( List marker ) , UCP ( Unlike coordinate phrase ) . For the annotation , chunks are represented using IOB format , which results in 22 tags in the dataset as reported in Table 4 .
CCG super - tags Hockenmaier ( 2006 ) developed , CCGbank , a dataset with Combinatory Categorial Grammar ( CCG ) derivations and dependency structures from the Penn Treebank . CCG is a lexicalized grammar formalism , which is expressive and efficiently parseable . It consists of 1272 tags .
A.4 Linguistic Ontologies :
WordNet : ( Miller , 1995 ) consists of 26 lexicographic senses for nouns , 2 for adjectives , and 1 for adverbs . Each of them represent a supersense and a hierarchy can be formed from hypernym to hyponym .
LIWC : Over the past few decades , Pennebaker et al . ( Pennebaker et al . , 2001 ) have designed psycholinguistic concepts using high frequency words . These word categories are mostly used to study gender , age , personality , and health to estimate the
C Clustering details
Algorithm 1 assigns each word to a separate cluster and then iteratively combines them based on Ward 's minimum variance criterion that minimizes intra - cluster variance . Distance between two vector representations is calculated with the squared Euclidean distance .
Algorithm 1 Clustering Procedure Input : â â y l : word representation of words Parameter : K : the total number of clusters 1 : for each word w i do The Elbow curve did not show any optimum clustering point , with the increase in number of clusters the distortion score kept decreasing , resulting in over - clustering ( a large number of clusters consisted of less than 5 words ) . The over - clustering resulted in high but wrong alignment scores e.g. consider a two word cluster having words " good " and " great " . The cluster will have a successful match with " adjective " since more than 90 % of the words in the cluster are adjectives . In this way , a lot of small clusters will have a successful match with many human - defined concepts and the resulting alignment scores will be high . On the other hand , Silhouette resulted in under - clustering , giving the best score at number of clusters = 10 . We handled this empirically by trying several values for the number of clusters i.e. , 200 to 1600 with step size 200 . We selected 1000 to find a good balance with over and under clustering . We understand that this may not be the best optimal point . We presented the results of 600 and 1000 clusters to show that our findings are not sensitive to the number of clusters parameter .
D Coarse vs. Fine - grained Categories
D.1 Coarse vs. Fine - grained Categories
Our analysis of compositional concepts showed that several fine - grained concepts could be combined to explain an unaligned concept . For example , by combining verb categories of POS to one coarse verb category , we can align the encoded concept present in Figure 5c . To probe this more formally , we collapsed POS and SEM fine - grained concepts into coarser categories ( 27 POS tags and 15 SEM tags ) . We then recomputed the alignment with the encoded concepts . For most of the models , the alignment doubled compared to the fine - grained categorizes with at least 39 % and at most 53 % percent match for POS . This reflects that in several cases , models learn the coarse language hierarchy . We further questioned how many encoded concepts can be explained using coarse human - defined concepts . Compared to Table 1 , the matches increased by at most 17 points in the case of BERT - uc . The XLM - R showed the highest matching percentage of 81 % . The higher alignment suggests that most of the encoded concepts learned by pre - trained models can be explained using human - defined concepts . ( See Appendix D for detailed results ) .
D.2 Corase POS and SEM labels
Tables 5 and 6 the alignment doubles in most of the cases which reflects that in several cases , models learn the coarse language hierarchy . However , they do not strictly adhere to fine - grained categories existed in humandefined concepts . We further extend the alignment of coarse POS and SEM categories to the overall alignment with the human - defined concepts . Table 8 presents the results . We see a match of up to 81 % in the case of XLM - R. The high alignment suggests that many of the encoded concepts can be explained using coarse human - defined concepts .
E Compositional Coverage
Table 9 shows the amount of coverage we obtain when aligning with the morphological concepts when allowing 90 % of the words in the cluster to be from N concepts .
F Robustness of Methodology across Datasets and Settings
Figure 8 shows the layer - wise patterns using 600 clusters instead of 1000 as used in the main paper . We observe that the overall trends largely remain the same .
To further demonstrate the robustness of our method with respect to dataset , we sub - sampled another dataset from the News corpus with a different vocabulary by selecting words that appear between 2 to 10 times in the corpus . Note that the selection of vocabulary is due to the memory and computation limitations . Figure 9 shows the results using this selection of data . Compared to Figure 4 , we can see that the overall patterns are largely similar and confirms the robustness of our findings . The slight difference in the patterns of WordNet and LIWC are due to the large selection of proper nouns in the second set of the data .
G Layer - wise results
Figure 10 present layer - wise results for all the understudied models .
Introduction
Approach
D = { U 1 , U 2 , ... , U T } = { x 1 , . . . , x n } ( 1 )
Datasets
Experimental Setup
Conclusion
A Related Work
B Implementation Details
Acknowledgement
This research is partially supported by NSFC Grant No . 91646205 , and SJTU - CMBCC Joint Research Scheme .
